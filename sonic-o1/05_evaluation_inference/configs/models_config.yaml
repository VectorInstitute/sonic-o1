# Dataset paths
dataset_path: "dataset"
vqa_path: "vqa"

results:
  predictions_path: "results/predictions"
  scores_path: "results/scores"
  
# Tasks to evaluate
tasks:
  - task1_summarization
  - task2_mcq
  - task3_temporal_localization

# Topics
topics:
  - "01_Patient-Doctor_Consultations"
  - "02_Job_Interviews"
  - "03_Parent-Teacher_Conferences"
  - "04_Customer_Service_Interactions"
  - "05_Courtroom_Proceedings"
  - "06_Emergency_Response_Scenarios"
  - "07_Public_Transportation_Conflicts"
  - "08_Workplace_Team_Meetings"
  - "09_HousingApartment_Tours"
  - "10_Restaurant_Service_Encounters"
  - "11_Mental_Health_Counseling"
  - "12_Community_Town_Halls"
  - "13_Olympics"

# Preprocessing settings
preprocessing:
  t2_t3:
    segment_max_duration: 180
    image_model_frames: 128
    video_model_fps: 1

# Retry logic
retry:
  max_attempts: 4  # Increase to 4 to match 4 fallback options
  fps_fallback: [1, 0.5, 0.25]  # Keep for reference (if needed for API models)
  frame_count_fallback: [256, 128, 64, 32]  # NEW: Your desired fallback
  audio_chunks_fallback: [null, 64, 32, 16] 
  audio_chunk_duration_sec: 10.0                  


metrics:
  # LLM Judge Model Configuration
  llm_judge_model: "Qwen/Qwen3-8B"  #  "Qwen/Qwen3-8B" / "gpt-5-mini"
  
  # Multi-GPU Distribution
  llm_judge_device_map: "auto"  # Options: "auto", "balanced", or custom dict
  
  # Precision
  llm_judge_dtype: "bfloat16"  # Options: "bfloat16", "float16", "float32", "auto"
  
  # Optional: Memory limits per GPU (in GiB)
  # Uncomment and adjust if you want to reserve GPU memory for other models
  llm_judge_max_memory:
    0: "30GiB"  # GPU 0: Reserve 20GB, leave 28GB for other models
    1: "30GiB"  # GPU 1: Can use up to 40GB
    2: "30GiB"  # GPU 2: Can use up to 40GB
    3: "30GiB"  # GPU 3: Can use up to 40GB
  
  # Generation Configuration
  llm_judge_generation:
    temperature: 0.0     # 0 = deterministic, >0 = stochastic
    top_p: 0.95           # Nucleus sampling
    max_new_tokens: 32768   # Max response length
  
  iou_thresholds: [0.3, 0.5, 0.7]

# Empathy Evaluation
empathy:
  enabled: true
  apply_to_tasks: ["task1_summarization"]

# Models
models:
  - name: "gemini"
    class: "Gemini"
    api_key_env: "GEMINI_API_KEY"
    model_version: "gemini-3-pro-preview"
    supports_video: true
    supports_audio: true
    generation_config:
      temperature: 0.7
      top_p: 0.95
      top_k: 40
      max_output_tokens: 8192
  
  - name: "qwen3"
    class: "Qwen3Omni"
    model_path: "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    supports_video: true
    supports_audio: true
    #audio_format: "wav"
    use_thinking: false
    device_map: "auto"
    dtype: "bfloat16"
    gpu_memory_utilization: 0.75  
    attn_implementation: "sdpa"
    max_frames: 256
    min_frames: 32 
    generation_config:
      temperature: 0.7
      top_p: 0.95
      max_new_tokens: 8192

  - name: minicpm-o-2.6
    class: MiniCPM
    model_path: openbmb/MiniCPM-o-2_6
    supports_video: true
    supports_audio: true
    audio_format: m4a
    
    # Device configuration
    device: cuda
    dtype: bfloat16  # or torch.bfloat16
    attn_implementation: sdpa  # or 'flash_attention_2' if flash-attn installed
    
    # Audio processing
    audio_sample_rate: 16000
    audio_mono: true
    
    # Model initialization
    init_vision: true
    init_audio: true
    init_tts: false
    language: en
    min_frames: 32
    max_frames: 256     
  
    # Generation configuration
    generation_config:
      temperature: 0.7
      top_p: 0.95
      max_new_tokens: 8192

  - name: "unimoe"
    class: "UniMoe"
    model_path: "HIT-TMG/Uni-MoE-2.0-Omni"
    
    # Optional: Path to Uni-MoE package if not in PYTHONPATH
    unimoe_package_path: "evaluation_inference/external_repos/Uni-MoE/Uni-MoE-2"
    
    supports_video: true
    supports_audio: true
    audio_format: "wav"
    
    # Device configuration
    #device: "cuda:0"
    device: "auto"
    dtype: "bfloat16"  # or "float16", "float32"
    
    # Frame sampling limits (spatial, not temporal)
    max_frames: 256
    min_frames: 32 
    # Generation configuration
    generation_config:
      temperature: 0.7
      top_p: 0.95
      max_new_tokens: 8192

  - name: "vita"
    class: "VITA"
    model_path: "VITA-MLLM/VITA-1.5"
    vita_repo_path: "evaluation_inference/external_repos/VITA"
    
    supports_video: true
    supports_audio: true
    
    model_type: "qwen2p5_instruct"
    conv_mode: "qwen2p5_instruct"
    
    max_frames: 64
    min_frames: 32
    video_framerate: 1
    image_aspect_ratio: "pad"

    
    generation_config:
      temperature: 0.7
      top_p: 0.95
      num_beams: 1
      max_new_tokens: 8192

  - name: "videollama"
    class: "VideoLLaMA2"
    model_path: "DAMO-NLP-SG/VideoLLaMA2.1-7B-AV"
    videollama_repo_path: "evaluation_inference/external_repos/VideoLLaMA2"
    
    supports_video: false
    supports_audio: true
    audio_format: "wav"
    
    # Device configuration
    device_map: "auto"
    dtype: "bfloat16"
    
    # Frame config
    max_frames: 128
    min_frames: 16
    retry_override:
      frame_count_fallback: [128, 64, 32, 16]  # Start lower
      audio_chunks_fallback: [64, 32, 16, 8]     # Always chunk, start lower
    # Generation configuration
    generation_config:
      temperature: 0.7
      top_p: 0.95
      max_new_tokens: 8192

  - name: "gpt4o"
    class: "GPT4o"
    api_key_env: "OPENAI_API_KEY"
    model_version: "gpt-4o"
    
    # Multimodal mode: both frames and captions
    supports_video: true
    supports_audio: false
    use_captions: false
    
    # Retry strategy: 'frame_count_caption', 'frame_count', 'fps', or 'auto'
    retry_strategy: "frame_count_caption"
    
    # Frame configuration
    max_frames: 128
    image_detail: "auto"
    
    # Retry configuration with progressive degradation
    retry_override:
      frame_count_fallback: [128, 64, 32, 16]
      caption_chunks_fallback: [null, 32, 16, 8]
    
    # API retry settings
    retry_attempts: 3
    retry_delay: 2
    
    # Generation configuration
    generation_config:
      temperature: 0.7
      max_tokens: 8192
      top_p: 0.95

  - name: "phi4"
    class: "Phi4"
    model_path: "microsoft/Phi-4-multimodal-instruct"
    
    supports_video: true
    supports_audio: true
    audio_format: "m4a"  # Supports m4a, wav, mp3, etc.
    
    # Device configuration
    device_map: "auto"  # Multi-GPU distribution
    torch_dtype: "bfloat16"  # Explicit dtype (or "auto" for flexibility)
    attn_implementation: "sdpa"  
    trust_remote_code: true
    
    # Frame config
    max_frames: 128  
    min_frames: 8   
    
    # Audio config
    audio_sample_rate: 16000  # Phi-4's audio encoder requirement
    audio_mono: true          # Speech models use mono
    # Generation config
    generation_config:
      temperature: 0.7
      top_p: 0.95
      max_new_tokens: 8192