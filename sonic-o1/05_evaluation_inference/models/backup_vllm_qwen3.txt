""""
models/qwen3_omni.py
Qwen3-Omni implementation with vLLM for efficient inference.
"""
import os
import logging
import time
import shutil
import gc
import multiprocessing
from typing import Optional, Dict, Any, Union
from pathlib import Path
import torch
import torch.distributed as dist
try:
    from vllm import LLM, SamplingParams
    from transformers import Qwen3OmniMoeProcessor
    from utils import process_mm_info
except ImportError as e:
    raise ImportError(
        f"Please install required packages: {e}\n"
        "pip install vllm transformers qwen-omni-utils"
    )
from .base_model import BaseModel
logger = logging.getLogger(__name__)
class Qwen3Omni(BaseModel):
    """
    Qwen3-Omni wrapper with vLLM for efficient multi-GPU inference.
    Supports both Instruct and Thinking variants with automatic audio truncation.
    """
    
    AUDIO_TOKENS_PER_SEC = 25
    VIDEO_TOKENS_PER_FRAME = 220
    TEXT_RESERVED_TOKENS = 500
    SAFETY_MARGIN_TOKENS = 2000
    OUTPUT_TOKEN_RESERVE = 8192
    
    def __init__(self, model_name: str, config: Dict[str, Any]):
        super().__init__(model_name, config)
        
        self.model_path = config.get('model_path', 'Qwen/Qwen3-Omni-30B-A3B-Instruct')
        self.use_thinking = config.get('use_thinking', False)
        
        self.gpu_memory_utilization = config.get('gpu_memory_utilization', 0.85)
        self.tensor_parallel_size = config.get('tensor_parallel_size', torch.cuda.device_count())
        self.max_num_seqs = config.get('max_num_seqs', 1)
        self.max_model_len = config.get('max_model_len', 65536)
        
        gen_config = config.get('generation_config', {})
        self.temperature = gen_config.get('temperature', 0.0)
        self.top_p = gen_config.get('top_p', 0.95)
        self.top_k = gen_config.get('top_k', 20)
        self.max_tokens = gen_config.get('max_new_tokens', 8192)
        
        self.max_frames = config.get('max_frames', 36)
        self.min_frames = config.get('min_frames', 28)
        
        self.enable_audio_truncation = config.get('enable_audio_truncation', True)
        self.max_audio_duration = config.get('max_audio_duration', None)
        
        self.limit_mm_per_prompt = config.get('limit_mm_per_prompt', {
            'image': 1,
            'video': 1,
            'audio': 1
        })
        
        self.llm = None
        self.processor = None
        
        self.stats = {
            'total_samples': 0,
            'truncated_audio': 0,
            'truncated_frames': 0,
        }
    
    def _calculate_max_audio_duration(self, num_frames: int) -> float:
        video_tokens = num_frames * self.VIDEO_TOKENS_PER_FRAME
        available_tokens = (
            self.max_model_len 
            - self.TEXT_RESERVED_TOKENS 
            - video_tokens 
            - self.SAFETY_MARGIN_TOKENS
            - self.OUTPUT_TOKEN_RESERVE
        )
        
        max_audio_tokens = max(0, available_tokens)
        max_duration_sec = max_audio_tokens / self.AUDIO_TOKENS_PER_SEC
        
        logger.debug(
            f"Token budget: video={video_tokens}, available_for_audio={max_audio_tokens}, "
            f"max_audio_duration={max_duration_sec:.1f}s"
        )
        
        return max_duration_sec
    
    def _clear_vllm_cache(self):
        vllm_cache = Path(os.environ.get('VLLM_CACHE_ROOT', Path.home() / '.cache/vllm'))
        mm_cache = vllm_cache / 'multimodal_cache'
        if mm_cache.exists():
            try:
                shutil.rmtree(mm_cache, ignore_errors=True)
                logger.debug(f"Cleared multimodal cache: {mm_cache}")
            except Exception as e:
                logger.warning(f"Failed to clear cache: {e}")
    
    def _is_engine_alive(self) -> bool:
        if self.llm is None:
            return False
        
        try:
            test_output = self.llm.generate([{
                'prompt': 'test',
                'multi_modal_data': {}
            }], SamplingParams(max_tokens=1))
            return True
        except Exception:
            return False
    
    def _reload_engine(self):
        logger.warning("Engine crashed, attempting reload")
        
        try:
            self.unload()
        except Exception as e:
            logger.warning(f"Error during unload: {e}")
        
        self._clear_vllm_cache()
        
        # INCREASED WAIT: Allow time for OS to reclaim VRAM from killed processes
        time.sleep(15)
        
        try:
            self.load()
            logger.info("Engine reloaded successfully")
        except Exception as e:
            logger.error(f"Failed to reload engine: {e}")
            raise RuntimeError(f"Could not recover from engine crash: {e}")
    
    def load(self):
        try:
            self._clear_vllm_cache()
            
            os.environ['VLLM_USE_V1'] = '0'
            os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
            if self.max_model_len > 65536:
                os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'
            
            logger.info(f"Loading Qwen3-Omni model from {self.model_path} with vLLM")
            logger.info(f"Using {self.tensor_parallel_size} GPUs for tensor parallelism")
            logger.info(f"Context length: {self.max_model_len} tokens")
            
            self.llm = LLM(
                model=self.model_path,
                trust_remote_code=True,
                gpu_memory_utilization=self.gpu_memory_utilization,
                tensor_parallel_size=self.tensor_parallel_size,
                limit_mm_per_prompt=self.limit_mm_per_prompt,
                max_num_seqs=self.max_num_seqs,
                max_model_len=self.max_model_len,
                seed=1234,
                disable_log_stats=True,
                enforce_eager=False,
                enable_prefix_caching=False,
                mm_processor_kwargs={"cache_gb": 0},
            )
            
            self.processor = Qwen3OmniMoeProcessor.from_pretrained(self.model_path)
            
            logger.info(f"Successfully loaded Qwen3-Omni with vLLM ({'Thinking' if self.use_thinking else 'Instruct'} mode)")
            logger.info(f"Audio truncation: {'enabled' if self.enable_audio_truncation else 'disabled'}")
            
        except Exception as e:
            raise RuntimeError(f"Failed to load Qwen3-Omni model with vLLM: {e}")
    
    def generate(
        self,
        frames: str,
        audio: Optional[str],
        prompt: str,
        fps: Optional[float] = None,
        video_category: Optional[str] = None,
        **kwargs
    ) -> str:
        if self.llm is None or self.processor is None:
            try:
                logger.warning("Model found unloaded in generate(), attempting lazy load...")
                self.load()
            except Exception as e:
                raise RuntimeError("Model not loaded. Call load() first.")
        
        if not isinstance(frames, str):
            raise ValueError(f"Qwen3-Omni requires video file path (str), got {type(frames)}")
        
        video_path = Path(frames)
        if not video_path.exists():
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        max_context_retries = 3
        frame_reduction_factor = 0.6 # More aggressive reduction
        audio_reduction_factor = 0.8
        
        current_max_frames = self.max_frames
        current_max_audio = None
        if self.enable_audio_truncation:
            current_max_audio = self._calculate_max_audio_duration(current_max_frames)
        
        last_error = None
        
        for context_attempt in range(max_context_retries):
            try:
                audio_str = f"{current_max_audio:.1f}s" if current_max_audio else "unlimited"
                logger.info(f"Attempt {context_attempt + 1}/{max_context_retries}: frames={current_max_frames}, audio={audio_str}")
                
                content = []
                
                video_content = {
                    "type": "video",
                    "video": str(video_path),
                    "max_frames": current_max_frames,
                    "min_frames": self.min_frames,
                }
                
                if fps is not None:
                    video_content["fps"] = fps
                
                content.append(video_content)
                
                if audio is not None and isinstance(audio, str) and os.path.exists(audio):
                    content.append({"type": "audio", "audio": str(audio)})
                
                content.append({"type": "text", "text": prompt})
                
                conversation = [{"role": "user", "content": content}]
                
                text = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
                
                audios, images, videos = process_mm_info(
                    conversation,
                    use_audio_in_video=False,
                    max_audio_duration=current_max_audio
                )
                
                self.stats['total_samples'] += 1
                if audios is not None and current_max_audio is not None:
                    actual_audio_duration = len(audios[0]) / 16000
                    if actual_audio_duration >= current_max_audio * 0.99:
                        self.stats['truncated_audio'] += 1
                
                inputs = {'prompt': text, 'multi_modal_data': {}}
                
                if audios is not None:
                    inputs['multi_modal_data']['audio'] = audios
                if images is not None:
                    inputs['multi_modal_data']['image'] = images
                if videos is not None:
                    inputs['multi_modal_data']['video'] = videos
                
                temperature = kwargs.get('temperature', self.temperature)
                top_p = kwargs.get('top_p', self.top_p)
                top_k = kwargs.get('top_k', self.top_k)
                max_tokens = kwargs.get('max_new_tokens', self.max_tokens)
                
                sampling_params = SamplingParams(
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_tokens=max_tokens,
                )
                
                logger.info("Generating response...")
                
                outputs = self.llm.generate([inputs], sampling_params=sampling_params)
                response_text = outputs[0].outputs[0].text
                
                logger.info(f"Generated response ({len(response_text)} chars)")
                
                return self.postprocess_output(response_text)
                
            except Exception as e:
                error_msg = str(e)
                last_error = error_msg
                
                is_context_error = any(keyword in error_msg.lower() for keyword in [
                    'context', 'token', 'length', 'limit', 'maximum', 'exceed', 'longer than'
                ])
                is_cache_error = "Expected a cached item" in error_msg or "mm_hash" in error_msg or "AssertionError" in error_msg
                is_engine_dead = "EngineDeadError" in error_msg or "EngineCore" in error_msg or "process_input_sockets" in error_msg
                
                if is_engine_dead or is_cache_error or is_context_error:
                    logger.error(f"Engine/Cache/Context error: {e}")
                    
                    if context_attempt < max_context_retries - 1:
                        self.unload() # FORCE UNLOAD before reload logic
                        self._reload_engine()
                        
                        current_max_frames = max(self.min_frames, int(current_max_frames * frame_reduction_factor))
                        if self.enable_audio_truncation:
                            new_audio_limit = self._calculate_max_audio_duration(current_max_frames)
                            current_max_audio = new_audio_limit * audio_reduction_factor
                        
                        audio_str = f"{current_max_audio:.1f}s" if current_max_audio else "unlimited"
                        logger.info(f"Retrying with frames={current_max_frames}, audio={audio_str}")
                        continue
                    else:
                        raise RuntimeError(f"Engine error after {max_context_retries} attempts: {e}")
                
                elif "out of memory" in error_msg.lower() or "OOM" in error_msg:
                    logger.error(f"OOM detected: {e}")
                    self.unload()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    self.load()
                    
                    if context_attempt < max_context_retries - 1:
                        current_max_frames = max(self.min_frames, int(current_max_frames * frame_reduction_factor))
                        if current_max_audio is not None:
                            current_max_audio = current_max_audio * audio_reduction_factor
                        logger.info(f"Retrying with frames={current_max_frames}, audio={current_max_audio:.1f}s")
                        continue
                    else:
                        raise RuntimeError(f"OOM error after {max_context_retries} attempts: {e}")
                
                else:
                    logger.error(f"Generation failed: {e}", exc_info=True)
                    raise RuntimeError(f"Generation failed: {e}")
        
        raise RuntimeError(f"Generation failed after {max_context_retries} attempts. Last error: {last_error}")
    
    def unload(self):
        """Aggressively cleanup vLLM to prevent zombie processes"""
        # 1. Standard Python deletion
        if self.llm is not None:
            try:
                del self.llm
            except Exception as e:
                logger.warning(f"Error deleting llm object: {e}")
            self.llm = None
        
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        # 2. Force GC and CUDA clear
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
        # 3. Destroy Distributed Group
        if dist.is_initialized():
            try:
                dist.destroy_process_group()
                logger.info("Distributed process group destroyed")
            except Exception as e:
                logger.warning(f"Failed to destroy process group: {e}")
                
        # 4. EXTREME MEASURE: Kill child processes manually
        # vLLM spawns worker processes that often hang. We find them and kill them.
        try:
            active_children = multiprocessing.active_children()
            if active_children:
                logger.info(f"Found {len(active_children)} active child processes. Terminating...")
                for child in active_children:
                    try:
                        child.terminate()
                        child.join(timeout=0.5)
                        if child.is_alive():
                            child.kill()
                    except Exception as e:
                        logger.warning(f"Failed to kill child {child.pid}: {e}")
        except Exception as e:
            logger.warning(f"Error during manual process cleanup: {e}")
        
        logger.info("Model unloaded, memory cleared, and child processes terminated")
    
    def get_model_info(self) -> Dict[str, Any]:
        info = super().get_model_info()
        info.update({
            'model_path': self.model_path,
            'model_type': 'Thinking' if self.use_thinking else 'Instruct',
            'backend': 'vLLM',
            'native_video': True,
            'native_audio': True,
            'tensor_parallel_size': self.tensor_parallel_size,
            'gpu_memory_utilization': self.gpu_memory_utilization,
            'max_frames': self.max_frames,
            'min_frames': self.min_frames,
            'max_model_len': self.max_model_len,
            'audio_truncation_enabled': self.enable_audio_truncation,
            'statistics': self.stats,
        })
        return info
    
    def get_statistics(self) -> Dict[str, Any]:
        total = self.stats['total_samples']
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            'truncation_rate': self.stats['truncated_audio'] / total * 100,
        }"