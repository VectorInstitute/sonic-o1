{
  "model": "videollama",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.13551019139489912,
            "rouge_l_std": 0.021780917539881216,
            "text_similarity_mean": 0.4852284602820873,
            "text_similarity_std": 0.0838750632510607,
            "llm_judge_score_mean": 2.3125,
            "llm_judge_score_std": 1.0439558180306292
          },
          "short": {
            "rouge_l_mean": 0.12338453310394276,
            "rouge_l_std": 0.030647442343329646,
            "text_similarity_mean": 0.4783601276576519,
            "text_similarity_std": 0.14301656961525308,
            "llm_judge_score_mean": 2.375,
            "llm_judge_score_std": 0.9270248108869579
          },
          "cider": {
            "cider_detailed": 6.206425412455525e-07,
            "cider_short": 0.0006346931991718894
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.11923059585661526,
            "rouge_l_std": 0.019781576359114653,
            "text_similarity_mean": 0.43119938316799344,
            "text_similarity_std": 0.15089244471798652,
            "llm_judge_score_mean": 2.1904761904761907,
            "llm_judge_score_std": 1.942483624225823
          },
          "short": {
            "rouge_l_mean": 0.1182214317635821,
            "rouge_l_std": 0.04906980736391158,
            "text_similarity_mean": 0.39170035116729285,
            "text_similarity_std": 0.19120446317663353,
            "llm_judge_score_mean": 2.1904761904761907,
            "llm_judge_score_std": 2.10710922760358
          },
          "cider": {
            "cider_detailed": 1.2205045847032245e-14,
            "cider_short": 0.00030373425907248737
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.1185047312472014,
            "rouge_l_std": 0.024723874776864867,
            "text_similarity_mean": 0.3842344532410304,
            "text_similarity_std": 0.1503840174326128,
            "llm_judge_score_mean": 2.2777777777777777,
            "llm_judge_score_std": 1.9092044752710648
          },
          "short": {
            "rouge_l_mean": 0.1127222333134526,
            "rouge_l_std": 0.03451205179374854,
            "text_similarity_mean": 0.3947582583253582,
            "text_similarity_std": 0.19739201209344404,
            "llm_judge_score_mean": 1.7777777777777777,
            "llm_judge_score_std": 1.7497795275581802
          },
          "cider": {
            "cider_detailed": 5.762688652028699e-23,
            "cider_short": 5.784536930018012e-07
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.12306638565209269,
            "rouge_l_std": 0.025563537789670732,
            "text_similarity_mean": 0.4340775926907857,
            "text_similarity_std": 0.12312268387598845,
            "llm_judge_score_mean": 1.2666666666666666,
            "llm_judge_score_std": 0.85374989832438
          },
          "short": {
            "rouge_l_mean": 0.1383410074268447,
            "rouge_l_std": 0.028423916994235146,
            "text_similarity_mean": 0.35508091847101847,
            "text_similarity_std": 0.13395330108826325,
            "llm_judge_score_mean": 1.4,
            "llm_judge_score_std": 0.9521904571390466
          },
          "cider": {
            "cider_detailed": 4.5501762016025024e-05,
            "cider_short": 0.0016884323791151047
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.11730119832665412,
            "rouge_l_std": 0.027639229103997166,
            "text_similarity_mean": 0.2678690180182457,
            "text_similarity_std": 0.14196635290819595,
            "llm_judge_score_mean": 0.8461538461538461,
            "llm_judge_score_std": 0.6617173282340483
          },
          "short": {
            "rouge_l_mean": 0.08513570178290493,
            "rouge_l_std": 0.03369872625570429,
            "text_similarity_mean": 0.1991989128291607,
            "text_similarity_std": 0.08721272478501071,
            "llm_judge_score_mean": 0.38461538461538464,
            "llm_judge_score_std": 0.6249260311258432
          },
          "cider": {
            "cider_detailed": 0.0013602118109593635,
            "cider_short": 1.720688550517036e-05
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.15444677835913614,
            "rouge_l_std": 0.026451900341143496,
            "text_similarity_mean": 0.45946509689092635,
            "text_similarity_std": 0.10399501562398826,
            "llm_judge_score_mean": 1.65,
            "llm_judge_score_std": 0.9096702699330126
          },
          "short": {
            "rouge_l_mean": 0.13720450461089534,
            "rouge_l_std": 0.03157264867840804,
            "text_similarity_mean": 0.4015477493405342,
            "text_similarity_std": 0.13594589134887297,
            "llm_judge_score_mean": 1.85,
            "llm_judge_score_std": 1.0136567466356647
          },
          "cider": {
            "cider_detailed": 3.613418227275014e-07,
            "cider_short": 0.011073636408911638
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.13632252634562428,
            "rouge_l_std": 0.028886805323138476,
            "text_similarity_mean": 0.4522193116801126,
            "text_similarity_std": 0.08981447854700553,
            "llm_judge_score_mean": 1.0,
            "llm_judge_score_std": 0.7559289460184544
          },
          "short": {
            "rouge_l_mean": 0.14456345447875454,
            "rouge_l_std": 0.03902803950614131,
            "text_similarity_mean": 0.3663447712148939,
            "text_similarity_std": 0.12607175212915459,
            "llm_judge_score_mean": 1.5,
            "llm_judge_score_std": 0.823754471047914
          },
          "cider": {
            "cider_detailed": 1.0194452810782799e-20,
            "cider_short": 0.004817928728754957
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.11014988877519172,
            "rouge_l_std": 0.03671527141089731,
            "text_similarity_mean": 0.30627944972366095,
            "text_similarity_std": 0.178206502692461,
            "llm_judge_score_mean": 1.1666666666666667,
            "llm_judge_score_std": 1.0671873729054748
          },
          "short": {
            "rouge_l_mean": 0.10093750269418739,
            "rouge_l_std": 0.0354520022414609,
            "text_similarity_mean": 0.2785364529117942,
            "text_similarity_std": 0.1826491010207303,
            "llm_judge_score_mean": 0.8333333333333334,
            "llm_judge_score_std": 1.0671873729054746
          },
          "cider": {
            "cider_detailed": 1.0506049757099236e-19,
            "cider_short": 0.0003033597236375514
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.14120667635319037,
            "rouge_l_std": 0.03985679940173366,
            "text_similarity_mean": 0.35387381197263795,
            "text_similarity_std": 0.1582393805349597,
            "llm_judge_score_mean": 1.5,
            "llm_judge_score_std": 1.118033988749895
          },
          "short": {
            "rouge_l_mean": 0.12854307097889947,
            "rouge_l_std": 0.055200685311809224,
            "text_similarity_mean": 0.29314560691515607,
            "text_similarity_std": 0.1599363154450268,
            "llm_judge_score_mean": 1.3333333333333333,
            "llm_judge_score_std": 0.9428090415820634
          },
          "cider": {
            "cider_detailed": 0.0020372325123493903,
            "cider_short": 0.0014419008385707375
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.1377262373458953,
            "rouge_l_std": 0.024059548945098604,
            "text_similarity_mean": 0.4206960337317508,
            "text_similarity_std": 0.14504084399938907,
            "llm_judge_score_mean": 1.6956521739130435,
            "llm_judge_score_std": 0.9971604296678012
          },
          "short": {
            "rouge_l_mean": 0.1301313733145277,
            "rouge_l_std": 0.023669903629701183,
            "text_similarity_mean": 0.34596285418323847,
            "text_similarity_std": 0.1230378626000777,
            "llm_judge_score_mean": 1.9130434782608696,
            "llm_judge_score_std": 1.212723972259248
          },
          "cider": {
            "cider_detailed": 0.00035268148258487287,
            "cider_short": 0.011282940721752567
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.13334439485139676,
            "rouge_l_std": 0.028161734578852637,
            "text_similarity_mean": 0.4242471227279076,
            "text_similarity_std": 0.15980995455737887,
            "llm_judge_score_mean": 2.076923076923077,
            "llm_judge_score_std": 1.1409536133993328
          },
          "short": {
            "rouge_l_mean": 0.12175876981595465,
            "rouge_l_std": 0.04067480714485729,
            "text_similarity_mean": 0.3767720816227106,
            "text_similarity_std": 0.16087300891777337,
            "llm_judge_score_mean": 2.230769230769231,
            "llm_judge_score_std": 1.1200169060431566
          },
          "cider": {
            "cider_detailed": 0.000364803816221046,
            "cider_short": 0.0020757977724705366
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.0981563932289348,
            "rouge_l_std": 0.022101526592276086,
            "text_similarity_mean": 0.1751133431163099,
            "text_similarity_std": 0.12022045542911258,
            "llm_judge_score_mean": 0.8333333333333334,
            "llm_judge_score_std": 0.5
          },
          "short": {
            "rouge_l_mean": 0.08459232701163083,
            "rouge_l_std": 0.02698918206904117,
            "text_similarity_mean": 0.17952356818649504,
            "text_similarity_std": 0.1238658634950915,
            "llm_judge_score_mean": 0.8888888888888888,
            "llm_judge_score_std": 0.5665577237325317
          },
          "cider": {
            "cider_detailed": 6.87579521739192e-17,
            "cider_short": 0.001007846043128182
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.12883785460556252,
            "rouge_l_std": 0.03759817238433356,
            "text_similarity_mean": 0.38995389899481897,
            "text_similarity_std": 0.1148272263249073,
            "llm_judge_score_mean": 1.1304347826086956,
            "llm_judge_score_std": 0.7969696860792764
          },
          "short": {
            "rouge_l_mean": 0.11980975828986103,
            "rouge_l_std": 0.04765707924788865,
            "text_similarity_mean": 0.3599074560662974,
            "text_similarity_std": 0.12617751501974744,
            "llm_judge_score_mean": 1.2173913043478262,
            "llm_judge_score_std": 0.9304754156101173
          },
          "cider": {
            "cider_detailed": 0.0019229584301979576,
            "cider_short": 0.019195273991477812
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.12721568094941496,
          "text_similarity_mean": 0.3834197674029437,
          "llm_judge_score_mean": 1.5343526549630229
        },
        "short": {
          "rouge_l_mean": 0.11887274373734139,
          "text_similarity_mean": 0.34006454683781556,
          "llm_judge_score_mean": 1.5303560709079103
        },
        "cider": {
          "cider_detailed_mean": 0.0004680285999003771,
          "cider_short_mean": 0.0041417945696355105
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.5686274509803921,
          "correct": 58,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.21528322834265565,
            "rouge_l_std": 0.08009458286147637,
            "text_similarity_mean": 0.6092753988124576,
            "text_similarity_std": 0.17055462249312253,
            "llm_judge_score_mean": 5.519607843137255,
            "llm_judge_score_std": 4.072216701726572
          },
          "rationale_cider": 0.05251277621592781
        },
        "02_Job_Interviews": {
          "accuracy": 0.64,
          "correct": 64,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2109796815631333,
            "rouge_l_std": 0.07801612642421477,
            "text_similarity_mean": 0.6091013212129474,
            "text_similarity_std": 0.17372174171377278,
            "llm_judge_score_mean": 6.18,
            "llm_judge_score_std": 3.5759753914142083
          },
          "rationale_cider": 0.07962694755317233
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.6173913043478261,
          "correct": 142,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.19298719623596855,
            "rouge_l_std": 0.07988110302195742,
            "text_similarity_mean": 0.5808534863040499,
            "text_similarity_std": 0.2045903380571368,
            "llm_judge_score_mean": 5.508695652173913,
            "llm_judge_score_std": 3.889496786886253
          },
          "rationale_cider": 0.03482602465447769
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.5897435897435898,
          "correct": 23,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.1861287859909223,
            "rouge_l_std": 0.07286896993775571,
            "text_similarity_mean": 0.6009887479818784,
            "text_similarity_std": 0.15709660559705368,
            "llm_judge_score_mean": 5.538461538461538,
            "llm_judge_score_std": 3.7950450410904644
          },
          "rationale_cider": 0.0028743824422677246
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.5130434782608696,
          "correct": 59,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.18252957512344659,
            "rouge_l_std": 0.08034890861022509,
            "text_similarity_mean": 0.544764101991187,
            "text_similarity_std": 0.19459112983513083,
            "llm_judge_score_mean": 4.156521739130435,
            "llm_judge_score_std": 4.065959011636741
          },
          "rationale_cider": 0.039737289025159196
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.5517241379310345,
          "correct": 48,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.17476187246783262,
            "rouge_l_std": 0.08408820562994003,
            "text_similarity_mean": 0.5450273703398376,
            "text_similarity_std": 0.2050354861397699,
            "llm_judge_score_mean": 4.942528735632184,
            "llm_judge_score_std": 4.075032816563667
          },
          "rationale_cider": 0.0313288343449389
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.49019607843137253,
          "correct": 25,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.18315453816258448,
            "rouge_l_std": 0.06212533566657988,
            "text_similarity_mean": 0.5868078531587825,
            "text_similarity_std": 0.13584622104457786,
            "llm_judge_score_mean": 5.235294117647059,
            "llm_judge_score_std": 3.9237248973848247
          },
          "rationale_cider": 0.030761350183667508
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 0.5373134328358209,
          "correct": 36,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.1937376192063251,
            "rouge_l_std": 0.07352144109099605,
            "text_similarity_mean": 0.5973308980186929,
            "text_similarity_std": 0.21509109338197777,
            "llm_judge_score_mean": 5.164179104477612,
            "llm_judge_score_std": 3.9797885450666812
          },
          "rationale_cider": 0.019443544268429702
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.5193798449612403,
          "correct": 67,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.19400466668231725,
            "rouge_l_std": 0.07978879357590947,
            "text_similarity_mean": 0.5617243667068177,
            "text_similarity_std": 0.1887305425010106,
            "llm_judge_score_mean": 4.37984496124031,
            "llm_judge_score_std": 3.902282771491087
          },
          "rationale_cider": 0.05145392503614997
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.45555555555555555,
          "correct": 41,
          "total": 90,
          "rationale": {
            "rouge_l_mean": 0.17755084385362158,
            "rouge_l_std": 0.08164358932731805,
            "text_similarity_mean": 0.5544552304678493,
            "text_similarity_std": 0.2193261860452853,
            "llm_judge_score_mean": 4.5,
            "llm_judge_score_std": 3.8822959987901777
          },
          "rationale_cider": 0.041420920298342614
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 0.5076923076923077,
          "correct": 33,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.1653964935784055,
            "rouge_l_std": 0.08214919929145709,
            "text_similarity_mean": 0.5168813862622931,
            "text_similarity_std": 0.22303350874261332,
            "llm_judge_score_mean": 4.8307692307692305,
            "llm_judge_score_std": 4.1863118733070666
          },
          "rationale_cider": 0.027971810313503966
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.582010582010582,
          "correct": 110,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.20468561177423117,
            "rouge_l_std": 0.0796731688949226,
            "text_similarity_mean": 0.5930756527004103,
            "text_similarity_std": 0.16098645303260142,
            "llm_judge_score_mean": 5.587301587301587,
            "llm_judge_score_std": 3.927793734151504
          },
          "rationale_cider": 0.0494281838081821
        },
        "13_Olympics": {
          "accuracy": 0.4927536231884058,
          "correct": 34,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.18005184366232066,
            "rouge_l_std": 0.07294731771800504,
            "text_similarity_mean": 0.6068350841169772,
            "text_similarity_std": 0.14219565437574977,
            "llm_judge_score_mean": 4.942028985507246,
            "llm_judge_score_std": 3.9995798981241375
          },
          "rationale_cider": 0.039448302731905804
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.5434947219953075,
        "rationale": {
          "rouge_l_mean": 0.18932707358798193,
          "text_similarity_mean": 0.5774708383133985,
          "llm_judge_score_mean": 5.114248730421413
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.03912795904513116,
          "std_iou": 0.0946137568583202,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02631578947368421,
            "count": 7,
            "total": 266
          },
          "R@0.5": {
            "recall": 0.011278195488721804,
            "count": 3,
            "total": 266
          },
          "R@0.7": {
            "recall": 0.0037593984962406013,
            "count": 1,
            "total": 266
          },
          "mae": {
            "start_mean": 350.86601127819546,
            "end_mean": 3878.3352406015038,
            "average_mean": 2114.600625939849
          },
          "rationale": {
            "rouge_l_mean": 0.2439523437445021,
            "rouge_l_std": 0.10393251594306427,
            "text_similarity_mean": 0.503786571512937,
            "text_similarity_std": 0.18421835970550549,
            "llm_judge_score_mean": 2.007518796992481,
            "llm_judge_score_std": 1.7896790177260218
          },
          "rationale_cider": 0.2924152435518991
        },
        "02_Job_Interviews": {
          "mean_iou": 0.04267816872308,
          "std_iou": 0.08698476030769249,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02390438247011952,
            "count": 6,
            "total": 251
          },
          "R@0.5": {
            "recall": 0.00398406374501992,
            "count": 1,
            "total": 251
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 251
          },
          "mae": {
            "start_mean": 217.65143824701195,
            "end_mean": 222.16033067729086,
            "average_mean": 219.9058844621514
          },
          "rationale": {
            "rouge_l_mean": 0.22696013146140406,
            "rouge_l_std": 0.09513388895756247,
            "text_similarity_mean": 0.49679284663759615,
            "text_similarity_std": 0.1929585914719815,
            "llm_judge_score_mean": 2.087649402390438,
            "llm_judge_score_std": 1.8957613794353463
          },
          "rationale_cider": 0.22785263419548976
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.031014575688554962,
          "std_iou": 0.06830419424346834,
          "median_iou": 0.0014285714285717534,
          "R@0.3": {
            "recall": 0.014925373134328358,
            "count": 8,
            "total": 536
          },
          "R@0.5": {
            "recall": 0.0018656716417910447,
            "count": 1,
            "total": 536
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 536
          },
          "mae": {
            "start_mean": 497.5863134328359,
            "end_mean": 523.5563414179105,
            "average_mean": 510.5713274253731
          },
          "rationale": {
            "rouge_l_mean": 0.22145802892618313,
            "rouge_l_std": 0.09775820605530748,
            "text_similarity_mean": 0.47122209229673356,
            "text_similarity_std": 0.20170863422818724,
            "llm_judge_score_mean": 2.218283582089552,
            "llm_judge_score_std": 2.0275439248860656
          },
          "rationale_cider": 0.21191222978399482
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.04714233143509154,
          "std_iou": 0.08783250660768664,
          "median_iou": 0.004514514514514734,
          "R@0.3": {
            "recall": 0.02608695652173913,
            "count": 3,
            "total": 115
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 115
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 115
          },
          "mae": {
            "start_mean": 124.54068695652174,
            "end_mean": 128.1818,
            "average_mean": 126.36124347826089
          },
          "rationale": {
            "rouge_l_mean": 0.22099561234236387,
            "rouge_l_std": 0.08985685842949316,
            "text_similarity_mean": 0.5018884548188551,
            "text_similarity_std": 0.16852662310454908,
            "llm_judge_score_mean": 1.9391304347826086,
            "llm_judge_score_std": 2.1560307996579557
          },
          "rationale_cider": 0.2201147250492846
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.0308384851338288,
          "std_iou": 0.07622189322870672,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.017543859649122806,
            "count": 6,
            "total": 342
          },
          "R@0.5": {
            "recall": 0.0029239766081871343,
            "count": 1,
            "total": 342
          },
          "R@0.7": {
            "recall": 0.0029239766081871343,
            "count": 1,
            "total": 342
          },
          "mae": {
            "start_mean": 476.93407894736845,
            "end_mean": 487.90845321637426,
            "average_mean": 482.42126608187135
          },
          "rationale": {
            "rouge_l_mean": 0.2281902660824798,
            "rouge_l_std": 0.0996222610578064,
            "text_similarity_mean": 0.49682986175325533,
            "text_similarity_std": 0.20876321960156233,
            "llm_judge_score_mean": 2.1842105263157894,
            "llm_judge_score_std": 2.0500996530065603
          },
          "rationale_cider": 0.16871047052821406
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.019092773083707423,
          "std_iou": 0.05530549930358966,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.007936507936507936,
            "count": 2,
            "total": 252
          },
          "R@0.5": {
            "recall": 0.003968253968253968,
            "count": 1,
            "total": 252
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 252
          },
          "mae": {
            "start_mean": 217.43515476190478,
            "end_mean": 248.17480555555554,
            "average_mean": 232.80498015873016
          },
          "rationale": {
            "rouge_l_mean": 0.23611029683783177,
            "rouge_l_std": 0.0921701750941652,
            "text_similarity_mean": 0.5240789959428921,
            "text_similarity_std": 0.1582428002287455,
            "llm_judge_score_mean": 2.484126984126984,
            "llm_judge_score_std": 2.4792283969397597
          },
          "rationale_cider": 0.30314251273491705
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.03364546465808189,
          "std_iou": 0.06825218459828929,
          "median_iou": 0.0037087087087087074,
          "R@0.3": {
            "recall": 0.01818181818181818,
            "count": 2,
            "total": 110
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 110
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 110
          },
          "mae": {
            "start_mean": 176.4342636363636,
            "end_mean": 182.97233636363634,
            "average_mean": 179.7033
          },
          "rationale": {
            "rouge_l_mean": 0.2033359455698057,
            "rouge_l_std": 0.09628565415280152,
            "text_similarity_mean": 0.45998478856953706,
            "text_similarity_std": 0.18857781736204587,
            "llm_judge_score_mean": 3.4727272727272727,
            "llm_judge_score_std": 2.4297739921425947
          },
          "rationale_cider": 0.251853457825602
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.04889636595904394,
          "std_iou": 0.08790466156490277,
          "median_iou": 0.009249971611340509,
          "R@0.3": {
            "recall": 0.02564102564102564,
            "count": 4,
            "total": 156
          },
          "R@0.5": {
            "recall": 0.01282051282051282,
            "count": 2,
            "total": 156
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 156
          },
          "mae": {
            "start_mean": 173.6856153846154,
            "end_mean": 269.5000641025642,
            "average_mean": 221.59283974358974
          },
          "rationale": {
            "rouge_l_mean": 0.23203124563396704,
            "rouge_l_std": 0.10766326596411342,
            "text_similarity_mean": 0.48029599423544145,
            "text_similarity_std": 0.19770402429428133,
            "llm_judge_score_mean": 2.448717948717949,
            "llm_judge_score_std": 1.8684110615188643
          },
          "rationale_cider": 0.3308220525893337
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.036050566724887746,
          "std_iou": 0.07241354795506984,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.012987012987012988,
            "count": 5,
            "total": 385
          },
          "R@0.5": {
            "recall": 0.005194805194805195,
            "count": 2,
            "total": 385
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 385
          },
          "mae": {
            "start_mean": 136.38324415584415,
            "end_mean": 132.93447792207795,
            "average_mean": 134.65886103896102
          },
          "rationale": {
            "rouge_l_mean": 0.25164381480330333,
            "rouge_l_std": 0.10498152139533103,
            "text_similarity_mean": 0.5032069261760342,
            "text_similarity_std": 0.18813160157448405,
            "llm_judge_score_mean": 2.3324675324675326,
            "llm_judge_score_std": 2.2351641654134617
          },
          "rationale_cider": 0.33567318762079046
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.03131293975023188,
          "std_iou": 0.06359578294496829,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.009900990099009901,
            "count": 2,
            "total": 202
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 202
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 202
          },
          "mae": {
            "start_mean": 124.38968316831685,
            "end_mean": 160.86188613861387,
            "average_mean": 142.62578465346536
          },
          "rationale": {
            "rouge_l_mean": 0.22368925307854506,
            "rouge_l_std": 0.09709398941687775,
            "text_similarity_mean": 0.4784103303474586,
            "text_similarity_std": 0.21501445497078758,
            "llm_judge_score_mean": 2.103960396039604,
            "llm_judge_score_std": 2.040210666276393
          },
          "rationale_cider": 0.15948682943727366
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.03213428737628332,
          "std_iou": 0.07201322317056541,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.013333333333333334,
            "count": 2,
            "total": 150
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 150
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 150
          },
          "mae": {
            "start_mean": 166.3986933333333,
            "end_mean": 170.59026666666668,
            "average_mean": 168.49447999999995
          },
          "rationale": {
            "rouge_l_mean": 0.21013949741757687,
            "rouge_l_std": 0.08556038958690249,
            "text_similarity_mean": 0.4999148878827691,
            "text_similarity_std": 0.20076013045984373,
            "llm_judge_score_mean": 1.9333333333333333,
            "llm_judge_score_std": 2.0997354330698164
          },
          "rationale_cider": 0.1969251774964378
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.03198842676692015,
          "std_iou": 0.06750329926323441,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015486725663716814,
            "count": 7,
            "total": 452
          },
          "R@0.5": {
            "recall": 0.0022123893805309734,
            "count": 1,
            "total": 452
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 452
          },
          "mae": {
            "start_mean": 258.2040265486726,
            "end_mean": 247.94099336283185,
            "average_mean": 253.07250995575222
          },
          "rationale": {
            "rouge_l_mean": 0.21619538382008988,
            "rouge_l_std": 0.09348920378440412,
            "text_similarity_mean": 0.4807007023734988,
            "text_similarity_std": 0.20831616531887365,
            "llm_judge_score_mean": 1.747787610619469,
            "llm_judge_score_std": 1.9829180850155919
          },
          "rationale_cider": 0.1369832632877772
        },
        "13_Olympics": {
          "mean_iou": 0.029830676297670146,
          "std_iou": 0.07596833016633141,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.010638297872340425,
            "count": 1,
            "total": 94
          },
          "R@0.5": {
            "recall": 0.010638297872340425,
            "count": 1,
            "total": 94
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 94
          },
          "mae": {
            "start_mean": 94.86015957446808,
            "end_mean": 102.64986170212765,
            "average_mean": 98.75501063829786
          },
          "rationale": {
            "rouge_l_mean": 0.21050636150337249,
            "rouge_l_std": 0.08816463764571847,
            "text_similarity_mean": 0.5161343037765077,
            "text_similarity_std": 0.19417789200267765,
            "llm_judge_score_mean": 2.1914893617021276,
            "llm_judge_score_std": 2.4765994004506173
          },
          "rationale_cider": 0.10993418894597566
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.034904078510962534,
        "mae_average": 375.8129318135617,
        "R@0.3": 0.017144774843366097,
        "R@0.5": 0.004222012824627945,
        "R@0.7": 0.000514105777263672,
        "rationale": {
          "rouge_l_mean": 0.22501601394010962,
          "text_similarity_mean": 0.4933266735633474,
          "llm_judge_score_mean": 2.2424156294080877
        }
      }
    }
  }
}