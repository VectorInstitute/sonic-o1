{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 13,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.11730119832665412,
      "rouge_l_std": 0.027639229103997166,
      "text_similarity_mean": 0.2678690180182457,
      "text_similarity_std": 0.14196635290819595,
      "llm_judge_score_mean": 0.8461538461538461,
      "llm_judge_score_std": 0.6617173282340483
    },
    "short": {
      "rouge_l_mean": 0.08513570178290493,
      "rouge_l_std": 0.03369872625570429,
      "text_similarity_mean": 0.1991989128291607,
      "text_similarity_std": 0.08721272478501071,
      "llm_judge_score_mean": 0.38461538461538464,
      "llm_judge_score_std": 0.6249260311258432
    },
    "cider": {
      "cider_detailed": 0.0013602118109593635,
      "cider_short": 1.720688550517036e-05
    }
  },
  "per_entry_results": [
    {
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.16533333333333333,
        "text_similarity": 0.5248087048530579,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures superficial visuals (courtroom setting and a censorship/YouTube-removal theme) but omits the main factual content: the Exeter sentencing hearing, the defendant Frank's disorderly conduct case and agitation, the dropped breach charge and trooper/attorney details, and it incorrectly asserts the topic involves the Boston Police while adding unsupported visual specifics."
      },
      "short": {
        "rouge_l": 0.11464968152866242,
        "text_similarity": 0.2357965111732483,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only lists vague visual elements in a courtroom and omits nearly all key factual content (the platform warning, legal developments, Frank's protests/quotes, and the FreeKeene censorship conclusion), so it fails to capture the video's substance."
      }
    },
    {
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.1602787456445993,
        "text_similarity": 0.17412835359573364,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only describes superficial visual/audio details and fails to capture any substantive content from the reference\u2014omitting the defendant's crimes and sentence, victims' impact statements, the defendant's lack of remorse, and the judge's response\u2014so it is almost entirely incorrect."
      },
      "short": {
        "rouge_l": 0.09333333333333334,
        "text_similarity": 0.15989138185977936,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives only trivial visual details unrelated to the substantive content of the correct answer, omitting all key facts about the criminal history, victim impact statements, the defendant's statements, and the judge's responses."
      }
    },
    {
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.116991643454039,
        "text_similarity": 0.31617921590805054,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only lists a few superficial visuals (people in masks, courtroom shots, a news anchor) and omits virtually all key factual content from the correct summary\u2014no verdict, charges, evidence, official statements, or sentencing details\u2014so it fails to capture the video's substantive meaning."
      },
      "short": {
        "rouge_l": 0.07619047619047618,
        "text_similarity": 0.28773173689842224,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only lists generic visual shots (masked people, reporters, anchors) and lacks essentially all key factual elements from the reference (guilty verdict, charges, evidence details, motions, sentencing timeline, DA comments), so it fails to capture the video's substantive summary."
      }
    },
    {
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.14168937329700274,
        "text_similarity": 0.47825050354003906,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely unrelated: it only vaguely matches the setting (man in a suit) but omits the central event (use of an AI-generated avatar), the judge's reaction, and the video's discussion of legal/ethical implications, and even adds unsupported claims (parody interpretation)."
      },
      "short": {
        "rouge_l": 0.07194244604316546,
        "text_similarity": 0.26359879970550537,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is unrelated and only lists generic visuals; it omits all key facts about the AI-generated lawyer avatar, the courtroom interaction, the man's admission/startup promotion, and the ensuing ethical/legal discussion."
      }
    },
    {
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.10101010101010101,
        "text_similarity": 0.35290905833244324,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction describes the visual scene of a man testifying but omits the key factual elements from the correct answer\u2014identifying Lyle Menendez and his allegations of sexual abuse by his father\u2014so it fails to capture the video's central topic."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": 0.2543761134147644,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only describes a generic courtroom scene and omits key factual elements from the correct answer\u2014Lyle Menendez's identity, his testimony about his father's alleged sexual abuse, and the Menendez brothers context."
      }
    },
    {
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.0707395498392283,
        "text_similarity": 0.22218292951583862,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is a generic, factually inaccurate summary that omits all key factual elements (case name, counsel, anti\u2011SLAPP issues, cited precedents, and specific disputes about Hothi's conduct) and incorrectly frames the discussion as about government regulation; only a vague mention of technology overlaps with the reference."
      },
      "short": {
        "rouge_l": 0.08737864077669905,
        "text_similarity": 0.28344112634658813,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is unrelated and incorrect\u2014it describes a generic virtual meeting about technology and regulation, while the correct answer summarizes a court oral argument in Hothi v. Musk with specific legal arguments, parties, and case-law references, all of which are missing or contradicted."
      }
    },
    {
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.08524590163934426,
        "text_similarity": 0.25265076756477356,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction offers only a vague visual description and omits the core facts: it fails to identify the Senate confirmation setting, Senator Cruz, Judge Ketanji Brown Jackson, or the substantive questioning about legal standing and self-identification, so it is largely incorrect."
      },
      "short": {
        "rouge_l": 0.08,
        "text_similarity": 0.11764848977327347,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is merely a generic visual description and omits all key factual elements from the correct summary (Cruz's questions, the hypotheticals about identity, Jackson's refusal to answer, and her explanation of judicial process), so it fails to match the reference."
      }
    },
    {
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.1407624633431085,
        "text_similarity": 0.27396881580352783,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives only vague visual shots and omits all key factual and narrative elements (courtroom cross-examination, Pettis's admission, Lankford's reaction, contempt finding and adjournment), so it fails to capture the correct video's content."
      },
      "short": {
        "rouge_l": 0.14942528735632185,
        "text_similarity": 0.1817905157804489,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction offers only superficial visual descriptions and omits all key factual elements from the correct answer (the cross-examination, Pettis's admission and naming of Lee Lankford, Lankford's outburst/contempt, and the recess), so it fails to match semantically."
      }
    },
    {
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.08823529411764706,
        "text_similarity": 0.23374377191066742,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer only describes superficial visual elements and infers a presentation, completely omitting the video's substantive content about legal practice, preparation, settlement advice, career guidance, research, management, and fitness emphasized by the speaker."
      },
      "short": {
        "rouge_l": 0.08450704225352113,
        "text_similarity": 0.19982856512069702,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer only describes superficial visual elements and inferred demeanor, and omits all substantive points about civil litigation procedures, advice, and recommendations present in the correct answer, so it fails to match the reference."
      }
    },
    {
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.10091743119266054,
        "text_similarity": 0.06329302489757538,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the video summary: it discusses English language instruction rather than the prosecutor's trial account, omitting all key factual details and introducing hallucinated content."
      },
      "short": {
        "rouge_l": 0.08383233532934131,
        "text_similarity": 0.065260149538517,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary; it describes an English-language lesson rather than the prosecutor's case about Carl Miller, the bottle exchange, flight, officer injury, Dr. Reyes' license-plate observation, and forensic cocaine evidence."
      }
    },
    {
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.12142857142857144,
        "text_similarity": 0.04951100796461105,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary: it describes an instructional/exam video with on-screen text, while the correct answer recounts a specific theft and arrest incident; there is no semantic overlap and all key facts are omitted."
      },
      "short": {
        "rouge_l": 0.0588235294117647,
        "text_similarity": 0.00978381559252739,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated to the video summary and contains none of the key facts (vandalism, theft, Merchant rummaging, arrest, items found, or Mendoza's ID); it fails to address the correct answer's content."
      }
    },
    {
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.12048192771084339,
        "text_similarity": 0.1283634603023529,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated and factually incorrect compared to the reference: it describes a generic speaker about government law-and-order measures, whereas the correct summary details an in-depth discussion on criminal appeals, appellate strategy, case examples, and advocacy techniques."
      },
      "short": {
        "rouge_l": 0.12334801762114536,
        "text_similarity": 0.21842914819717407,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated and contradicts the correct answer: it talks about generic government enforcement and order, omitting all key elements about appellate advocacy, categories of appeals, technical drafting, oral advocacy, case examples, and practical guidance present in the correct summary."
      }
    },
    {
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.11180124223602485,
        "text_similarity": 0.41230762004852295,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer only gives superficial visual descriptions and omits all substantive content from the correct answer (the interview with Paul Gilbert, the discussion of witness nervousness, cross-examination challenges, the definition of witness familiarisation, and the theory-plus-mock training methodology), and it includes an unsubstantiated logo claim."
      },
      "short": {
        "rouge_l": 0.08333333333333333,
        "text_similarity": 0.3120095133781433,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer only lists visual shots and a logo, failing to summarize the video's core message about witness preparation, challenges, and familiarisation training; it contains no substantive overlap with the correct summary."
      }
    }
  ]
}