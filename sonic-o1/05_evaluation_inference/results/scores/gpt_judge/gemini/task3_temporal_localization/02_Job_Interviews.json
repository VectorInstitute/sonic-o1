{
  "topic_id": 2,
  "topic_name": "Job Interviews",
  "num_evaluated": 255,
  "aggregated_metrics": {
    "mean_iou": 0.31895370565318176,
    "std_iou": 0.3222556169575355,
    "median_iou": 0.26178010471204155,
    "R@0.3": {
      "recall": 0.47058823529411764,
      "count": 120,
      "total": 255
    },
    "R@0.5": {
      "recall": 0.30980392156862746,
      "count": 79,
      "total": 255
    },
    "R@0.7": {
      "recall": 0.17254901960784313,
      "count": 44,
      "total": 255
    },
    "mae": {
      "start_mean": 8.640117647058817,
      "end_mean": 10.636450980392153,
      "average_mean": 9.638284313725485
    },
    "rationale": {
      "rouge_l_mean": 0.29668575493434585,
      "rouge_l_std": 0.09200241816212403,
      "text_similarity_mean": 0.669644813619408,
      "text_similarity_std": 0.12131736908037669,
      "llm_judge_score_mean": 5.352941176470588,
      "llm_judge_score_std": 2.684914497574344
    },
    "rationale_cider": 0.24398839295755445
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the man asks the woman to sell him the pen, when does the woman start describing the pen as fantastic and smooth?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 3.47,
        "end": 8.757
      },
      "pred_interval": {
        "start": 4.5,
        "end": 8.0
      },
      "iou": 0.6620011348590884,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0299999999999998,
        "end": 0.7569999999999997,
        "average": 0.8934999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.7422834038734436,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the sequence (anchor before the woman's description) and gives reasonable approximate timings, but its timestamps are off by about 0.5\u20131.0s compared to the reference and it adds a minor extra detail ('Yes sir') not present in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes asking why the man needs the pen, when does the man reply, 'Ah, exactly. Now, this is the point...'?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 24.55,
        "end": 30.536
      },
      "pred_interval": {
        "start": 24.5,
        "end": 29.5
      },
      "iou": 0.8200795228628227,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.05000000000000071,
        "end": 1.0360000000000014,
        "average": 0.543000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315788,
        "text_similarity": 0.6871903538703918,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer matches the reference both semantically and temporally \u2014 the anchor ends around 24.3\u201324.5s and the man's reply begins immediately after (\u224824.5s) and continues through ~29.5\u201330.5s, preserving the original meaning."
      }
    },
    {
      "question_id": "003",
      "question": "After the man says 'You are not there just to carry the product to the customer', when does he list reasons like color or thick writing for wanting a pen?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 39.244,
        "end": 50.436
      },
      "pred_interval": {
        "start": 41.5,
        "end": 45.0
      },
      "iou": 0.31272337383845605,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2560000000000002,
        "end": 5.436,
        "average": 3.846
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953488,
        "text_similarity": 0.5994396209716797,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that the reasons (e.g., 'color', 'thick writing') are listed after the anchor remark, but the temporal boundaries are noticeably off (predicts start ~41.5s and end ~45.0s versus reference 39.244s\u201350.436s) and it introduces a minor unsupported detail about 'looking for a solution.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces herself as a former visa officer, when does she explain what American officials expect during an interview?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 34.481,
        "end": 40.61
      },
      "pred_interval": {
        "start": 34.5,
        "end": 40.5
      },
      "iou": 0.9789525208027414,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.01899999999999835,
        "end": 0.10999999999999943,
        "average": 0.06449999999999889
      },
      "rationale_metrics": {
        "rouge_l": 0.24444444444444444,
        "text_similarity": 0.6540008187294006,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relationship, with E2 timestamps nearly identical and E1 start only ~1s off and lacking a precise end time; minor timing imprecision warrants a small deduction."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman states that the visa officer is judging you, when does the text 'BE CONFIDENT!' appear on screen?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 106.12,
        "end": 111.935
      },
      "pred_interval": {
        "start": 66.0,
        "end": 67.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.120000000000005,
        "end": 44.435,
        "average": 42.2775
      },
      "rationale_metrics": {
        "rouge_l": 0.2278481012658228,
        "text_similarity": 0.6918543577194214,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor around 47s, but the target timing is completely wrong (66s vs. ground truth 106.12\u2013111.935s), omitting the key factual element and contradicting the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes her advice about maintaining general eye contact, when does she begin talking about a slight smile?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 149.259,
        "end": 151.34
      },
      "pred_interval": {
        "start": 109.5,
        "end": 112.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.758999999999986,
        "end": 38.84,
        "average": 39.299499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.6913450360298157,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the slight-smile remark follows immediately after the eye-contact advice, but the timestamps are substantially incorrect (predicted ~109.5\u2013112.5s vs. ground truth ~149.239\u2013151.34s), so it is factually wrong on key details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes stating that the visa officer will not have a favorable impression, when does she mention that this could influence a refusal?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 155.0,
        "end": 156.5
      },
      "pred_interval": {
        "start": 154.5,
        "end": 156.8
      },
      "iou": 0.652173913043475,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 0.30000000000001137,
        "average": 0.4000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.7027696371078491,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the relation (E2 immediately follows E1) and the quoted content, but the timestamps differ slightly from the ground truth (E1 is 0.5s early and E2 is shifted/0.3s longer), so it's mostly correct with minor temporal inaccuracies."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman asks what to do to gain confidence, when does she state that practicing is first and foremost?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 159.766,
        "end": 161.729
      },
      "pred_interval": {
        "start": 160.0,
        "end": 162.3
      },
      "iou": 0.6823204419889501,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.23400000000000887,
        "end": 0.570999999999998,
        "average": 0.4025000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.6120272874832153,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target relation (E2 after E1) and the utterance timing of 'First and foremost is to practice', with only minor timestamp offsets (\u22480.2\u20130.6s) and omission of E1's start time; semantics and relation match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes explaining how Argo Visa can help gain confidence for visa issuance, when does the screen transition to 'Follow us:'?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 187.512,
        "end": 200.0
      },
      "pred_interval": {
        "start": 188.0,
        "end": 200.0
      },
      "iou": 0.9609224855861628,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.48799999999999955,
        "end": 0.0,
        "average": 0.24399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903615,
        "text_similarity": 0.6537153720855713,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the relation (transition occurs after the anchor finishes) and that the 'Follow us:' screen lasts until 200.0s, but its timestamps are ~0.5s later than the ground-truth intervals, so it is not fully accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the second house rule about turning on video, when does she state that without video the interview will not be conducted?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 29.731,
        "end": 32.777
      },
      "pred_interval": {
        "start": 29.8,
        "end": 32.6
      },
      "iou": 0.9192383453709787,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.06899999999999906,
        "end": 0.1769999999999996,
        "average": 0.12299999999999933
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.6546926498413086,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer accurately matches the reference: timestamps closely align (minor rounding differences), the quoted target phrase is correct, and it correctly states the target occurs after the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how to use the chat icon, when does she start explaining how to use the raise hand icon?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 50.521,
        "end": 57.454
      },
      "pred_interval": {
        "start": 51.1,
        "end": 54.5
      },
      "iou": 0.490408192701572,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5790000000000006,
        "end": 2.9540000000000006,
        "average": 1.7665000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.24175824175824176,
        "text_similarity": 0.643004834651947,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the raise-hand explanation immediately follows the chat explanation and captures the speaker's wording, but its timestamps are noticeably later than the ground-truth times, so the timing details are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing all the continents where TTEC is located, when does she state that TTEC India is located in Ahmedabad?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 84.04,
        "end": 88.665
      },
      "pred_interval": {
        "start": 84.0,
        "end": 89.0
      },
      "iou": 0.925,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.04000000000000625,
        "end": 0.33499999999999375,
        "average": 0.1875
      },
      "rationale_metrics": {
        "rouge_l": 0.5348837209302325,
        "text_similarity": 0.8616747856140137,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer accurately matches the content and timing of both segments with only negligible (<1s) timing differences and reproduces the India statement exactly, so it is a faithful match to the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing the first reason for leaving a job, when does she start explaining the second reason?",
      "video_id": "2dgx53kiOBQ",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 46.0
      },
      "gt_interval": {
        "start": 10.023,
        "end": 15.169
      },
      "pred_interval": {
        "start": 10.0,
        "end": 15.0
      },
      "iou": 0.962855484619849,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.022999999999999687,
        "end": 0.16900000000000048,
        "average": 0.09600000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.4655470550060272,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that the second reason follows immediately and gives similar start/end times, but it misstates the first reason's end time (9.5s vs 10.003s) and adds an unverified phrase ('can't stand your boss'), introducing inaccuracies."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks 'So what can you say?', when does she provide the suggested response?",
      "video_id": "2dgx53kiOBQ",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 46.0
      },
      "gt_interval": {
        "start": 37.076,
        "end": 40.609
      },
      "pred_interval": {
        "start": 37.6,
        "end": 40.7
      },
      "iou": 0.8302980132450327,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5240000000000009,
        "end": 0.09100000000000108,
        "average": 0.307500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.19047619047619047,
        "text_similarity": 0.3867405652999878,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures that the response follows immediately and the target end time is close, but it misstates the anchor timing (no start given and end time is ~0.54s later than ground truth) and omits precise start times for both segments."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'you're on the hunt', when does he say '\u6211\u5728\u627e\u5de5\u4f5c'?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 3.0,
        "end": 4.9
      },
      "pred_interval": {
        "start": 3.9,
        "end": 5.2
      },
      "iou": 0.4545454545454547,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8999999999999999,
        "end": 0.2999999999999998,
        "average": 0.5999999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.42666666666666675,
        "text_similarity": 0.7785638570785522,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both utterances, their order ('after'), and a valid translation, but the timestamps differ notably from the reference (E1 predicted ~1.0s vs 1.633s; E2 predicted 3.9\u20135.2s vs 3.0\u20134.9s), so timing accuracy is lacking."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying '\u5c65\u5386\u8868' (CV), when does he say '\u5e94\u5f81\u5de5\u4f5c' (To apply for jobs)?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 15.5,
        "end": 16.5
      },
      "pred_interval": {
        "start": 15.9,
        "end": 17.4
      },
      "iou": 0.31578947368421056,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.40000000000000036,
        "end": 0.8999999999999986,
        "average": 0.6499999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.7296099662780762,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both utterances and the 'once_finished' relation; the timestamps are slightly offset (anchor -0.2s, target start +0.4s, target end +0.9s) but the semantic alignment and ordering are preserved."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'when's the interview?', when does he ask about the salary for the position in Mandarin?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 33.0,
        "end": 36.4
      },
      "pred_interval": {
        "start": 33.2,
        "end": 36.5
      },
      "iou": 0.914285714285713,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.20000000000000284,
        "end": 0.10000000000000142,
        "average": 0.15000000000000213
      },
      "rationale_metrics": {
        "rouge_l": 0.5074626865671642,
        "text_similarity": 0.7716387510299683,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events, the Mandarin phrasing, and the 'after' relation, with E2 timing nearly identical; however the anchor timing differs from the reference (predicted 22.0s vs correct finish 23.821s), a notable but minor discrepancy."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the first tip about dressing to impress, when does she start giving the second tip?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 11.708,
        "end": 14.471
      },
      "pred_interval": {
        "start": 11.5,
        "end": 14.1
      },
      "iou": 0.8051161225176706,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.20800000000000018,
        "end": 0.37100000000000044,
        "average": 0.2895000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.6237686276435852,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the end of the first tip and the start of the second tip and preserves the relation; the reported timestamps are slightly off by ~0.15\u20130.4s but are semantically accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the three quick tips for a job fair, when does she explain the second tip about doing research?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 14.811,
        "end": 20.96
      },
      "pred_interval": {
        "start": 11.5,
        "end": 20.9
      },
      "iou": 0.6436575052854121,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.311,
        "end": 0.060000000000002274,
        "average": 1.685500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28205128205128205,
        "text_similarity": 0.6779946088790894,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor as the introduction and the relation (E2 after E1) and matches the end time closely, but it gives an inaccurate earlier start time for the second tip (11.5s vs 14.811s), a notable factual discrepancy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Nothing else will make you stand out more than your personality', when does she say 'Be sure to use that'?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 28.591,
        "end": 29.734
      },
      "pred_interval": {
        "start": 28.2,
        "end": 29.8
      },
      "iou": 0.7143749999999998,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.3910000000000018,
        "end": 0.06599999999999895,
        "average": 0.22850000000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.3768115942028986,
        "text_similarity": 0.638785719871521,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target and their relation, but the timestamps are slightly off (E1 predicted end 28.2s vs 28.371s; E2 predicted start 28.2s vs 28.591s and end 29.8s vs 29.734s), representing minor timing inaccuracies."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking \"Tell me about yourself\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 10.0,
        "end": 16.993
      },
      "pred_interval": {
        "start": 10.0,
        "end": 16.0
      },
      "iou": 0.8580008580008581,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.0,
        "end": 0.9929999999999986,
        "average": 0.4964999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.29473684210526313,
        "text_similarity": 0.7077618837356567,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures the sequence, relation, and full display window (\u224810.0s\u201317s) and the relation 'after' matches 'once_finished'; it slightly omits the precise E1 start time (8.643s) and gives a minor rounding difference on the question end time."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes asking \"Why should we hire you?\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 29.937,
        "end": 39.249
      },
      "pred_interval": {
        "start": 30.0,
        "end": 38.0
      },
      "iou": 0.8591065292096219,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.06299999999999883,
        "end": 1.2490000000000023,
        "average": 0.6560000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.30434782608695654,
        "text_similarity": 0.6631476283073425,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies that the green text appears immediately after the question and gives timing that closely matches the reference (\u224829.94s\u219230s start, \u224839.25s\u219239s end); it omits the exact E1 start time and uses a looser relation label ('after' vs 'once_finished') but is otherwise accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes announcing \"Let's check your pronunciation. Listen and repeat.\", when does he start repeating the first smart answer?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 121.718,
        "end": 126.147
      },
      "pred_interval": {
        "start": 122.0,
        "end": 126.0
      },
      "iou": 0.903138405960713,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.2819999999999965,
        "end": 0.14700000000000557,
        "average": 0.21450000000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.6280621290206909,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the sequence (the speaker repeats immediately after the announcement) and approximate durations, but the absolute timestamps are consistently about 60 seconds earlier than the reference and the relation label is less specific ('after' vs 'once_finished'), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of the virtual interview, when does she list the three things to set the right first impression?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 13.846,
        "end": 19.861
      },
      "pred_interval": {
        "start": 14.0,
        "end": 19.5
      },
      "iou": 0.9143807148794679,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.15399999999999991,
        "end": 0.36100000000000065,
        "average": 0.2575000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.6956608891487122,
        "llm_judge_score": 8,
        "llm_judge_justification": "The predicted answer correctly identifies the target segment (E2) timing and the 'after' relation, but it misstates the introduction (E1) timing by a large margin (predicts end ~12s vs gold 3.557s), so it's mostly correct but not fully accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to use a virtual background or blur it, when does she start talking about sound and internet connection?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 40.176,
        "end": 42.981
      },
      "pred_interval": {
        "start": 40.0,
        "end": 43.0
      },
      "iou": 0.9349999999999999,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.17600000000000193,
        "end": 0.01899999999999835,
        "average": 0.09750000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.39473684210526316,
        "text_similarity": 0.7751455307006836,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer matches the reference in event boundaries (minor rounding differences only) and the relation label; timings and semantic content align closely with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker recommends connecting laptops to an ethernet cable, when is the next piece of advice she gives regarding avoiding distractions?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 50.012,
        "end": 59.987
      },
      "pred_interval": {
        "start": 50.0,
        "end": 54.0
      },
      "iou": 0.399319114849304,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.012000000000000455,
        "end": 5.987000000000002,
        "average": 2.999500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837834,
        "text_similarity": 0.7719429135322571,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the ethernet anchor and the subsequent 'do not disturb' phone advice with nearly matching start times and an equivalent 'after/next' relation; the only minor discrepancy is the shorter end time for E2 compared to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the video's topic about crushing a sales job interview, when does the animated logo for 'The Elliott Group' appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.378,
        "end": 13.048
      },
      "pred_interval": {
        "start": 7.0,
        "end": 13.0
      },
      "iou": 0.9295634920634921,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.3780000000000001,
        "end": 0.04800000000000004,
        "average": 0.21300000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.4411764705882353,
        "text_similarity": 0.9236372709274292,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction largely matches the event order and end time, but the start time is off (~7.0s vs 7.378s) and it introduces an unverified 'fire effect'; the relationship ('after/immediately after') is acceptable."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that many applicants are 'unprepared' for sales jobs, when does the text overlay 'COME PREPARED' appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 55.459,
        "end": 56.559
      },
      "pred_interval": {
        "start": 54.0,
        "end": 60.0
      },
      "iou": 0.18333333333333238,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4590000000000032,
        "end": 3.4410000000000025,
        "average": 2.450000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.45901639344262296,
        "text_similarity": 0.8164732456207275,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the events and their temporal relation ('after') and approximates E1, but it misestimates E2 timing\u2014the overlay's onset and especially its disappearance differ noticeably from the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker describes looking 'unmanicured' for an interview, when does he make a specific hand gesture with both hands to emphasize his point?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 322.0,
        "end": 323.0
      },
      "pred_interval": {
        "start": 202.0,
        "end": 204.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 120.0,
        "end": 119.0,
        "average": 119.5
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.8217774629592896,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies a simultaneous hand-gesture and the 'during' relation and notes both hands, but the timestamps are far off from the reference (202\u2013204s vs. 321\u2013324.8s) and it adds specific wording not present in the ground truth, so it is largely temporally incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that resumes are not needed, when does he ask what dealerships want instead?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 175.098,
        "end": 175.998
      },
      "pred_interval": {
        "start": 177.0,
        "end": 179.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.9019999999999868,
        "end": 3.0020000000000095,
        "average": 2.451999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.23188405797101447,
        "text_similarity": 0.6636700630187988,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and that the target follows, but the target timestamp is off (predicted ~177.0s vs correct ~175.1\u2013176.0s) and the timing/video-time labels are inconsistent, so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's explanation of what 'the machine' is, when does a visual of a man and woman in a car showroom appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 307.098,
        "end": 311.098
      },
      "pred_interval": {
        "start": 309.0,
        "end": 312.0
      },
      "iou": 0.4279885760913951,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.9019999999999868,
        "end": 0.9019999999999868,
        "average": 1.4019999999999868
      },
      "rationale_metrics": {
        "rouge_l": 0.2916666666666667,
        "text_similarity": 0.534329354763031,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes a showroom visual appearing during the speaker's answer, but its timestamps conflict with the reference (starts and ends later and extends beyond the anchor speech), contradicting the correct containment and timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining that customers and managers will see you as someone who 'works at the mall' if you are dressed poorly, when does he directly advise to 'dress nice'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 273.401,
        "end": 274.923
      },
      "pred_interval": {
        "start": 281.0,
        "end": 283.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5989999999999895,
        "end": 8.076999999999998,
        "average": 7.837999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.25882352941176473,
        "text_similarity": 0.44883543252944946,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the speaker tells listeners to 'dress nice' but the timestamps are substantially off and contradict the reference (prediction places the advice much later and adds a longer pause), so it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if people are hiring someone average, when does he mention they are trying to hire a 'difference maker'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.877,
        "end": 375.04
      },
      "pred_interval": {
        "start": 376.0,
        "end": 378.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.1229999999999905,
        "end": 2.9599999999999795,
        "average": 4.041499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.26506024096385544,
        "text_similarity": 0.8614183068275452,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the relative ordering (the 'difference maker' remark follows the question about hiring an average person) but the reported time boundaries are notably inaccurate\u2014each interval is off by about 3\u20135 seconds and does not match the correct start/end timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he is 'very coachable', when does the text overlay 'I'M VERY COACHABLE' appear on screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 414.192,
        "end": 418.53
      },
      "pred_interval": {
        "start": 417.0,
        "end": 421.0
      },
      "iou": 0.22473560517038402,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8079999999999927,
        "end": 2.4700000000000273,
        "average": 2.63900000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21621621621621623,
        "text_similarity": 0.6901618838310242,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the visual overlay follows the speech, but the timestamps are significantly off (predicted ~417s vs ground truth ~414s) and the predicted overlay end time (~421s) greatly contradicts the reference (4148.53s), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises to have 'eye contact', when does he directly look at the camera and say 'just like I'm looking at you in the camera'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 533.923,
        "end": 537.649
      },
      "pred_interval": {
        "start": 537.0,
        "end": 540.0
      },
      "iou": 0.10679611650485456,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.076999999999998,
        "end": 2.350999999999999,
        "average": 2.7139999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.37209302325581395,
        "text_similarity": 0.8334765434265137,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted E1 time (535.0s) roughly falls within the correct E1 interval, but the predicted E2 times are substantially off\u2014it shifts the start much later (537.0s vs 533.923s) and extends beyond the true end (540.0s vs 537.649s), mislocating when the quoted line is spoken."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"eye contact, write that down,\" when does he demonstrate eye contact with his hands?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 535.23,
        "end": 537.26
      },
      "pred_interval": {
        "start": 538.2,
        "end": 540.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.9700000000000273,
        "end": 3.240000000000009,
        "average": 3.105000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.32967032967032966,
        "text_similarity": 0.8223719596862793,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the gesture and that it occurs after the spoken line, but the timestamps are substantially off (E1 and E2 shifted by ~3\u20134 seconds) and it fails to reflect that the action immediately follows the instruction, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"How stupid would that be?\" when does he say, \"Eye contact, look.\"",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 549.39,
        "end": 551.41
      },
      "pred_interval": {
        "start": 554.3,
        "end": 555.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.909999999999968,
        "end": 4.389999999999986,
        "average": 4.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.4047619047619048,
        "text_similarity": 0.8404573798179626,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation correct (E2 occurs after E1) but both event timestamps are substantially shifted (~3\u20135 seconds later) compared to the ground truth, making the times factually incorrect; the added remark about a brief intervening sentence is unsupported by the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'll self-educate\" for the second time, when does the text overlay \"I'LL SELF EDUCATE\" appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 637.11,
        "end": 642.12
      },
      "pred_interval": {
        "start": 641.1,
        "end": 646.2
      },
      "iou": 0.11221122112210982,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.990000000000009,
        "end": 4.080000000000041,
        "average": 4.035000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.3298969072164949,
        "text_similarity": 0.8519734144210815,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it misplaces the second utterance (predicts 640.5\u2013642.0s vs correct 535.09\u2013540.11s), gives incorrect overlay timing (641.1\u2013646.2s vs correct 637.11\u2013642.12s), and wrongly states the text appears during the speech rather than immediately after."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"We help you land your dream job\", when does the text \"TRAGIC ENDINGS\" appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.968,
        "end": 13.737
      },
      "pred_interval": {
        "start": 12.72,
        "end": 14.28
      },
      "iou": 0.4398788927335639,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7520000000000007,
        "end": 0.5429999999999993,
        "average": 0.6475
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.7730209827423096,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the relation ('after') and gives reasonable approximate times, but the reported start (12.72s vs 11.968s) and end (14.28s vs 13.737s) timestamps are off by about 0.5\u20130.75 seconds, so it's not perfectly precise."
      }
    },
    {
      "question_id": "003",
      "question": "Once the 'My Practice Interview' logo and text animation finishes, when does the speaker return on screen and say \"So, we're talking about casual interviews\"?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 50.841,
        "end": 52.768
      },
      "pred_interval": {
        "start": 51.0,
        "end": 53.6
      },
      "iou": 0.64081188836535,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.15899999999999892,
        "end": 0.8320000000000007,
        "average": 0.49549999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.6106992363929749,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures that the speaker returns right after the logo animation and gives start time very close to the reference, but it rounds the intro finish and overestimates the utterance end by about 0.8s, so timing is not exact."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the client receiving an email stating 'Sorry, we're going with somebody else,' when does he say the client was devastated?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 176.5,
        "end": 177.7
      },
      "pred_interval": {
        "start": 177.6,
        "end": 179.4
      },
      "iou": 0.03448275862068763,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0999999999999943,
        "end": 1.700000000000017,
        "average": 1.4000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.25974025974025977,
        "text_similarity": 0.6789804100990295,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same events (the quoted email and the client being devastated) but gives notably different timestamps (E1 placed earlier and E2 starting and ending later than the reference) and characterizes the relation as 'immediate succession' rather than the referenced 'after', so it is a partial but imperfect match."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is saying 'you come across as sounding actually inexperienced', when does the word 'INEXPERIENCED' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 225.1,
        "end": 228.2
      },
      "pred_interval": {
        "start": 225.9,
        "end": 227.2
      },
      "iou": 0.41935483870967266,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8000000000000114,
        "end": 1.0,
        "average": 0.9000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6532143354415894,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events but omits the speaker timing and gives incorrect text timings (predicts 225.9\u2013227.2s vs. ground truth 225.1\u2013228.2s), incorrectly claiming synchronization with speech and adding an unsupported detail about text color."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing the three tips for casual interviews, when does 'KEY TIP #1 NEVER TREAT AN INTERVIEW AS A CASUAL AFFAIR' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 270.6,
        "end": 275.0
      },
      "pred_interval": {
        "start": 272.2,
        "end": 276.9
      },
      "iou": 0.44444444444444947,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.599999999999966,
        "end": 1.8999999999999773,
        "average": 1.7499999999999716
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.476662278175354,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially later timestamps: anchor finish 271.5s vs 270.7s and key-tip 272.2\u2013276.9s vs 270.6\u2013275.0s. There is partial overlap, but the timings are inaccurate, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'So, those are my three tips', when does the text 'YOU NEED TO PREPARE THE SAME WAY' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 379.3,
        "end": 382.2
      },
      "pred_interval": {
        "start": 380.0,
        "end": 382.5
      },
      "iou": 0.6874999999999989,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6999999999999886,
        "end": 0.30000000000001137,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.5432098765432098,
        "text_similarity": 0.6034132242202759,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the text, its start/end times and the 'after' relation; E2 times are very close to the reference. However, E1 is off by ~3.1s, a minor but noticeable timing discrepancy."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining about the interview preparation videos, when does he start talking about his free ebook, 'My Practice Interview'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 401.4,
        "end": 409.8
      },
      "pred_interval": {
        "start": 404.8,
        "end": 420.5
      },
      "iou": 0.26178010471204155,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.400000000000034,
        "end": 10.699999999999989,
        "average": 7.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2828282828282828,
        "text_similarity": 0.7513988018035889,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'once_finished' relation and mentions the ebook, but the key timestamps disagree substantially with the ground truth (E1 and E2 start/end times differ by several seconds) and the predicted E2 end time extends well beyond the reference, so it is factually inaccurate on crucial details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing his free ebook, when is the next time he mentions 'My Interview Accelerator Workshop'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 418.2,
        "end": 421.9
      },
      "pred_interval": {
        "start": 421.0,
        "end": 426.5
      },
      "iou": 0.10843373493975615,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8000000000000114,
        "end": 4.600000000000023,
        "average": 3.700000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.31325301204819284,
        "text_similarity": 0.7297202348709106,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same next resource and that it occurs after the ebook mention, but the timestamps are significantly off (predicted 421.0s vs correct 418.2s) and it uses an on-screen text cue rather than the earlier spoken mention, so it does not align precisely with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces herself as a licensed hairdresser, when does she explain why she decided to take a break from the hair industry?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.03,
        "end": 28.03
      },
      "pred_interval": {
        "start": 22.2,
        "end": 39.5
      },
      "iou": 0.2890173410404624,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8300000000000018,
        "end": 11.469999999999999,
        "average": 6.15
      },
      "rationale_metrics": {
        "rouge_l": 0.4403669724770642,
        "text_similarity": 0.44385820627212524,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the explanation occurs after the introduction and roughly around 22\u201323s, but misstates the introduction time (4s vs 5.66s) and significantly overextends and fabricates the explanation content/duration (ends at 39.5s vs 28.03s), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states she needs to get ready, when does she announce that her hair and makeup are done?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.66,
        "end": 113.61
      },
      "pred_interval": {
        "start": 150.8,
        "end": 153.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.140000000000015,
        "end": 39.89,
        "average": 40.01500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2823529411764706,
        "text_similarity": 0.47236230969429016,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance and the quoted line 'Hair is done, makeup is done,' but it gives a substantially incorrect timestamp for the target event (150.8\u2013153.5s vs. 110.66\u2013113.61s) and adds an unverified 'jump cut' detail, so the temporal information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says she will try on outfits, when does she begin to show her chosen interview outfit in the mirror?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 277.7,
        "end": 279.6
      },
      "pred_interval": {
        "start": 241.0,
        "end": 246.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.69999999999999,
        "end": 33.60000000000002,
        "average": 35.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.20754716981132074,
        "text_similarity": 0.5198184251785278,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but misreports both event times by large margins (E1 ~45s early, E2 ~36\u201339s early) and omits the detail that the outfit is shown fully by 279.6s, so it fails on key factual accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman declares she has finally 'got the outfit down', when does she describe the specific clothing items she is wearing?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 257.6,
        "end": 272.0
      },
      "pred_interval": {
        "start": 262.0,
        "end": 276.0
      },
      "iou": 0.5434782608695659,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.399999999999977,
        "end": 4.0,
        "average": 4.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2772277227722772,
        "text_similarity": 0.41214704513549805,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that the clothing description follows the anchor and identifies start/end of the listing, but all three timestamps deviate noticeably from the ground truth (E1 ~7.5s early; E2 start/end ~4s late), so the timing is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions her discount code, when does she start explaining Dossier's reward system?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 423.05,
        "end": 433.322
      },
      "pred_interval": {
        "start": 424.0,
        "end": 438.0
      },
      "iou": 0.6235451505016729,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9499999999999886,
        "end": 4.677999999999997,
        "average": 2.813999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6847625970840454,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' and roughly locates the discount code within the broader time window, but the target start/end times are off (starts ~0.95s late and ends ~4.7s late) and it adds extra details (checkout process) not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes spraying perfume on her neck/hair, when does she spray perfume on her wrist?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 365.341,
        "end": 366.421
      },
      "pred_interval": {
        "start": 367.0,
        "end": 369.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6589999999999918,
        "end": 2.5790000000000077,
        "average": 2.1189999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.827811598777771,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'once_finished' relation, but the reported timestamps are noticeably later than the ground truth (off by ~1\u20132.6s), so the temporal localization is imprecise."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker suggests bringing a resume, when does she explain why it's a good idea?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 440.04,
        "end": 452.824
      },
      "pred_interval": {
        "start": 452.0,
        "end": 488.0
      },
      "iou": 0.01718098415346148,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.95999999999998,
        "end": 35.17599999999999,
        "average": 23.567999999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.2736842105263158,
        "text_similarity": 0.7278008460998535,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relation 'once_finished' is correct, the predicted timestamps are substantially misaligned with the reference (anchor shifted ~8\u201312s later, explanation start occurs near the reference explanation's end and the predicted end extends far beyond), so it fails to capture the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman advises writing down a list of questions, when does she suggest asking about work hours?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 537.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 539.5,
        "end": 542.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5,
        "end": 2.5,
        "average": 2.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3902439024390244,
        "text_similarity": 0.5562034845352173,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct utterance and the 'after' relationship, but the timestamps are noticeably shifted (~2.5\u20133s later) and do not match the accurate intervals given in the ground truth, so timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman suggests researching the salon's social media and website, when does she explain why this research is important?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 652.5,
        "end": 659.0
      },
      "pred_interval": {
        "start": 662.0,
        "end": 673.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.5,
        "end": 14.5,
        "average": 12.0
      },
      "rationale_metrics": {
        "rouge_l": 0.22916666666666669,
        "text_similarity": 0.44293832778930664,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly captures that she explains why research matters, but the timestamps are substantially off (E1 wrong by ~36s; E2 shifted and extended), and the quoted wording adds details not in the reference, so it is only a partial, inaccurate match."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman recommends having a portfolio ready to show, when does she emphasize the importance of social media in the salon world?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 702.0
      },
      "pred_interval": {
        "start": 698.5,
        "end": 704.0
      },
      "iou": 0.4375,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5,
        "end": 2.0,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4772727272727273,
        "text_similarity": 0.6233314275741577,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events and their 'after' relationship, but the timestamps are shifted by ~2\u20132.5s (and E1 given as a single time rather than the full interval) and E2's end is extended, so timings are slightly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman talks about social media being a big thing in the salon world, when does she explain that social media marketing can bring more clients into the salon?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 733.4,
        "end": 797.5
      },
      "pred_interval": {
        "start": 754.2,
        "end": 761.6
      },
      "iou": 0.11544461778471099,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.800000000000068,
        "end": 35.89999999999998,
        "average": 28.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.6245781183242798,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted intervals for E1 (700.0s) and E2 (754.2\u2013761.6s) fall within the reference ranges and correctly identify the content and 'after' relation, with no added incorrect details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes saying that confidence and the way one presents themselves goes a long way, when does she talk about preferring a personable applicant?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 784.0,
        "end": 794.9
      },
      "pred_interval": {
        "start": 786.5,
        "end": 793.5
      },
      "iou": 0.6422018348623867,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5,
        "end": 1.3999999999999773,
        "average": 1.9499999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951805,
        "text_similarity": 0.7105432748794556,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the same relation and reasonably matches E2, but E1's end time is substantially off (771.0s vs. 783.8s), which omits a key temporal fact and reduces overall accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman talks about waiting for the car's air conditioning to cool down, when does she advise giving enough time to arrive early for the interview?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 854.5,
        "end": 861.7
      },
      "pred_interval": {
        "start": 821.1,
        "end": 827.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.39999999999998,
        "end": 34.10000000000002,
        "average": 33.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.6093761324882507,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the semantic relation ('after') and the content (AC comment then advice) but the timestamps are substantially off (~41\u201343 seconds earlier) and do not match the reference timing, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'wish me luck', when does she say 'I'm back from the interview'?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1004.0
      },
      "gt_interval": {
        "start": 882.5,
        "end": 883.5
      },
      "pred_interval": {
        "start": 886.6,
        "end": 888.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.100000000000023,
        "end": 4.7999999999999545,
        "average": 4.449999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168672,
        "text_similarity": 0.700095534324646,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction preserves the correct temporal relation (anchor before target) but gives timestamps that are off by roughly 4 seconds for both events and thus does not match the precise ground-truth timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says the interview was 'super easy', when does she mention there was 'none of that typical what are your strengths and weaknesses'?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1004.0
      },
      "gt_interval": {
        "start": 927.9,
        "end": 932.1
      },
      "pred_interval": {
        "start": 931.5,
        "end": 936.5
      },
      "iou": 0.06976744186046757,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.6000000000000227,
        "end": 4.399999999999977,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.18749999999999997,
        "text_similarity": 0.4926159381866455,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the order and the quoted phrase, but both anchor and target timestamps are several seconds later than the reference and the prediction adds extra trailing text not present in the ground truth, so the timing/alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the animated intro sequence concludes, when does the speaker greet the audience with \"Morning, everyone\"?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 51.533,
        "end": 52.234
      },
      "pred_interval": {
        "start": 51.0,
        "end": 52.5
      },
      "iou": 0.46733333333333366,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5330000000000013,
        "end": 0.26599999999999824,
        "average": 0.39949999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.5714285714285715,
        "text_similarity": 0.8384758234024048,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the relation and general timing, but the timestamps are slightly off (E1 ~0.488s late; E2 start ~0.533s early and finish ~0.266s late) so it is mostly correct but not exact."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"First, context\", when does the text \"Design exercise \u2260 white boarding\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.396,
        "end": 101.982
      },
      "pred_interval": {
        "start": 56.0,
        "end": 101.0
      },
      "iou": 0.9700317515549562,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.3960000000000008,
        "end": 0.9819999999999993,
        "average": 0.6890000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.5432098765432098,
        "text_similarity": 0.8089922070503235,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the sequence and visibility of the text and relation, but has small timing inaccuracies (E1: 55.0 vs 56.156; E2: 56.0 vs 56.396; end: 101.0 vs 101.982), so it's largely correct but not exact."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that the next thing to get right is the deliverables, when does the text introduce action item number two?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 195.3,
        "end": 198.0
      },
      "pred_interval": {
        "start": 196.0,
        "end": 199.0
      },
      "iou": 0.5405405405405422,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6999999999999886,
        "end": 1.0,
        "average": 0.8499999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.6469118595123291,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures the anchor/target order and gives timestamps very close to the ground truth; differences are minor (~0.4\u20131.0s later) and do not change the answer's correctness."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he clearly missed the biggest thing, when does the text describe the final deliverable for a take-home exercise?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 256.5,
        "end": 261.7
      },
      "pred_interval": {
        "start": 256.0,
        "end": 262.0
      },
      "iou": 0.8666666666666648,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5,
        "end": 0.30000000000001137,
        "average": 0.4000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.13861386138613863,
        "text_similarity": 0.7317376136779785,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the target text and its timing roughly (256.0\u2013262.0s vs 256.5\u2013261.7s) and the deliverable wording, but it mislocates the anchor (E1) at ~244s rather than ~254.8s, a significant error for an anchor-relative question."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"action item number three\", when does the text overlay \"Use standard patterns\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 348.0,
        "end": 352.0
      },
      "pred_interval": {
        "start": 349.0,
        "end": 352.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 0.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.6482983827590942,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the target text's disappearance time (~352.0s) and that the overlay follows the anchor, but it significantly misstates the anchor timing (predicts anchor ends at 348.8s vs ground truth ~344.5s) and shifts the target appearance (~349.0s vs 348.0s), contradicting the correct temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is talking about looking at other apps for design inspiration, when does the text overlay \"By looking at Google MD, Apple HIG\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.0,
        "end": 378.0
      },
      "pred_interval": {
        "start": 370.5,
        "end": 382.8
      },
      "iou": 0.5859374999999994,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 4.800000000000011,
        "average": 2.6500000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.2268041237113402,
        "text_similarity": 0.5663192272186279,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the text overlay start roughly right (within 0.5s) but incorrectly shifts the speaker's anchor much later and extends the overlay end time beyond the correct end (382.8s vs 378.0s), producing significant timing mismatches and an incorrect overlap."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"Action item number four\", when does the text overlay \"Pick the right prompt\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 382.7,
        "end": 386.0
      },
      "pred_interval": {
        "start": 383.0,
        "end": 386.0
      },
      "iou": 0.909090909090906,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.30000000000001137,
        "end": 0.0,
        "average": 0.15000000000000568
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272727,
        "text_similarity": 0.6627339124679565,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction misstates the anchor timing (saying E1 ends at 383.0s vs the correct 379.3s) and incorrectly claims the text appears immediately after the speech, whereas the correct answer notes a ~3.4s delay; the predicted duration of the overlay roughly matches the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"Now is the fun one, fun time,\" when does the text \"Hot Take\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 528.0,
        "end": 533.5
      },
      "pred_interval": {
        "start": 530.8,
        "end": 533.8
      },
      "iou": 0.4655172413793218,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7999999999999545,
        "end": 0.2999999999999545,
        "average": 1.5499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.27499999999999997,
        "text_similarity": 0.69959956407547,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct event order (target after anchor) but gives substantially incorrect timestamps for both the anchor and target start times and adds an unsupported detail (fire emojis); only the target end time is roughly close."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions his 'cover letter video', when does the YouTube video thumbnail related to cover letters appear on the screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 566.5,
        "end": 615.0
      },
      "pred_interval": {
        "start": 568.1,
        "end": 571.1
      },
      "iou": 0.061855670103092786,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6000000000000227,
        "end": 43.89999999999998,
        "average": 22.75
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.7391071319580078,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right (anchor then thumbnail) but the timestamps are substantially incorrect\u2014the anchor is placed ~5s late and the thumbnail\u2019s start and especially end times are wrong (predicted a brief 3s display vs actual ~48.5s), omitting the correct duration."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker says \"smash the like button for the YouTube algorithm,\" when does he make a 'smashing' gesture with his arms?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 607.0,
        "end": 609.0
      },
      "pred_interval": {
        "start": 598.6,
        "end": 599.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.399999999999977,
        "end": 9.399999999999977,
        "average": 8.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7452033758163452,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly timestamps both the anchor and gesture (~598s\u2013599s) instead of the reference times (E1: 605.0\u2013608.0, E2: 607.0\u2013609.0), so it fails to match the correct timing and overlap."
      }
    },
    {
      "question_id": "001",
      "question": "After the host states that Mr. Syed Hassan is a top LinkedIn voice with good experience in recruitment and HR, when does Syed greet the host?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.826,
        "end": 23.329
      },
      "pred_interval": {
        "start": 21.0,
        "end": 24.0
      },
      "iou": 0.501,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8260000000000005,
        "end": 0.6709999999999994,
        "average": 0.7484999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.16129032258064516,
        "text_similarity": 0.8318727016448975,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events, their relative order, and Syed's immediate greeting; timestamps are slightly off (E1 ~19.3s vs ~20s, E2 starts ~21.8s vs 21.0s and ends ~23.3s vs ~24.0s) but differences are minor and do not change the meaning."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes asking what applicants should consider before applying for jobs, when does Syed begin his answer by saying 'Well, that's a very good question'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 74.466,
        "end": 81.582
      },
      "pred_interval": {
        "start": 114.0,
        "end": 117.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.534000000000006,
        "end": 35.418000000000006,
        "average": 37.476000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.21333333333333332,
        "text_similarity": 0.6816257238388062,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's temporal relation (immediate start) matches the correct relation, but its timestamps (114s\u2013117s) contradict the accurate times (anchor ~64.26\u201373.355s; target ~74.466\u201381.3s), so it is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Syed states that many people apply for irrelevant jobs, when does he start explaining that most companies in Dubai use ATS systems?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.0,
        "end": 105.605
      },
      "pred_interval": {
        "start": 141.0,
        "end": 146.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 40.394999999999996,
        "average": 38.6975
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.8000678420066833,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the relative order (target after anchor) but both event timestamps are substantially incorrect\u2014E1 is placed around 141s vs 87\u201391.85s and E2 at 141\u2013146s vs 104\u2013105.6s\u2014so it fails to match the correct temporal locations."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first speaker finishes explaining that rejections in Dubai are automated, when does the second speaker give positive feedback?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 162.4,
        "end": 164.8
      },
      "pred_interval": {
        "start": 163.0,
        "end": 166.2
      },
      "iou": 0.4736842105263209,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5999999999999943,
        "end": 1.3999999999999773,
        "average": 0.9999999999999858
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.6513129472732544,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the relation (second speaker gives positive feedback after the first finishes) and timings are close, but the provided timestamps differ moderately from the reference (E1 ~0.7s later, E2 start ~0.6s later and E2 end ~1.4s later), so it's not a perfect match."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker mentions that developers are mostly in demand in the UAE, when does he start listing specific types of developers?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.1,
        "end": 255.2
      },
      "pred_interval": {
        "start": 252.0,
        "end": 257.0
      },
      "iou": 0.5423728813559298,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9000000000000057,
        "end": 1.8000000000000114,
        "average": 1.3500000000000085
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.4702652096748352,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the sequence and content (listing developer types) and aligns closely with the reference timestamps, but the predicted start/end times differ slightly (E1 +0.8s, E2 start +0.9s, E2 end +1.8s) so it's not an exact match."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the left (Hassan) starts explaining his screening process for resumes, when does he first mention checking the candidate's years of experience?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 364.35,
        "end": 366.36
      },
      "pred_interval": {
        "start": 370.0,
        "end": 372.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.649999999999977,
        "end": 5.639999999999986,
        "average": 5.644999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6323482394218445,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that Hassan mentions checking years of experience, but the provided timestamps are substantially off and internally inconsistent (wrong mm:ss conversions) compared to the reference, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the left (Hassan) mentions asking for a screening call, when does he advise to check for red flags during that call?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 429.39,
        "end": 432.42
      },
      "pred_interval": {
        "start": 434.0,
        "end": 437.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.610000000000014,
        "end": 4.579999999999984,
        "average": 4.594999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3146067415730337,
        "text_similarity": 0.5850545763969421,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct instruction and relation (advice to check red flags during the screening call), but the provided timestamps are inaccurate relative to the reference and include an inconsistent video-time mapping, so timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the left (Hassan) states they shortlist a candidate, when does he mention calling them to assess them in person?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.58,
        "end": 443.3
      },
      "pred_interval": {
        "start": 445.0,
        "end": 448.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.420000000000016,
        "end": 4.699999999999989,
        "average": 4.060000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903615,
        "text_similarity": 0.6672101616859436,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the sequence and quoted phrase (calling to assess in person) and that it follows shortlisting, but the provided timestamps are several seconds later than the ground-truth intervals, so the timing is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the right finishes talking about the content being helpful for many working professionals, when does he mention sharing Mr. Hassan's profile?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 523.7,
        "end": 526.1
      },
      "pred_interval": {
        "start": 516.9,
        "end": 522.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.800000000000068,
        "end": 3.5,
        "average": 5.150000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.6055048704147339,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events and their order but the timestamps are substantially incorrect (both anchor and target are shifted several seconds earlier and the target's end time differs), so it fails to match the ground-truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the right asks the audience if they have any questions, when does he tell them to write in the comments?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 542.0,
        "end": 543.5
      },
      "pred_interval": {
        "start": 535.3,
        "end": 536.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.7000000000000455,
        "end": 6.7000000000000455,
        "average": 6.7000000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.21333333333333335,
        "text_similarity": 0.5518801212310791,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general actions (a question and an instruction to write in comments) but misrepresents both events' timing and content\u2014the anchor is mischaracterized and the target is placed much earlier and said to follow immediately, contradicting the correct timestamps and relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the right finishes stating that he and Mr. Hassan will review and answer questions, when does the man on the left say 'Definitely, definitely'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 546.5,
        "end": 547.5
      },
      "pred_interval": {
        "start": 540.2,
        "end": 541.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.2999999999999545,
        "end": 5.899999999999977,
        "average": 6.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.4383561643835616,
        "text_similarity": 0.661371648311615,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relation (E2 follows immediately after E1) but the absolute timestamps are substantially off (~6.3s earlier) and the duration of E2 differs, so it is not temporally accurate to the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says her first job interview was through LinkedIn, when does she mention the job tab on LinkedIn?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 112.525,
        "end": 116.189
      },
      "pred_interval": {
        "start": 72.5,
        "end": 76.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.025000000000006,
        "end": 39.68899999999999,
        "average": 39.857
      },
      "rationale_metrics": {
        "rouge_l": 0.17948717948717952,
        "text_similarity": 0.7098908424377441,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor and the 'after' relation, but it gives substantially incorrect timestamps for the target event (72.5\u201376.5s vs. the true 112.525\u2013116.189s), so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is demonstrating on her phone how to search for a job on LinkedIn, when does she verbally instruct to write 'architect or interior designer jobs'?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 146.179,
        "end": 148.622
      },
      "pred_interval": {
        "start": 145.0,
        "end": 149.0
      },
      "iou": 0.610750000000003,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.179000000000002,
        "end": 0.3779999999999859,
        "average": 0.778499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.7651227712631226,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor as the phone demonstration, the target speech as occurring during that demo, and the temporal relation; the predicted timestamps slightly differ from the reference by about \u00b11s but still encompass the correct interval."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'keywords plays a very important role', when does her finger scroll down the list of job posts on the phone screen?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 170.3
      },
      "pred_interval": {
        "start": 167.0,
        "end": 172.0
      },
      "iou": 0.060000000000002274,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 1.6999999999999886,
        "average": 2.3499999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.7251044511795044,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly identifies the anchor but misstates its end by ~0.4s and, critically, places the scroll much earlier (starts at 167.0s vs 170.0s) and much longer (ends 172.0s vs 170.3s), adding an unverified selection detail; timing and extra details therefore do not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions finding multiple tabs when searching for architect jobs, when does she instruct the viewer to go to the 'posts' tab?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.4,
        "end": 158.9
      },
      "pred_interval": {
        "start": 158.0,
        "end": 160.5
      },
      "iou": 0.29032258064516364,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5999999999999943,
        "end": 1.5999999999999943,
        "average": 1.0999999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.4700363278388977,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the instruction and quoted phrase and aligns semantically with the reference; timestamps are slightly offset (predicted E2 starts ~0.6s later and ends ~1.6s later than the reference) but this is a minor discrepancy."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises cross-checking company details before sending a CV, when does she suggest calling the company's mentioned number for verification?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 383.586,
        "end": 388.331
      },
      "pred_interval": {
        "start": 216.5,
        "end": 223.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 167.086,
        "end": 164.83100000000002,
        "average": 165.95850000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.15999999999999998,
        "text_similarity": 0.42331498861312866,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the same suggestion content (calling to check hiring) but the temporal boundaries are substantially different from the ground-truth anchor/target times, so the localization is incorrect despite semantic match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says she went to the company's profile, when does she mention finding the number there?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 381.5,
        "end": 383.466
      },
      "pred_interval": {
        "start": 385.2,
        "end": 387.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.6999999999999886,
        "end": 3.6340000000000146,
        "average": 3.6670000000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.7192609906196594,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (immediately after/once_finished) but misstates both event timestamps by several seconds (shifting E1 and E2 later), contradicting the provided ground-truth timings and thus failing on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the company asked her to share her CV via email, when does she say she actually shared it?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 401.369,
        "end": 404.314
      },
      "pred_interval": {
        "start": 405.2,
        "end": 408.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.8309999999999604,
        "end": 4.185999999999979,
        "average": 4.0084999999999695
      },
      "rationale_metrics": {
        "rouge_l": 0.58,
        "text_similarity": 0.7884365320205688,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the target sentence content and that it follows the anchor, but it gives substantially different timestamps and a different relation label ('immediately after' vs 'once_finished'), so the timing/factual details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says she gave the company a call to check on hiring, when does she confirm that she did call and they were looking for a candidate?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 388.572,
        "end": 395.942
      },
      "pred_interval": {
        "start": 392.1,
        "end": 400.5
      },
      "iou": 0.3220992622401061,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.52800000000002,
        "end": 4.557999999999993,
        "average": 4.043000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3551401869158878,
        "text_similarity": 0.7556940317153931,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the key semantic content (she called and confirmed they were hiring) but the timestamps are shifted later and the target segment is extended beyond the reference; the relation label is paraphrased ('immediately after' vs 'once_finished'), so there are partial temporal mismatches."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that not getting a job at a big company is not the case, when does she start introducing strategies for getting a job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 191.24,
        "end": 195.36
      },
      "pred_interval": {
        "start": 193.0,
        "end": 200.0
      },
      "iou": 0.26940639269406574,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.759999999999991,
        "end": 4.639999999999986,
        "average": 3.1999999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.4918032786885246,
        "text_similarity": 0.7201724052429199,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the event order and that the strategies begin immediately after the anchor, but the provided timestamps are noticeably shifted (anchor ~5.8s late, target start ~1.8s late and end ~4.6s late) and thus not fully aligned with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the 'BEFORE INTERVIEW' text appears on screen, when does the 'DURING INTERVIEW (ONSITE & OFFSITE)' text appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.12,
        "end": 209.32
      },
      "pred_interval": {
        "start": 206.0,
        "end": 210.0
      },
      "iou": 0.33603238866396706,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.8799999999999955,
        "end": 0.6800000000000068,
        "average": 3.280000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.276595744680851,
        "text_similarity": 0.7708905935287476,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct next phase but the timestamps are substantially wrong: the anchor is placed ~5s late (203.0s vs 198.0s) and the target start is ~6s late (206.0s vs 200.12s), with only the end time roughly similar; thus it does not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that you should get ready technically, when does she start listing examples of how to get ready technically?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 338.44,
        "end": 347.64
      },
      "pred_interval": {
        "start": 340.0,
        "end": 348.0
      },
      "iou": 0.7991631799163164,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.5600000000000023,
        "end": 0.36000000000001364,
        "average": 0.960000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.1388888888888889,
        "text_similarity": 0.5665974020957947,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures that E2 immediately follows E1 and covers the full list, with only minor timing offsets (~1.5s later start and ~0.36s longer end) compared to the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says that the next topic is things to do during the interview, when does she state the first thing to do?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 405.02,
        "end": 415.34
      },
      "pred_interval": {
        "start": 411.0,
        "end": 415.5
      },
      "iou": 0.41412213740457704,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.980000000000018,
        "end": 0.160000000000025,
        "average": 3.0700000000000216
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.4954519271850586,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer roughly matches the end of E2 but significantly misplaces the anchor and start times (E1 ends at 411.0s vs 404.02s, E2 starts at 411.0s vs 405.02s) and omits the noted brief pause, so timing alignment is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker tells the audience to ask interviewers questions about themselves during the chat, when does she start giving an example related to infrastructure as code?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 470.16,
        "end": 495.68
      },
      "pred_interval": {
        "start": 468.0,
        "end": 492.0
      },
      "iou": 0.7890173410404613,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 2.160000000000025,
        "end": 3.680000000000007,
        "average": 2.920000000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.46320226788520813,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction gets the long example (E2) roughly correct (start/end within a few seconds of the reference), but it misplaces the anchor (E1) significantly (~416s vs 450.8\u2013455.9s), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises to 'get to know your interviewer, build a relationship with the person', when does she state that doing so leaves an impression?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.0,
        "end": 532.52
      },
      "pred_interval": {
        "start": 533.0,
        "end": 537.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 4.480000000000018,
        "average": 4.240000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.28260869565217395,
        "text_similarity": 0.7027021646499634,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct events and the temporal relation (following/once finished), but the provided timestamps are notably shifted later and E2 is overextended compared to the ground truth, so the localization is imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the point 'Be yourself', when does she explain the consequences of trying to be fake?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 544.33,
        "end": 584.4
      },
      "pred_interval": {
        "start": 543.0,
        "end": 554.0
      },
      "iou": 0.23357487922705228,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.330000000000041,
        "end": 30.399999999999977,
        "average": 15.865000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.20833333333333331,
        "text_similarity": 0.699387788772583,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction roughly identifies the intro and the explanation start, but timings are misaligned (E1 shifted several seconds, E2 end truncated by ~30s) and the relation is labeled 'during' instead of the correct 'after', omitting a large portion of the explained content."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that interviews are typically 'peer programming sessions', when does she give an example of reducing latency from 100 milliseconds to 8 milliseconds?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 668.88,
        "end": 677.08
      },
      "pred_interval": {
        "start": 671.0,
        "end": 677.0
      },
      "iou": 0.7317073170731667,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 2.1200000000000045,
        "end": 0.08000000000004093,
        "average": 1.1000000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.684187650680542,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted timestamps fall within or very close to the reference intervals (E1 at 580.0s is inside 575.07\u2013581.09; E2 671.0\u2013677.0s aligns with 668.88\u2013677.08s) and the temporal relation 'after' is correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes advising to 'add numbers as well if they are numbers to add', when does she say that 'there are sometimes that there won't be numbers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 704.38,
        "end": 708.06
      },
      "pred_interval": {
        "start": 710.2,
        "end": 712.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.82000000000005,
        "end": 4.740000000000009,
        "average": 5.28000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.17821782178217818,
        "text_similarity": 0.5947593450546265,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the target phrase and that it immediately follows the anchor, but the timestamps are substantially off (reference anchor ~703.38s and target 704.38\u2013708.06s vs predicted 710.2s and 710.2\u2013712.8s), so it is factually inaccurate on timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions an impact on '10 different teams', when does she talk about '10 million users or 10 million customers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 723.39,
        "end": 725.25
      },
      "pred_interval": {
        "start": 726.8,
        "end": 729.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.409999999999968,
        "end": 4.5499999999999545,
        "average": 3.9799999999999613
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.6819709539413452,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target occurs after it, but the predicted time window for the '10 million users or 10 million customers' phrase is several seconds later than the ground truth and includes an incorrect video time mapping."
      }
    },
    {
      "question_id": "003",
      "question": "When does the next numbered text overlay appear after the overlay '6. Mention past achievements you can comfortably talk about.'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 796.43,
        "end": 800.39
      },
      "pred_interval": {
        "start": 798.8,
        "end": 807.5
      },
      "iou": 0.14363143631436537,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3700000000000045,
        "end": 7.110000000000014,
        "average": 4.740000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.6733852624893188,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction reasonably matches the start of overlay '7' (798.8s vs 796.43s) but misstates the time for overlay '6' (755s vs 795.23s), overestimates the end of '7' (807.5s vs 800.39s), and provides an inconsistent wall-clock mapping, so several key details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions doing what you should be doing after the interview, when does the text overlay 'Sit back and focus on yourself.' appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 899.5,
        "end": 901.9
      },
      "pred_interval": {
        "start": 900.0,
        "end": 904.0
      },
      "iou": 0.42222222222221717,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 2.1000000000000227,
        "average": 1.3000000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.4649416208267212,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the overlay text and approximate start time (~900s) but misstates the speaker timing, wrongly says the overlay appears as she speaks (should be after), gives an inaccurate end time (~904s vs 901.9s), and adds an unsupported location detail."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that it's either you get an offer or a rejection, when does she say 'If you get an offer, congratulations to you'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 917.6,
        "end": 919.6
      },
      "pred_interval": {
        "start": 918.0,
        "end": 920.5
      },
      "iou": 0.5517241379310467,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.39999999999997726,
        "end": 0.8999999999999773,
        "average": 0.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.34146341463414637,
        "text_similarity": 0.6646823287010193,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly states the line occurs after the rejection and gives start/end times very close to the reference (918s vs 917.6s start; 920.5s vs 919.6s end); minor timing offsets (and an extra reassuring phrase) are small deviations but do not change the answer."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's invitation for help on social media, when do her social media handles appear on screen?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 983.0,
        "end": 987.0
      },
      "pred_interval": {
        "start": 953.0,
        "end": 957.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 30.0,
        "average": 30.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953489,
        "text_similarity": 0.5411081314086914,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the handles appear during the invitation and notes their on/off presence, but the timestamps are significantly incorrect (off by ~31 seconds), and it adds platform details not specified in the ground truth; these factual timing errors warrant a low score."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of '3 Things to know when INTERVIEWING', when does she start talking about people being chosen relative to other candidates?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 32.8,
        "end": 38.0
      },
      "pred_interval": {
        "start": 34.0,
        "end": 39.0
      },
      "iou": 0.6451612903225803,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.2000000000000028,
        "end": 1.0,
        "average": 1.1000000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.34408602150537637,
        "text_similarity": 0.7372540831565857,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction preserves the correct relation ('after') and the content of both events; timestamps are slightly offset (E1 ~+2s start/+1s end, E2 ~+1.2s start/+1s end) but differences are minor and do not change the semantics."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker talks about false positives and negatives in terms of accuracy, when does she explain that companies care about not hiring bad talents?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 103.0,
        "end": 106.0
      },
      "pred_interval": {
        "start": 144.0,
        "end": 154.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 48.0,
        "average": 44.5
      },
      "rationale_metrics": {
        "rouge_l": 0.33999999999999997,
        "text_similarity": 0.7746426463127136,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same relation and wording (that companies care about not hiring bad talents) but gives substantially incorrect timestamps compared to the reference, so it fails on the key factual timing element."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying what you should be doing after the interview, when does she mention that the video is about getting your dream job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1019.1500000000001
      },
      "gt_interval": {
        "start": 890.5,
        "end": 894.9
      },
      "pred_interval": {
        "start": 891.0,
        "end": 897.5
      },
      "iou": 0.5571428571428539,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 2.6000000000000227,
        "average": 1.5500000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.18749999999999997,
        "text_similarity": 0.7081525325775146,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly locates the anchor and that the target immediately follows, with a start time close to the ground truth, but the predicted end time (897.5s) is significantly later than the correct 894.9s and thus adds inaccurate/extra timing detail."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman mentions the STAR method, when does she explain what it stands for?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 159.92,
        "end": 164.1
      },
      "pred_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.080000000000013,
        "end": 7.900000000000006,
        "average": 8.490000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.8284517526626587,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the events and their semantic content (mention then explanation of STAR), but the timestamps are significantly offset from the ground truth and the temporal relation labeling differs, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the man introduces the topic of what not to say, when does the woman advise against bad-mouthing former employers?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 185.4,
        "end": 191.0
      },
      "pred_interval": {
        "start": 186.0,
        "end": 190.0
      },
      "iou": 0.714285714285715,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5999999999999943,
        "end": 1.0,
        "average": 0.7999999999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.26804123711340205,
        "text_similarity": 0.6372567415237427,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the general temporal relation and approximate E2 timing but misstates E1 (180.0s vs 174.5s), misattributes the speaker (says the man rather than the woman), and omits the 'Big red flag' phrasing and exact E2 boundary\u2014significant factual discrepancies."
      }
    },
    {
      "question_id": "003",
      "question": "After the man states that preparation ties back to building skills, when does he explain what deep research involves?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 247.12,
        "end": 252.48
      },
      "pred_interval": {
        "start": 251.0,
        "end": 263.0
      },
      "iou": 0.09319899244332432,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.8799999999999955,
        "end": 10.52000000000001,
        "average": 7.200000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.25210084033613445,
        "text_similarity": 0.6615713834762573,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the semantic content of the deep-research explanation and that it occurs after the anchor, but the timestamps are substantially inaccurate (both E1 and E2 are mislocated and E2 end time is off), and it introduces speculative/misleading notes about phrasing and timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the man sips his coffee, when does he say 'it builds skills'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 342.0,
        "end": 343.0
      },
      "pred_interval": {
        "start": 343.4,
        "end": 344.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3999999999999773,
        "end": 1.3000000000000114,
        "average": 1.3499999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.3548387096774193,
        "text_similarity": 0.6253535747528076,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the same events and preserves the 'after' relation, but it misstates the E2 time (about 1.4s later than the reference) and omits the E1 end time, so it does not fully match the ground truth timings."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying 'every single time', when does he start saying 'You show up differently'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 347.5,
        "end": 348.9
      },
      "pred_interval": {
        "start": 348.0,
        "end": 349.2
      },
      "iou": 0.5294117647058725,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 0.30000000000001137,
        "average": 0.4000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.5546869039535522,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction gives very similar start/end times (within ~0.5s) and correctly identifies E2 timing, but it shifts both timestamps slightly later and does not explicitly state the immediate 'once_finished' relation, so there's a minor timing/relation discrepancy."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces the 'deep dive' into interview preparation, when does she mention covering 'surprising insights and steps'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 26.0,
        "end": 29.5
      },
      "pred_interval": {
        "start": 25.5,
        "end": 30.5
      },
      "iou": 0.7,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5,
        "end": 1.0,
        "average": 0.75
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.6463881134986877,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction captures the correct events and order and only differs by small timing offsets (intro ~1s late; the insight segment starts ~0.5s earlier and ends ~1s later), so meaning is preserved with minor timestamp discrepancies."
      }
    },
    {
      "question_id": "002",
      "question": "During the man's explanation of how dressing formally applies to online interviews, when does he mention the concept of 'enclothed cognition'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 77.0,
        "end": 80.0
      },
      "pred_interval": {
        "start": 116.5,
        "end": 120.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.5,
        "end": 40.5,
        "average": 40.0
      },
      "rationale_metrics": {
        "rouge_l": 0.1509433962264151,
        "text_similarity": 0.4934098720550537,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the concept ('enclothed cognition') but gives a substantially incorrect timestamp (116.5\u2013120.5s vs the ground-truth 77.0\u201380.0s), so it fails to match the reference timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying \"makes you stand out\", when does he say \"absolutely\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 335.4,
        "end": 336.0
      },
      "pred_interval": {
        "start": 335.5,
        "end": 336.5
      },
      "iou": 0.45454545454544515,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.10000000000002274,
        "end": 0.5,
        "average": 0.30000000000001137
      },
      "rationale_metrics": {
        "rouge_l": 0.32911392405063294,
        "text_similarity": 0.6064413785934448,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and the 'immediately after' relation, and the E2 start time is nearly identical; however, the E1 end time (335.4s vs 334.7s) and E2 end time (336.5s vs 336.0s) have small but notable timing inaccuracies."
      }
    },
    {
      "question_id": "002",
      "question": "After the man finishes sipping from his white cup, when does he say \"it builds skills\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 343.0,
        "end": 343.6
      },
      "pred_interval": {
        "start": 344.2,
        "end": 345.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1999999999999886,
        "end": 1.7999999999999545,
        "average": 1.4999999999999716
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.6543788909912109,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies E1 and the 'after' relation and E1 timing is close to the ground truth, but it misplaces E2 by over a second and invents intervening phrases not present in the reference, so the answer is factually inaccurate/incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions his parents advised him to always go on interviews, when does he explicitly state that interviews are practice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.064,
        "end": 43.554
      },
      "pred_interval": {
        "start": 40.0,
        "end": 43.5
      },
      "iou": 0.7795100222717146,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.9359999999999999,
        "end": 0.054000000000002046,
        "average": 0.495000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.597830057144165,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'it's practice' segment and its end time and preserves the 'after' relation, but it mislocates E1 substantially (00:05-00:15 vs ground-truth ~22.24s), so the anchor timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising not to overstate qualifications, when does he tell viewers to know their worth for negotiation?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.944,
        "end": 117.861
      },
      "pred_interval": {
        "start": 106.0,
        "end": 113.0
      },
      "iou": 0.58739615675086,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.055999999999997385,
        "end": 4.861000000000004,
        "average": 2.458500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2666666666666666,
        "text_similarity": 0.6840037107467651,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer captures the sequence and paraphrases the advice about knowing your worth, but the timestamps are substantially inaccurate versus the ground truth, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions strengths and weaknesses, when does he mention why you want this particular job?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 179.8,
        "end": 182.4
      },
      "pred_interval": {
        "start": 180.0,
        "end": 183.0
      },
      "iou": 0.7500000000000044,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.19999999999998863,
        "end": 0.5999999999999943,
        "average": 0.3999999999999915
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7565741539001465,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target intervals and their 'after' relationship; the timestamps fall within or very close to the reference ranges and the content matches (aside from a minor wording typo and a 0.6s end-time difference)."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's discussion about having coaches being important for self-improvement, when does he mention Roger Wakefield?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 215.9,
        "end": 218.0
      },
      "pred_interval": {
        "start": 217.0,
        "end": 219.0
      },
      "iou": 0.3225806451612909,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0999999999999943,
        "end": 1.0,
        "average": 1.0499999999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7796421051025391,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor start time and locates the target within the correct interval (centered at ~217s and citing the Roger Wakefield quote); minor issues are the missing anchor end time and a 1s discrepancy on the target end time."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes elaborating on researching a company and asking about their projects, when does he transition to discussing common questions about training and education?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.3,
        "end": 314.7
      },
      "pred_interval": {
        "start": 313.0,
        "end": 316.0
      },
      "iou": 0.1588785046728963,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.699999999999989,
        "end": 1.3000000000000114,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.20224719101123598,
        "text_similarity": 0.7136366367340088,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct target topic but the timestamps are notably off and the relation between anchor and target is mischaracterized (anchor end and target start differ from the reference), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what goals need to be achieved for more money, when does he say that the company will pay that money?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.5,
        "end": 340.9
      },
      "pred_interval": {
        "start": 344.0,
        "end": 346.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.5,
        "end": 5.100000000000023,
        "average": 4.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.30107526881720426,
        "text_similarity": 0.8093631863594055,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but both event time intervals are significantly shifted and do not overlap the ground-truth spans, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions going for a union job, when does he describe understanding the process of safety and hazard assessments?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 381.5
      },
      "pred_interval": {
        "start": 379.0,
        "end": 386.0
      },
      "iou": 0.2118644067796608,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.800000000000011,
        "end": 4.5,
        "average": 4.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.28260869565217395,
        "text_similarity": 0.8364771008491516,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the content and that the target occurs after the anchor, but the timestamps are substantially shifted (~4\u20135s) and the predicted anchor time falls within the correct target window, so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about expressing passion for the job, when does he advise being a student of construction?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 531.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 534.0,
        "end": 541.0
      },
      "iou": 0.55,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 1.5,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2696629213483146,
        "text_similarity": 0.6686153411865234,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies both events and their content, preserves the correct order, and the timestamps are consistently offset (absolute\u2192relative), so the relative alignment is accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks about the key responsibilities of a construction worker, when does he list these responsibilities?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 588.0,
        "end": 610.0
      },
      "pred_interval": {
        "start": 589.0,
        "end": 616.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 6.0,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.19047619047619052,
        "text_similarity": 0.5250974893569946,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the ask vs. list events and even the listed responsibilities, but the timestamps are significantly different from the ground truth and the immediate-following temporal relation is not clearly preserved, so the temporal information is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks about dealing with an unhappy supervisor, when does he advise owning up to your mistakes?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 705.0,
        "end": 711.0
      },
      "pred_interval": {
        "start": 708.0,
        "end": 711.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 0.0,
        "average": 1.5
      },
      "rationale_metrics": {
        "rouge_l": 0.29729729729729726,
        "text_similarity": 0.5970001220703125,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly identifies the speaker reading the question and then advising to own up to mistakes, preserving the required temporal order (E2 after E1); differences are only in absolute timestamps, which the prompt treats as relative."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that the foreman will then explain to the general contractor, when does he start describing a similar dynamic between a journeyman and an apprentice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 744.67,
        "end": 790.86
      },
      "pred_interval": {
        "start": 749.16,
        "end": 812.2
      },
      "iou": 0.6175033318525098,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.490000000000009,
        "end": 21.340000000000032,
        "average": 12.91500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820515,
        "text_similarity": 0.4429772198200226,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes that the journeyman\u2013apprentice description follows the foreman explanation, but the start and end timestamps are substantially off (predicted ~749.16\u2013812.2s vs correct 743.38 then 744.67\u2013790.86s) and it adds unverified phrasing, so the timing and some details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes reading the question about developing skill sets, when does he begin explaining his method of doing so?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 903.0
      },
      "pred_interval": {
        "start": 894.0,
        "end": 910.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 7.0,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.30588235294117644,
        "text_similarity": 0.6538881063461304,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that E2 is the speaker's explanation of his method, but it misaligns the timings\u2014E2 is said to begin at 894.0s and end at 910.0s versus the reference 892.0\u2013903.0s\u2014thus failing the precise temporal relation (should start immediately at 892.0s) and overstating the duration."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining due diligence regarding unsafe acts, when does he start discussing strengths and weaknesses for an interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 960.4,
        "end": 976.0
      },
      "pred_interval": {
        "start": 966.0,
        "end": 982.0
      },
      "iou": 0.46296296296296247,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.600000000000023,
        "end": 6.0,
        "average": 5.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.743806779384613,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the interview strengths/weaknesses discussion follows the due-diligence remark and matches the relation and content, but the timestamps are substantially incorrect (E1 end and E2 start/end differ significantly from the reference), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker highlights the 'Practice makes perfect' section, when does he advise being cool, collected, and confident for the upcoming interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1113.03,
        "end": 1118.08
      },
      "pred_interval": {
        "start": 1107.0,
        "end": 1113.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.029999999999973,
        "end": 5.079999999999927,
        "average": 5.55499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481927,
        "text_similarity": 0.8190336227416992,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the quoted phrase but the timestamps are substantially incorrect: E1 is placed at 1059s instead of 1110.3\u20131112.2, and E2 is shifted earlier and ends too soon (1107\u20131113s vs 1113.03\u20131118.08), so the temporal alignment is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker reads 'What is your main weakness?', when does he advise turning a weakness into a positive?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1213.307,
        "end": 1216.671
      },
      "pred_interval": {
        "start": 1247.0,
        "end": 1250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.692999999999984,
        "end": 33.32899999999995,
        "average": 33.51099999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481927,
        "text_similarity": 0.832026481628418,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the events and their ordering (anchor question followed by advice to turn a weakness into a positive) and even quotes the advice, but the provided timestamps do not match the ground-truth timings and thus are significantly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'dress to impress', when does he read the sentence 'What you wear to an interview is very important'?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1258.4,
        "end": 1261.5
      },
      "pred_interval": {
        "start": 1260.5,
        "end": 1264.0
      },
      "iou": 0.17857142857143146,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.099999999999909,
        "end": 2.5,
        "average": 2.2999999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.47191011235955055,
        "text_similarity": 0.8745438456535339,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the order (target after anchor) and identifies both events, but the timestamps are shifted later by ~2\u20133 seconds (and E1 lacks the precise start/end interval), and the prediction adds minor unfounded detail (stuttering) and a longer E2 duration."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker reads 'dress professionally, not casually', when does he start reading the specific advice about what women should not wear?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1272.8,
        "end": 1277.3
      },
      "pred_interval": {
        "start": 1275.5,
        "end": 1280.0
      },
      "iou": 0.24999999999999212,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7000000000000455,
        "end": 2.7000000000000455,
        "average": 2.7000000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.8193991184234619,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly preserves the order (target after anchor) and the content of both segments, but the reported time spans are shifted a few seconds later than the reference and it adds an unverified intermediate sentence about confidence."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes reading the dressing advice for women, when does he read the dressing advice for men?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1277.7,
        "end": 1282.0
      },
      "pred_interval": {
        "start": 1280.0,
        "end": 1284.5
      },
      "iou": 0.2941176470588255,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2999999999999545,
        "end": 2.5,
        "average": 2.3999999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5765975713729858,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly indicates the men's advice immediately follows the women's and preserves the sequence, but the reported timestamps are off by about 2.5\u20132.7 seconds compared to the ground truth, so the timing is not fully accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker welcomes to the Job Search Preparation Resume Writing Workshop, when does she introduce herself as Cynthia White?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 9.61,
        "end": 15.95
      },
      "pred_interval": {
        "start": 9.68,
        "end": 16.0
      },
      "iou": 0.9812206572769951,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.07000000000000028,
        "end": 0.05000000000000071,
        "average": 0.0600000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117643,
        "text_similarity": 0.5621313452720642,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer accurately matches the reference timing and relation (E2 begins immediately after E1) with only trivial rounding differences in the timestamps and no factual contradictions or omissions."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that employers spend about 60 to 90 seconds reviewing a resume, when does she explain the purpose of a cover letter?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 93.56,
        "end": 100.47
      },
      "pred_interval": {
        "start": 133.8,
        "end": 140.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.24000000000001,
        "end": 39.72999999999999,
        "average": 39.985
      },
      "rationale_metrics": {
        "rouge_l": 0.3296703296703296,
        "text_similarity": 0.7536012530326843,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies the same E1 and E2 time spans and wording (59\u201371s and ~1:33\u20131:40), captures the cover letter purpose quote, and correctly states the temporal relation as 'after.'"
      }
    },
    {
      "question_id": "001",
      "question": "During the initial 'You will learn' slide, when does the speaker mention how to format an electronic resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 172.9
      },
      "pred_interval": {
        "start": 171.5,
        "end": 173.8
      },
      "iou": 0.3684210526315793,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5,
        "end": 0.9000000000000057,
        "average": 1.2000000000000028
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307692,
        "text_similarity": 0.7537327408790588,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction roughly locates the mention near 171\u2013172s, but it misstates the anchor start (150.0s vs 154.0s) and extends the target to 173.8s, which goes past the anchor end (172.9s), so it fails to correctly place the event wholly during the slide."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'Design a resume that is simple and clean', when is the next checklist item, 'Limit the resume to two pages maximum', mentioned?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 233.0,
        "end": 235.8
      },
      "pred_interval": {
        "start": 234.4,
        "end": 237.4
      },
      "iou": 0.31818181818181906,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4000000000000057,
        "end": 1.5999999999999943,
        "average": 1.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36842105263157887,
        "text_similarity": 0.621076226234436,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the next checklist item and its timing, but the reported start/end times are slightly later than the ground truth (anchor off by ~1.4s and target start/end about 1.4\u20131.6s later), so it's mostly accurate but not exact."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing the tip 'Review the job announcement', when does she begin discussing 'Consider the employer's perspective'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 274.9,
        "end": 307.2
      },
      "pred_interval": {
        "start": 277.3,
        "end": 279.6
      },
      "iou": 0.0712074303405576,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.400000000000034,
        "end": 27.599999999999966,
        "average": 15.0
      },
      "rationale_metrics": {
        "rouge_l": 0.28,
        "text_similarity": 0.5396600961685181,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates key timestamps and duration: it places the end of E1 ~1.9s later and the start of E2 ~2.4s later than the reference, and drastically shortens E2 (ending at 279.6s vs 307.2s), so it does not match the correct, seamless transition."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that 'one size does not fit all', when does she finish explaining the need to edit a basic resume for each job?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.18,
        "end": 330.25
      },
      "pred_interval": {
        "start": 350.0,
        "end": 358.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.819999999999993,
        "end": 27.75,
        "average": 23.784999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2718446601941748,
        "text_similarity": 0.6193606853485107,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and the segment ordering, but the provided timestamps are substantially incorrect (about 20s later and with very different durations) versus the reference, so it fails on factual temporal accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing the chronological resume, when does she start describing the skills-based resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.0,
        "end": 510.0
      },
      "pred_interval": {
        "start": 472.0,
        "end": 478.5
      },
      "iou": 0.14473684210526316,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 31.5,
        "average": 16.25
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.7466735243797302,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the skills-based segment follows the chronological resume, but it misstates several key timestamps (E1 off by ~1s, E2 start ~1s early and E2 end much earlier than the reference 510.0s), and thus fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that they need to be careful about concealing information, when does the 'Style: Combination Resume - Sample #1' title appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.5,
        "end": 519.3
      },
      "pred_interval": {
        "start": 515.0,
        "end": 540.5
      },
      "iou": 0.14901960784313548,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 21.200000000000045,
        "average": 10.850000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272727,
        "text_similarity": 0.5102207660675049,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction roughly matches the events but has important timing and relation errors: E1/E2 times are off by ~0.7s/0.5s and it states the title appears simultaneously with the speaker finishing rather than shortly after; it also adds an end time for the slide not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the 'Style: Combination Resume - Sample #2' title appears, when does the speaker start describing the benefits for job seekers like those changing industries or reentering the workforce?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 542.7,
        "end": 556.7
      },
      "pred_interval": {
        "start": 545.2,
        "end": 560.0
      },
      "iou": 0.6647398843930653,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5,
        "end": 3.2999999999999545,
        "average": 2.8999999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.5393069386482239,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the slide, the speaker's explanatory segment and the 'after' relation, and matches the content (benefits for reentering workers/veterans); however the timestamps are slightly offset from the reference (start/end times differ by a few seconds)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes summarizing James' scenario, when does she state the recommended resume type for him?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 667.3,
        "end": 674.9
      },
      "pred_interval": {
        "start": 670.4,
        "end": 674.1
      },
      "iou": 0.4868421052631624,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.1000000000000227,
        "end": 0.7999999999999545,
        "average": 1.9499999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.6307096481323242,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor, the recommendation, and the 'once_finished' relation, with the core statement end time closely matching; however, the predicted start times are off by ~2\u20133 seconds from the reference, so there are minor temporal inaccuracies."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes discussing how to choose the appropriate resume style, when does she start talking about the actual resume and its contents?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 877.86,
        "end": 884.43
      },
      "pred_interval": {
        "start": 882.0,
        "end": 889.0
      },
      "iou": 0.21813285457809273,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.139999999999986,
        "end": 4.57000000000005,
        "average": 4.355000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.13888888888888887,
        "text_similarity": 0.5314372777938843,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction's timings are several seconds later than the ground truth (predicts 882.0\u2013889.0s vs correct 877.86\u2013884.43s) and claims a specific quoted utterance, contradicting the reference that the next segment starts immediately at 877.86s; thus it is factually inaccurate though roughly in the same region."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'name and contact information' as a basic category for a resume, when does she list the 'skills and accomplishments' category?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.09,
        "end": 922.44
      },
      "pred_interval": {
        "start": 925.0,
        "end": 927.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.909999999999968,
        "end": 4.559999999999945,
        "average": 4.734999999999957
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666669,
        "text_similarity": 0.5626096725463867,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that 'skills and accomplishments' directly follows 'name and contact information', but the provided timestamps are several seconds later and do not match the reference intervals, making the answer factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions that many employers are beginning to use emails for communication with job candidates, when does she advise opening a new email address?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1011.0,
        "end": 1024.0
      },
      "pred_interval": {
        "start": 1015.0,
        "end": 1029.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 5.0,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4130434782608696,
        "text_similarity": 0.6540670394897461,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the advice phrase and its content, but the timestamps are imprecise (E1 timing is vague and E2 start/end are shifted several seconds later than the reference), so it is largely correct but not exact."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Skills & Accomplishments' section, when does she suggest using mynextmove.org?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1121.7,
        "end": 1126.15
      },
      "pred_interval": {
        "start": 1124.0,
        "end": 1126.0
      },
      "iou": 0.4494382022471864,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2999999999999545,
        "end": 0.15000000000009095,
        "average": 1.2250000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.3125,
        "text_similarity": 0.7073133587837219,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the relative order and closely matches the E2 timing, but the anchor E1 timestamp (1053.0s) differs substantially from the ground-truth anchor interval, so the anchor timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes mentioning the onetonline.org website, when does the 'New Graduate' text appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1199.0,
        "end": 1199.5
      },
      "pred_interval": {
        "start": 1156.0,
        "end": 1158.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.0,
        "end": 41.5,
        "average": 42.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.8105430603027344,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal order right (E2 after E1) but the timestamps are significantly incorrect (predicted E1 at ~1130s vs 1172s actual, predicted E2 at 1156s vs 1199\u20131199.5s actual), so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "003",
      "question": "When is the next time a new category of individual appears on screen after the 'New Graduate' text is displayed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1202.0,
        "end": 1202.5
      },
      "pred_interval": {
        "start": 1204.0,
        "end": 1206.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 3.5,
        "average": 2.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.8342585563659668,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the next category ('Formerly Incarcerated') but gives incorrect timing for E1 (1156.0s vs 1199.0s) and a notably wrong time for E2 (1204.0s vs ~1202.0\u20131202.5s), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about 'fragments in your work history', when does she introduce 'Summary Statements'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.3,
        "end": 1283.6
      },
      "pred_interval": {
        "start": 1279.0,
        "end": 1283.0
      },
      "iou": 0.754716981132082,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.7000000000000455,
        "end": 0.599999999999909,
        "average": 0.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962022,
        "text_similarity": 0.6595669984817505,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and gives approximate start/end times matching the reference; only minor timestamp offsets (~0.7\u20131.1s) exist, which do not change the temporal relation or omit key details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states the importance of including summary statements, when does she explain how they clearly communicate qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1341.0,
        "end": 1351.0
      },
      "pred_interval": {
        "start": 1306.0,
        "end": 1313.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.0,
        "end": 38.0,
        "average": 36.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2637362637362637,
        "text_similarity": 0.779759407043457,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly captures the idea that summary statements highlight qualifications, but it misstates both anchor/target timestamps by ~33 seconds and quotes different target wording and end time, so it does not align with the correct answer's timing or exact content."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining how to list relevant skills using bullets, when does the 'Skills/Summary of Skills' section appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1430.0,
        "end": 1431.0
      },
      "pred_interval": {
        "start": 1429.0,
        "end": 1444.0
      },
      "iou": 0.06666666666666667,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 13.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.31460674157303375,
        "text_similarity": 0.6626896262168884,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction is moderately aligned temporally but not precise: E1 is 3s later than the reference and described differently, E2 start is 1s earlier and the prediction omits the 'fully visible by 1431s' detail while adding an extra end time (1444s). Overall it captures the next-section relation but misses key timing and label specifics."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'This is an accomplishment statement', when does the 'Example 4: Accomplishment Statement' text box appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1466.0,
        "end": 1466.5
      },
      "pred_interval": {
        "start": 1466.0,
        "end": 1497.0
      },
      "iou": 0.016129032258064516,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.0,
        "end": 30.5,
        "average": 15.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2682926829268293,
        "text_similarity": 0.7600342035293579,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor timing (\u22481457s) and the E2 start at 1466.0s, but it wrongly extends the E2 end to 1497.0s and omits the correct E2 completion time (1466.5s), adding an inaccurate long-duration claim."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing job duties and contributions, when does she start to explain how to list the most recent job first?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1599.24,
        "end": 1604.0
      },
      "pred_interval": {
        "start": 1601.8,
        "end": 1607.5
      },
      "iou": 0.26634382566586534,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5599999999999454,
        "end": 3.5,
        "average": 3.0299999999999727
      },
      "rationale_metrics": {
        "rouge_l": 0.25806451612903225,
        "text_similarity": 0.6609052419662476,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the transition and content (listing most recent job first) but the provided timestamps are notably shifted later compared to the ground truth (anchor and target start/end times differ by ~2\u20134 seconds), so it does not accurately align with the reference timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the yellow hexagonal graphics for 'Education/Training' fully appear on the screen, when does the speaker begin to list the types of earned qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1622.7,
        "end": 1628.27
      },
      "pred_interval": {
        "start": 1625.5,
        "end": 1631.2
      },
      "iou": 0.32588235294117435,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7999999999999545,
        "end": 2.9300000000000637,
        "average": 2.865000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.2888888888888889,
        "text_similarity": 0.5872573256492615,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the same events and their order and provides start/end times, but the timestamps are slightly shifted later by about 1\u20133 seconds compared to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Body' section of the cover letter, when does she provide an example of an introduction?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1798.91,
        "end": 1805.84
      },
      "pred_interval": {
        "start": 1801.5,
        "end": 1811.5
      },
      "iou": 0.34471803018268043,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.589999999999918,
        "end": 5.660000000000082,
        "average": 4.125
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6292639970779419,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target and preserves the 'after' relation with timings close to the reference; minor penalties for slight time offsets, omission of E1's end time, and added/unverified phrasing and an extended end time for E2."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide changes to display 'Sample Cover Letter', when does the speaker begin describing the included elements?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1893.78,
        "end": 1906.58
      },
      "pred_interval": {
        "start": 1899.0,
        "end": 1909.5
      },
      "iou": 0.4821882951653889,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.220000000000027,
        "end": 2.9200000000000728,
        "average": 4.07000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.43373493975903615,
        "text_similarity": 0.7310016751289368,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the slide anchor and that the speaker lists elements, but the reported start time (1899.0s vs 1893.78s) and end time (1909.5s vs 1906.58s) notably disagree with the reference, so the timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes the cover letter tip about checking for errors, when does the slide transition to 'Electronic Resume'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1944.0,
        "end": 1944.99
      },
      "pred_interval": {
        "start": 1945.0,
        "end": 1946.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 1.009999999999991,
        "average": 1.0049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.7284163236618042,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the anchor/target roles but gives noticeably different timestamps (E1 at 1944.5s vs 1943.92s; E2 at 1945.0s vs transition 1944.0\u20131944.99s) and collapses a transition range to a single instant, so it contradicts the correct timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions applications and resumes being submitted online, when does she explain that an electronic resume will contain the same information as a standard resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1969.8,
        "end": 1974.8
      },
      "pred_interval": {
        "start": 1974.5,
        "end": 1980.0
      },
      "iou": 0.029411764705877762,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.7000000000000455,
        "end": 5.2000000000000455,
        "average": 4.9500000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962025,
        "text_similarity": 0.5109213590621948,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (E2 occurs after E1) and the content of E2, but the provided timestamps are noticeably offset from the reference (several seconds later), so timing accuracy is imperfect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that electronic information needs to be in plain text, when does she mention that bolded or underlined text needs to be removed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1980.1,
        "end": 1986.8
      },
      "pred_interval": {
        "start": 1984.0,
        "end": 1992.0
      },
      "iou": 0.2352941176470532,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.900000000000091,
        "end": 5.2000000000000455,
        "average": 4.550000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.24719101123595508,
        "text_similarity": 0.6005603075027466,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relation and content (that bold/underlined formatting must be removed and it follows immediately), but the timestamp boundaries are substantially off (E1 ends at 1980.1s in the reference vs 1984.0s predicted; E2 also shifted ~3.9\u20135.2s later), so timing accuracy is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Electronic Resume Tips' slide appears, when does the speaker advise to limit each line to 65 characters?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2027.3,
        "end": 2029.4
      },
      "pred_interval": {
        "start": 2030.0,
        "end": 2034.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7000000000000455,
        "end": 4.599999999999909,
        "average": 3.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.7380959987640381,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the same slide, the quoted utterance, and the 'after' relation; only the reported timestamps are slightly off by a few seconds (small temporal offsets), so the content is essentially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating their website address, when does she state her contact information is again right there?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2148.0,
        "end": 2152.0
      },
      "pred_interval": {
        "start": 2148.0,
        "end": 2151.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.0,
        "end": 1.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6283491849899292,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction accurately captures the relative ordering and content, with only minor timing discrepancies (~0.5\u20131.0s) from the reference and no contradictory or hallucinated details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes thanking the viewer for taking a look at the workshop, when does the screen transition to the Extension logo?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2160.1,
        "end": 2161.0
      },
      "pred_interval": {
        "start": 2155.0,
        "end": 2161.0
      },
      "iou": 0.15000000000001515,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.099999999999909,
        "end": 0.0,
        "average": 2.5499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.28846153846153844,
        "text_similarity": 0.800788164138794,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly records the anchor ending at 2155.0s but contradicts the key timing of the transition and logo (predicts 2155.0/2156.0s vs. reference 2160.1/2160.8s), so it fails on critical factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Britney' scenario, when does she explain that Britney should highlight skills from volunteering, sports, clubs, and other activities?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 729.63,
        "end": 736.05
      },
      "pred_interval": {
        "start": 724.0,
        "end": 740.0
      },
      "iou": 0.40124999999999744,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.6299999999999955,
        "end": 3.9500000000000455,
        "average": 4.7900000000000205
      },
      "rationale_metrics": {
        "rouge_l": 0.3516483516483517,
        "text_similarity": 0.853708803653717,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both anchor and target events and their 'after' relationship; the predicted interval fully contains the true target interval and the anchor timestamp is only 3s off, so minor timing inaccuracies warrant a small deduction."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes Martha's current job as a teacher's assistant with three years of experience, when does the speaker state that Martha should use a skills-based resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 788.07,
        "end": 792.83
      },
      "pred_interval": {
        "start": 791.0,
        "end": 795.0
      },
      "iou": 0.2640692640692719,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.92999999999995,
        "end": 2.169999999999959,
        "average": 2.5499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.37894736842105264,
        "text_similarity": 0.8229920864105225,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their 'after' relationship, with timings that largely overlap the ground truth; minor discrepancies in exact start/end times (especially the target end) prevent a perfect score."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning contacting them for interactive sessions on various topics, when does she start talking about the website?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2140.17,
        "end": 2150.24
      },
      "pred_interval": {
        "start": 2142.3,
        "end": 2147.5
      },
      "iou": 0.516385302879838,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.130000000000109,
        "end": 2.7399999999997817,
        "average": 2.4349999999999454
      },
      "rationale_metrics": {
        "rouge_l": 0.2278481012658228,
        "text_similarity": 0.5798581838607788,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the website discussion follows the contact mention (same relation) and provides a nearby time window, but the timestamps are off by ~2\u20133 seconds and the predicted end time (2147.5s) omits the actual URL finish at 2150.24s and mislabels the anchor phrase, so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states her name, Cynthia White, when does she thank the viewers for watching the workshop?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2151.26,
        "end": 2155.3
      },
      "pred_interval": {
        "start": 2151.7,
        "end": 2155.0
      },
      "iou": 0.8168316831683692,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.4399999999995998,
        "end": 0.3000000000001819,
        "average": 0.36999999999989086
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.5928927659988403,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the thank-you segment and its relation to the name utterance and gives matching start/end times for E2 (within ~0.5s), but the E1 timestamp is off by about 1.24s from the reference, a minor timing discrepancy."
      }
    },
    {
      "question_id": "001",
      "question": "After Michael Emery introduces himself as the Director of Human Resources at IOM, when does he explain that competency-based interviews are also referred to as behavioral or situational interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 17.414,
        "end": 23.021
      },
      "pred_interval": {
        "start": 18.2,
        "end": 23.5
      },
      "iou": 0.7921459086427871,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.7859999999999978,
        "end": 0.4789999999999992,
        "average": 0.6324999999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.2696629213483146,
        "text_similarity": 0.6021161079406738,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer accurately matches the reference timestamps and ordering (introduction ~5s and explanation ~18\u201323s), preserving all key details with no contradictions or omissions."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'why do we use competency-based interviews?', when does he mention that a competency-based interview has a 0.51 predictor of success?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 91.2,
        "end": 97.969
      },
      "pred_interval": {
        "start": 131.5,
        "end": 137.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.3,
        "end": 39.531000000000006,
        "average": 39.9155
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.725838840007782,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted timestamps align with the reference intervals (question ~75.6\u201379.3s vs 1:16, predictor mention ~91.2\u201397.97s vs 1:31\u20131:37) and correctly indicates the target event follows the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes listing examples of what to prepare for, when does he start giving the second piece of advice about saving the vacancy notice?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.8,
        "end": 158.0
      },
      "pred_interval": {
        "start": 217.0,
        "end": 224.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.19999999999999,
        "end": 66.5,
        "average": 65.35
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.514467716217041,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is essentially incorrect: the timestamps and quoted phrases differ substantially from the ground truth (prediction places events ~60s later and cites different utterances), so it fails to match the reference timing or content."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes explaining why it's important to save the vacancy notice by saying 'it's gone off the web', when does he start talking about the structure of an interview?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.8,
        "end": 209.8
      },
      "pred_interval": {
        "start": 273.0,
        "end": 276.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.19999999999999,
        "end": 66.19999999999999,
        "average": 65.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3469387755102041,
        "text_similarity": 0.5481290817260742,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially different timestamps and a different E1 quote than the ground truth; while it correctly cites the E2 phrase, the start/end times are off by ~66\u2013105s, so it does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the fourth letter for the CAR principles, when does he explain what panels often ask about this fourth letter?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.34,
        "end": 330.41
      },
      "pred_interval": {
        "start": 338.0,
        "end": 347.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.660000000000025,
        "end": 16.589999999999975,
        "average": 12.125
      },
      "rationale_metrics": {
        "rouge_l": 0.3418803418803419,
        "text_similarity": 0.8383640050888062,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the target content (panels asking about learning) and that it follows the anchor, but it misplaces the anchor time by ~30s and has notable discrepancies in event boundaries, so the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks how long a response should be, when does he warn that candidates can be marked down?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 419.29,
        "end": 427.37
      },
      "pred_interval": {
        "start": 395.0,
        "end": 402.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.29000000000002,
        "end": 25.370000000000005,
        "average": 24.830000000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.5217391304347826,
        "text_similarity": 0.8385134935379028,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same utterances and correct temporal relation (target after anchor), but both anchor and target timestamps are significantly incorrect (off by ~22\u201325s), so it omits key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises having a pre-prepared statement for questions to the panel, when does he mention good 'bog standard questions'?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 494.0,
        "end": 501.0
      },
      "pred_interval": {
        "start": 497.0,
        "end": 508.0
      },
      "iou": 0.2857142857142857,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 7.0,
        "average": 5.0
      },
      "rationale_metrics": {
        "rouge_l": 0.43076923076923074,
        "text_similarity": 0.7605466842651367,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies the anchor at 463.0s, captures the target utterance within the correct interval (starts at 497.0s, overlapping 494.0\u2013501.0s) and correctly states the temporal relation as 'after'; minor differences in end time do not change the meaning."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the panel assessing your answers, when does he first equate this to diving in the Olympics?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.94,
        "end": 530.52
      },
      "pred_interval": {
        "start": 532.0,
        "end": 536.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.059999999999945,
        "end": 5.980000000000018,
        "average": 6.019999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.625,
        "text_similarity": 0.9214544296264648,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor content and the 'after' relationship and even captures the target phrase, but it mislocates the target event timing by about 6 seconds (and similarly shifts the end time), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises choosing examples commensurate with the position level, when does he give the specific example of a candidate accused of cheating in high school?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 612.01,
        "end": 619.36
      },
      "pred_interval": {
        "start": 578.0,
        "end": 586.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.00999999999999,
        "end": 33.360000000000014,
        "average": 33.685
      },
      "rationale_metrics": {
        "rouge_l": 0.34862385321100914,
        "text_similarity": 0.8488607406616211,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor content and the 'after' relation, but the temporal localizations are substantially wrong\u2014especially E2 (predicted 578\u2013586s vs correct 612\u2013619s)\u2014so the answer fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing the three different formats for interviews, when does the '3 types of interviews' graphic appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.1,
        "end": 710.8
      },
      "pred_interval": {
        "start": 701.0,
        "end": 711.0
      },
      "iou": 0.8990825688073372,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.8999999999999773,
        "end": 0.20000000000004547,
        "average": 0.5500000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.8008760809898376,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction matches the events and durations closely but has small timing offsets (~1.1s early for E1, ~0.9s late for E2 start, ~0.2s late for E2 end) and slightly contradicts the immediacy of the graphic appearance."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'There's telephone interviews', when does the advice '1. Stand up' appear on screen for telephone interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 717.2,
        "end": 807.3
      },
      "pred_interval": {
        "start": 717.0,
        "end": 731.0
      },
      "iou": 0.15282392026578032,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.20000000000004547,
        "end": 76.29999999999995,
        "average": 38.25
      },
      "rationale_metrics": {
        "rouge_l": 0.30107526881720426,
        "text_similarity": 0.6799434423446655,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly places the visual '1. Stand up' appearing around 717s and the overall 'after' relationship, but it is ambiguous about E1 timing (offers conflicting times) and significantly understates E2's duration (731.0s vs. correct 807.3s), so it is only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises on making sure the background is tidy for video interviews, when does the visual text '3. Appropriate clothing' appear?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 815.0
      },
      "pred_interval": {
        "start": 799.0,
        "end": 815.0
      },
      "iou": 0.9375,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 0.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4266666666666667,
        "text_similarity": 0.6546885371208191,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the correct 'after' relationship and matches the E2 end time, with only minor timestamp discrepancies (E1 off by ~3.7s and E2 start off by ~1s); no major factual contradictions or omissions."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the importance of eye contact in a face-to-face interview, when does he explain how to involve the entire panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.8,
        "end": 897.0
      },
      "pred_interval": {
        "start": 884.0,
        "end": 892.5
      },
      "iou": 0.5923076923076958,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7999999999999545,
        "end": 4.5,
        "average": 2.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.31818181818181823,
        "text_similarity": 0.7637532949447632,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures both events, their order, and the described actions, with only minor timing offsets (E1 within the reference window and E2 shifted slightly earlier and ending sooner than the ground truth). These small temporal differences do not change the semantic content."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the young man's inappropriate attire during an interview, when does he advise to wear appropriate clothing?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 927.1,
        "end": 929.2
      },
      "pred_interval": {
        "start": 928.0,
        "end": 930.5
      },
      "iou": 0.35294117647060397,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8999999999999773,
        "end": 1.2999999999999545,
        "average": 1.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.6970319747924805,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the sequence (description then immediate advice) and gives close timestamps, but the provided intervals are shifted later by ~0.9\u20132s compared to the reference and it adds brief quoted phrasing not in the ground truth, so it's not fully accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says connecting with panel members on social media is a 'no-no', when does he say that sending a thank you is appropriate?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1087.0,
        "end": 1088.5
      },
      "pred_interval": {
        "start": 1087.2,
        "end": 1091.0
      },
      "iou": 0.32499999999998863,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.20000000000004547,
        "end": 2.5,
        "average": 1.3500000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.5511921048164368,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies E1 and E2, their order (after), and quotes the speaker's wording; timestamps are essentially matching though the predicted E2 end time is slightly later (1091.0s vs 1088.5s), a minor discrepancy."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker gives the example question about a dysfunctional team, when does he ask about the three most important values?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1154.0,
        "end": 1158.0
      },
      "pred_interval": {
        "start": 1156.3,
        "end": 1163.1
      },
      "iou": 0.18681318681319367,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2999999999999545,
        "end": 5.099999999999909,
        "average": 3.699999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.2439024390243902,
        "text_similarity": 0.590499222278595,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that the values question comes after the dysfunctional-team question and locates E2 approximately, but it misstates E1 timing (omits the start and gives an end ~10s later than reference) and gives an E2 end time several seconds off, so key timing details are inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes saying the phrase \"contrary evidence questions\", when does the specific text for \"5. Contrary-evidence questions\" appear on the blue panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1237.0,
        "end": 1257.7
      },
      "pred_interval": {
        "start": 1238.0,
        "end": 1258.0
      },
      "iou": 0.9380952380952403,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 0.2999999999999545,
        "average": 0.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.39080459770114945,
        "text_similarity": 0.7206222414970398,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly indicates the text appears after the phrase and the visibility duration is roughly right, but the timestamps are noticeably off (E1 and E2 are both given as 1238.0s, conflating the events) and the exact start/end times differ from the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining contrary-evidence questions, when does the \"Closing words\" slide appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1257.7,
        "end": 1259.0
      },
      "pred_interval": {
        "start": 1258.0,
        "end": 1260.0
      },
      "iou": 0.43478260869566077,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.2999999999999545,
        "end": 1.0,
        "average": 0.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.17647058823529413,
        "text_similarity": 0.5248185396194458,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the events and relation (once_finished) and only differs slightly in timing (start ~0.3s later and end ~1.0s later than reference); it adds a harmless visual detail but has a modest end-time discrepancy."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes stating that he hopes the tutorial was useful, when does he recommend visiting the IOM website for jobs?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1275.9,
        "end": 1284.3
      },
      "pred_interval": {
        "start": 1278.0,
        "end": 1286.0
      },
      "iou": 0.6237623762376249,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.099999999999909,
        "end": 1.7000000000000455,
        "average": 1.8999999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.2391304347826087,
        "text_similarity": 0.5848737955093384,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer accurately captures the events, timing (all offsets within ~2 seconds), and the 'after' relation, and preserves the key content about recommending the IOM website for careers/vacancies."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the session as 'interviewing prep 101', when does he state that the session will build on other career presentations?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 27.466,
        "end": 37.226
      },
      "pred_interval": {
        "start": 27.0,
        "end": 37.0
      },
      "iou": 0.9323293565421474,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.4660000000000011,
        "end": 0.2259999999999991,
        "average": 0.3460000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903614,
        "text_similarity": 0.7337788343429565,
        "llm_judge_score": 9,
        "llm_judge_justification": "The predicted answer correctly identifies the anchor and target, their immediate sequential relationship, and nearly matches the timestamps (differences under ~0.5s); minor timing imprecision is the only issue."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces himself as Daniel Moglen, when does he state where he works?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 66.316,
        "end": 68.83
      },
      "pred_interval": {
        "start": 66.0,
        "end": 68.5
      },
      "iou": 0.7717314487632505,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.3160000000000025,
        "end": 0.3299999999999983,
        "average": 0.3230000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7146425247192383,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures that the target immediately follows the anchor and the target timings are close to the reference (within ~0.3s), but the anchor time is noticeably off (~1.7s earlier than the ground truth), so the timing alignment is not fully accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that the workshops are just the beginning, when does he encourage everyone to stay in touch?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 169.9,
        "end": 175.8
      },
      "pred_interval": {
        "start": 168.5,
        "end": 173.5
      },
      "iou": 0.4931506849315053,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4000000000000057,
        "end": 2.3000000000000114,
        "average": 1.8500000000000085
      },
      "rationale_metrics": {
        "rouge_l": 0.5591397849462365,
        "text_similarity": 0.7588208913803101,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the correct relation and quoted utterances, but the timestamp estimates are moderately off (\u22481.5\u20132.6s shifts and a ~2.3s shorter end), so it's largely correct but not perfectly aligned temporally."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, cool', when does he welcome everyone?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 202.8,
        "end": 203.6
      },
      "pred_interval": {
        "start": 207.0,
        "end": 209.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.199999999999989,
        "end": 5.400000000000006,
        "average": 4.799999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.7623430490493774,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly preserves the order (welcome follows the 'All right, cool' utterance) but the timestamps are significantly off (anchor and target times differ by ~3\u20135s and the target end is incorrect), and the relation is weakened to 'after' instead of the immediate 'once_finished', so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker shares his screen, when does he ask the audience to reflect on job interviews?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 298.6,
        "end": 303.3
      },
      "pred_interval": {
        "start": 272.0,
        "end": 277.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.600000000000023,
        "end": 25.80000000000001,
        "average": 26.200000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.3925233644859813,
        "text_similarity": 0.8405285477638245,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the prompt to reflect and the 'after' relation, but the anchor and target timestamps are substantially incorrect (off by ~25\u201335 seconds) compared to the reference, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks which interview questions the audience is most concerned about, when does he instruct them to type those questions in the chatbox?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.283,
        "end": 336.694
      },
      "pred_interval": {
        "start": 336.1,
        "end": 340.2
      },
      "iou": 0.10038871049518283,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.8170000000000073,
        "end": 3.505999999999972,
        "average": 2.6614999999999895
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666669,
        "text_similarity": 0.7414180040359497,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the time spans are notably off: the anchor is ~1.4s early and the target start is ~1.8s late, causing the predicted target to miss part of the instruction; it also adds an extra phrase not reflected in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker encourages the audience to keep all questions in mind, when does he start describing how he measures a successful interview?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 510.0,
        "end": 564.0
      },
      "pred_interval": {
        "start": 497.2,
        "end": 509.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.800000000000011,
        "end": 55.0,
        "average": 33.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.416,
        "text_similarity": 0.7068516612052917,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speaker's introductory phrase but the timestamps are substantially off: the anchor end and target start differ from the reference by ~10\u201313s, and the predicted target ends much earlier (509.0s vs 564.0s), omitting most of the described segment. The overall temporal alignment is therefore largely incorrect despite semantic overlap in the opening line."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if there's anything he would have wanted to do differently in an interview, when does he ask if he said everything he wanted to say?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 520.97,
        "end": 525.55
      },
      "pred_interval": {
        "start": 525.5,
        "end": 530.2
      },
      "iou": 0.005417118093169494,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.529999999999973,
        "end": 4.650000000000091,
        "average": 4.590000000000032
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.7009382247924805,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their semantic content/order, but the temporal boundaries are noticeably off (E2 is placed ~4\u20135s later and does not immediately follow the anchor as in the ground truth), so the localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, so let's dive in here', when does the text 'DO NOT JUDGE YOUR PERFORMANCE - OR SELF WORTH - ON A JOB OFFER' appear on screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 570.09,
        "end": 574.39
      },
      "pred_interval": {
        "start": 613.5,
        "end": 619.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.40999999999997,
        "end": 45.110000000000014,
        "average": 44.25999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.76888108253479,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the event order (text appears after the prompt) but the timestamps are substantially off (predicted ~43s later than ground truth) and it adds an unsupported claim that the speaker reads the text aloud, so it is largely factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker discusses applying for jobs that ask for too much experience, when does he state that getting interviews indicates a good resume and cover letter?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 606.28,
        "end": 616.41
      },
      "pred_interval": {
        "start": 651.0,
        "end": 660.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.72000000000003,
        "end": 43.59000000000003,
        "average": 44.15500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.6300923824310303,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct semantic point (that interviews indicate a good resume/cover letter) but the provided timestamps are substantially off from the ground truth (~46\u201350 seconds later), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing his initial application for a graduate writing specialist position at UCSB, when does he mention being a finalist but not getting the job?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.0,
        "end": 718.5
      },
      "pred_interval": {
        "start": 719.5,
        "end": 725.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5,
        "end": 7.0,
        "average": 6.25
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111105,
        "text_similarity": 0.6670951843261719,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation and quoted phrases right but the anchor/target timestamps are significantly incorrect (off by ~5\u20136s), so the temporal alignment is wrong and key factual timing details are not preserved."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining that he and his family were not ready to move to apply for the second position, when does he reiterate that not getting a job interview is not necessarily unsuccessful?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.248,
        "end": 773.62
      },
      "pred_interval": {
        "start": 781.5,
        "end": 792.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.251999999999953,
        "end": 18.879999999999995,
        "average": 16.065999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.2325581395348837,
        "text_similarity": 0.6743007898330688,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct relation and the semantic content (reiterating that not getting a job isn't necessarily a failure) but the provided timestamps are substantially off (~13\u201319 seconds later than the reference) and it introduces quoted phrases not present in the ground truth, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks what the main thing hiring managers are looking for when interviewing people, when does he state their response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 874.4,
        "end": 883.6
      },
      "pred_interval": {
        "start": 858.0,
        "end": 863.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.399999999999977,
        "end": 20.600000000000023,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2696629213483146,
        "text_similarity": 0.7672710418701172,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relation and that the speaker comments before revealing the answer, the timestamps are substantially incorrect (predicted anchor 827.8s vs ground truth 852.0s; predicted target 858.0\u2013863.0s vs 874.4\u2013883.6s), so key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks for thoughts on 'likability' and mentions its subjective nature, when does he read the comment 'Doesn't sound fair' from the chat box?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 897.4,
        "end": 898.3
      },
      "pred_interval": {
        "start": 904.0,
        "end": 905.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.600000000000023,
        "end": 7.2000000000000455,
        "average": 6.900000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481928,
        "text_similarity": 0.5238486528396606,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the comment is read after the likability prompt, but the provided timestamps are significantly off (predicts ~904\u2013905.5s vs ground truth 897.4\u2013898.3s) and includes inconsistent player-time mapping, so timing is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes reading the chat comment 'I'm disheartened by that', when does he describe his own reaction to the concept of 'likability'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 937.485,
        "end": 939.668
      },
      "pred_interval": {
        "start": 944.0,
        "end": 949.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.514999999999986,
        "end": 9.331999999999994,
        "average": 7.92349999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2916666666666667,
        "text_similarity": 0.5758321285247803,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the speaker's reaction occurs after the chat line and even quotes the reaction, but the timestamps are substantially off (predicted 944\u2013949s vs correct 937.485\u2013939.668s) and it incorrectly represents the timing as immediate, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating that 'Interviewing is an inexact science', when does he elaborate on this by posing a rhetorical question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 974.6,
        "end": 985.7
      },
      "pred_interval": {
        "start": 980.0,
        "end": 992.5
      },
      "iou": 0.3184357541899471,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.399999999999977,
        "end": 6.7999999999999545,
        "average": 6.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677416,
        "text_similarity": 0.44957083463668823,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the rhetorical question and its placement immediately after the statement and even quotes it, but the reported timings are inaccurate (start \u22485.4s late and end \u22486.8s late) and the player/global time labeling is inconsistent, so it is only a partial match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if something makes sense, when does he start talking about thinking about your audience?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1085.885,
        "end": 1093.694
      },
      "pred_interval": {
        "start": 1091.5,
        "end": 1093.5
      },
      "iou": 0.2561147394032537,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.615000000000009,
        "end": 0.19399999999995998,
        "average": 2.9044999999999845
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818185,
        "text_similarity": 0.7063859701156616,
        "llm_judge_score": 6,
        "llm_judge_justification": "The anchor is identified accurately (close timing and phrasing), but the predicted target starts substantially later than the ground truth (1091.5s vs 1085.885s), capturing only the tail of the correct segment and omitting the earlier portion."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces HR interview/phone screens, when does he refer to it as a 'gatekeeper'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1125.176,
        "end": 1128.0
      },
      "pred_interval": {
        "start": 1156.0,
        "end": 1160.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.82400000000007,
        "end": 32.0,
        "average": 31.412000000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.8209349513053894,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies a 'gatekeeper' mention but both the anchor and target timestamps are substantially later and do not match the ground-truth spans, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's discussion of site visits, when does he describe the current form of these visits?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1176.087,
        "end": 1183.755
      },
      "pred_interval": {
        "start": 1247.0,
        "end": 1253.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.91300000000001,
        "end": 69.24499999999989,
        "average": 70.07899999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.6333513259887695,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the semantic content (description of the current virtual/Zoom site-visit format) but gives substantially different anchor/target timestamps than the ground truth, so it fails on the key factual element of timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that panel interviews maintain fairness, when does he mention there's no feedback or response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1247.396,
        "end": 1252.49
      },
      "pred_interval": {
        "start": 1249.0,
        "end": 1254.0
      },
      "iou": 0.5284675953967273,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6040000000000418,
        "end": 1.509999999999991,
        "average": 1.5570000000000164
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526316,
        "text_similarity": 0.8167684674263,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction accurately captures the quoted line and anchor event, but the timestamps are slightly shifted later (anchor ~0.6s off; target start/end ~1.6s off), so mostly correct with minor timing inaccuracies."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker recommends being on a hiring committee, when does he share his personal experience as a grad student?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1287.554,
        "end": 1295.994
      },
      "pred_interval": {
        "start": 1282.0,
        "end": 1288.0
      },
      "iou": 0.03187080177218204,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.554000000000087,
        "end": 7.9939999999999145,
        "average": 6.774000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.44943820224719094,
        "text_similarity": 0.8244609236717224,
        "llm_judge_score": 4,
        "llm_judge_justification": "The anchor timing is close to the ground truth, but the predicted target segment starts ~5.5s too early and ends ~8s too early, truncating the later content of the speaker's grad-student anecdote and thus missing key material."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions that the community is invited to sit on interviews for higher positions, when does he advise attending them to learn?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.58,
        "end": 1299.06
      },
      "pred_interval": {
        "start": 1349.0,
        "end": 1357.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.42000000000007,
        "end": 57.940000000000055,
        "average": 57.680000000000064
      },
      "rationale_metrics": {
        "rouge_l": 0.4186046511627907,
        "text_similarity": 0.7744216918945312,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the semantic content (the immediate advice 'And so go to those too'), but the timestamps are significantly shifted (~57 s) from the ground truth, so the temporal correctness is poor."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to always save the job description as a PDF, when does he explain that the job posting is often removed after expiry?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1452.209,
        "end": 1457.575
      },
      "pred_interval": {
        "start": 1454.0,
        "end": 1459.0
      },
      "iou": 0.5264320424090821,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.79099999999994,
        "end": 1.4249999999999545,
        "average": 1.6079999999999472
      },
      "rationale_metrics": {
        "rouge_l": 0.27956989247311825,
        "text_similarity": 0.6136904954910278,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the content and that the target follows the advice, but the absolute timestamps deviate by about 1\u20132 seconds (anchor end and target start/end) and the relation is less precise than 'immediately following,' so it is only partially aligned with the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining to turn qualifications into questions, when does he give the example 'must be familiar with discourse analysis'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1492.512,
        "end": 1496.48
      },
      "pred_interval": {
        "start": 1497.0,
        "end": 1500.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.488000000000056,
        "end": 3.519999999999982,
        "average": 4.004000000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.4042553191489362,
        "text_similarity": 0.578491747379303,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event order right (the example occurs after the instruction) but the timestamps are substantially off (predicted anchor ends at 1495.0 vs 1492.22, and predicted target 1497.0\u20131500.0 vs 1492.512\u20131496.480) and it omits the correct immediate/illustrative relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the idea of a 'bad response', when does he describe the specific bad response example?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1804.78,
        "end": 1808.35
      },
      "pred_interval": {
        "start": 1805.8,
        "end": 1812.2
      },
      "iou": 0.3436657681940606,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0199999999999818,
        "end": 3.8500000000001364,
        "average": 2.435000000000059
      },
      "rationale_metrics": {
        "rouge_l": 0.2708333333333333,
        "text_similarity": 0.7980408668518066,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target segments and that the specific example follows the introduction, with start times close to the reference; however, the E2 end time is several seconds later than the ground truth and the answer adds quoted detail not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces his 'go-to response', when does he state that his weakness is being conflict-avoidant?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1887.2,
        "end": 1890.9
      },
      "pred_interval": {
        "start": 1850.5,
        "end": 1853.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.700000000000045,
        "end": 37.40000000000009,
        "average": 37.05000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.3058823529411765,
        "text_similarity": 0.8137658834457397,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target utterance content and relative order, but the anchor and target timestamps are substantially and factually incorrect compared to the ground truth, omitting the key timing information."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"So I just dug in. I said,\" when does he start listing possible uses for a brick?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2144.2,
        "end": 2157.5
      },
      "pred_interval": {
        "start": 2150.0,
        "end": 2157.0
      },
      "iou": 0.526315789473677,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.800000000000182,
        "end": 0.5,
        "average": 3.150000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.3191489361702127,
        "text_similarity": 0.36775875091552734,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the listing follows the quoted phrase and gets the end time roughly right, but it misstates the start time (\u22482150.0s vs ground truth 2144.2s), a significant factual error for the asked timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"Is everyone still with me? Are we good?\" when does the slide transition to show the 'S(T)AR' method?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2189.8,
        "end": 2191.0
      },
      "pred_interval": {
        "start": 2189.0,
        "end": 2191.0
      },
      "iou": 0.599999999999909,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8000000000001819,
        "end": 0.0,
        "average": 0.40000000000009095
      },
      "rationale_metrics": {
        "rouge_l": 0.3469387755102041,
        "text_similarity": 0.6510725617408752,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures that the slide appears after the speaker and gives a close slide time, but it misstates the speaker end time (\u22482185s vs 2179.5s) and adds unverified/extra detail about the slide content, so it is only partially aligned."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the action taken, when does he finish describing the result?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2376.449,
        "end": 2382.556
      },
      "pred_interval": {
        "start": 2382.0,
        "end": 2409.0
      },
      "iou": 0.017080888451968947,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.550999999999931,
        "end": 26.44399999999996,
        "average": 15.997499999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.5965818166732788,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures a sequential/after relation, but the timestamps for E1 and E2 differ substantially from the reference and the predicted E2 finish time and descriptive details appear unsupported/hallucinated, so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes talking about the program becoming institutionalized, when does he mention the 'tags' at the bottom of the slide?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2407.152,
        "end": 2412.282
      },
      "pred_interval": {
        "start": 2413.0,
        "end": 2418.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.847999999999956,
        "end": 5.717999999999847,
        "average": 5.782999999999902
      },
      "rationale_metrics": {
        "rouge_l": 0.2127659574468085,
        "text_similarity": 0.6161844730377197,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events, their order, and the topic-transition relation, but the reported timestamps are several seconds later than the reference (E1 ~+2.7s, E2 start ~+5.8s, E2 end ~+5.7s), so the temporal boundaries are noticeably misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions moving into the mock interview, when does he start explaining what seminal experiences are?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2572.085,
        "end": 2581.418
      },
      "pred_interval": {
        "start": 2572.5,
        "end": 2582.5
      },
      "iou": 0.8562650024003986,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.4149999999999636,
        "end": 1.08199999999988,
        "average": 0.7484999999999218
      },
      "rationale_metrics": {
        "rouge_l": 0.24444444444444444,
        "text_similarity": 0.6631336808204651,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction accurately captures and timestamps E2 (start and end) and the relation (after), but the E1 anchor time is significantly off (~47s earlier than the reference), making the overall alignment partially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the bullet point 'Create your talking points -> 3-4 SAR examples', when does he introduce the next bullet point about 'tagging' SAR examples?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2602.202,
        "end": 2611.574
      },
      "pred_interval": {
        "start": 2596.5,
        "end": 2602.5
      },
      "iou": 0.01976913891467249,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.7020000000002256,
        "end": 9.07400000000007,
        "average": 7.388000000000147
      },
      "rationale_metrics": {
        "rouge_l": 0.2666666666666666,
        "text_similarity": 0.4449261426925659,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the next bullet is about tagging, but the timestamps diverge: E1 omits the finish time and E2's start/end times are several seconds off from the reference, so important temporal details are missing or inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"So five minutes on the clock\", when does he tell the audience to look at the Muse article?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2689.809,
        "end": 2694.275
      },
      "pred_interval": {
        "start": 2691.0,
        "end": 2693.0
      },
      "iou": 0.44782803403494115,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1909999999998035,
        "end": 1.275000000000091,
        "average": 1.2329999999999472
      },
      "rationale_metrics": {
        "rouge_l": 0.33707865168539325,
        "text_similarity": 0.794037401676178,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states that the Muse-article instruction follows the 'five minutes' remark and gives plausible timestamps, but the reported anchor and target times are shifted and the predicted E2 duration is shorter than the ground truth, so the timing is imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states his two criteria for experience, when does he advise on what career stage experiences to focus on?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2808.94,
        "end": 2831.958
      },
      "pred_interval": {
        "start": 2809.0,
        "end": 2833.0
      },
      "iou": 0.9541978387364978,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.05999999999994543,
        "end": 1.0419999999999163,
        "average": 0.5509999999999309
      },
      "rationale_metrics": {
        "rouge_l": 0.19607843137254902,
        "text_similarity": 0.702635645866394,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies that the advice occurs after the criteria and matches the E2 time window and content (focus on grad school/earlier relevance) with only minor timing discrepancies (E1 end about 9s later and E2 end ~1s later)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'm gonna speak the question for folks who are watching the recording here, so 'cause I think it's a really good question\", when does he read out the question from the chat?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2867.48,
        "end": 2878.688
      },
      "pred_interval": {
        "start": 2869.0,
        "end": 2879.0
      },
      "iou": 0.8409722222222324,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.5199999999999818,
        "end": 0.31199999999989814,
        "average": 0.91599999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.1904761904761905,
        "text_similarity": 0.5164299011230469,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the event sequence (setup then immediate reading and the question text) and the end time is close, but the timestamps are slightly offset (~1.5s later for E1/E2 start and ~0.3s for E2 end) compared to the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if it's useful to give a STAR story from a non-work context, when does he start discussing family examples as potentially too personal?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2888.2,
        "end": 2891.7
      },
      "pred_interval": {
        "start": 2885.0,
        "end": 2891.0
      },
      "iou": 0.41791044776123254,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.199999999999818,
        "end": 0.6999999999998181,
        "average": 1.949999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.19801980198019803,
        "text_similarity": 0.6989740133285522,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction captures the correct semantic relation and the speaker's caution about family examples, but the reported timestamps are slightly off from the reference (E1 a few seconds early and E2 begins ~3s earlier and ends marginally earlier), so it is not an exact match."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will outline what they are going to do first, when does the screen transition to show the next set of instructions for the mock interview?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.0,
        "end": 2920.0
      },
      "pred_interval": {
        "start": 2913.0,
        "end": 2915.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 5.0,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.25531914893617025,
        "text_similarity": 0.8286663293838501,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted anchor time (2913s) is within the correct anchor interval, but the predicted target timing and relationship contradict the reference: the prediction claims the transition starts at 2913s and is fully visible by 2915s, whereas the ground truth states the transition begins at 2916s and is fully visible by 2920s, so the temporal ordering/timings are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking 'Why are you interested in this position?', when does he mention 'Tell me about yourself' as an alternative?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3061.797,
        "end": 3062.728
      },
      "pred_interval": {
        "start": 3065.8,
        "end": 3067.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.003000000000156,
        "end": 4.7719999999999345,
        "average": 4.3875000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7259490489959717,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that the alternative question immediately follows and reproduces the phrasing, but the provided timestamps differ significantly from the ground-truth anchor and target times, so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will put the interview schedule into the chat box, when does he display the Muse article on behavioral interview questions?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.0,
        "end": 3125.6
      },
      "pred_interval": {
        "start": 3123.0,
        "end": 3128.0
      },
      "iou": 0.32499999999998863,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 2.400000000000091,
        "average": 2.7000000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.2765957446808511,
        "text_similarity": 0.708730936050415,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the sequence (display occurs after the statement) and gives time ranges that overlap the ground truth, but the reported start/end times are slightly shifted (E1 starts later and E2 is ~3s later start and ends later than the reference)."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if there are any questions before opening breakout rooms, when does he describe the group sizes?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3206.19,
        "end": 3214.181
      },
      "pred_interval": {
        "start": 3212.0,
        "end": 3218.0
      },
      "iou": 0.18467400508044454,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.809999999999945,
        "end": 3.81899999999996,
        "average": 4.814499999999953
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.6798564195632935,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the utterance describing group sizes occurring after the question, but the provided timestamps significantly differ from the reference (several seconds off) and the anchor's start/end times are not matched, so the answer is factually inaccurate on timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'So, just plan your time accordingly', when does he say 'And you'll have about 25 minutes and we'll call you back at the end'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3215.09,
        "end": 3217.67
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3224.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.9099999999998545,
        "end": 6.329999999999927,
        "average": 5.619999999999891
      },
      "rationale_metrics": {
        "rouge_l": 0.1951219512195122,
        "text_similarity": 0.5762330293655396,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct ordering (target after anchor) but the reported timestamps are substantially and consistently offset (by ~6\u20139 seconds) and durations differ from the ground truth, so the answer is largely temporally inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'If we're ready to go, we'll stop the recording and we'll send you off', when does the black screen with text 'For the remainder of the session...' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3231.62,
        "end": 3239.85
      },
      "pred_interval": {
        "start": 3231.0,
        "end": 3239.0
      },
      "iou": 0.8338983050847667,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.6199999999998909,
        "end": 0.849999999999909,
        "average": 0.7349999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1590909090909091,
        "text_similarity": 0.5864952802658081,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gives target start/end times close to the reference and correctly states the target occurs after the anchor, but it misreports the anchor end time by ~5 seconds (3230s vs 3224.79s), a notable factual discrepancy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they will be doing a mock interview today, when does he start explaining what a mock interview is?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1623.186,
        "end": 1643.288
      },
      "pred_interval": {
        "start": 1625.56,
        "end": 1656.0
      },
      "iou": 0.5402572072895723,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3740000000000236,
        "end": 12.711999999999989,
        "average": 7.543000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.5284453630447388,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the events and their order and paraphrases the utterances, but the timestamps differ noticeably from the reference (E1 ~4.5s late, E2 start ~2.4s late and end ~12.7s later), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the 'Tell me about yourself (TMAY)' question, when does he introduce 'Behavioral Questions'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1740.196,
        "end": 1747.784
      },
      "pred_interval": {
        "start": 1713.88,
        "end": 1743.6
      },
      "iou": 0.10040113260972147,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.315999999999804,
        "end": 4.1840000000001965,
        "average": 15.25
      },
      "rationale_metrics": {
        "rouge_l": 0.18604651162790697,
        "text_similarity": 0.45029759407043457,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and their sequencing (relation=next), but the timestamps are substantially off: E1 end is given ~1713s vs 1700.62s, and E2 start is given ~1713.88s vs 1740.20s (only the predicted E2 end partially overlaps the reference). The large timing discrepancies make the prediction largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains that \"put you on the spot questions\" are by definition questions you can't prepare for, when does he explicitly say he has an example of such a question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2004.224,
        "end": 2006.086
      },
      "pred_interval": {
        "start": 2007.5,
        "end": 2010.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.2760000000000673,
        "end": 4.413999999999987,
        "average": 3.8450000000000273
      },
      "rationale_metrics": {
        "rouge_l": 0.32941176470588235,
        "text_similarity": 0.7216479778289795,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the E2 utterance ('I have an example...') and places it near the correct time, but it gives a wildly incorrect time for E1 (1970s vs ~15\u201319s in the ground truth), so the key temporal alignment is wrong despite preserving the 'after' relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"And here's the question\" to introduce the example, when does the slide titled \"THE BRICK QUESTION\" appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2048.659,
        "end": 2048.899
      },
      "pred_interval": {
        "start": 2051.2,
        "end": 2052.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5409999999997126,
        "end": 3.300999999999931,
        "average": 2.9209999999998217
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.7680559158325195,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and gives E1 timing consistent with the reference mapping; E2 is slightly off (~2\u20133s later) compared to the ground truth, a minor timing discrepancy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks the audience \"What is the point of this question?\", when does he state that the entire purpose of the question is that you can't prepare for it?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2115.539,
        "end": 2118.102
      },
      "pred_interval": {
        "start": 2119.0,
        "end": 2123.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4609999999997854,
        "end": 4.898000000000138,
        "average": 4.179499999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.4835164835164835,
        "text_similarity": 0.6859477758407593,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events, the quoted E2 line, and the correct temporal relation (after); however, the E1 timestamp is inconsistent/likely mistyped (2068s vs ~134s), so there's a minor timing error despite overall semantic agreement."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying they will stop the recording and send the attendees off, when does the black screen with information about practicing interviews appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3225.795,
        "end": 3228.795
      },
      "pred_interval": {
        "start": 3231.2,
        "end": 3239.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.404999999999745,
        "end": 10.304999999999836,
        "average": 7.854999999999791
      },
      "rationale_metrics": {
        "rouge_l": 0.22448979591836735,
        "text_similarity": 0.7485634684562683,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly shifts both event timestamps by ~9\u201310s and vastly overstates the black screen duration and content, though it correctly notes the black screen follows the speaker; these factual timing errors are substantial."
      }
    },
    {
      "question_id": "002",
      "question": "After the screen displays text encouraging attendees to practice interviews, when is the next text displayed on the screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3236.0,
        "end": 3240.0
      },
      "pred_interval": {
        "start": 3239.1,
        "end": 3246.1
      },
      "iou": 0.08910891089109892,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.099999999999909,
        "end": 6.099999999999909,
        "average": 4.599999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.2380952380952381,
        "text_similarity": 0.7466334104537964,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same next text content but contradicts the reference timestamps (predicted start/end times 3239.1\u20133246.1 vs correct 3236\u20133240) and thus misstates key temporal facts."
      }
    },
    {
      "question_id": "003",
      "question": "Once the screen finishes displaying information about other LCL videos, when do the credits for 'Linguistics Career Launch 2021' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3241.0,
        "end": 3243.0
      },
      "pred_interval": {
        "start": 3246.1,
        "end": 3257.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.099999999999909,
        "end": 14.099999999999909,
        "average": 9.599999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.6829104423522949,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps and durations conflict significantly with the ground truth (predicted E1/E2 at ~3246.1s\u20133257.1s vs. ground truth E1 3236.3\u20133239.36s and E2 starting 3241s and present at 3243s), so it is largely incorrect despite both noting credits follow the informational text."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first woman finishes describing the traits she looks for in people to hire, when does Bartolo introduce himself?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.812,
        "end": 9.402
      },
      "pred_interval": {
        "start": 8.0,
        "end": 11.0
      },
      "iou": 0.4397741530740274,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.18799999999999972,
        "end": 1.5980000000000008,
        "average": 0.8930000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.14705882352941177,
        "text_similarity": 0.41983580589294434,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys that Bartolo speaks after the woman, but it gives incorrect timestamps (woman finishing at ~7.0s vs 7.711s, Bartolo ending at ~11.0s vs 9.402s) and adds a likely hallucinated utterance, so it is largely inaccurate on key details."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'English with Gallery Teachers' title card, when does the background music play?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.0,
        "end": 25.6
      },
      "pred_interval": {
        "start": 21.0,
        "end": 26.0
      },
      "iou": 0.9200000000000003,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.0,
        "end": 0.3999999999999986,
        "average": 0.1999999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.14925373134328357,
        "text_similarity": 0.32858985662460327,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly indicates the music plays during the title card and matches the start time (~21.0s), but it overstays the end by ~0.4s (predicts 26.0s vs correct 25.6s) and omits the precise anchor interval."
      }
    },
    {
      "question_id": "003",
      "question": "After Rita mentions her company is not a pyramid system, when does she clarify she never uses the word 'employees'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 114.588,
        "end": 116.843
      },
      "pred_interval": {
        "start": 151.0,
        "end": 157.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.412000000000006,
        "end": 40.157,
        "average": 38.2845
      },
      "rationale_metrics": {
        "rouge_l": 0.12658227848101267,
        "text_similarity": 0.09733687341213226,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer contains the correct content phrase but gives completely different timestamps and sequence (144\u2013157s vs. 108.435\u2013116.843s) and thus fails to match the ground-truth timing and relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the female speaker states she never reads CVs, when does she mention that the HR director reads them?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 190.4,
        "end": 199.43
      },
      "pred_interval": {
        "start": 192.0,
        "end": 195.0
      },
      "iou": 0.3322259136212624,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5999999999999943,
        "end": 4.430000000000007,
        "average": 3.0150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.32653061224489793,
        "text_similarity": 0.7182224988937378,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (E2 occurs after E1) and the content that the HR director reads CVs, but the timestamps differ notably from the reference (E1 ~4s early, E2 start ~1.6s late and end ~4.4s early), and the predicted E2 duration is shorter than the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman explains that certifications like CELTA or TEFL are not the sole hiring criteria, when does she state what essential qualities she looks for?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.0,
        "end": 343.3
      },
      "pred_interval": {
        "start": 340.8,
        "end": 344.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.8000000000000114,
        "end": 0.6999999999999886,
        "average": 1.25
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.860087513923645,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the content of the target (the listed qualities) and the general region, but the reported start/end times differ from the reference (predicted start is ~1.8s late and end ~0.7s late) and the anchor's end time is not specified precisely."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man mentions that he considers people with scout experience because it indicates passion and ability to manage people, when does the woman add that it shows likability?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 369.0,
        "end": 372.5
      },
      "pred_interval": {
        "start": 369.8,
        "end": 373.2
      },
      "iou": 0.6428571428571419,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8000000000000114,
        "end": 0.6999999999999886,
        "average": 0.75
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7660284042358398,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction captures the main relation\u2014the woman immediately responds after the man and adds that it shows likability\u2014while anchor/target timings are close; however, the timestamps differ slightly and the prediction adds an unsupported detail ('likable as a teacher'), so it's not a perfect match."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman mentions having 'trick questions' for interviews, such as 'what makes you happiest in teaching?', when does she describe what an ideal answer to that question sounds like?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 530.0,
        "end": 535.0
      },
      "pred_interval": {
        "start": 458.8,
        "end": 468.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.19999999999999,
        "end": 66.5,
        "average": 68.85
      },
      "rationale_metrics": {
        "rouge_l": 0.1839080459770115,
        "text_similarity": 0.6145858764648438,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer places both the anchor and the target much earlier and gives different time ranges than the ground truth; while it captures a similar thematic role-play, it fails to match the correct timestamps and event alignment, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks if they've not hired someone for crossing arms, when does the woman respond about not hiring people who didn't smile?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.6,
        "end": 521.2
      },
      "pred_interval": {
        "start": 515.2,
        "end": 520.5
      },
      "iou": 0.8166666666666629,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.39999999999997726,
        "end": 0.7000000000000455,
        "average": 0.5500000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.6760395765304565,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and direct response and preserves the sequential relationship; the timestamps are close but slightly offset from the reference (differences up to ~1.3s), so minor timing inaccuracies cost one point."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says reading body language is very important, when does she give the example of loving children with a poker face?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.4,
        "end": 632.7
      },
      "pred_interval": {
        "start": 629.0,
        "end": 638.0
      },
      "iou": 0.3854166666666705,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6000000000000227,
        "end": 5.2999999999999545,
        "average": 2.9499999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.0816326530612245,
        "text_similarity": 0.7213997840881348,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly identifies the anchor and target utterances but the timestamps are substantially inaccurate (E1 ends ~625.6s not ~628s; E2 is 628.4\u2013632.7s, not 629.0\u2013638.0s) and it adds/extends content beyond the ground truth, so it is only a loose match."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about the opportunity to offer educational courses after switching to online learning, when does she mention people outside Chisinau who always wanted this alternative?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 707.0,
        "end": 715.5
      },
      "pred_interval": {
        "start": 711.0,
        "end": 720.0
      },
      "iou": 0.34615384615384615,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 4.5,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.33707865168539325,
        "text_similarity": 0.8429546356201172,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures the anchor/topic and that the target mention of people outside Chisinau occurs after the anchor; timing is close but slightly offset (predicted start ~711s vs 707s and predicted end extends beyond the reference by a few seconds)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the pandemic simply forced them with the reality of online learning, when does she explain that they were partly stressed and partly happy about the transition?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 817.899,
        "end": 828.773
      },
      "pred_interval": {
        "start": 777.0,
        "end": 783.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.899,
        "end": 45.773000000000025,
        "average": 43.33600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7606033682823181,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the immediate follow-up utterance and its content, but the provided timestamps are significantly inaccurate and the target segment duration is much shorter than the ground truth, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'It's not exactly like in class. I'm not gonna lie, it is not.', when does she give examples of what is missed in online classes?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 863.0,
        "end": 869.0
      },
      "pred_interval": {
        "start": 869.0,
        "end": 878.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 9.0,
        "average": 7.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2765957446808511,
        "text_similarity": 0.7977992296218872,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces the target interval\u2014claiming E2 starts at 869.0s and ends at 878.0s\u2014whereas the reference specifies E2 spans 863.0\u2013869.0s; the anchor timing is also slightly off. This contradicts the key temporal facts and adds unsupported content, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the female speaker finishes saying \"So why not see that?\", when does the male speaker begin to speak?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 992.475,
        "end": 994.716
      },
      "pred_interval": {
        "start": 996.6,
        "end": 1000.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.125,
        "end": 5.283999999999992,
        "average": 4.704499999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.5564001202583313,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys that the male speaks immediately after the female (once_finished) but gives incorrect timestamps (predicts 996.6s vs correct start 992.475s) and asserts a simultaneous 0.0s gap and quoted wording not supported by the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female speaker finishes stating that Moldova and Romania have a huge number of emigrants, when does she list the countries where they live?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 903.0,
        "end": 908.8
      },
      "pred_interval": {
        "start": 907.5,
        "end": 910.5
      },
      "iou": 0.17333333333332726,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.5,
        "end": 1.7000000000000455,
        "average": 3.1000000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.2077922077922078,
        "text_similarity": 0.4057697653770447,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relation (target starts immediately after the anchor) but gives substantially different timestamps and an incorrect end time, and it adds a specific country list not present in the reference; these factual and timing mismatches reduce correctness."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male speaker finishes saying \"I agree with you completely\", when does he start talking about writing an article on the future of TEFL?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 996.979,
        "end": 1001.302
      },
      "pred_interval": {
        "start": 1001.1,
        "end": 1004.5
      },
      "iou": 0.026858130567743558,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.120999999999981,
        "end": 3.197999999999979,
        "average": 3.65949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.5986440181732178,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction contradicts the reference: it gives a different anchor end time (~1001.1s vs 996.658s) and claims the target starts at 1001.1s, whereas the ground truth says the target began at 877.0s and finished at 1001.302s. These timing discrepancies and the incorrect start time make the answer largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'So you have to be adaptable,' when does she say 'only the strongest survive'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1076.398,
        "end": 1078.041
      },
      "pred_interval": {
        "start": 1080.9,
        "end": 1082.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.50200000000018,
        "end": 4.359000000000151,
        "average": 4.4305000000001655
      },
      "rationale_metrics": {
        "rouge_l": 0.4137931034482758,
        "text_similarity": 0.6931464076042175,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('after'), its timestamps are substantially inaccurate: the anchor is ~1s late and the target is ~4.5s later than the ground truth, and it fails to reflect that the phrase follows directly immediately after the prior statement."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says, 'You cannot come at a lesson in a hoodie,' when does the man appear on screen wearing a red hoodie?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1112.044,
        "end": 1114.077
      },
      "pred_interval": {
        "start": 1114.1,
        "end": 1116.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0559999999998126,
        "end": 2.522999999999911,
        "average": 2.2894999999998618
      },
      "rationale_metrics": {
        "rouge_l": 0.28828828828828823,
        "text_similarity": 0.6613292694091797,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the visual interjection (man in red hoodie pulling hood up) and the 'after' relationship, but the timestamps are noticeably off (E1 ~2s early and E2 ~2s+ late compared to the reference), so the temporal alignment is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes saying, 'It's been really nice to have you again on this channel,' when does the Facebook page overlay appear on screen?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1184.146,
        "end": 1186.146
      },
      "pred_interval": {
        "start": 1186.1,
        "end": 1190.4
      },
      "iou": 0.007355292612735551,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.9539999999999509,
        "end": 4.254000000000133,
        "average": 3.104000000000042
      },
      "rationale_metrics": {
        "rouge_l": 0.5274725274725275,
        "text_similarity": 0.776608943939209,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies both events and that the Facebook overlay follows the statement, but the timestamps and durations are substantially off (E1/E2 times differ by ~2\u20134 seconds from the reference), so it contradicts the precise temporal information in the correct answer."
      }
    }
  ]
}