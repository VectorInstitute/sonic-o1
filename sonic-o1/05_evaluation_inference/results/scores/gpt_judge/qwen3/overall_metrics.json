{
  "model": "qwen3",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.24600238920419246,
            "rouge_l_std": 0.057389273830450935,
            "text_similarity_mean": 0.7847794108092785,
            "text_similarity_std": 0.05245448725525773,
            "llm_judge_score_mean": 7.375,
            "llm_judge_score_std": 1.4947825928876748
          },
          "short": {
            "rouge_l_mean": 0.24679482024625857,
            "rouge_l_std": 0.07179387985626856,
            "text_similarity_mean": 0.7313427925109863,
            "text_similarity_std": 0.09838356818858225,
            "llm_judge_score_mean": 6.125,
            "llm_judge_score_std": 2.0879116360612584
          },
          "cider": {
            "cider_detailed": 0.0703977965342299,
            "cider_short": 0.028692854365624573
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.2551545247798038,
            "rouge_l_std": 0.06167050394942512,
            "text_similarity_mean": 0.7552508541515895,
            "text_similarity_std": 0.07531129006026764,
            "llm_judge_score_mean": 7.142857142857143,
            "llm_judge_score_std": 1.8331787195162057
          },
          "short": {
            "rouge_l_mean": 0.22223900883088946,
            "rouge_l_std": 0.0609285470133077,
            "text_similarity_mean": 0.6662091981796991,
            "text_similarity_std": 0.10595562526543538,
            "llm_judge_score_mean": 6.380952380952381,
            "llm_judge_score_std": 2.148669366224472
          },
          "cider": {
            "cider_detailed": 0.07182284342572959,
            "cider_short": 0.026418083880735715
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.22165959623345202,
            "rouge_l_std": 0.061502252256264864,
            "text_similarity_mean": 0.7442486749755012,
            "text_similarity_std": 0.07769980255893436,
            "llm_judge_score_mean": 6.722222222222222,
            "llm_judge_score_std": 1.789026909789402
          },
          "short": {
            "rouge_l_mean": 0.19099946479228191,
            "rouge_l_std": 0.08019427490801391,
            "text_similarity_mean": 0.6999572018782297,
            "text_similarity_std": 0.10674651617153469,
            "llm_judge_score_mean": 5.833333333333333,
            "llm_judge_score_std": 2.1921577396609844
          },
          "cider": {
            "cider_detailed": 0.0031601929904943437,
            "cider_short": 0.006200466846002987
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.25584674072801233,
            "rouge_l_std": 0.058589349457914636,
            "text_similarity_mean": 0.743283220132192,
            "text_similarity_std": 0.05400428231514748,
            "llm_judge_score_mean": 6.133333333333334,
            "llm_judge_score_std": 2.028683207293725
          },
          "short": {
            "rouge_l_mean": 0.25954060777936466,
            "rouge_l_std": 0.0982784502299469,
            "text_similarity_mean": 0.7116631547609965,
            "text_similarity_std": 0.0994753660486911,
            "llm_judge_score_mean": 5.333333333333333,
            "llm_judge_score_std": 2.0548046676563256
          },
          "cider": {
            "cider_detailed": 0.0015516857723181265,
            "cider_short": 0.00010746093209841347
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.22769203220449327,
            "rouge_l_std": 0.06725175490182297,
            "text_similarity_mean": 0.703391934816654,
            "text_similarity_std": 0.1369829152572223,
            "llm_judge_score_mean": 6.0,
            "llm_judge_score_std": 1.7541160386140584
          },
          "short": {
            "rouge_l_mean": 0.1762975608269051,
            "rouge_l_std": 0.06931536510323989,
            "text_similarity_mean": 0.6202043615854703,
            "text_similarity_std": 0.1958283193762138,
            "llm_judge_score_mean": 5.0,
            "llm_judge_score_std": 1.7974340685458343
          },
          "cider": {
            "cider_detailed": 0.05115336721216622,
            "cider_short": 1.808439287059384e-05
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.2277334010271749,
            "rouge_l_std": 0.048963740085165255,
            "text_similarity_mean": 0.6864487200975418,
            "text_similarity_std": 0.12773485860908715,
            "llm_judge_score_mean": 5.1,
            "llm_judge_score_std": 2.1656407827707715
          },
          "short": {
            "rouge_l_mean": 0.2362205214316866,
            "rouge_l_std": 0.061924179801017866,
            "text_similarity_mean": 0.6382859408855438,
            "text_similarity_std": 0.11944400393215013,
            "llm_judge_score_mean": 5.45,
            "llm_judge_score_std": 1.9868316486305528
          },
          "cider": {
            "cider_detailed": 0.00615018385086958,
            "cider_short": 0.005662754357492989
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.21248374867797795,
            "rouge_l_std": 0.03785238563792681,
            "text_similarity_mean": 0.7015314783368792,
            "text_similarity_std": 0.06278825222498566,
            "llm_judge_score_mean": 4.857142857142857,
            "llm_judge_score_std": 1.8844151368961313
          },
          "short": {
            "rouge_l_mean": 0.2261980420709555,
            "rouge_l_std": 0.10786209521014074,
            "text_similarity_mean": 0.6285376357180732,
            "text_similarity_std": 0.12459898846998037,
            "llm_judge_score_mean": 4.142857142857143,
            "llm_judge_score_std": 2.099562636671296
          },
          "cider": {
            "cider_detailed": 8.599152335031883e-14,
            "cider_short": 0.02112115429730733
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.20828030153547364,
            "rouge_l_std": 0.04040066194077727,
            "text_similarity_mean": 0.7206427355607351,
            "text_similarity_std": 0.07248365599051207,
            "llm_judge_score_mean": 6.083333333333333,
            "llm_judge_score_std": 1.4409680388158819
          },
          "short": {
            "rouge_l_mean": 0.20645842791318103,
            "rouge_l_std": 0.04169323773437657,
            "text_similarity_mean": 0.7518357982238134,
            "text_similarity_std": 0.09370315090141283,
            "llm_judge_score_mean": 5.666666666666667,
            "llm_judge_score_std": 1.49071198499986
          },
          "cider": {
            "cider_detailed": 0.010954958966278266,
            "cider_short": 0.03062382244618478
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.23003935068781592,
            "rouge_l_std": 0.048711761253101914,
            "text_similarity_mean": 0.6705864084263643,
            "text_similarity_std": 0.10020906479021284,
            "llm_judge_score_mean": 4.583333333333333,
            "llm_judge_score_std": 1.2219065248845982
          },
          "short": {
            "rouge_l_mean": 0.23526326076800488,
            "rouge_l_std": 0.07319964788963433,
            "text_similarity_mean": 0.6309349648654461,
            "text_similarity_std": 0.1416898162575893,
            "llm_judge_score_mean": 4.166666666666667,
            "llm_judge_score_std": 1.3123346456686351
          },
          "cider": {
            "cider_detailed": 0.01970285258842953,
            "cider_short": 0.03503909375073718
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.23154172909912205,
            "rouge_l_std": 0.05053724053704597,
            "text_similarity_mean": 0.7214138857696367,
            "text_similarity_std": 0.16137404416262732,
            "llm_judge_score_mean": 5.0,
            "llm_judge_score_std": 2.206709137469482
          },
          "short": {
            "rouge_l_mean": 0.22480809209152952,
            "rouge_l_std": 0.06362368277703472,
            "text_similarity_mean": 0.673420663761056,
            "text_similarity_std": 0.1532641025805256,
            "llm_judge_score_mean": 4.739130434782608,
            "llm_judge_score_std": 2.1912353336268087
          },
          "cider": {
            "cider_detailed": 0.009141550147914918,
            "cider_short": 0.002831608523940962
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.26265920580316404,
            "rouge_l_std": 0.08685579057611527,
            "text_similarity_mean": 0.7733268600243789,
            "text_similarity_std": 0.09909447951159207,
            "llm_judge_score_mean": 7.230769230769231,
            "llm_judge_score_std": 1.5268794800984007
          },
          "short": {
            "rouge_l_mean": 0.23327117733447086,
            "rouge_l_std": 0.04396157557279631,
            "text_similarity_mean": 0.7368500370245713,
            "text_similarity_std": 0.0659989559053582,
            "llm_judge_score_mean": 6.6923076923076925,
            "llm_judge_score_std": 1.5384615384615385
          },
          "cider": {
            "cider_detailed": 0.0965528091909538,
            "cider_short": 0.1634930711028072
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.18802063751128648,
            "rouge_l_std": 0.051402912872420686,
            "text_similarity_mean": 0.5990190257628759,
            "text_similarity_std": 0.10822686537630889,
            "llm_judge_score_mean": 4.722222222222222,
            "llm_judge_score_std": 2.0764701767811955
          },
          "short": {
            "rouge_l_mean": 0.20043974244816093,
            "rouge_l_std": 0.09541808235909495,
            "text_similarity_mean": 0.6067505478858948,
            "text_similarity_std": 0.14160173819593672,
            "llm_judge_score_mean": 4.222222222222222,
            "llm_judge_score_std": 1.9876159799998132
          },
          "cider": {
            "cider_detailed": 0.01686715565718008,
            "cider_short": 0.007957326690984512
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.19238163419311424,
            "rouge_l_std": 0.03519500729043123,
            "text_similarity_mean": 0.6706354125686314,
            "text_similarity_std": 0.10042283191820832,
            "llm_judge_score_mean": 3.391304347826087,
            "llm_judge_score_std": 1.2419875527900608
          },
          "short": {
            "rouge_l_mean": 0.17932895592899606,
            "rouge_l_std": 0.053523431826387836,
            "text_similarity_mean": 0.6259701187195985,
            "text_similarity_std": 0.1326291454149458,
            "llm_judge_score_mean": 3.0434782608695654,
            "llm_judge_score_std": 1.2675982380098478
          },
          "cider": {
            "cider_detailed": 0.020402006959950453,
            "cider_short": 0.13264168864903117
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.22765348397577562,
          "text_similarity_mean": 0.7134275862640199,
          "llm_judge_score_mean": 5.718578309464598
        },
        "short": {
          "rouge_l_mean": 0.21829689865097576,
          "text_similarity_mean": 0.670920185846106,
          "llm_judge_score_mean": 5.138149856460894
        },
        "cider": {
          "cider_detailed_mean": 0.02906595409973852,
          "cider_short_mean": 0.03544672847967834
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9411764705882353,
          "correct": 96,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.4000871668957517,
            "rouge_l_std": 0.11679140883095027,
            "text_similarity_mean": 0.7904383873238283,
            "text_similarity_std": 0.08852692180065361,
            "llm_judge_score_mean": 9.42156862745098,
            "llm_judge_score_std": 1.7117602138161754
          },
          "rationale_cider": 0.4536507912878533
        },
        "02_Job_Interviews": {
          "accuracy": 0.98,
          "correct": 98,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.37607149078716345,
            "rouge_l_std": 0.12088865245685315,
            "text_similarity_mean": 0.7737454190850258,
            "text_similarity_std": 0.09003163667227343,
            "llm_judge_score_mean": 9.57,
            "llm_judge_score_std": 1.1336225121265013
          },
          "rationale_cider": 0.1793139206256632
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.9521739130434783,
          "correct": 219,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.358061378589774,
            "rouge_l_std": 0.11322596151769616,
            "text_similarity_mean": 0.7867043057213658,
            "text_similarity_std": 0.08835861473943242,
            "llm_judge_score_mean": 9.252173913043478,
            "llm_judge_score_std": 1.7437764030022018
          },
          "rationale_cider": 0.26406937422363586
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.9230769230769231,
          "correct": 36,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.28485826726327484,
            "rouge_l_std": 0.06760603630761919,
            "text_similarity_mean": 0.7685620601360614,
            "text_similarity_std": 0.09962459024829541,
            "llm_judge_score_mean": 8.615384615384615,
            "llm_judge_score_std": 2.1196925831220903
          },
          "rationale_cider": 0.09864588372495227
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9826086956521739,
          "correct": 113,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.3684471777915373,
            "rouge_l_std": 0.11576651940826292,
            "text_similarity_mean": 0.7878272605169079,
            "text_similarity_std": 0.12067026687191618,
            "llm_judge_score_mean": 9.278260869565218,
            "llm_judge_score_std": 1.2617987580672734
          },
          "rationale_cider": 0.4712847920416611
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.8735632183908046,
          "correct": 76,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.28609917074545094,
            "rouge_l_std": 0.09615562448733234,
            "text_similarity_mean": 0.7565155582523894,
            "text_similarity_std": 0.11612656289257002,
            "llm_judge_score_mean": 8.39080459770115,
            "llm_judge_score_std": 2.588019798815172
          },
          "rationale_cider": 0.17329996994199176
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.8235294117647058,
          "correct": 42,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.26645614751453534,
            "rouge_l_std": 0.07637473274067325,
            "text_similarity_mean": 0.740025509221881,
            "text_similarity_std": 0.10323850595918514,
            "llm_judge_score_mean": 7.7254901960784315,
            "llm_judge_score_std": 3.087472672722116
          },
          "rationale_cider": 0.08007541174798269
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 1.0,
          "correct": 67,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.3674231479229126,
            "rouge_l_std": 0.12227576252628602,
            "text_similarity_mean": 0.7765602006841061,
            "text_similarity_std": 0.10361194476354658,
            "llm_judge_score_mean": 9.567164179104477,
            "llm_judge_score_std": 0.8144933522174479
          },
          "rationale_cider": 0.3865848065694783
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.937984496124031,
          "correct": 121,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.3679469880273811,
            "rouge_l_std": 0.1283028786065397,
            "text_similarity_mean": 0.7824666846630185,
            "text_similarity_std": 0.10414107091664605,
            "llm_judge_score_mean": 8.790697674418604,
            "llm_judge_score_std": 2.0333438248731155
          },
          "rationale_cider": 0.24302590782422964
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.9444444444444444,
          "correct": 85,
          "total": 90,
          "rationale": {
            "rouge_l_mean": 0.3419815026983208,
            "rouge_l_std": 0.10992927171500237,
            "text_similarity_mean": 0.7927709539731344,
            "text_similarity_std": 0.1113391094306147,
            "llm_judge_score_mean": 8.766666666666667,
            "llm_judge_score_std": 2.0002777584903333
          },
          "rationale_cider": 0.23919043783468374
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 1.0,
          "correct": 65,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.34286381450625464,
            "rouge_l_std": 0.10074821765835647,
            "text_similarity_mean": 0.797924071091872,
            "text_similarity_std": 0.08572889983053261,
            "llm_judge_score_mean": 9.569230769230769,
            "llm_judge_score_std": 0.8032478450588685
          },
          "rationale_cider": 0.2097057714033707
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.9417989417989417,
          "correct": 178,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.3613231310835102,
            "rouge_l_std": 0.11036114665764568,
            "text_similarity_mean": 0.7815186509852687,
            "text_similarity_std": 0.09951067460755025,
            "llm_judge_score_mean": 9.126984126984127,
            "llm_judge_score_std": 1.62804843237563
          },
          "rationale_cider": 0.31398946393790506
        },
        "13_Olympics": {
          "accuracy": 0.8695652173913043,
          "correct": 60,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.270440678906004,
            "rouge_l_std": 0.0884734775837112,
            "text_similarity_mean": 0.7674428276393724,
            "text_similarity_std": 0.09219249041926204,
            "llm_judge_score_mean": 7.391304347826087,
            "llm_judge_score_std": 2.9003617530709422
          },
          "rationale_cider": 0.22057333133163795
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9361478255596187,
        "rationale": {
          "rouge_l_mean": 0.33785077405629776,
          "text_similarity_mean": 0.7771155299457101,
          "llm_judge_score_mean": 8.881979275650354
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.05019979314086843,
          "std_iou": 0.15486742720751487,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.07063197026022305,
            "count": 19,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.04460966542750929,
            "count": 12,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.01858736059479554,
            "count": 5,
            "total": 269
          },
          "mae": {
            "start_mean": 57.058669144981415,
            "end_mean": 3533.8897063197023,
            "average_mean": 1795.474187732342
          },
          "rationale": {
            "rouge_l_mean": 0.2953847183422093,
            "rouge_l_std": 0.084970390365069,
            "text_similarity_mean": 0.6884478209851843,
            "text_similarity_std": 0.10272012602558761,
            "llm_judge_score_mean": 2.7769516728624537,
            "llm_judge_score_std": 1.6704484854810402
          },
          "rationale_cider": 0.0930178927954836
        },
        "02_Job_Interviews": {
          "mean_iou": 0.06281126096219115,
          "std_iou": 0.17796261027597848,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.08695652173913043,
            "count": 22,
            "total": 253
          },
          "R@0.5": {
            "recall": 0.05533596837944664,
            "count": 14,
            "total": 253
          },
          "R@0.7": {
            "recall": 0.02766798418972332,
            "count": 7,
            "total": 253
          },
          "mae": {
            "start_mean": 58.87799604743083,
            "end_mean": 58.98194861660079,
            "average_mean": 58.929972332015815
          },
          "rationale": {
            "rouge_l_mean": 0.28740887638057094,
            "rouge_l_std": 0.09255716929886333,
            "text_similarity_mean": 0.6984047541034081,
            "text_similarity_std": 0.09611726924143103,
            "llm_judge_score_mean": 2.9446640316205532,
            "llm_judge_score_std": 1.8147673730675635
          },
          "rationale_cider": 0.1035913954002984
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.03079604586141732,
          "std_iou": 0.11308141982829019,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.042435424354243544,
            "count": 23,
            "total": 542
          },
          "R@0.5": {
            "recall": 0.014760147601476014,
            "count": 8,
            "total": 542
          },
          "R@0.7": {
            "recall": 0.007380073800738007,
            "count": 4,
            "total": 542
          },
          "mae": {
            "start_mean": 48.992547970479684,
            "end_mean": 50.71632103321032,
            "average_mean": 49.854434501845
          },
          "rationale": {
            "rouge_l_mean": 0.27024022401236236,
            "rouge_l_std": 0.07732513913994483,
            "text_similarity_mean": 0.6845427879933061,
            "text_similarity_std": 0.09492748772928487,
            "llm_judge_score_mean": 2.6549815498154983,
            "llm_judge_score_std": 1.5537607927656063
          },
          "rationale_cider": 0.08426831613768121
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.03691121038296297,
          "std_iou": 0.13678198304986666,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05172413793103448,
            "count": 6,
            "total": 116
          },
          "R@0.5": {
            "recall": 0.034482758620689655,
            "count": 4,
            "total": 116
          },
          "R@0.7": {
            "recall": 0.017241379310344827,
            "count": 2,
            "total": 116
          },
          "mae": {
            "start_mean": 62.4364224137931,
            "end_mean": 63.01431896551723,
            "average_mean": 62.725370689655165
          },
          "rationale": {
            "rouge_l_mean": 0.2824120309114433,
            "rouge_l_std": 0.07415091085746506,
            "text_similarity_mean": 0.7130639722635006,
            "text_similarity_std": 0.07427545921032126,
            "llm_judge_score_mean": 2.3275862068965516,
            "llm_judge_score_std": 1.2089883350522905
          },
          "rationale_cider": 0.03303794047827465
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.038057915742107894,
          "std_iou": 0.12816600408162263,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.04956268221574344,
            "count": 17,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.029154518950437316,
            "count": 10,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.011661807580174927,
            "count": 4,
            "total": 343
          },
          "mae": {
            "start_mean": 40.75253352769679,
            "end_mean": 41.95436443148689,
            "average_mean": 41.35344897959184
          },
          "rationale": {
            "rouge_l_mean": 0.27432859262260045,
            "rouge_l_std": 0.07899811865984849,
            "text_similarity_mean": 0.7036587452575694,
            "text_similarity_std": 0.12125329767438138,
            "llm_judge_score_mean": 2.63265306122449,
            "llm_judge_score_std": 1.382643647833006
          },
          "rationale_cider": 0.044170541344094066
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.02996694006457039,
          "std_iou": 0.116527514167614,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.04365079365079365,
            "count": 11,
            "total": 252
          },
          "R@0.5": {
            "recall": 0.023809523809523808,
            "count": 6,
            "total": 252
          },
          "R@0.7": {
            "recall": 0.003968253968253968,
            "count": 1,
            "total": 252
          },
          "mae": {
            "start_mean": 43.02369841269841,
            "end_mean": 75.02791269841269,
            "average_mean": 59.02580555555555
          },
          "rationale": {
            "rouge_l_mean": 0.305283810148991,
            "rouge_l_std": 0.08135470796636955,
            "text_similarity_mean": 0.7503931289390912,
            "text_similarity_std": 0.07931038458687802,
            "llm_judge_score_mean": 2.5476190476190474,
            "llm_judge_score_std": 1.3978846993414948
          },
          "rationale_cider": 0.08216461249731075
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.013613272217173629,
          "std_iou": 0.06372164473651731,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.017699115044247787,
            "count": 2,
            "total": 113
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 113
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 113
          },
          "mae": {
            "start_mean": 44.610805309734516,
            "end_mean": 45.04632743362832,
            "average_mean": 44.82856637168142
          },
          "rationale": {
            "rouge_l_mean": 0.27279160058397145,
            "rouge_l_std": 0.07070512069125034,
            "text_similarity_mean": 0.6786084847640147,
            "text_similarity_std": 0.0979602465148236,
            "llm_judge_score_mean": 2.6548672566371683,
            "llm_judge_score_std": 1.5037999743683244
          },
          "rationale_cider": 0.06265768368037619
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.0405721263652345,
          "std_iou": 0.12733821171135085,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05732484076433121,
            "count": 9,
            "total": 157
          },
          "R@0.5": {
            "recall": 0.03184713375796178,
            "count": 5,
            "total": 157
          },
          "R@0.7": {
            "recall": 0.006369426751592357,
            "count": 1,
            "total": 157
          },
          "mae": {
            "start_mean": 58.678668789808924,
            "end_mean": 151.09687898089172,
            "average_mean": 104.88777388535031
          },
          "rationale": {
            "rouge_l_mean": 0.27675195709720685,
            "rouge_l_std": 0.07870025821116142,
            "text_similarity_mean": 0.6914395558985935,
            "text_similarity_std": 0.09841501869504303,
            "llm_judge_score_mean": 2.3503184713375798,
            "llm_judge_score_std": 1.1937536049271826
          },
          "rationale_cider": 0.09156659522902547
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.032053626763395814,
          "std_iou": 0.12392766267212985,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.04639175257731959,
            "count": 18,
            "total": 388
          },
          "R@0.5": {
            "recall": 0.028350515463917526,
            "count": 11,
            "total": 388
          },
          "R@0.7": {
            "recall": 0.005154639175257732,
            "count": 2,
            "total": 388
          },
          "mae": {
            "start_mean": 73.22859536082474,
            "end_mean": 75.52183247422681,
            "average_mean": 74.37521391752577
          },
          "rationale": {
            "rouge_l_mean": 0.2885644318591461,
            "rouge_l_std": 0.08839536827167895,
            "text_similarity_mean": 0.7192499015441874,
            "text_similarity_std": 0.107461416714913,
            "llm_judge_score_mean": 2.252577319587629,
            "llm_judge_score_std": 1.3153019161468669
          },
          "rationale_cider": 0.04628405326818369
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.05555510156190417,
          "std_iou": 0.1512460385831761,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0673076923076923,
            "count": 14,
            "total": 208
          },
          "R@0.5": {
            "recall": 0.04326923076923077,
            "count": 9,
            "total": 208
          },
          "R@0.7": {
            "recall": 0.014423076923076924,
            "count": 3,
            "total": 208
          },
          "mae": {
            "start_mean": 82.10576923076923,
            "end_mean": 109.37236538461539,
            "average_mean": 95.7390673076923
          },
          "rationale": {
            "rouge_l_mean": 0.261873685356086,
            "rouge_l_std": 0.07558559299335749,
            "text_similarity_mean": 0.6726789278193162,
            "text_similarity_std": 0.11320594736206807,
            "llm_judge_score_mean": 2.6778846153846154,
            "llm_judge_score_std": 1.4958568891575945
          },
          "rationale_cider": 0.06991656598652853
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.031633193195012246,
          "std_iou": 0.11399845372973669,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.039473684210526314,
            "count": 6,
            "total": 152
          },
          "R@0.5": {
            "recall": 0.02631578947368421,
            "count": 4,
            "total": 152
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 152
          },
          "mae": {
            "start_mean": 67.16935526315788,
            "end_mean": 68.22278289473684,
            "average_mean": 67.69606907894736
          },
          "rationale": {
            "rouge_l_mean": 0.26844616769625124,
            "rouge_l_std": 0.0830284595768486,
            "text_similarity_mean": 0.6887322246636215,
            "text_similarity_std": 0.10136505116921354,
            "llm_judge_score_mean": 2.710526315789474,
            "llm_judge_score_std": 1.4762665345006052
          },
          "rationale_cider": 0.07722634082743757
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.0417810548282182,
          "std_iou": 0.13670955887173572,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0670995670995671,
            "count": 31,
            "total": 462
          },
          "R@0.5": {
            "recall": 0.030303030303030304,
            "count": 14,
            "total": 462
          },
          "R@0.7": {
            "recall": 0.008658008658008658,
            "count": 4,
            "total": 462
          },
          "mae": {
            "start_mean": 110.1253896103896,
            "end_mean": 81.25270995670994,
            "average_mean": 95.68904978354979
          },
          "rationale": {
            "rouge_l_mean": 0.2733659324271303,
            "rouge_l_std": 0.08213558256028167,
            "text_similarity_mean": 0.6791674613194677,
            "text_similarity_std": 0.10115466256999714,
            "llm_judge_score_mean": 2.935064935064935,
            "llm_judge_score_std": 1.6967322477217088
          },
          "rationale_cider": 0.07333813932779179
        },
        "13_Olympics": {
          "mean_iou": 0.012618821132946989,
          "std_iou": 0.06048687501625176,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.020202020202020204,
            "count": 2,
            "total": 99
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 99
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 99
          },
          "mae": {
            "start_mean": 48.13487878787879,
            "end_mean": 48.366707070707065,
            "average_mean": 48.25079292929293
          },
          "rationale": {
            "rouge_l_mean": 0.2863179441528371,
            "rouge_l_std": 0.07506424089319604,
            "text_similarity_mean": 0.7085690597693125,
            "text_similarity_std": 0.09513817273629427,
            "llm_judge_score_mean": 2.3333333333333335,
            "llm_judge_score_std": 1.2949006435891817
          },
          "rationale_cider": 0.0859734474018579
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.03665925863215413,
        "mae_average": 199.9099810050035,
        "R@0.3": 0.0508046309505287,
        "R@0.5": 0.027864483273608256,
        "R@0.7": 0.009316308534766636,
        "rationale": {
          "rouge_l_mean": 0.2802438439685236,
          "text_similarity_mean": 0.6982274481015825,
          "llm_judge_score_mean": 2.5999252167056404
        }
      }
    }
  }
}