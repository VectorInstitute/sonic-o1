{
  "model": "minicpm-o-2.6",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.15791989379902355,
            "rouge_l_std": 0.02783449281433809,
            "text_similarity_mean": 0.6950965002179146,
            "text_similarity_std": 0.06941687450134958,
            "llm_judge_score_mean": 4.9375,
            "llm_judge_score_std": 1.919269066598011
          },
          "short": {
            "rouge_l_mean": 0.10185036281569146,
            "rouge_l_std": 0.04036852297885348,
            "text_similarity_mean": 0.5717672556638718,
            "text_similarity_std": 0.13978580123165915,
            "llm_judge_score_mean": 3.875,
            "llm_judge_score_std": 1.7633419974582356
          },
          "cider": {
            "cider_detailed": 0.029803928942011153,
            "cider_short": 9.062776208724314e-07
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.1549943723190393,
            "rouge_l_std": 0.03324691436681503,
            "text_similarity_mean": 0.5869202670596895,
            "text_similarity_std": 0.14402794967669835,
            "llm_judge_score_mean": 4.190476190476191,
            "llm_judge_score_std": 2.1295885499997995
          },
          "short": {
            "rouge_l_mean": 0.10229786480470397,
            "rouge_l_std": 0.05387440725534158,
            "text_similarity_mean": 0.5056423005603609,
            "text_similarity_std": 0.15286035350399393,
            "llm_judge_score_mean": 3.6666666666666665,
            "llm_judge_score_std": 2.0079208230769536
          },
          "cider": {
            "cider_detailed": 0.011264094503831158,
            "cider_short": 0.00042254553762291627
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.13835375529728072,
            "rouge_l_std": 0.02554163460765692,
            "text_similarity_mean": 0.601847805082798,
            "text_similarity_std": 0.17075743569889865,
            "llm_judge_score_mean": 4.277777777777778,
            "llm_judge_score_std": 2.5123153454655585
          },
          "short": {
            "rouge_l_mean": 0.10018371400146345,
            "rouge_l_std": 0.05311590060554425,
            "text_similarity_mean": 0.5315046509106954,
            "text_similarity_std": 0.1988854765467706,
            "llm_judge_score_mean": 3.2777777777777777,
            "llm_judge_score_std": 2.0764701767811955
          },
          "cider": {
            "cider_detailed": 0.0037781294854639536,
            "cider_short": 2.1358718782126128e-11
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.14660879809656127,
            "rouge_l_std": 0.02446921990129999,
            "text_similarity_mean": 0.5908027152220409,
            "text_similarity_std": 0.10169345911239219,
            "llm_judge_score_mean": 3.2,
            "llm_judge_score_std": 2.1354156504062622
          },
          "short": {
            "rouge_l_mean": 0.08384693389700883,
            "rouge_l_std": 0.03348199698598676,
            "text_similarity_mean": 0.4694525440533956,
            "text_similarity_std": 0.14275780337477761,
            "llm_judge_score_mean": 1.9333333333333333,
            "llm_judge_score_std": 0.8537498983243799
          },
          "cider": {
            "cider_detailed": 6.458853205345195e-08,
            "cider_short": 1.730930546237192e-06
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.14245901788663878,
            "rouge_l_std": 0.04681752718690547,
            "text_similarity_mean": 0.48581524995657116,
            "text_similarity_std": 0.20214316745229144,
            "llm_judge_score_mean": 3.1538461538461537,
            "llm_judge_score_std": 2.4446536280108777
          },
          "short": {
            "rouge_l_mean": 0.07825050779295131,
            "rouge_l_std": 0.06041955278833854,
            "text_similarity_mean": 0.452728013579662,
            "text_similarity_std": 0.1811997802896769,
            "llm_judge_score_mean": 2.1538461538461537,
            "llm_judge_score_std": 1.0262818510866412
          },
          "cider": {
            "cider_detailed": 0.0002678973439734874,
            "cider_short": 7.858356457201233e-07
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.15970611760314207,
            "rouge_l_std": 0.039306666274795066,
            "text_similarity_mean": 0.5785381674766541,
            "text_similarity_std": 0.15732447600478286,
            "llm_judge_score_mean": 3.35,
            "llm_judge_score_std": 1.9045996954740911
          },
          "short": {
            "rouge_l_mean": 0.10511072384781953,
            "rouge_l_std": 0.04607016299073092,
            "text_similarity_mean": 0.4981545329093933,
            "text_similarity_std": 0.13514985392046683,
            "llm_judge_score_mean": 3.1,
            "llm_judge_score_std": 1.4798648586948742
          },
          "cider": {
            "cider_detailed": 0.02629862731160364,
            "cider_short": 0.0018287703426092667
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.14239363983121875,
            "rouge_l_std": 0.022572253324974857,
            "text_similarity_mean": 0.5229383472885404,
            "text_similarity_std": 0.1136727153298864,
            "llm_judge_score_mean": 2.2142857142857144,
            "llm_judge_score_std": 1.4725377234348787
          },
          "short": {
            "rouge_l_mean": 0.1060891025068583,
            "rouge_l_std": 0.08054667806783257,
            "text_similarity_mean": 0.4302914972816195,
            "text_similarity_std": 0.14763039549209356,
            "llm_judge_score_mean": 2.142857142857143,
            "llm_judge_score_std": 1.4568627181693672
          },
          "cider": {
            "cider_detailed": 0.003256004929427736,
            "cider_short": 1.9099983647360507e-10
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.13387894417544385,
            "rouge_l_std": 0.030478174709726944,
            "text_similarity_mean": 0.5082657085731626,
            "text_similarity_std": 0.22233077541518365,
            "llm_judge_score_mean": 3.1666666666666665,
            "llm_judge_score_std": 1.9075871903765995
          },
          "short": {
            "rouge_l_mean": 0.08685722465094527,
            "rouge_l_std": 0.048078628925964505,
            "text_similarity_mean": 0.4349144430210193,
            "text_similarity_std": 0.2051905809219237,
            "llm_judge_score_mean": 2.1666666666666665,
            "llm_judge_score_std": 1.21335164821342
          },
          "cider": {
            "cider_detailed": 0.014136992899731714,
            "cider_short": 1.464802480303815e-16
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.14917350622309739,
            "rouge_l_std": 0.02792367297953055,
            "text_similarity_mean": 0.49712598447998363,
            "text_similarity_std": 0.1280016933075071,
            "llm_judge_score_mean": 2.875,
            "llm_judge_score_std": 1.3635890143294642
          },
          "short": {
            "rouge_l_mean": 0.08603708557938777,
            "rouge_l_std": 0.03854069621494866,
            "text_similarity_mean": 0.42378266838689643,
            "text_similarity_std": 0.10446566309180216,
            "llm_judge_score_mean": 2.6666666666666665,
            "llm_judge_score_std": 0.9428090415820634
          },
          "cider": {
            "cider_detailed": 0.004462975337730058,
            "cider_short": 0.00206350478043158
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.1529563081435949,
            "rouge_l_std": 0.0219173461246786,
            "text_similarity_mean": 0.5957852537217347,
            "text_similarity_std": 0.16173437913207828,
            "llm_judge_score_mean": 3.130434782608696,
            "llm_judge_score_std": 1.7767666731915257
          },
          "short": {
            "rouge_l_mean": 0.10123110298060381,
            "rouge_l_std": 0.03760244945673548,
            "text_similarity_mean": 0.4848004417574924,
            "text_similarity_std": 0.18310798133173303,
            "llm_judge_score_mean": 2.782608695652174,
            "llm_judge_score_std": 1.3499282344573933
          },
          "cider": {
            "cider_detailed": 0.01306508676711751,
            "cider_short": 0.0002474674292481541
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.15629854881736388,
            "rouge_l_std": 0.038096567378384565,
            "text_similarity_mean": 0.6306699904111716,
            "text_similarity_std": 0.11735839102366534,
            "llm_judge_score_mean": 4.461538461538462,
            "llm_judge_score_std": 1.9851519847021448
          },
          "short": {
            "rouge_l_mean": 0.11846576791318457,
            "rouge_l_std": 0.04280399018490756,
            "text_similarity_mean": 0.6219335473500766,
            "text_similarity_std": 0.09803272455242235,
            "llm_judge_score_mean": 3.769230769230769,
            "llm_judge_score_std": 1.7166087388016458
          },
          "cider": {
            "cider_detailed": 2.543817367804251e-06,
            "cider_short": 0.016515715880062655
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.11703033161148202,
            "rouge_l_std": 0.021318274095431702,
            "text_similarity_mean": 0.3902517110109329,
            "text_similarity_std": 0.14788265339590032,
            "llm_judge_score_mean": 2.0555555555555554,
            "llm_judge_score_std": 1.1290223018278653
          },
          "short": {
            "rouge_l_mean": 0.06471491586930551,
            "rouge_l_std": 0.04248829431574872,
            "text_similarity_mean": 0.34109233174886966,
            "text_similarity_std": 0.1705667845862657,
            "llm_judge_score_mean": 1.5555555555555556,
            "llm_judge_score_std": 0.8958064164776166
          },
          "cider": {
            "cider_detailed": 0.003523326101384092,
            "cider_short": 3.961172493444511e-11
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.16522450966637697,
            "rouge_l_std": 0.03419405286773445,
            "text_similarity_mean": 0.6307390591372615,
            "text_similarity_std": 0.1226480346274087,
            "llm_judge_score_mean": 2.4347826086956523,
            "llm_judge_score_std": 1.0964800185146513
          },
          "short": {
            "rouge_l_mean": 0.10654207694403721,
            "rouge_l_std": 0.04519422966725866,
            "text_similarity_mean": 0.5131650515224623,
            "text_similarity_std": 0.1531535701292537,
            "llm_judge_score_mean": 2.0,
            "llm_judge_score_std": 1.0215078369104984
          },
          "cider": {
            "cider_detailed": 0.05674520092648377,
            "cider_short": 0.009568560215617344
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.14746136488232794,
          "text_similarity_mean": 0.5626766738183427,
          "llm_judge_score_mean": 3.3421433778039127
        },
        "short": {
          "rouge_l_mean": 0.09549826027722777,
          "text_similarity_mean": 0.48301763682660115,
          "llm_judge_score_mean": 2.6992468790963775
        },
        "cider": {
          "cider_detailed_mean": 0.012815759458050625,
          "cider_short_mean": 0.0023576913447211673
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9215686274509803,
          "correct": 94,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.2702236888059469,
            "rouge_l_std": 0.09402658995872817,
            "text_similarity_mean": 0.7420175335570878,
            "text_similarity_std": 0.10482143925326806,
            "llm_judge_score_mean": 8.549019607843137,
            "llm_judge_score_std": 2.31206321144842
          },
          "rationale_cider": 0.16609688056183636
        },
        "02_Job_Interviews": {
          "accuracy": 0.95,
          "correct": 95,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2580216803176492,
            "rouge_l_std": 0.06995559903765076,
            "text_similarity_mean": 0.7248580968379974,
            "text_similarity_std": 0.09637647149682914,
            "llm_judge_score_mean": 8.94,
            "llm_judge_score_std": 1.6841615124446943
          },
          "rationale_cider": 0.18434510455174494
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.9260869565217391,
          "correct": 213,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.2529846254537641,
            "rouge_l_std": 0.07372769561630109,
            "text_similarity_mean": 0.7303403461756913,
            "text_similarity_std": 0.10658679927247929,
            "llm_judge_score_mean": 8.521739130434783,
            "llm_judge_score_std": 1.9774723703317147
          },
          "rationale_cider": 0.14464254670935234
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.8461538461538461,
          "correct": 33,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.2467319569191433,
            "rouge_l_std": 0.08065091457964564,
            "text_similarity_mean": 0.7324834825136722,
            "text_similarity_std": 0.09892968985686369,
            "llm_judge_score_mean": 7.615384615384615,
            "llm_judge_score_std": 2.6565389534214194
          },
          "rationale_cider": 0.14067630183783786
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.8695652173913043,
          "correct": 100,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2571555635634705,
            "rouge_l_std": 0.07778907899812351,
            "text_similarity_mean": 0.7316236217870661,
            "text_similarity_std": 0.12398469752819963,
            "llm_judge_score_mean": 8.373913043478261,
            "llm_judge_score_std": 2.472319346071681
          },
          "rationale_cider": 0.09297703241999312
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.8505747126436781,
          "correct": 74,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.23700345914301105,
            "rouge_l_std": 0.07918765502556697,
            "text_similarity_mean": 0.7173822181320738,
            "text_similarity_std": 0.12940185066269563,
            "llm_judge_score_mean": 7.241379310344827,
            "llm_judge_score_std": 3.2054749042647006
          },
          "rationale_cider": 0.08461818159607958
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.8235294117647058,
          "correct": 42,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.22623678455117516,
            "rouge_l_std": 0.05350260605226065,
            "text_similarity_mean": 0.7124962514522029,
            "text_similarity_std": 0.12390644457077324,
            "llm_judge_score_mean": 7.333333333333333,
            "llm_judge_score_std": 2.961628239679207
          },
          "rationale_cider": 0.14832438125394584
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 0.9552238805970149,
          "correct": 64,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.23552484750566724,
            "rouge_l_std": 0.0597364297163882,
            "text_similarity_mean": 0.7568402463820443,
            "text_similarity_std": 0.11427472066440154,
            "llm_judge_score_mean": 8.611940298507463,
            "llm_judge_score_std": 1.9079974958570898
          },
          "rationale_cider": 0.11031581491264011
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.8062015503875969,
          "correct": 104,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.24273759571834475,
            "rouge_l_std": 0.0714259779453791,
            "text_similarity_mean": 0.7013135284416435,
            "text_similarity_std": 0.13461595170779772,
            "llm_judge_score_mean": 7.255813953488372,
            "llm_judge_score_std": 3.1703057093864135
          },
          "rationale_cider": 0.09348317477100772
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.8235294117647058,
          "correct": 70,
          "total": 85,
          "rationale": {
            "rouge_l_mean": 0.24810578037459208,
            "rouge_l_std": 0.07293413397689982,
            "text_similarity_mean": 0.743838882446289,
            "text_similarity_std": 0.11870944186313181,
            "llm_judge_score_mean": 7.788235294117647,
            "llm_judge_score_std": 2.731490080941854
          },
          "rationale_cider": 0.18106493451975214
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 0.8923076923076924,
          "correct": 58,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.23208682620963894,
            "rouge_l_std": 0.06470240158302064,
            "text_similarity_mean": 0.7443461303527539,
            "text_similarity_std": 0.0991891271360038,
            "llm_judge_score_mean": 8.476923076923077,
            "llm_judge_score_std": 2.3606237266235857
          },
          "rationale_cider": 0.08576200446960208
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.9206349206349206,
          "correct": 174,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.2427443007104362,
            "rouge_l_std": 0.07564357871972868,
            "text_similarity_mean": 0.7094581677484765,
            "text_similarity_std": 0.10242621782337322,
            "llm_judge_score_mean": 8.248677248677248,
            "llm_judge_score_std": 2.329148366412668
          },
          "rationale_cider": 0.09217741399734292
        },
        "13_Olympics": {
          "accuracy": 0.782608695652174,
          "correct": 54,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.22297174990263366,
            "rouge_l_std": 0.07493692451865161,
            "text_similarity_mean": 0.7318898897240127,
            "text_similarity_std": 0.10015861616948918,
            "llm_judge_score_mean": 7.130434782608695,
            "llm_judge_score_std": 3.2744644145674315
          },
          "rationale_cider": 0.11820325915752843
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8744603787131044,
        "rationale": {
          "rouge_l_mean": 0.2440406814750364,
          "text_similarity_mean": 0.7291452611962317,
          "llm_judge_score_mean": 8.006676438087805
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.014482665099741129,
          "std_iou": 0.06561070667351358,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015267175572519083,
            "count": 4,
            "total": 262
          },
          "R@0.5": {
            "recall": 0.003816793893129771,
            "count": 1,
            "total": 262
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 262
          },
          "mae": {
            "start_mean": 658.3624847328246,
            "end_mean": 4226.351893129771,
            "average_mean": 2442.357188931298
          },
          "rationale": {
            "rouge_l_mean": 0.21592267264889473,
            "rouge_l_std": 0.09984178384327684,
            "text_similarity_mean": 0.429159315270976,
            "text_similarity_std": 0.18535207180346194,
            "llm_judge_score_mean": 3.2748091603053435,
            "llm_judge_score_std": 2.2481661232610644
          },
          "rationale_cider": 0.24760826300201494
        },
        "02_Job_Interviews": {
          "mean_iou": 0.03270429233420023,
          "std_iou": 0.1169718734737444,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.052845528455284556,
            "count": 13,
            "total": 246
          },
          "R@0.5": {
            "recall": 0.016260162601626018,
            "count": 4,
            "total": 246
          },
          "R@0.7": {
            "recall": 0.0040650406504065045,
            "count": 1,
            "total": 246
          },
          "mae": {
            "start_mean": 375.5312520325203,
            "end_mean": 368.0999878048781,
            "average_mean": 371.8156199186992
          },
          "rationale": {
            "rouge_l_mean": 0.19791591300200187,
            "rouge_l_std": 0.0911502499817805,
            "text_similarity_mean": 0.3836054064303152,
            "text_similarity_std": 0.1751655011792324,
            "llm_judge_score_mean": 3.3577235772357725,
            "llm_judge_score_std": 2.4697522383252144
          },
          "rationale_cider": 0.24711940037125188
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.02101621231123869,
          "std_iou": 0.09365195083048253,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.030828516377649325,
            "count": 16,
            "total": 519
          },
          "R@0.5": {
            "recall": 0.015414258188824663,
            "count": 8,
            "total": 519
          },
          "R@0.7": {
            "recall": 0.0019267822736030828,
            "count": 1,
            "total": 519
          },
          "mae": {
            "start_mean": 1042.4920211946048,
            "end_mean": 1040.3418901734103,
            "average_mean": 1041.4169556840077
          },
          "rationale": {
            "rouge_l_mean": 0.18424253949729022,
            "rouge_l_std": 0.09147001101708899,
            "text_similarity_mean": 0.38460938602341393,
            "text_similarity_std": 0.18160557040979336,
            "llm_judge_score_mean": 3.277456647398844,
            "llm_judge_score_std": 2.4416292622522064
          },
          "rationale_cider": 0.23368982975716915
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.02139003715609796,
          "std_iou": 0.10943055202629653,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.043478260869565216,
            "count": 5,
            "total": 115
          },
          "R@0.5": {
            "recall": 0.008695652173913044,
            "count": 1,
            "total": 115
          },
          "R@0.7": {
            "recall": 0.008695652173913044,
            "count": 1,
            "total": 115
          },
          "mae": {
            "start_mean": 123.35614782608694,
            "end_mean": 123.16967826086955,
            "average_mean": 123.26291304347826
          },
          "rationale": {
            "rouge_l_mean": 0.18758158640318182,
            "rouge_l_std": 0.0966120418958786,
            "text_similarity_mean": 0.4038185607156028,
            "text_similarity_std": 0.1592596956258718,
            "llm_judge_score_mean": 4.3304347826086955,
            "llm_judge_score_std": 2.515073649644876
          },
          "rationale_cider": 0.30456482296296716
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.022559954737346826,
          "std_iou": 0.09930309494152856,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.026239067055393587,
            "count": 9,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.014577259475218658,
            "count": 5,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0058309037900874635,
            "count": 2,
            "total": 343
          },
          "mae": {
            "start_mean": 654.3455714285714,
            "end_mean": 655.5992099125364,
            "average_mean": 654.9723906705539
          },
          "rationale": {
            "rouge_l_mean": 0.19103853338987728,
            "rouge_l_std": 0.09828893583833948,
            "text_similarity_mean": 0.37901132975218943,
            "text_similarity_std": 0.20043368543184176,
            "llm_judge_score_mean": 3.3323615160349855,
            "llm_judge_score_std": 2.4459824781261874
          },
          "rationale_cider": 0.21764524994225015
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.010637065084053106,
          "std_iou": 0.050960788268986874,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.00796812749003984,
            "count": 2,
            "total": 251
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 251
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 251
          },
          "mae": {
            "start_mean": 238.0745378486056,
            "end_mean": 272.88820318725107,
            "average_mean": 255.48137051792833
          },
          "rationale": {
            "rouge_l_mean": 0.20688072132670066,
            "rouge_l_std": 0.10096164110830645,
            "text_similarity_mean": 0.4355150994907812,
            "text_similarity_std": 0.16464813130063755,
            "llm_judge_score_mean": 4.199203187250996,
            "llm_judge_score_std": 2.5216511385778637
          },
          "rationale_cider": 0.34858011435224673
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.003248835436044256,
          "std_iou": 0.02279342644562709,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "mae": {
            "start_mean": 269.7770693069307,
            "end_mean": 256.21581188118813,
            "average_mean": 262.99644059405944
          },
          "rationale": {
            "rouge_l_mean": 0.16721434109517763,
            "rouge_l_std": 0.08443324034445797,
            "text_similarity_mean": 0.41039216946257223,
            "text_similarity_std": 0.17042061545014478,
            "llm_judge_score_mean": 3.9702970297029703,
            "llm_judge_score_std": 2.7447716114281837
          },
          "rationale_cider": 0.25145116809410545
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.015530412205444426,
          "std_iou": 0.06830434192614128,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.006622516556291391,
            "count": 1,
            "total": 151
          },
          "R@0.5": {
            "recall": 0.006622516556291391,
            "count": 1,
            "total": 151
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 151
          },
          "mae": {
            "start_mean": 301.70353642384106,
            "end_mean": 398.23956291390726,
            "average_mean": 349.9715496688742
          },
          "rationale": {
            "rouge_l_mean": 0.20637120756002944,
            "rouge_l_std": 0.10181989871568056,
            "text_similarity_mean": 0.41927941807157154,
            "text_similarity_std": 0.18894329781785435,
            "llm_judge_score_mean": 3.1125827814569536,
            "llm_judge_score_std": 2.248010294211823
          },
          "rationale_cider": 0.2891809943615627
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.02195987237121976,
          "std_iou": 0.09222722937561278,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.028423772609819122,
            "count": 11,
            "total": 387
          },
          "R@0.5": {
            "recall": 0.012919896640826873,
            "count": 5,
            "total": 387
          },
          "R@0.7": {
            "recall": 0.002583979328165375,
            "count": 1,
            "total": 387
          },
          "mae": {
            "start_mean": 170.82260465116278,
            "end_mean": 171.84018863049096,
            "average_mean": 171.3313966408269
          },
          "rationale": {
            "rouge_l_mean": 0.2098887111144131,
            "rouge_l_std": 0.1000693008660326,
            "text_similarity_mean": 0.4159569602738825,
            "text_similarity_std": 0.1757787462658677,
            "llm_judge_score_mean": 3.449612403100775,
            "llm_judge_score_std": 2.3036693804941986
          },
          "rationale_cider": 0.31747473592305847
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.018502921580203762,
          "std_iou": 0.08526821294456159,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02631578947368421,
            "count": 5,
            "total": 190
          },
          "R@0.5": {
            "recall": 0.005263157894736842,
            "count": 1,
            "total": 190
          },
          "R@0.7": {
            "recall": 0.005263157894736842,
            "count": 1,
            "total": 190
          },
          "mae": {
            "start_mean": 199.97283684210527,
            "end_mean": 231.63778421052635,
            "average_mean": 215.8053105263158
          },
          "rationale": {
            "rouge_l_mean": 0.17715213994727455,
            "rouge_l_std": 0.09483980121028213,
            "text_similarity_mean": 0.36601856495008656,
            "text_similarity_std": 0.1980212222240642,
            "llm_judge_score_mean": 3.9894736842105263,
            "llm_judge_score_std": 2.768220925326001
          },
          "rationale_cider": 0.2050814544576896
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.028428613740708996,
          "std_iou": 0.1129337342340637,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.041379310344827586,
            "count": 6,
            "total": 145
          },
          "R@0.5": {
            "recall": 0.006896551724137931,
            "count": 1,
            "total": 145
          },
          "R@0.7": {
            "recall": 0.006896551724137931,
            "count": 1,
            "total": 145
          },
          "mae": {
            "start_mean": 206.57364137931035,
            "end_mean": 208.70534482758623,
            "average_mean": 207.63949310344827
          },
          "rationale": {
            "rouge_l_mean": 0.1782191645542518,
            "rouge_l_std": 0.08675831569962948,
            "text_similarity_mean": 0.42324410276808616,
            "text_similarity_std": 0.18930155921265673,
            "llm_judge_score_mean": 3.5724137931034483,
            "llm_judge_score_std": 2.5072166944468917
          },
          "rationale_cider": 0.20793654023139063
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.021754607783230472,
          "std_iou": 0.08153436979421272,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.029411764705882353,
            "count": 13,
            "total": 442
          },
          "R@0.5": {
            "recall": 0.004524886877828055,
            "count": 2,
            "total": 442
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 442
          },
          "mae": {
            "start_mean": 484.80385520361995,
            "end_mean": 454.64426470588234,
            "average_mean": 469.72405995475117
          },
          "rationale": {
            "rouge_l_mean": 0.18236998367897836,
            "rouge_l_std": 0.10398725324135792,
            "text_similarity_mean": 0.36370570317534434,
            "text_similarity_std": 0.1949182771607262,
            "llm_judge_score_mean": 3.479638009049774,
            "llm_judge_score_std": 2.4826623891793687
          },
          "rationale_cider": 0.18459453101621895
        },
        "13_Olympics": {
          "mean_iou": 0.0032380463892322134,
          "std_iou": 0.021010887840272236,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "mae": {
            "start_mean": 70.7790705882353,
            "end_mean": 74.8578117647059,
            "average_mean": 72.8184411764706
          },
          "rationale": {
            "rouge_l_mean": 0.20187987505217833,
            "rouge_l_std": 0.09910401779046918,
            "text_similarity_mean": 0.45833102740785653,
            "text_similarity_std": 0.20527056102192195,
            "llm_judge_score_mean": 3.376470588235294,
            "llm_judge_score_std": 2.7050126479566665
          },
          "rationale_cider": 0.16894062287262598
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.018111810479135525,
        "mae_average": 510.7379331100547,
        "R@0.3": 0.023752294577765867,
        "R@0.5": 0.0073070104635794796,
        "R@0.7": 0.0027124667565423264,
        "rationale": {
          "rouge_l_mean": 0.19282133763617304,
          "text_similarity_mean": 0.40558823413789835,
          "llm_judge_score_mean": 3.594036704591875
        }
      }
    }
  }
}