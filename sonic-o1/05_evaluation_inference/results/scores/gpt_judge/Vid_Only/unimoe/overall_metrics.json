{
  "model": "unimoe",
  "experiment_name": "Vid_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.2135031143871741,
            "rouge_l_std": 0.04371812276467782,
            "text_similarity_mean": 0.7352305464446545,
            "text_similarity_std": 0.07497948025917764,
            "llm_judge_score_mean": 4.6875,
            "llm_judge_score_std": 2.171369095755026
          },
          "short": {
            "rouge_l_mean": 0.15862685806887158,
            "rouge_l_std": 0.05122587576357171,
            "text_similarity_mean": 0.6191866155713797,
            "text_similarity_std": 0.1431229101429527,
            "llm_judge_score_mean": 3.8125,
            "llm_judge_score_std": 1.589762167747113
          },
          "cider": {
            "cider_detailed": 0.016941001477675147,
            "cider_short": 3.12721250068559e-06
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.17428913441264363,
            "rouge_l_std": 0.03247737481908311,
            "text_similarity_mean": 0.6203617907705761,
            "text_similarity_std": 0.14129162066277104,
            "llm_judge_score_mean": 4.619047619047619,
            "llm_judge_score_std": 2.419684955411905
          },
          "short": {
            "rouge_l_mean": 0.12955269400837036,
            "rouge_l_std": 0.07137840903179565,
            "text_similarity_mean": 0.5112557467960176,
            "text_similarity_std": 0.1440386658861845,
            "llm_judge_score_mean": 3.8095238095238093,
            "llm_judge_score_std": 2.084387488403742
          },
          "cider": {
            "cider_detailed": 0.005190508834542835,
            "cider_short": 0.01037355794759239
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.15544845343632943,
            "rouge_l_std": 0.03988361143721955,
            "text_similarity_mean": 0.4601102631825667,
            "text_similarity_std": 0.18362758750714617,
            "llm_judge_score_mean": 2.0,
            "llm_judge_score_std": 1.4142135623730951
          },
          "short": {
            "rouge_l_mean": 0.08613465696486526,
            "rouge_l_std": 0.06869238472116433,
            "text_similarity_mean": 0.4339413350591293,
            "text_similarity_std": 0.2406066441088093,
            "llm_judge_score_mean": 2.1538461538461537,
            "llm_judge_score_std": 1.9153691689213435
          },
          "cider": {
            "cider_detailed": 0.0008212820160541216,
            "cider_short": 0.0005070106796157219
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.18108023407871573,
          "text_similarity_mean": 0.6052342001325991,
          "llm_judge_score_mean": 3.768849206349206
        },
        "short": {
          "rouge_l_mean": 0.12477140301403573,
          "text_similarity_mean": 0.5214612324755089,
          "llm_judge_score_mean": 3.2586233211233213
        },
        "cider": {
          "cider_detailed_mean": 0.0076509307760907015,
          "cider_short_mean": 0.003627898613236266
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.8823529411764706,
          "correct": 90,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.2673156607130253,
            "rouge_l_std": 0.08418783472203012,
            "text_similarity_mean": 0.7136285280187925,
            "text_similarity_std": 0.1438794586307367,
            "llm_judge_score_mean": 7.970588235294118,
            "llm_judge_score_std": 3.085245985530836
          },
          "rationale_cider": 0.18734126825373168
        },
        "02_Job_Interviews": {
          "accuracy": 0.83,
          "correct": 83,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2514845976892455,
            "rouge_l_std": 0.07640867473694084,
            "text_similarity_mean": 0.6957167264819145,
            "text_similarity_std": 0.12368408904530377,
            "llm_judge_score_mean": 8.44,
            "llm_judge_score_std": 2.5310867231290195
          },
          "rationale_cider": 0.1376051574937436
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.808695652173913,
          "correct": 93,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2495410114473337,
            "rouge_l_std": 0.07899692020911694,
            "text_similarity_mean": 0.713605348587684,
            "text_similarity_std": 0.1446811457555151,
            "llm_judge_score_mean": 7.904347826086957,
            "llm_judge_score_std": 3.243653101710231
          },
          "rationale_cider": 0.0722429541447219
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8403495311167944,
        "rationale": {
          "rouge_l_mean": 0.25611375661653485,
          "text_similarity_mean": 0.7076502010294637,
          "llm_judge_score_mean": 8.104978687127025
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.013357955003012846,
          "std_iou": 0.044678665866361274,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0038461538461538464,
            "count": 1,
            "total": 260
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 260
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 260
          },
          "mae": {
            "start_mean": 638.804303328827,
            "end_mean": 4256.119351276005,
            "average_mean": 2447.461827302416
          },
          "rationale": {
            "rouge_l_mean": 0.2222635616433116,
            "rouge_l_std": 0.08998723068495255,
            "text_similarity_mean": 0.5186569354461076,
            "text_similarity_std": 0.2007034582096514,
            "llm_judge_score_mean": 2.326923076923077,
            "llm_judge_score_std": 1.8326226881602672
          },
          "rationale_cider": 0.24859170981198625
        },
        "02_Job_Interviews": {
          "mean_iou": 0.029335339050434256,
          "std_iou": 0.09151553158167026,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.031746031746031744,
            "count": 8,
            "total": 252
          },
          "R@0.5": {
            "recall": 0.011904761904761904,
            "count": 3,
            "total": 252
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 252
          },
          "mae": {
            "start_mean": 378.2297769824256,
            "end_mean": 370.29923710403824,
            "average_mean": 374.26450704323196
          },
          "rationale": {
            "rouge_l_mean": 0.2195918123105472,
            "rouge_l_std": 0.08372391884282099,
            "text_similarity_mean": 0.5263393232451071,
            "text_similarity_std": 0.17974091214833635,
            "llm_judge_score_mean": 2.6031746031746033,
            "llm_judge_score_std": 2.1697025071455607
          },
          "rationale_cider": 0.1985406911091619
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.013020664089412323,
          "std_iou": 0.06711218029340588,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.01466275659824047,
            "count": 5,
            "total": 341
          },
          "R@0.5": {
            "recall": 0.005865102639296188,
            "count": 2,
            "total": 341
          },
          "R@0.7": {
            "recall": 0.002932551319648094,
            "count": 1,
            "total": 341
          },
          "mae": {
            "start_mean": 1189.6495188349702,
            "end_mean": 1200.67695458824,
            "average_mean": 1195.1632367116051
          },
          "rationale": {
            "rouge_l_mean": 0.21953959190838496,
            "rouge_l_std": 0.08982819743172438,
            "text_similarity_mean": 0.5334238810492051,
            "text_similarity_std": 0.21439231830058766,
            "llm_judge_score_mean": 2.4252199413489737,
            "llm_judge_score_std": 2.0242975020598486
          },
          "rationale_cider": 0.1390544521357703
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.018571319380953143,
        "mae_average": 1338.9631903524178,
        "R@0.3": 0.016751647396808688,
        "R@0.5": 0.005923288181352698,
        "R@0.7": 0.0009775171065493646,
        "rationale": {
          "rouge_l_mean": 0.22046498862074793,
          "text_similarity_mean": 0.5261400465801399,
          "llm_judge_score_mean": 2.4517725404822177
        }
      }
    }
  }
}