{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 343,
  "aggregated_metrics": {
    "mean_iou": 0.05064466082692683,
    "std_iou": 0.15011236764977398,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.07871720116618076,
      "count": 27,
      "total": 343
    },
    "R@0.5": {
      "recall": 0.043731778425655975,
      "count": 15,
      "total": 343
    },
    "R@0.7": {
      "recall": 0.011661807580174927,
      "count": 4,
      "total": 343
    },
    "mae": {
      "start_mean": 40.48511078717201,
      "end_mean": 41.98221865889213,
      "average_mean": 41.23366472303207
    },
    "rationale": {
      "rouge_l_mean": 0.32314523731532535,
      "rouge_l_std": 0.08155204560242811,
      "text_similarity_mean": 0.7486053441947125,
      "text_similarity_std": 0.08346735249898996,
      "llm_judge_score_mean": 2.673469387755102,
      "llm_judge_score_std": 1.5380770352176956
    },
    "rationale_cider": 0.3228037402892504
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the attorney states they will only proceed with the sentencing hearing for disorderly conduct and not the breach of bail, when does Frank ask if the breach of bail charge is being dropped?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.292,
        "end": 41.433
      },
      "pred_interval": {
        "start": 60.0,
        "end": 62.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.708,
        "end": 21.067,
        "average": 20.3875
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.6076223254203796,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relationship, it gives completely different timestamps for both E1 and E2 that contradict the ground-truth times, so the key factual elements (accurate timestamps) are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Frank states he is being persecuted for exercising his First Amendment rights, when does he say he was found guilty for being loud due to a disability?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 133.165,
        "end": 141.734
      },
      "pred_interval": {
        "start": 120.0,
        "end": 122.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.164999999999992,
        "end": 19.23400000000001,
        "average": 16.1995
      },
      "rationale_metrics": {
        "rouge_l": 0.37974683544303806,
        "text_similarity": 0.782180905342102,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the two speech events and their temporal relation ('after'), but the provided timestamps are substantially inaccurate compared to the reference (both E1 and E2 times are wrong and durations are much shorter), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the attorney asks Frank why he is getting angry, when does Frank respond that he is angry because he is there for nothing?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 119.983,
        "end": 123.427
      },
      "pred_interval": {
        "start": 150.0,
        "end": 152.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.016999999999996,
        "end": 29.072999999999993,
        "average": 29.544999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7393086552619934,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the events and their temporal relation ('after'), but the provided timestamps are substantially different from the ground truth, so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'Frank, you're in a courthouse,' when does the man respond by telling her to arrest him?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 173.35,
        "end": 176.25
      },
      "pred_interval": {
        "start": 155.3,
        "end": 157.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.049999999999983,
        "end": 18.75,
        "average": 18.39999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.5730146169662476,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same utterances but gives times that are significantly different from the reference (off by ~13\u201318s) and mislabels the relation as an 'immediate response' rather than 'after,' so it fails to match the key temporal facts."
      }
    },
    {
      "question_id": "003",
      "question": "Once the text 'Libraries already protected more than one video that YouTube took down' finishes being described by the narrator, when does the text describing YouTube's strike appear?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 169.23,
        "end": 175.0
      },
      "pred_interval": {
        "start": 220.0,
        "end": 222.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.77000000000001,
        "end": 47.19999999999999,
        "average": 48.985
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6041972637176514,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the qualitative relation (immediate follow-up) but the start/end timestamps for both E1 and E2 differ substantially from the reference, so the core factual timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the defendant was convicted of three counts of homicide by intoxicated use of a motor vehicle, when does he mention a count of injury by intoxicated use of a motor vehicle?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 16.341,
        "end": 19.565
      },
      "pred_interval": {
        "start": 27.0,
        "end": 30.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.658999999999999,
        "end": 10.434999999999999,
        "average": 10.546999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.22784810126582278,
        "text_similarity": 0.7096043229103088,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but gives completely different timestamps and an incorrect temporal relation (says 'after' with much later times rather than 'once_finished' immediately after); thus it fails on factual timing and relation despite matching event content."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the defendant was serving a significant prison sentence for those convictions, when does he specify the total sentence length?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.5,
        "end": 46.7
      },
      "pred_interval": {
        "start": 55.0,
        "end": 58.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.5,
        "end": 11.299999999999997,
        "average": 12.399999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7666910886764526,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but both E1 and E2 timestamps are substantially incorrect (and E1 end time is omitted), so it fails to match the ground-truth timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker laments that Tim had a long life ahead of him, when does he state that rehabilitation is one of the reasons people are sent to prison?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.786,
        "end": 207.069
      },
      "pred_interval": {
        "start": 130.0,
        "end": 134.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.786,
        "end": 73.06899999999999,
        "average": 73.4275
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.7159398794174194,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the event timestamps are largely incorrect (both events placed much earlier, E1 end time omitted) and the predicted temporal gap contradicts the noted noticeable gap in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male attorney finishes his address to the court, when does the judge ask if there are representatives of the victim?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 304.214,
        "end": 307.942
      },
      "pred_interval": {
        "start": 150.0,
        "end": 160.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 154.214,
        "end": 147.942,
        "average": 151.078
      },
      "rationale_metrics": {
        "rouge_l": 0.3692307692307692,
        "text_similarity": 0.7708144187927246,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the immediate 'once_finished' relationship, but the timestamps are materially incorrect (150.0s vs correct ~300\u2013307s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge states that the person has to use the microphone, when does the man in the white t-shirt (victim) begin moving to the microphone?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 356.0
      },
      "pred_interval": {
        "start": 165.0,
        "end": 170.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 187.0,
        "end": 186.0,
        "average": 186.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4819277108433735,
        "text_similarity": 0.7380125522613525,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly reports both event timestamps (165.0s vs correct ~350s) and states the victim starts moving at the same time as the judge, contradicting the reference; it only minimally captures a sequential relation but is factually wrong on timing and relation detail."
      }
    },
    {
      "question_id": "003",
      "question": "During the man's speech to the court, when does he say, 'You killed somebody that meant a lot'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 401.276,
        "end": 403.024
      },
      "pred_interval": {
        "start": 200.0,
        "end": 205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 201.276,
        "end": 198.024,
        "average": 199.65
      },
      "rationale_metrics": {
        "rouge_l": 0.24561403508771928,
        "text_similarity": 0.6805480122566223,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels the relationship as occurring within the speech, but it fails to provide the anchor timestamp and gives a clearly incorrect E2 time (200.0s vs the true 401.276\u2013403.024s), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge warns the man in the white shirt about interrupting, when does the judge get up and leave the bench?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.13,
        "end": 331.15
      },
      "pred_interval": {
        "start": 480.0,
        "end": 490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 148.87,
        "end": 158.85000000000002,
        "average": 153.86
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.8250141739845276,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the event timestamps are wildly inaccurate (predicted ~470\u2013490s vs ground truth ~331s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes asking the man if he wishes to address him, when does the man respond?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.38,
        "end": 331.39
      },
      "pred_interval": {
        "start": 515.0,
        "end": 520.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 183.62,
        "end": 188.61,
        "average": 186.115
      },
      "rationale_metrics": {
        "rouge_l": 0.1917808219178082,
        "text_similarity": 0.7424885630607605,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') but misplaces both events with entirely incorrect timestamps and durations, introducing fabricated timing details; because the core temporal locations are wrong, the answer is largely incorrect despite the correct relation label."
      }
    },
    {
      "question_id": "003",
      "question": "After the man in the white shirt states his child's birth date, when does he describe being happy and proud about becoming a parent?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.55,
        "end": 331.58
      },
      "pred_interval": {
        "start": 535.0,
        "end": 540.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 203.45,
        "end": 208.42000000000002,
        "average": 205.935
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.8421000242233276,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation as 'after', but the event timestamps and intervals are substantially incorrect compared to the reference, so key factual details are mismatched."
      }
    },
    {
      "question_id": "001",
      "question": "After the man finishes his emotional statement, when does the woman with long dark hair start walking towards the speaker's table?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.096,
        "end": 512.12
      },
      "pred_interval": {
        "start": 602.5,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.404,
        "end": 92.88,
        "average": 91.642
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.6575722694396973,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation (after) and movement cue right but majorly misstates the event timings (predicts ~602.5s vs. correct ~511.6\u2013512.1s) and adds 'immediately' without support, so it contradicts key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman with long dark hair finishes sitting down at the table, when does she say 'Good afternoon'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.245,
        "end": 512.259
      },
      "pred_interval": {
        "start": 611.0,
        "end": 612.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 98.755,
        "end": 99.74099999999999,
        "average": 99.24799999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7100301384925842,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies a verbal greeting after the sit but gives completely wrong timestamps (~100s off), mislabels the anchor, and downgrades the precise 'once_finished' (immediate) relation to a vague 'after', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman says 'my son did not deserve this', when does she list his family relationships?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 513.109,
        "end": 513.197
      },
      "pred_interval": {
        "start": 618.0,
        "end": 620.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.89099999999996,
        "end": 106.803,
        "average": 105.84699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.6176537871360779,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the listing occurs after the quote, but it gives entirely different timestamps (617s/618s vs 512.593s/513.109\u2013513.197s) and wrongly describes the audio as a continuation rather than the short pause/crying noted in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states she misses the things that irritated her about her son, when does she request life without the possibility of parole?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 779.1,
        "end": 786.0
      },
      "pred_interval": {
        "start": 700.5,
        "end": 702.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.60000000000002,
        "end": 83.29999999999995,
        "average": 80.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.6670119762420654,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect and inconsistent timestamps (E2 at 700.5\u2013702.7s vs reference 779.1\u2013786.0s) and omits E1's correct times; the stated relation 'after' also conflicts with the provided predicted times."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first woman finishes her statement saying 'That's it', when does attorney Koenig begin describing the defendant's difficult upbringing?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 829.7,
        "end": 831.0
      },
      "pred_interval": {
        "start": 715.2,
        "end": 717.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 114.5,
        "end": 113.70000000000005,
        "average": 114.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4102564102564103,
        "text_similarity": 0.7675405144691467,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect event times (E2 at 715.2\u2013717.3 vs correct 829.7\u2013831.0) and omits a numeric time for E1, and its stated relationship ('after') contradicts the supplied predicted timestamps; key factual elements are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After attorney Koenig explains that the defendant did not pursue an NGI defense, when does she state that the NGI defense actually came from her and attorney Zawada?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 900.0
      },
      "pred_interval": {
        "start": 860.4,
        "end": 863.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.600000000000023,
        "end": 36.89999999999998,
        "average": 34.25
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.6093042492866516,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor content and the relation label, but it gives no times for E1 and provides E2 times (860.4\u2013863.1s) that contradict the ground truth ordering and times (E1 ~879\u2013889s, E2 ~892\u2013900s), so the temporal information is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"mental illness\", when does she state that Joshua Skolman needs treatment in the institution?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.573,
        "end": 922.798
      },
      "pred_interval": {
        "start": 922.5,
        "end": 924.0
      },
      "iou": 0.08695652173913043,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.927000000000021,
        "end": 1.2019999999999982,
        "average": 1.5645000000000095
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.667790412902832,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') and roughly locates the target, but it significantly misplaces the anchor (predicts ~920s vs correct ~907s) and slightly shifts the target boundaries, so the timings are semantically inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks Joshua Skolman if he has anything to tell him, when does Skolman explicitly deny having mental illness?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1001.283,
        "end": 1002.784
      },
      "pred_interval": {
        "start": 950.5,
        "end": 952.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.783000000000015,
        "end": 50.78399999999999,
        "average": 50.783500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.7046627402305603,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the denial and the 'after' relationship, but the timestamps are substantially incorrect (predicted ~950s vs. correct ~1000s), so it fails to locate the event accurately."
      }
    },
    {
      "question_id": "003",
      "question": "After Joshua Skolman states \"I don't have mental illness\", when does he next explicitly claim that \"you guys\" have the mental illness or defect?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1006.129,
        "end": 1009.331
      },
      "pred_interval": {
        "start": 965.0,
        "end": 966.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.12900000000002,
        "end": 42.83100000000002,
        "average": 41.98000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.7857400178909302,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship but the anchor and target timestamps and durations are substantially wrong (off by ~40\u201350s) and do not match the ground-truth immediate next occurrence, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the Judge says, 'Thank you, Mr. Scolman', when does he state that only God creates life?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1149.0,
        "end": 1151.0
      },
      "pred_interval": {
        "start": 1110.0,
        "end": 1114.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.0,
        "end": 37.0,
        "average": 38.0
      },
      "rationale_metrics": {
        "rouge_l": 0.41269841269841273,
        "text_similarity": 0.7509536743164062,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterances but gives substantially incorrect timestamps (placing the target at 1110\u20131114s instead of 1149\u20131151s) and the wrong temporal relation (overlap/'immediately after' vs. target occurring later), so it fails on key factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the clerk finishes commanding silence, when does the deputy say 'Be seated'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1109.8,
        "end": 1110.5
      },
      "pred_interval": {
        "start": 1150.0,
        "end": 1152.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.200000000000045,
        "end": 41.5,
        "average": 40.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7273560762405396,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('immediately after') but the timestamps and durations are significantly incorrect (1150.0\u20131152.0 vs. ground truth 1109.6\u20131110.5), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the Judge states 'Every life has value', when does he begin speaking about humanity's 'terrible, terrible dark side'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1164.5,
        "end": 1169.5
      },
      "pred_interval": {
        "start": 1185.0,
        "end": 1190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.5,
        "end": 20.5,
        "average": 20.5
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7160629630088806,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both anchor and target are substantially different from the ground truth (off by ~29s) and the temporal relation is misstated, so it fails to match the correct intervals and relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states he doesn't think it's a mental illness, when does he define a thoughtful conviction?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.453,
        "end": 1238.14
      },
      "pred_interval": {
        "start": 1300.5,
        "end": 1305.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.04700000000003,
        "end": 66.8599999999999,
        "average": 66.95349999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.34375000000000006,
        "text_similarity": 0.6689175367355347,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after'), but both anchor and target timestamps are substantially inaccurate compared to the ground truth, so the key factual elements (correct time spans) are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if he needs to recite historical crimes, when does he state that it causes one to pause and lose their breath?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1259.682,
        "end": 1264.588
      },
      "pred_interval": {
        "start": 1350.2,
        "end": 1355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.51800000000003,
        "end": 90.41200000000003,
        "average": 90.46500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7746363282203674,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering and descriptions right but the timestamps are far off (about 90+ seconds) and the anchor end time is omitted, so it fails to match the ground-truth temporal locations."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge declares that civilized society values life, when does he state that taking a life is the highest crime?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1362.484,
        "end": 1366.753
      },
      "pred_interval": {
        "start": 1405.3,
        "end": 1410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.81600000000003,
        "end": 43.24700000000007,
        "average": 43.03150000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.5564809441566467,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') correct but the timestamps are substantially off (both events shifted ~43\u201346s later) and the anchor end time is omitted; it also omits the camera/audio note, so it fails to match the reference timing and details."
      }
    },
    {
      "question_id": "001",
      "question": "After the inmate first looks down at the paper handed to him, when is he handed paper again by an officer?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1603.0,
        "end": 1603.4
      },
      "pred_interval": {
        "start": 1595.0,
        "end": 1597.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 6.400000000000091,
        "average": 7.2000000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.862839937210083,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event order ('after') and roughly locates the first look, but it places the second handing much earlier (1595.0s vs. 1603.0s), a significant timing error that undermines correctness."
      }
    },
    {
      "question_id": "002",
      "question": "After the inmate turns his head to the side, when does he start walking away, escorted by the officers?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1626.0,
        "end": 1627.0
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1607.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 20.0,
        "average": 20.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.7923500537872314,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the temporal relation ('after') correct, its event timestamps are substantially incorrect (off by ~2\u20133s for E1 and ~21s for E2) and it omits the correct end times, so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the distinct sound of a door or gate opening, when does the inmate walk through the door?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1637.0
      },
      "pred_interval": {
        "start": 1630.0,
        "end": 1632.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 5.0,
        "average": 5.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.8960776329040527,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the correct events and their order ('after') but both timestamps are substantially incorrect (should be ~1603.2\u20131603.3 for E1 and ~1636.0\u20131637.0 for E2) and it misses the large temporal gap, so it is mostly wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states, \"You will never be released from the state prison system,\" when does he mention a \"compass evaluation\" as part of the extended supervision conditions?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1433.0,
        "end": 1436.0
      },
      "pred_interval": {
        "start": 1425.0,
        "end": 1429.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 7.0,
        "average": 7.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3820224719101123,
        "text_similarity": 0.7946205735206604,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation correct ('after') and roughly locates E1, but E2's timestamps are notably earlier (\u22488s start / \u22487s end difference) and do not match the correct interval nor indicate capturing the full phrase, so the localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge mentions that the 25-year sentence for Count 2 will be consecutive to any sentence the defendant is now serving, when does the camera cut to a shot of the back of the defendant's head?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.8,
        "end": 1440.5
      },
      "pred_interval": {
        "start": 1500.0,
        "end": 1505.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.200000000000045,
        "end": 64.5,
        "average": 62.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.7232146859169006,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the timestamps are substantially off (about 60s later) and do not match the precise start/establishment times given in the ground truth, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge orders the $5,000 paid by the Compensation Panel to be part of the restitution order, when does the defendant stand up in the courtroom?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1539.0,
        "end": 1542.0
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.0,
        "end": 68.0,
        "average": 67.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7371715307235718,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relationship, but the timestamps are substantially inaccurate (off by ~135\u2013150 seconds) and the predicted start/end times do not match the reference, so it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the man looks up and turns his head to his left, when do the man and deputies open the door and exit the room?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1642.992
      },
      "gt_interval": {
        "start": 1631.0,
        "end": 1634.0
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1635.0
      },
      "iou": 0.2,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 1.0,
        "average": 6.0
      },
      "rationale_metrics": {
        "rouge_l": 0.396039603960396,
        "text_similarity": 0.7757576704025269,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the sequence ('after') and the visual cue (movement toward door), and its end time roughly matches, but its E1 and E2 start times are substantially incorrect (E1 at ~1610s vs 1625\u20131627s; E2 start at 1620s vs 1631s), so it fails to align with key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "While the male anchor announces a jury has reached a verdict in the Chandler Halderson case, when does the on-screen text 'JURY REACHES VERDICT' first appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 4.6,
        "end": 22.8
      },
      "pred_interval": {
        "start": 10.5,
        "end": 11.0
      },
      "iou": 0.02747252747252747,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.9,
        "end": 11.8,
        "average": 8.850000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.889373242855072,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: the anchor actually covers ~0.03\u20136.6s (not starting at 5.0s) and the on-screen text first appears at 4.6s (and remains) rather than at 10.5\u201311.0s, so stating the text occurs 'after' contradicts the true 'during' overlap."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female anchor states Halderson faces eight charges, when does the on-screen graphic detailing the charges appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.7,
        "end": 35.8
      },
      "pred_interval": {
        "start": 45.0,
        "end": 48.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.3,
        "end": 12.200000000000003,
        "average": 16.75
      },
      "rationale_metrics": {
        "rouge_l": 0.32142857142857145,
        "text_similarity": 0.7916589379310608,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the ground truth by giving completely different timestamps and durations (E1 at 40.0s vs actual finish 23.6s; E2 at 45.0\u201348.0s vs actual 23.7\u201335.8s); only the qualitative 'after' relationship matches."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male anchor says 'Let's actually go to the courtroom now live', when does the judge begin speaking?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.7,
        "end": 204.9
      },
      "pred_interval": {
        "start": 72.0,
        "end": 75.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 131.7,
        "end": 129.9,
        "average": 130.8
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.8272101879119873,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the sequence ('after') but the timestamps and durations are substantially incorrect compared to the ground truth, omitting the key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking if there is anything from the state, when does the state reply?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.02,
        "end": 151.03
      },
      "pred_interval": {
        "start": 325.0,
        "end": 326.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 173.98,
        "end": 175.47,
        "average": 174.725
      },
      "rationale_metrics": {
        "rouge_l": 0.29999999999999993,
        "text_similarity": 0.7722254991531372,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events and their ordering, but the timestamps and durations are grossly incorrect (324\u2013326.5s vs 150.58\u2013151.03s) and the relation label is only loosely equivalent, so it mostly mismatches the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the female reporter states the jury deliberated for just over two hours, when does the male reporter comment on the quick verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.45,
        "end": 152.5
      },
      "pred_interval": {
        "start": 290.0,
        "end": 292.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.55,
        "end": 139.5,
        "average": 138.525
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.5311968326568604,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'after/shortly after' relation but the timestamps are grossly incorrect (288\u2013292s vs. 152.07\u2013152.5s) and durations/matching events do not align, so the answer is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes asking if the jury reached a verdict on all counts, when does the jury foreman respond?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.2,
        "end": 153.2
      },
      "pred_interval": {
        "start": 240.5,
        "end": 242.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.30000000000001,
        "end": 88.80000000000001,
        "average": 88.05000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21212121212121213,
        "text_similarity": 0.7017662525177002,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the roles and sequence roughly right (judge then foreman, relation 'immediately after'), but the timestamps are substantially incorrect and the foreman's timing/duration contradicts the precise ground-truth times, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the foreperson states that the jury reached a verdict, when does the court staff receive the verdict folder?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.2,
        "end": 357.9
      },
      "pred_interval": {
        "start": 335.0,
        "end": 338.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.19999999999999,
        "end": 19.899999999999977,
        "average": 21.049999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.3544303797468355,
        "text_similarity": 0.6831964254379272,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the timestamps are substantially different from the ground truth and durations are incorrect, so it fails to match key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes reading the verdict for Count 1, when does he begin reading the verdict for Count 2?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.7,
        "end": 445.2
      },
      "pred_interval": {
        "start": 353.0,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.69999999999999,
        "end": 90.19999999999999,
        "average": 89.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.7623500227928162,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the qualitative relation ('once') but gives substantially incorrect timestamps and omits the correct immediate start time (441.7s) and the concluding phrase end (445.2s), thus failing on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge finishes reading the verdict for Count 8, when does he state that 'not guilty' verdict forms were also returned?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 630.9,
        "end": 641.0
      },
      "pred_interval": {
        "start": 514.0,
        "end": 516.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 116.89999999999998,
        "end": 125.0,
        "average": 120.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4383561643835617,
        "text_similarity": 0.7728057503700256,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the event timestamps are substantially incorrect and do not match the reference intervals, so it fails on factual alignment of when the judge finished Count 8 and when the 'not guilty' statement occurred."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge finishes reading the final guilty verdict for count 8, when does he begin to inquire if the jury members agreed with the verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 528.9,
        "end": 619.0
      },
      "pred_interval": {
        "start": 540.5,
        "end": 542.0
      },
      "iou": 0.016648168701442836,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.600000000000023,
        "end": 77.0,
        "average": 44.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.5128205128205128,
        "text_similarity": 0.7914729118347168,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially incorrect timestamps (540.5s vs ground-truth 513.0s for E1, and 540.5s vs 528.9s start for E2) and wrongly labels the relationship as 'immediate' despite a ~15.9s gap in the reference; key timing details are thus incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the last juror confirms their agreement to the verdicts, when does the judge begin his speech thanking the jury for their service?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 621.0,
        "end": 665.0
      },
      "pred_interval": {
        "start": 560.0,
        "end": 562.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.0,
        "end": 102.5,
        "average": 81.75
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.6926776170730591,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives both event times (560.0s) that contradict the ground-truth times (E1=617.0s, E2 start=621.0s) and incorrectly states they are simultaneous; it is factually incorrect on all key elements."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes instructing the defendant's table to be seated, when does Attorney Brown make his motion?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 741.0
      },
      "pred_interval": {
        "start": 600.0,
        "end": 602.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.0,
        "end": 139.0,
        "average": 138.0
      },
      "rationale_metrics": {
        "rouge_l": 0.44067796610169496,
        "text_similarity": 0.8007993698120117,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely incorrect times (600.0s vs correct 732.0s anchor and 737.0\u2013741.0s target) and wrongly states the motion is immediate/simultaneous, whereas the motion actually starts 5 seconds after the judge's instruction; major factual discrepancies."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking about any pre-sentence request, when does Attorney Brown state that a pre-sentence investigation should be performed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "pred_interval": {
        "start": 701.0,
        "end": 705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 7.5,
        "average": 6.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3508771929824561,
        "text_similarity": 0.8426655530929565,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that Attorney Brown speaks after the judge, but the timestamps are off by ~5\u20138 seconds and the relation 'after' is less precise than the reference's 'once_finished' (immediate following)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge orders a pre-sentence investigation, when does he specify that there should not be any recommendations made within the report?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.6,
        "end": 754.5
      },
      "pred_interval": {
        "start": 710.0,
        "end": 715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.60000000000002,
        "end": 39.5,
        "average": 39.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8486599922180176,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative relation (E2 immediately follows E1) but both event timestamps are about 40 seconds earlier than the ground truth, so the temporal localization is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the news anchor finishes stating the sentencing hearing dates, when does the District Attorney begin his statement to the media?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 935.0,
        "end": 938.5
      },
      "pred_interval": {
        "start": 720.0,
        "end": 725.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 215.0,
        "end": 213.5,
        "average": 214.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7955129146575928,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering (E2 after E1) but the timestamps and durations are substantially incorrect (off by ~180+ seconds) and it incorrectly asserts the DA spoke 'immediately after' rather than after a ~31s gap, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the system does not work without jurors, when does he mention that the jury of 18 sacrificed a month of their life?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 901.4,
        "end": 908.9
      },
      "pred_interval": {
        "start": 925.0,
        "end": 930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.600000000000023,
        "end": 21.100000000000023,
        "average": 22.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.6611334085464478,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relationship ('after') right but substantially misplaces both anchor and target timestamps (off by ~25\u201330 seconds) and therefore does not match the correct answer's exact timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the District Attorney states that the process had to take a whole week off because of the virus, when does he commend people for sacrificing their time and potentially their safety?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 971.4,
        "end": 982.1
      },
      "pred_interval": {
        "start": 950.0,
        "end": 955.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.399999999999977,
        "end": 27.100000000000023,
        "average": 24.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6978542804718018,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the ground truth (predicted 945\u2013955s vs. actual 970.8\u2013982.1s); although both indicate 'after', the timing and intervals are incorrect, so the answer is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer asks for a message to Barton Krista's family, when does the District Attorney state he will speak to them after?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1027.2,
        "end": 1028.7
      },
      "pred_interval": {
        "start": 1030.0,
        "end": 1035.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7999999999999545,
        "end": 6.2999999999999545,
        "average": 4.5499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.5931929349899292,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates all key timestamps (E1 and E2 times differ significantly from the reference) and thus is factually incorrect, despite correctly indicating an 'after' relationship; it omits the correct start/end times provided in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the Sheriff's Department could get the case to trial in short order, when does he talk about the professionalism and integrity of law enforcement?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1088.6,
        "end": 1095.4
      },
      "pred_interval": {
        "start": 1075.0,
        "end": 1085.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.599999999999909,
        "end": 10.400000000000091,
        "average": 12.0
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.788038432598114,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and that he praises professionalism, but the timestamps and quoted phrasing differ substantially from the ground truth (off by ~10\u201316s and incorrect start/end text), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer asks about the historic significance and challenges of the trial, when does the District Attorney confirm the trial was unprecedented?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.2,
        "end": 1202.0
      },
      "pred_interval": {
        "start": 1100.0,
        "end": 1110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.20000000000005,
        "end": 92.0,
        "average": 96.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.42105263157894735,
        "text_similarity": 0.6965553760528564,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the anchor/target timestamps are off by ~100 seconds (1100s\u20131110s vs 1199.0s\u20131202.0s) and the relation label doesn't match the ground truth 'once_finished'. While the predicted utterance content is similar, the key factual timing and relation are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the District Attorney says they may never know 'the why' behind the crime, when does the news anchor cut in to summarize this point?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1358.6,
        "end": 1367.8
      },
      "pred_interval": {
        "start": 1125.0,
        "end": 1135.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 233.5999999999999,
        "end": 232.79999999999995,
        "average": 233.19999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.9282524585723877,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the anchor summarizes following the DA's remark, but the provided timestamps are substantially wrong and the event boundaries/relation label do not match the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the news anchor finishes stating that the system worked and the verdict graphic is fully shown, when does the narrator begin listing the specific guilty verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1265.0,
        "end": 1275.0
      },
      "pred_interval": {
        "start": 1248.5,
        "end": 1252.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.5,
        "end": 23.0,
        "average": 19.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4383561643835617,
        "text_similarity": 0.8111149072647095,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the rough temporal ordering (narrator after anchor) but the anchor and narrator start/end timestamps are substantially incorrect and the relation label differs from the reference; key timing details (1257.0s, 1265.0s\u20131275.0s) are not matched."
      }
    },
    {
      "question_id": "002",
      "question": "After reporter Jaymes Langrehr finishes saying that the sheer volume of evidence proved to be overwhelming for the jurors, when does he explain that DNA analysts proved bloodstains belonged to the parents?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1350.0,
        "end": 1364.0
      },
      "pred_interval": {
        "start": 1303.5,
        "end": 1307.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.5,
        "end": 57.0,
        "average": 51.75
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.7414001822471619,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the timestamps for both E1 and E2 are substantially incorrect (off by ~30\u201350+ seconds) and the E2 interval does not match the reference, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once reporter Jaymes Langrehr finishes saying there was no one specific piece of evidence that 'really stuck out', when is the next time he mentions bringing in the DNA analysts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1348.0,
        "end": 1352.0
      },
      "pred_interval": {
        "start": 1350.0,
        "end": 1354.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 2.0,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.7820718288421631,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the subsequent mention of DNA analysts but the timestamps are notably inaccurate (E1 off by ~8.5s, E2 shifted ~2s) and the relation 'after' is less precise than the correct 'next' immediate transition."
      }
    },
    {
      "question_id": "001",
      "question": "After MS. NULAND finishes asking \"What's your response to that?\", when does the Sheriff begin responding about his faith in the team?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.47,
        "end": 1430.395
      },
      "pred_interval": {
        "start": 1475.0,
        "end": 1480.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.52999999999997,
        "end": 49.60500000000002,
        "average": 49.067499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7126559019088745,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after') but the provided timestamps are significantly incorrect (about 48s later than the ground truth), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Sheriff finishes explaining what he will remember most about the case, when does a reporter ask about the case informing future work?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1491.452,
        "end": 1494.597
      },
      "pred_interval": {
        "start": 1530.0,
        "end": 1535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.548,
        "end": 40.40300000000002,
        "average": 39.47550000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21212121212121213,
        "text_similarity": 0.7014160752296448,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures a generic 'E2 after E1' ordering but gives substantially incorrect timestamps (off by ~38s) and labels, and it uses the wrong relation ('after' instead of the specific 'once_finished'), so it fails on factual and semantic alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the reporter finishes their commentary thanking Sheriff Calvin Baer and his deputies for their hard work, when does the reporter next mention that \"A lot of resources involved for a very long time\"?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1528.402,
        "end": 1530.927
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.59799999999996,
        "end": 74.07300000000009,
        "average": 72.83550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2077922077922078,
        "text_similarity": 0.6678528785705566,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially incorrect timestamps (off by ~71s) and wrong durations, and labels the relation as 'after' rather than the correct 'next', so it fails to match the reference. "
      }
    },
    {
      "question_id": "001",
      "question": "After the main anchor introduces Tahlil Maudeen, when does Tahlil begin reporting on the emotion in the gallery?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1701.902,
        "end": 1708.327
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.90200000000004,
        "end": 93.327,
        "average": 95.11450000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4827586206896552,
        "text_similarity": 0.8799939751625061,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps are substantially incorrect (off by ~100s) and misstates the anchor timing, omitting the correct start/end times\u2014major factual elements are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the main anchor asks Tahlil if she talked to Chandler Halderson's defense attorneys, when does the anchor next ask about whom Tahlil interviewed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1765.828,
        "end": 1767.07
      },
      "pred_interval": {
        "start": 1675.0,
        "end": 1680.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.82799999999997,
        "end": 87.06999999999994,
        "average": 88.94899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.36923076923076925,
        "text_similarity": 0.8546825647354126,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering ('after/next') but the timestamps for both E1 and E2 are substantially incorrect compared to the reference, so it fails to provide the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After Tahlil Maudeen reports that the defense attorneys were unavailable for interviews, when does she report that the DA seemed pleased with the verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1769.555,
        "end": 1783.597
      },
      "pred_interval": {
        "start": 1705.0,
        "end": 1710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.55500000000006,
        "end": 73.59699999999998,
        "average": 69.07600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.46874999999999994,
        "text_similarity": 0.7338962554931641,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the event order and content (defense unavailable, DA pleased) and relation 'after', but the timestamps are substantially incorrect compared to the ground truth, so it is largely wrong on the requested timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the reporter asks if people were surprised by the quick jury return, when does she explain why they were expecting it?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1789.692,
        "end": 1798.408
      },
      "pred_interval": {
        "start": 1772.5,
        "end": 1778.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.192000000000007,
        "end": 20.4079999999999,
        "average": 18.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.19718309859154928,
        "text_similarity": 0.8205613493919373,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation correct ('after') but the event timestamps are substantially wrong compared to the ground truth (E1/E2 start/end times differ by ~14\u201319s and E2 end time is incorrect), so key factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host says, 'Thank you all,' when does she begin to introduce the website for more information on the case?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1809.891,
        "end": 1815.742
      },
      "pred_interval": {
        "start": 1820.5,
        "end": 1830.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.608999999999924,
        "end": 14.258000000000038,
        "average": 12.433499999999981
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7737375497817993,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their order, but the timestamps are substantially shifted (off by ~9\u201314s) and the relationship 'after' omits the correct 'immediately follows' timing; durations/end times are also incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the host says, 'Thanks for joining us,' when does she announce the return to regular programming?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1830.005,
        "end": 1831.628
      },
      "pred_interval": {
        "start": 1845.0,
        "end": 1850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.99499999999989,
        "end": 18.37200000000007,
        "average": 16.68349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285714,
        "text_similarity": 0.8117981553077698,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and correct ordering but mislocalizes both timestamps by ~14.7s (and gives a different E2 end time) and fails to state that the target 'immediately follows' the anchor, so it is factually incorrect on timing and temporal relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the narrator explains that the judges stopped the video, when does the judge in the embedded video actually stop the video and question the man?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 217.92,
        "end": 221.605
      },
      "pred_interval": {
        "start": 30.0,
        "end": 35.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 187.92,
        "end": 186.605,
        "average": 187.2625
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.7856853008270264,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies E1 and the 'after' relation, but it mislocates E2 by a large margin (predicts ~30\u201335s vs. ground-truth ~217.9\u2013221.6s), so the key temporal annotation is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if the video is counsel for the case, when does the man reply, 'I generated that'?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 224.77,
        "end": 225.951
      },
      "pred_interval": {
        "start": 40.0,
        "end": 42.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 184.77,
        "end": 183.951,
        "average": 184.3605
      },
      "rationale_metrics": {
        "rouge_l": 0.39344262295081966,
        "text_similarity": 0.6252589225769043,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are completely incorrect compared to the reference (predicted E1/E2 at ~30\u201342s vs. ground truth ~218\u2013226s), so it fails to match the key factual timing information."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes telling the man he is not going to use the courtroom as a business launch, when does she instruct him to stand up for oral argument time?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 323.425,
        "end": 328.018
      },
      "pred_interval": {
        "start": 195.0,
        "end": 200.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 128.425,
        "end": 128.01799999999997,
        "average": 128.2215
      },
      "rationale_metrics": {
        "rouge_l": 0.3658536585365854,
        "text_similarity": 0.7497997283935547,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly captures the temporal relation ('after') and the event descriptions, but it gives completely different timestamps for both events compared to the ground truth, so it fails to accurately locate the events in time."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says \"That is not a real person\", when does the judge say it would have been nice to know that information?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 154.321,
        "end": 158.401
      },
      "pred_interval": {
        "start": 154.5,
        "end": 156.7
      },
      "iou": 0.5392156862745053,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.17900000000000205,
        "end": 1.7010000000000218,
        "average": 0.9400000000000119
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7082003355026245,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures the temporal order and provides timestamps that fall within the reference intervals for both events; it omits the anchor end time and gives a slightly shorter end time for the target, so minor temporal details are missing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes describing the man's past verbal conversations with her staff, when does she say she doesn't appreciate being misled?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 177.031,
        "end": 181.351
      },
      "pred_interval": {
        "start": 175.2,
        "end": 176.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.8310000000000173,
        "end": 4.550999999999988,
        "average": 3.1910000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.6661030054092407,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target follows ('once'), but the timestamp offsets are notably inaccurate: the predicted anchor end is ~1.5s early and the predicted target starts ~1.8s earlier and ends ~4.6s earlier than the ground truth, so the target interval is misaligned."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge states that the man is not going to use the courtroom as a launch for his business, when does she command him to \"shut that off\"?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 200.411,
        "end": 201.981
      },
      "pred_interval": {
        "start": 192.5,
        "end": 193.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.911000000000001,
        "end": 8.281000000000006,
        "average": 8.096000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25352112676056343,
        "text_similarity": 0.7774658203125,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the prediction correctly identifies the temporal relation as 'after,' the reported timestamps are substantially incorrect (placing the target within the anchor window rather than after it) and therefore fail to match the reference event boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interrogator asks what kind of objects were used, when does the witness name a toothbrush and shaving utensil?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 150.14,
        "end": 150.22
      },
      "pred_interval": {
        "start": 152.5,
        "end": 154.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3600000000000136,
        "end": 4.5800000000000125,
        "average": 3.470000000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.7837049961090088,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misreports the witness event timing by over 2 seconds and gives incorrect end times, contradicting the correct close temporal adjacency; only the question label matches, so it largely fails to align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interrogator asks if the abuser decided to use something besides a toothbrush, when does the witness answer 'Yes'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.11,
        "end": 151.12
      },
      "pred_interval": {
        "start": 175.3,
        "end": 176.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.189999999999998,
        "end": 25.379999999999995,
        "average": 24.784999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7718082666397095,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the order (witness says 'Yes' after the question) but is highly inaccurate on timestamps and durations (predicted ~173\u2013176.5s vs correct ~151.01\u2013151.12s) and thus fails to localize the events correctly."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interrogator asks what his mom said, when does the witness recount his mother's full response?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.06,
        "end": 153.23
      },
      "pred_interval": {
        "start": 210.0,
        "end": 215.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.94,
        "end": 62.47,
        "average": 59.705
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962022,
        "text_similarity": 0.7837563753128052,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the witness's recounting follows the interrogator's question, but it gives completely different timestamps and durations (208\u2013215.7s vs. 153.03\u2013153.23s), which contradicts the ground truth and indicates major factual errors."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What did your mom say?\", when does the man respond about his mom telling him to stop and that he was exaggerating?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 340.0
      },
      "pred_interval": {
        "start": 375.0,
        "end": 380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 40.0,
        "average": 40.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.8309603929519653,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct relation ('after') and the described content, but the timestamps are substantially incorrect (shifted by ~35\u201340s), which contradicts the key factual timing in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he was afraid to tell anyone because his dad didn't want him to, when does he reveal his dad said it was 'our secret'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 387.0,
        "end": 389.0
      },
      "pred_interval": {
        "start": 420.0,
        "end": 425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 36.0,
        "average": 34.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.8328141570091248,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the events and the 'after' relation, but both event timestamps deviate substantially from the ground truth (off by ~30\u201340s), so the timing is incorrect and only partial credit is warranted."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes asking, \"What did you do to your brother?\", when does the man describe taking him to the woods and playing with a toothbrush in the same way?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 427.0,
        "end": 438.0
      },
      "pred_interval": {
        "start": 480.0,
        "end": 485.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.0,
        "end": 47.0,
        "average": 50.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.7108644247055054,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially different timestamps and a less specific relation ('after' vs 'once_finished'), so it fails on temporal accuracy and relation labeling."
      }
    },
    {
      "question_id": "001",
      "question": "Once Lyle Menendez removes his hand from his mouth/face, when does he put his fingers back into his mouth while crying?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 515.8,
        "end": 517.7
      },
      "pred_interval": {
        "start": 515.0,
        "end": 518.0
      },
      "iou": 0.6333333333333636,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7999999999999545,
        "end": 0.2999999999999545,
        "average": 0.5499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.4395943284034729,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the order right but the timestamps are noticeably off (E1 at 514.0s vs 515.7s correct; E2 start at 515.0s vs 515.8s) and implies a delay rather than the nearly-immediate transition described, so it is only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "While the female voice asks, 'It stopped for you with your dad when you were eight,' when is Erik Menendez first shown on screen?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 536.0,
        "end": 579.0
      },
      "pred_interval": {
        "start": 525.5,
        "end": 526.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.5,
        "end": 52.5,
        "average": 31.5
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.8121461868286133,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and durations differ substantially from the ground truth (predicted E1/E2 around 524\u2013526.5s vs. actual E1 533.5\u2013536.8s and E2 starting at 536.0s and continuing to 579.0s), so it is incorrect about when Erik Menendez first appears and about the event duration/relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After Erik Menendez says 'Yes' in response to doing something about it, when does the female voice ask 'What did you do?'",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 560.0,
        "end": 560.8
      },
      "pred_interval": {
        "start": 565.0,
        "end": 567.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 6.2000000000000455,
        "average": 5.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.31746031746031744,
        "text_similarity": 0.6038541793823242,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the temporal relation (the female voice occurs after), but the reported timestamps and durations are substantially different from the ground truth (off by ~5\u20138s) and it does not match the brief pause/timing details, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the camera shows Lyle Menendez crying and sniffling, when does the female voice ask about the abuse stopping when he was eight?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 533.5,
        "end": 536.5
      },
      "pred_interval": {
        "start": 522.5,
        "end": 525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 11.5,
        "average": 11.25
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.6984107494354248,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' but gives incorrect and inconsistent timestamps (E2 is placed ~11s earlier than the reference and E1 times are omitted/vague), so it fails to match key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "During the female voice asking if he thought it might be happening to someone else, when is Erik Menendez shown with a distressed facial expression?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 539.0,
        "end": 545.8
      },
      "pred_interval": {
        "start": 545.0,
        "end": 548.0
      },
      "iou": 0.08888888888888384,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 2.2000000000000455,
        "average": 4.100000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.325,
        "text_similarity": 0.7138839960098267,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the timings and contradicts the ground truth: correct E1 is 539.0\u2013545.8 with E2 present throughout that interval, whereas the prediction gives E1 ~544.0 (no end) and E2 545.0\u2013548.0, failing to show E2 spanning the entire question and omitting E1's end."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female voice finishes asking 'And who did you think it was happening to?', when does Erik Menendez answer 'Eric'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 551.0,
        "end": 551.5
      },
      "pred_interval": {
        "start": 570.0,
        "end": 572.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 20.5,
        "average": 19.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.6518899202346802,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relation (Erik answers immediately after the question), but the timestamps are significantly off (predicted ~570\u2013572s vs. ground truth ~548.8\u2013551.5s) and durations are incorrect, so the answer is not accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the Presiding Justice asks for the appearance of counsel, when does the appellant's counsel introduce himself?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.401,
        "end": 18.83
      },
      "pred_interval": {
        "start": 5.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.401,
        "end": 8.829999999999998,
        "average": 7.615499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.7031223773956299,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship but the key temporal facts are incorrect: both E1 and E2 start/stop times differ substantially from the ground truth, so the answer is largely factually wrong despite matching the relation."
      }
    },
    {
      "question_id": "002",
      "question": "During the Presiding Justice's question about public importance, when does Mr. Lifrak remain silent and attentive?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.5,
        "end": 103.0
      },
      "pred_interval": {
        "start": 120.0,
        "end": 130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.5,
        "end": 27.0,
        "average": 53.75
      },
      "rationale_metrics": {
        "rouge_l": 0.360655737704918,
        "text_similarity": 0.7675600647926331,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction keeps the same 'during' relation and that Mr. Lifrak is silent/attentive, but the timestamps are incorrect and do not overlap the ground-truth interval (E1 should be 39.5\u2013103.0s), so key factual timing is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Lifrak finishes asking to reserve time for rebuttal, when does the Presiding Justice grant permission?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 109.412,
        "end": 110.2
      },
      "pred_interval": {
        "start": 200.0,
        "end": 205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.588,
        "end": 94.8,
        "average": 92.69399999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3939393939393939,
        "text_similarity": 0.8170573711395264,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'once' temporal relation (Presiding Justice grants permission after Mr. Lifrak's request) but the provided timestamps are substantially incorrect and inconsistent with the reference, so key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (bottom left) mentions Mr. Hothi injected himself into controversies, when does he state that Mr. Hothi said very publicly on Twitter that Mr. Musk and Tesla were lying?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 196.5,
        "end": 201.5
      },
      "pred_interval": {
        "start": 180.5,
        "end": 182.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 19.0,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.710206151008606,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the quoted content and the 'after' relation right, but both anchor and target timestamps are substantially incorrect and do not match the ground-truth intervals, so it fails to localize the events accurately."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker (bottom left) is detailing Mr. Hothi's actions, when does he mention Mr. Hothi hitting an employee with a car?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 283.6,
        "end": 285.5
      },
      "pred_interval": {
        "start": 195.0,
        "end": 197.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.60000000000002,
        "end": 88.5,
        "average": 88.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.19718309859154928,
        "text_similarity": 0.6035716533660889,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and an equivalent relation ('while'), but the timestamps are substantially incorrect (190/195\u2013197s vs correct 278.5/283.6\u2013285.5s), so it fails on critical temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge (top left) finishes expressing her doubt about the relevance of Mr. Hothi's actions, when does the speaker (bottom left) begin his response?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 339.9,
        "end": 350.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 212.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.89999999999998,
        "end": 137.5,
        "average": 133.7
      },
      "rationale_metrics": {
        "rouge_l": 0.5806451612903225,
        "text_similarity": 0.8536771535873413,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the same event types (judge's doubt and speaker's response) but the timestamps are far off and E1/E2 boundaries are incorrect compared to the reference; the relation label ('once' vs 'once_finished') is also not an exact match."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge (top right) asks what if Mr. Musk made an ad hominem attack on Mr. Hothi, when does the lawyer (bottom left) respond by giving examples of unrelated attacks?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 380.5
      },
      "pred_interval": {
        "start": 340.0,
        "end": 350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.19999999999999,
        "end": 30.5,
        "average": 32.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.812721312046051,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the judge's question (anchor) but places the lawyer's response at 340.0\u2013350.0s instead of the correct 374.2\u2013380.5s and incorrectly labels the relationship as 'immediately after,' so the target timing and relation are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge (top center) states that the lawsuit is about Mr. Hothi almost killing Tesla employees, when does the lawyer (bottom left) clarify Mr. Musk's statements regarding Mr. Hothi harassing employees and almost killing one?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 553.0,
        "end": 561.0
      },
      "pred_interval": {
        "start": 365.0,
        "end": 375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 188.0,
        "end": 186.0,
        "average": 187.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.7345616817474365,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: both anchor and target timestamps differ from the reference (predicted ~365s vs correct 479\u2013483s and 553\u2013561s), and the asserted 'immediately after' relationship is not supported by the correct timings."
      }
    },
    {
      "question_id": "003",
      "question": "Once the lawyer (bottom left) finishes explaining why 'almost killed' is not a false statement of fact using a sidewalk analogy, when does the judge (top center) ask if that just means the lawyer might win the lawsuit?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 586.8
      },
      "pred_interval": {
        "start": 410.0,
        "end": 420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 174.0,
        "end": 166.79999999999995,
        "average": 170.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.8376510143280029,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relationship ('immediately after') and that the judge's question follows the lawyer's statement, but it gives completely incorrect timestamps and durations for both anchor and target, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (bottom left) states the problem is that two people would be debating under different rules because Mr. Hothi is protected by anti-SLAPP, when does he state that the target of Mr. Hothi's speech would not have the same protection?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.405,
        "end": 511.559
      },
      "pred_interval": {
        "start": 516.0,
        "end": 520.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.595000000000027,
        "end": 8.440999999999974,
        "average": 6.518000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.699688196182251,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the target follows the anchor, but the timestamps are substantially inaccurate (especially E2) and it fails to reflect the immediate adjacency/causal relation specified in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom right) begins his turn by saying 'I mean, I think that the difficulty, Mr. Lefebvre, that I have is', when does he explain that the argument is based on Mr. Hothi entering the public sphere?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.597,
        "end": 512.074
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.40300000000002,
        "end": 92.92600000000004,
        "average": 90.66450000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.7856463193893433,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering/relationship ('immediately after') but is factually incorrect on key details: the anchor and target timestamps differ substantially from the reference (predicted ~595\u2013605s vs correct ~511.57\u2013512.07s) and the predicted durations/positions do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker (bottom right) finishes giving the example of Mr. Musk commenting on Mr. Houthi's methodology and data, when does he question the relevance of 'almost killing someone in the parking lot'?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.302,
        "end": 512.387
      },
      "pred_interval": {
        "start": 650.0,
        "end": 655.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.69799999999998,
        "end": 142.61300000000006,
        "average": 140.15550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7229889631271362,
        "llm_judge_score": 0,
        "llm_judge_justification": "Error parsing LLM response: Expecting ',' delimiter: line 3 column 73 (char 88)"
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks why this is less attenuated than in the Wilson case, when does he begin explaining the issue?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 703.5
      },
      "pred_interval": {
        "start": 695.0,
        "end": 705.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 1.5,
        "average": 1.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3880597014925374,
        "text_similarity": 0.6772252917289734,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but omits E1 timing, gives E2 timings that conflict with the reference (starting before the reported E1 end and ending later), and labels the relation as 'after' rather than the specified 'once_finished', so it is factually inconsistent and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that their analysis is relevant to 'what he did', when does he give the example of trespassing?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 764.0,
        "end": 768.7
      },
      "pred_interval": {
        "start": 745.0,
        "end": 750.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 18.700000000000045,
        "average": 18.850000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.6380574703216553,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps (E2 at 745\u2013750s vs. correct 764.0\u2013768.7s) and omits E1's time, and it asserts the relation 'after' despite the provided times contradicting the correct chronology\u2014thus largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the presiding justice turns the floor over to the opponent for replies, when does the opponent begin speaking?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 802.5
      },
      "pred_interval": {
        "start": 855.0,
        "end": 860.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.0,
        "end": 57.5,
        "average": 56.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3508771929824562,
        "text_similarity": 0.8237117528915405,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but omits the anchor time (E1) and gives significantly incorrect timestamps for E2 (855.0s\u2013860.0s vs. the correct 800.0s\u2013802.5s), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mr. Greenspan finishes saying that the purpose was not to monitor or harass employees, when does he begin explaining that the person was counting cars off the production line?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.67,
        "end": 1058.61
      },
      "pred_interval": {
        "start": 1082.5,
        "end": 1087.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.829999999999927,
        "end": 28.690000000000055,
        "average": 28.75999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2702702702702703,
        "text_similarity": 0.7796489000320435,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the temporal relation (E2 immediately follows E1), but the provided timestamps are substantially different from the reference (off by ~28\u201330 seconds and mismatched intervals), so the answer is factually incorrect on key timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Greenspan finishes asking if there are any specific questions, when does the Presiding Justice respond by asking the panel if they have questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.969,
        "end": 1135.213
      },
      "pred_interval": {
        "start": 1164.8,
        "end": 1166.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.830999999999904,
        "end": 30.98700000000008,
        "average": 31.908999999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7233799695968628,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('immediately after') correct but the event timestamps are substantially different from the ground truth (off by ~30\u201344 seconds), so the prediction is largely incorrect on the key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Liffrec finishes explaining that the Nidal case was about a university building something in a public park and that was the public debate, when does he state that the university then made statements about protesters inciting violence?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1159.099,
        "end": 1164.925
      },
      "pred_interval": {
        "start": 1217.4,
        "end": 1221.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.30100000000016,
        "end": 56.174999999999955,
        "average": 57.238000000000056
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.6575545072555542,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the temporal relation ('immediately after') but the timestamps substantially diverge from the reference (E1 start omitted and both E1/E2 times are ~57s later), so key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's discussion about harassment, when does he mention \"extensive evidence of harassment\"?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1240.5,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 1305.0,
        "end": 1307.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.5,
        "end": 65.5,
        "average": 65.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.7897194623947144,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event timings (shifted ~64s later) and incorrectly labels the relation as 'after' whereas the correct annotation has E2 occurring during E1, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom left) concludes his argument, when does the Presiding Justice (center top) begin asking if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1345.0,
        "end": 1348.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.215999999999894,
        "end": 48.77099999999996,
        "average": 48.993499999999926
      },
      "rationale_metrics": {
        "rouge_l": 0.29508196721311475,
        "text_similarity": 0.7684157490730286,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation as 'after', but the provided timestamps are substantially different from the ground truth and misalign the events (hallucinated/incorrect times), so it is mostly incorrect. "
      }
    },
    {
      "question_id": "003",
      "question": "After the Justice (top right) states that the Nadel case came out in 1994, when does he ask if the court would have considered things the same way with the Filmon Supreme Court's decision?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1310.105,
        "end": 1318.758
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1323.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.894999999999982,
        "end": 4.241999999999962,
        "average": 7.068499999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.7022891640663147,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer largely misaligns the timestamps (E1/E2 times differ substantially and E1 is given as a start time rather than the referenced end time), though it correctly captures the temporal relation ('after'). The incorrect and inconsistent timing details make it mostly incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'And with that, I'd submit', when does the Presiding Justice ask if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1363.1,
        "end": 1364.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.3159999999998,
        "end": 65.27099999999996,
        "average": 66.29349999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7504303455352783,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the order and near-immediate relation between events, but it gives substantially incorrect timestamps (E1/E2 times are ~68s later than the ground truth), so it is factually inaccurate on the key timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the Presiding Justice asks if there are any other questions, when does Justice Sanchez start speaking about the Nadel case?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1300.609,
        "end": 1302.492
      },
      "pred_interval": {
        "start": 1365.0,
        "end": 1367.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.39100000000008,
        "end": 65.00800000000004,
        "average": 64.69950000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.688866138458252,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that Justice Sanchez speaks about the Nadel case after the Presiding Justice's question, but the timestamps are significantly incorrect and it omits the intermediate brief 'No' and the precise E2 timing, so it is not factually accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After Senator Cruz asks if the same principle applies to other protected characteristics, when does he ask if he could decide he was an Asian man?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 14.506,
        "end": 16.388
      },
      "pred_interval": {
        "start": 12.5,
        "end": 14.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0060000000000002,
        "end": 2.3880000000000017,
        "average": 2.197000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.380952380952381,
        "text_similarity": 0.7418504357337952,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation (following/after) and describes the E2 span conceptually, but it fails to provide the specific timestamps and precise boundaries given in the ground truth, omitting key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Judge Jackson finishes explaining that she is not able to answer hypotheticals, when does Senator Cruz interrupt to re-ask his question about assessing standing?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 30.6,
        "end": 32.439
      },
      "pred_interval": {
        "start": 33.0,
        "end": 35.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3999999999999986,
        "end": 2.561,
        "average": 2.4804999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.7710534334182739,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the order and that Cruz interrupts right after Jackson's comment (paraphrased as 'immediately after'), but it omits the precise timestamps given in the reference (29.7s, 30.6s, 32.439s) and is less specific about the exact start/end times."
      }
    },
    {
      "question_id": "003",
      "question": "While Judge Jackson is explaining how she would assess standing, when does she state that she would 'consider the relevant precedents'?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 45.0,
        "end": 47.8
      },
      "pred_interval": {
        "start": 46.0,
        "end": 48.0
      },
      "iou": 0.5999999999999991,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 0.20000000000000284,
        "average": 0.6000000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.46428571428571425,
        "text_similarity": 0.6644908785820007,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the relation ('during') and the event descriptions, but it omits the key factual timestamps (36.4\u201352.805s and 45.0\u201347.8s) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain why that detective asked her to do it?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 37.335,
        "end": 44.121
      },
      "pred_interval": {
        "start": 47.0,
        "end": 50.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.665,
        "end": 5.878999999999998,
        "average": 7.7719999999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.7119489908218384,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the events, but fails to provide the correct time span for E1 and significantly mislocalizes E2 (47.0\u201350.0s vs the reference 37.335\u201344.121s), so the temporal grounding is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mickey Haller finishes stating that the witness pointed to Detective Lee Langford, when does Langford begin his angry outburst?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 66.887,
        "end": 72.795
      },
      "pred_interval": {
        "start": 59.0,
        "end": 62.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.8870000000000005,
        "end": 10.795000000000002,
        "average": 9.341000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.5923557281494141,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same events but gives completely incorrect timestamps for E2 (59.0\u201362.0s vs correct start 66.887s) and a relation ('after') that contradicts its own timing and the correct 'once_finished' relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Detective Lee Langford's angry outburst about the witness, when does the judge declare the court in recess?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 82.826,
        "end": 85.59
      },
      "pred_interval": {
        "start": 75.0,
        "end": 78.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.825999999999993,
        "end": 7.590000000000003,
        "average": 7.707999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.4150943396226415,
        "text_similarity": 0.6636160612106323,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but it fails to provide the anchor's timestamp and gives substantially incorrect start/end times for the judge's recess compared to the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mickey Haller finishes asking Detective Pettis if she was in the Bonaventure Hotel on October 20th, when does Detective Pettis answer?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 16.239,
        "end": 16.76
      },
      "pred_interval": {
        "start": 44.0,
        "end": 45.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.761,
        "end": 28.24,
        "average": 28.0005
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.8428179025650024,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relationship correct (target immediately follows anchor) but the anchor and target timestamps are substantially incorrect and do not match the ground truth, so the response is largely factually wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain her personal motivation for complying?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 46.707,
        "end": 55.417
      },
      "pred_interval": {
        "start": 55.0,
        "end": 57.0
      },
      "iou": 0.04051296997959794,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.293,
        "end": 1.5829999999999984,
        "average": 4.937999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4675324675324675,
        "text_similarity": 0.7899441719055176,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but mislocates both events substantially: the anchor is given as 52.0s (correct 32.008s) and the target interval 55.0\u201357.0s does not match the correct 46.707\u201355.417s, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Mickey Haller asks if the man Detective Pettis made a deal with is in the courtroom, when does she point him out?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 61.92,
        "end": 62.701
      },
      "pred_interval": {
        "start": 81.0,
        "end": 82.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.08,
        "end": 19.799,
        "average": 19.4395
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.8043709397315979,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and relative order (target after anchor) but the timestamps are substantially incorrect (57.56s vs 80s anchor; 61.92\u201362.70s vs 81\u201382.5s target), so it misaligns the events and adds unsupported timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chaturved introduces Mr. Uday Hula, when does he state that Mr. Uday Hula has become a popular name pan India and beyond?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.33,
        "end": 43.1
      },
      "pred_interval": {
        "start": 15.0,
        "end": 19.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.33,
        "end": 24.1,
        "average": 25.215
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7187485098838806,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation right but both event timestamps are incorrect (E1 predicted ~10s vs 19.992s; E2 predicted 15\u201319s vs 41.33\u201343.1s), so it fails to match key factual information."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Vikas Chaturved says 'Over to you Mr. Trikram', when does Mr. Trikram start speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 153.433,
        "end": 154.776
      },
      "pred_interval": {
        "start": 48.0,
        "end": 49.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.43299999999999,
        "end": 105.77600000000001,
        "average": 105.6045
      },
      "rationale_metrics": {
        "rouge_l": 0.4590163934426229,
        "text_similarity": 0.7973793148994446,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and duration (\u224847\u201349s) are vastly different from the ground truth (151.953s and 153.433\u2013154.776s), and the relation label ('immediate') does not match 'once_finished'; only the ordering (Trikram speaks after Vikas) is similar."
      }
    },
    {
      "question_id": "003",
      "question": "After Mr. Trikram finishes his welcome, when does Mr. Uday Holla begin his speech?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "pred_interval": {
        "start": 105.0,
        "end": 106.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.0,
        "end": 66.0,
        "average": 65.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4074074074074074,
        "text_similarity": 0.7847456932067871,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the 'after' relation, the timestamps and durations are wildly incorrect compared to the reference (147.207s vs 104s for E1 and 169\u2013172s vs 105\u2013106s for E2), so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker emphasizes that facts are important, when does he state that a lawyer must have the patience of a crane?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 354.8
      },
      "pred_interval": {
        "start": 345.0,
        "end": 352.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 2.8000000000000114,
        "average": 4.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363637,
        "text_similarity": 0.7122012376785278,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the events' content and their ordering ('after'), but the timestamps are substantially incorrect for both events (E1 predicted ~15s earlier and E2 ~7s earlier and with a wrong end), so it fails to match the ground-truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that appearing for government becomes more difficult, when does he recount his personal experience as Advocate General?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.1,
        "end": 412.9
      },
      "pred_interval": {
        "start": 398.0,
        "end": 412.0
      },
      "iou": 0.5302013422818784,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.100000000000023,
        "end": 0.8999999999999773,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6291934251785278,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relationship right and roughly locates E2, but both event timestamps are inaccurate: E1 is placed much earlier than the reference (352.0s vs 379.9\u2013383.9s) and E2's start/end are off (398.0\u2013412.0s vs 404.1\u2013412.9s), so key temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a lawyer must know the law to draft better, when does he offer an illustration?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 504.9,
        "end": 506.3
      },
      "pred_interval": {
        "start": 478.0,
        "end": 490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.899999999999977,
        "end": 16.30000000000001,
        "average": 21.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.40625000000000006,
        "text_similarity": 0.6899768114089966,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has major timestamp discrepancies (both E1 and E2 times differ substantially from the reference and E1 end time is missing), so the events are not aligned with the ground truth; it only matches the 'after' relationship but gets the key temporal details wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that there has been an amendment, when does he state that everment is taken out?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.3,
        "end": 532.0
      },
      "pred_interval": {
        "start": 540.0,
        "end": 545.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.700000000000045,
        "end": 13.0,
        "average": 11.850000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.7352247834205627,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and that the target follows the amendment, but the timestamps are substantially incorrect (shifted ~12\u201317s later) and thus contradict the precise ground-truth timings."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that readiness and willingness was mandatory prior to 2018, when does he explicitly state that the suit was liable to be dismissed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.568,
        "end": 583.193
      },
      "pred_interval": {
        "start": 600.0,
        "end": 610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.432000000000016,
        "end": 26.807000000000016,
        "average": 23.619500000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.4819277108433735,
        "text_similarity": 0.762069582939148,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the timestamps for both the anchor and target events are incorrect and do not match the reference intervals, so the prediction is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the balance of the purchase amount, when does he explain the necessity for the plaintiff to prove they had the necessary funds?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 634.565,
        "end": 644.056
      },
      "pred_interval": {
        "start": 670.0,
        "end": 680.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.434999999999945,
        "end": 35.94399999999996,
        "average": 35.68949999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164376,
        "text_similarity": 0.6457752585411072,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the timestamps are substantially incorrect (anchor ~670s vs 628.8\u2013633.7s and target timing ending ~680s vs 634.565\u2013644.056s), so it fails to match the reference timings."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a well-drafted plaint helps portray personality to the judge, when does he begin talking about the second benefit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.9,
        "end": 708.7
      },
      "pred_interval": {
        "start": 714.0,
        "end": 718.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.100000000000023,
        "end": 9.299999999999955,
        "average": 11.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526315,
        "text_similarity": 0.8162331581115723,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') but misstates the key timestamps: the anchor actually ends at 699.7s and the target starts at 700.9s (per ground truth), whereas the prediction gives 714.0s/718.0s, a significant factual mismatch."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a civil case lingers for a number of years, when does he explain that one becomes a senior by the time the case comes up for evidence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 719.5,
        "end": 724.9
      },
      "pred_interval": {
        "start": 765.0,
        "end": 770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.5,
        "end": 45.10000000000002,
        "average": 45.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2784810126582279,
        "text_similarity": 0.7829421758651733,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the anchor/target timestamps are substantially incorrect and do not match the reference segment boundaries, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing client satisfaction with plaint length and associated fees, when does he introduce the strategy of putting facts at the beginning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 795.264,
        "end": 805.511
      },
      "pred_interval": {
        "start": 845.0,
        "end": 850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.73599999999999,
        "end": 44.48900000000003,
        "average": 47.11250000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.34210526315789475,
        "text_similarity": 0.7712541818618774,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets only the coarse ordering ('after') right but the timestamps are significantly incorrect compared to the reference (845s vs 794\u2013795s start and 850s vs 805.511s end), omitting the precise times given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Supreme Court, when does he state the specific paragraph number of the judgment?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 925.404,
        "end": 950.0
      },
      "pred_interval": {
        "start": 930.5,
        "end": 933.0
      },
      "iou": 0.10164254350300861,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.096000000000004,
        "end": 17.0,
        "average": 11.048000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.31746031746031744,
        "text_similarity": 0.8469773530960083,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: anchor and target timestamps differ substantially and the predicted paragraph number ('67') does not match the correct '240'; only the temporal relation 'after' is correct."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing Justice Sanjay Kishan Kaul's judgment, when does he mention the 'Ren and Martin principle of pressie writing'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.581,
        "end": 990.021
      },
      "pred_interval": {
        "start": 960.0,
        "end": 965.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.581000000000017,
        "end": 25.020999999999958,
        "average": 25.800999999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.8576560616493225,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies both events but the timestamps are substantially incorrect for both the anchor (predicts 950.0s vs 972.941\u2013975.001s) and the target (predicts 960.0\u2013965.0s vs 986.581\u2013990.021s), so it fails to match the correct temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using 'short paragraphs, short sentences, direct sentences' in a plaint, when does he warn that a judge will lose patience with lengthy paragraphs?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1005.669,
        "end": 1012.012
      },
      "pred_interval": {
        "start": 1010.0,
        "end": 1015.0
      },
      "iou": 0.21562533490514843,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.331000000000017,
        "end": 2.9880000000000564,
        "average": 3.659500000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.7591913342475891,
        "llm_judge_score": 9,
        "llm_judge_justification": "The predicted answer correctly identifies the anchor and target events and their temporal relation ('after'), and the predicted start times fall within the reference intervals; minor discrepancies are the missing anchor end time and the predicted target end (1015.0s) extends slightly beyond the reference end (1012.01s)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a civil dispute is always before a civil court, when does he emphasize that the lawyer must be thorough with the civil procedure code?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1077.5,
        "end": 1083.7
      },
      "pred_interval": {
        "start": 1085.0,
        "end": 1095.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5,
        "end": 11.299999999999955,
        "average": 9.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.78737872838974,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer preserves the correct temporal relation (target after anchor) but the timestamps are substantially inaccurate: the predicted anchor and target are shifted later and the predicted anchor even falls into the ground-truth target interval, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that settlement will not benefit in the short term, when does he explain how it will benefit in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1250.671,
        "end": 1254.734
      },
      "pred_interval": {
        "start": 1120.0,
        "end": 1130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 130.67100000000005,
        "end": 124.73399999999992,
        "average": 127.70249999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164387,
        "text_similarity": 0.8737931847572327,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the timestamps are substantially different and contradict the ground-truth timecodes, so it fails to match the key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that the entire procedure for civil disputes is in the civil procedure code, when does he mention that states also have civil rules of practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1098.4,
        "end": 1101.7
      },
      "pred_interval": {
        "start": 1160.0,
        "end": 1170.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.59999999999991,
        "end": 68.29999999999995,
        "average": 64.94999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.8133337497711182,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the anchor and target timestamps are significantly different from the ground truth, so the timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how settlement will benefit you, when does he state that it will definitely help in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1238.9,
        "end": 1241.9
      },
      "pred_interval": {
        "start": 1280.0,
        "end": 1285.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.09999999999991,
        "end": 43.09999999999991,
        "average": 42.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.6974706053733826,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same anchor and target events and their order, but the timestamps are substantially different (~40s later), the anchor end time is omitted, and the relation loses the key detail that the target immediately follows the speaker's statement."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker first mentions there's a saying in Kannada, when does he explain the saying about winners and losers in court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1273.6,
        "end": 1278.4
      },
      "pred_interval": {
        "start": 1350.0,
        "end": 1355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 76.40000000000009,
        "end": 76.59999999999991,
        "average": 76.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1846153846153846,
        "text_similarity": 0.683574378490448,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor as the initial mention, the target as the subsequent explanation, and the 'after' relationship, but the provided timestamps are substantially incorrect (off by ~80s) and the anchor end time is missing, so it fails to match the reference timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker begins discussing the drafting of the plaint, when does he advise to draft it in a professional manner?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1319.385,
        "end": 1344.757
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.61500000000001,
        "end": 80.24299999999994,
        "average": 90.42899999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2121212121212121,
        "text_similarity": 0.8370726108551025,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are clearly incorrect (about 100s later) and do not match the reference intervals for E1 or E2, and the predicted E2 duration and relation do not align with the ground truth. This is a near-complete mismatch of key factual elements (timings/intervals)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states 'You have order two, rule two, which specifies that', when does he elaborate on what it specifies?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.147,
        "end": 1451.484
      },
      "pred_interval": {
        "start": 1417.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.146999999999935,
        "end": 26.483999999999924,
        "average": 24.31549999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.7769215106964111,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'when' relation but the event timestamps and boundaries diverge substantially from the reference (anchor and elaboration times are off by tens of seconds), so it fails to match the key temporal details; note the reference also contains an inconsistent end time. "
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'life of the lawyer becomes easier', when does he list Order Six, Seven, and Eight?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1460.096,
        "end": 1410.578
      },
      "pred_interval": {
        "start": 1442.0,
        "end": 1450.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.096000000000004,
        "end": 39.422000000000025,
        "average": 28.759000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.3939393939393939,
        "text_similarity": 0.7122084498405457,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and ordering differ substantially from the reference (predicted times are ~20s earlier and the relation 'after' contradicts the reference timing/ordering), so it fails to match the correct event boundaries and relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Then comes the stage of evidence', when does he explain that a lawyer must sit with clients to understand what kind of evidence to lead?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1553.662,
        "end": 1566.557
      },
      "pred_interval": {
        "start": 1465.0,
        "end": 1473.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.66200000000003,
        "end": 93.55700000000002,
        "average": 91.10950000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272724,
        "text_similarity": 0.7656607627868652,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and intervals conflict substantially with the reference (off by ~58\u201390s) and thus do not match the correct temporal locations; the relationship label is acceptable but the timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the written statement, when does he begin talking about Order 8?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.811,
        "end": 1624.02
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.810999999999922,
        "end": 9.019999999999982,
        "average": 12.415499999999952
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.7876010537147522,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the semantic relation ('after') right but the timestamps are substantially incorrect (both E1 and E2 start/end times differ by >10s from the reference and E1 end is omitted), so it fails to match the key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a general denial is not sufficient, when does he mention the mistake lawyers normally commit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1651.02,
        "end": 1671.11
      },
      "pred_interval": {
        "start": 1625.0,
        "end": 1635.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.019999999999982,
        "end": 36.1099999999999,
        "average": 31.06499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7110819816589355,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the correct 'after' relation, but the reported start/end times are significantly incorrect for both E1 and E2 and E2's duration is far shorter than the reference, so it fails on precise localization."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that a lawyer must know the law, when does he explain the specific areas a lawyer should be thorough with?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.607,
        "end": 1763.816
      },
      "pred_interval": {
        "start": 1650.0,
        "end": 1665.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.60699999999997,
        "end": 98.81600000000003,
        "average": 103.7115
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.6889536380767822,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', its timestamps for both E1 and E2 are substantially different from the reference, so it fails to provide the correct timing of the explanation."
      }
    },
    {
      "question_id": "001",
      "question": "When is the next time the speaker mentions an 'Order six, Rule' after mentioning 'Order six, Rule four'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.2,
        "end": 1830.9
      },
      "pred_interval": {
        "start": 1795.0,
        "end": 1805.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.200000000000045,
        "end": 25.90000000000009,
        "average": 29.050000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.7910873889923096,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies an 'after' relationship but the timestamps are substantially wrong for both events and the second mention does not match the correct 'Order six, Rule eight' at 1827.2\u20131830.9s, so it fails to align with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker explains that fraud and undue influence require specific pleas, when does he state that a general plea is not sufficient?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1802.1,
        "end": 1806.5
      },
      "pred_interval": {
        "start": 1850.0,
        "end": 1860.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.90000000000009,
        "end": 53.5,
        "average": 50.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.6908060312271118,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the relative order ('after') right but the event timestamps are substantially incorrect (off by ~45\u201350s) and it misses that the second event immediately follows the first; therefore it fails to match the ground truth timing and sequencing precisely."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using short sentences and small paragraphs in a written statement, when does he transition to discussing 'evidence'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1908.4,
        "end": 1914.4
      },
      "pred_interval": {
        "start": 1925.0,
        "end": 1935.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.59999999999991,
        "end": 20.59999999999991,
        "average": 18.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.15625,
        "text_similarity": 0.6587557792663574,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the coarse relation ('after') right but the timestamps are substantially off for both segments compared to the reference (predicted times are ~15\u201324s later), so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that leading questions are to be eschewed, when does he advise on how to prepare for examination?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1964.967,
        "end": 1965.937
      },
      "pred_interval": {
        "start": 1982.0,
        "end": 2000.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0329999999999,
        "end": 34.0630000000001,
        "average": 25.548000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6906440258026123,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer preserves the correct semantic relation ('after') and general content about preparing, but the reported timestamps are substantially inaccurate compared to the reference (off by many seconds) and the prediction adds unverified detail about case facts; therefore it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes how some lawyers approach cross-examination without preparation, when does he explain what a good lawyer always does?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2010.4,
        "end": 2018.651
      },
      "pred_interval": {
        "start": 2030.0,
        "end": 2045.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.59999999999991,
        "end": 26.348999999999933,
        "average": 22.97449999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.38888888888888884,
        "text_similarity": 0.73484206199646,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the gist and relation ('once') but the temporal boundaries are substantially incorrect compared to the reference (both E1 and E2 timings and durations mismatch), so it fails the required localization accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's explanation of pitfalls when unprepared for cross-examination, when does he mention forgetting to ask relevant questions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2044.393,
        "end": 2049.878
      },
      "pred_interval": {
        "start": 2100.0,
        "end": 2110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.60699999999997,
        "end": 60.121999999999844,
        "average": 57.86449999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.727515697479248,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event content and the 'during' relation, but the timestamps are substantially shifted from the ground-truth intervals (roughly ~50\u201360s off), so the temporal information is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that cross-examination is an art by itself, when does he state that watching a good cross-examiner will help enormously?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2187.557,
        "end": 2201.817
      },
      "pred_interval": {
        "start": 2155.0,
        "end": 2165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.55699999999979,
        "end": 36.81700000000001,
        "average": 34.6869999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.8500595092773438,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the quoted phrase and that the target follows the anchor, but the anchor and target timestamps are substantially off from the reference (errors of ~50s and ~30s), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker describes law as a noble profession, when does he mention lawyers dedicating themselves to clients to ensure justice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2222.5,
        "end": 2233.2
      },
      "pred_interval": {
        "start": 2200.0,
        "end": 2210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.5,
        "end": 23.199999999999818,
        "average": 22.84999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.7576842904090881,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general content (law as noble, dedication to clients) but the anchor/target time spans are incorrect and the relation ('while') differs from the correct explanation relationship, so it is largely misaligned with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that delays are endemic, when does he provide the reason as a call for settlement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2338.838,
        "end": 2346.208
      },
      "pred_interval": {
        "start": 2230.0,
        "end": 2240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.83800000000019,
        "end": 106.20800000000008,
        "average": 107.52300000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.8566389083862305,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the reason follows the anchor and labels it a 'call for settlement', but the timestamps and spans are substantially incorrect (off by ~110s and missing the anchor end), so it does not match the ground-truth segments."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that delays are endemic, when does he ask to ensure settlements occur?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2346.0
      },
      "pred_interval": {
        "start": 2375.0,
        "end": 2380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.0,
        "end": 34.0,
        "average": 34.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.8103653192520142,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps and event boundaries are substantially off from the reference (roughly 34s later and different durations), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker notes it's already 40 minutes, when does he state that he will give some time for questioning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.8,
        "end": 2371.0
      },
      "pred_interval": {
        "start": 2450.0,
        "end": 2455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.19999999999982,
        "end": 84.0,
        "average": 83.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.5194805194805195,
        "text_similarity": 0.8909651041030884,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target and their order, but the timestamps are substantially shifted from the ground truth, the anchor end time is omitted, and the temporal relation is labeled 'after' rather than the correct 'immediately follows'. These factual/time errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions postponing because he was busy, when does he thank someone for 'pestering' him to educate the lawyer community?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2391.454,
        "end": 2399.123
      },
      "pred_interval": {
        "start": 2500.0,
        "end": 2505.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.54599999999982,
        "end": 105.87699999999995,
        "average": 107.21149999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164387,
        "text_similarity": 0.857606053352356,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the coarse order (thank you occurs after the postponement) but the timestamps are significantly off (~105s later) and it labels the relation as simply 'after' instead of the reference's 'immediately follows', so it fails on key temporal accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "How long does the speaker talk about the importance of reading entire judgments, not just highlighted portions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2578.041,
        "end": 2584.494
      },
      "pred_interval": {
        "start": 2505.0,
        "end": 2530.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.04100000000017,
        "end": 54.49400000000014,
        "average": 63.767500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.21538461538461537,
        "text_similarity": 0.5833438634872437,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly labels the relationship as 'during' but gives start/end times (2505.0\u20132530.0s) that conflict with the ground truth interval (2568.041\u20132578.041s), so the duration is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes saying he will take questions, when does Mr. Vikas begin speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2614.902,
        "end": 2617.184
      },
      "pred_interval": {
        "start": 2685.0,
        "end": 2690.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.09799999999996,
        "end": 72.8159999999998,
        "average": 71.45699999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.7785126566886902,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the event order (anchor then Mr. Vikas) but gives timestamps that are substantially wrong (off by ~88s), labels the relation as merely 'after' instead of a direct transition, and omits the target event end time, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that online law lexicons are not sufficient, when does he directly say 'You must read. A lawyer must read.'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2522.2,
        "end": 2525.3
      },
      "pred_interval": {
        "start": 2635.0,
        "end": 2637.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 112.80000000000018,
        "end": 111.69999999999982,
        "average": 112.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4415584415584415,
        "text_similarity": 0.7514470815658569,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the target line follows immediately, but the provided timestamps are grossly incorrect and do not match the reference, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he would fall asleep reading the Civil Procedure Code as it is, when does he explain that he would be enthusiastic if he had a case on hand?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2688.6,
        "end": 2699.0
      },
      "pred_interval": {
        "start": 2720.0,
        "end": 2728.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.40000000000009,
        "end": 29.0,
        "average": 30.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.7104249000549316,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the provided timestamps are substantially different from the reference (both anchor and target are shifted by ~15\u201330+ seconds), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'Specific relief act, what are the sections?', when does he advise to go to the AR manual?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2720.5,
        "end": 2722.3
      },
      "pred_interval": {
        "start": 2755.0,
        "end": 2763.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.5,
        "end": 40.69999999999982,
        "average": 37.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.47887323943661975,
        "text_similarity": 0.7913827896118164,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relationship, but the timestamps are substantially inaccurate (off by ~29\u201341 seconds) and the target duration is incorrect, which are key factual errors for video-based grounding."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions he suffered from COVID, when does he start describing a doctor's experience during the peak of COVID?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.119,
        "end": 2850.7
      },
      "pred_interval": {
        "start": 2800.0,
        "end": 2812.0
      },
      "iou": 0.09627218934910997,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.119000000000142,
        "end": 38.69999999999982,
        "average": 22.90949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7148882746696472,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relationship right ('after') but provides significantly incorrect timestamps (E1 start off by ~10s, E2 start ~7s early, E2 end different) and omits E1's end time, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Vikas Chatrath finishes asking how to structure arguments, when does Udaya Holla begin to explain what one must always remember?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.46,
        "end": 2963.1
      },
      "pred_interval": {
        "start": 2905.0,
        "end": 2910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.460000000000036,
        "end": 53.09999999999991,
        "average": 32.27999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.6390687227249146,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct speakers and that Udaya explains after Vikas, but the timestamps are substantially off and the temporal relation is labeled 'after' rather than the immediate 'once_finished' (contiguous) indicated in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After Udaya Holla suggests preparing a list of dates and synopsis, when does he mention that the High Court has adopted this practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2941.0,
        "end": 2942.8
      },
      "pred_interval": {
        "start": 2955.0,
        "end": 2960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.0,
        "end": 17.199999999999818,
        "average": 15.599999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.2816901408450704,
        "text_similarity": 0.7484731674194336,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship but gives substantially incorrect timestamps (both anchor and target are shifted ~10\u201320s later than the reference) and adds an unfounded remark about a speech-content shift; thus key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes asking how to specifically plead in a summary suit, when does Udaya Holla ask for clarification 'In a summary suit, is it?'",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2999.596,
        "end": 3000.717
      },
      "pred_interval": {
        "start": 3005.0,
        "end": 3010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.403999999999996,
        "end": 9.282999999999902,
        "average": 7.343499999999949
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.6697863340377808,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the target start time (3005.0s vs 2999.596s) and thus incorrectly characterizes the clarification as occurring later ('after') rather than as an immediate response; it also adds an unverified comment about tone."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes lawyers arguing endlessly for hours on end, when does he state that the judge then sleeps?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3046.2,
        "end": 3047.7
      },
      "pred_interval": {
        "start": 3065.0,
        "end": 3070.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.800000000000182,
        "end": 22.300000000000182,
        "average": 20.550000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.8465019464492798,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relationship, but the provided timestamps are significantly offset from the ground truth (~20s later) and the event boundaries/durations do not match, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker confirms the order number as '37, correct. Sorry.', when does the other speaker (Vikas Chaprath) begin to introduce a common question?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3157.242,
        "end": 3163.028
      },
      "pred_interval": {
        "start": 3150.0,
        "end": 3155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.242000000000189,
        "end": 8.027999999999793,
        "average": 7.634999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.8365257382392883,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the qualitative relation ('after') but the timestamps are significantly inaccurate and inconsistent with the reference (E1 and E2 times differ by several seconds and the predicted intervals do not align with the correct ones), so it fails on factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that preliminary objections concerning maintainability must appear in the beginning of a written statement, when does he begin describing how lawyers often present their defense?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3301.7,
        "end": 3309.9
      },
      "pred_interval": {
        "start": 3200.0,
        "end": 3205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.69999999999982,
        "end": 104.90000000000009,
        "average": 103.29999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.271604938271605,
        "text_similarity": 0.7889981269836426,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but both event time ranges are substantially misaligned with the ground truth (off by ~80\u2013100 seconds), so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker recommends setting out your own facts in the written statement, when does he mention preliminary objections?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3223.3,
        "end": 3224.548
      },
      "pred_interval": {
        "start": 3235.0,
        "end": 3240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.699999999999818,
        "end": 15.452000000000226,
        "average": 13.576000000000022
      },
      "rationale_metrics": {
        "rouge_l": 0.37681159420289856,
        "text_similarity": 0.7161252498626709,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the provided start/end timestamps are substantially incorrect compared to the reference, so the answer is largely temporally inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions misjoinder or non-joinder of parties, when does he list territorial lack of jurisdiction as an objection?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3254.547,
        "end": 3258.914
      },
      "pred_interval": {
        "start": 3260.0,
        "end": 3265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.4529999999999745,
        "end": 6.085999999999785,
        "average": 5.76949999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.41269841269841273,
        "text_similarity": 0.734277069568634,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but the timestamps are substantially shifted and inconsistent with the reference (both start times are later), and the relation 'after' is less precise than the correct 'next' \u2014 overall the answer is mostly incorrect. "
      }
    },
    {
      "question_id": "003",
      "question": "After Vikas finishes asking his question about the Advocate General's experience, when does Udaya Holla begin to respond about the State Council's disabilities?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3417.766,
        "end": 3429.231
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.766000000000076,
        "end": 19.231000000000222,
        "average": 15.99850000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285715,
        "text_similarity": 0.6907140016555786,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the 'after' relation but the event timestamps are substantially off from the ground truth (both start and end times differ by over 12\u201319 seconds), and the predicted duration is incorrect, so it does not accurately match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying the colloquial Kannada phrase, when does he translate it to English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.2,
        "end": 3417.7
      },
      "pred_interval": {
        "start": 3450.0,
        "end": 3455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.80000000000018,
        "end": 37.30000000000018,
        "average": 37.05000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131145,
        "text_similarity": 0.7784639000892639,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the relation that the English translation follows immediately after the Kannada phrase, but it gives significantly different/incorrect timestamps (\u22483450s\u20133455s versus ground-truth 3411.0\u20133417.7s) and lacks the precise start time, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes talking about the difficulties faced by state councils in tough situations, when does the second speaker say 'Vikram'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.82,
        "end": 3472.161
      },
      "pred_interval": {
        "start": 3520.0,
        "end": 3522.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.179999999999836,
        "end": 49.83899999999994,
        "average": 49.00949999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.21875,
        "text_similarity": 0.654778003692627,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering right (second speaker says 'Vikram' after the first finishes) but the timestamps are significantly off (~50s discrepancy) and the relation label differs, so it does not match the reference. "
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises Kannada medium students to acquire more knowledge of English, when does he suggest joining websites for better English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3527.318,
        "end": 3535.0
      },
      "pred_interval": {
        "start": 3580.0,
        "end": 3585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.68199999999979,
        "end": 50.0,
        "average": 51.340999999999894
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.554240345954895,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event ordering (E2 follows E1) but misplaces both timestamps by a large margin and changes the relation to 'immediately after' (where the ground truth has a ~28s gap). These factual timing errors and the stronger temporal claim reduce correctness."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker talks about drafting in Kannada, when does he emphasize having mastery over the Kannada language?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3590.3,
        "end": 3592.0
      },
      "pred_interval": {
        "start": 3640.0,
        "end": 3650.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.69999999999982,
        "end": 58.0,
        "average": 53.84999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.35000000000000003,
        "text_similarity": 0.6700617074966431,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the mastery remark follows the anchor and captures the content, but the timestamps are substantially off (by ~45\u201360s), the target duration is exaggerated, and it fails to note the immediate adjacency\u2014making it largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks about scheduling a lawyer's day, when does the first speaker refer to the question as a 'multi-million dollar question'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3696.0,
        "end": 3697.2
      },
      "pred_interval": {
        "start": 3675.0,
        "end": 3685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 12.199999999999818,
        "average": 16.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.24,
        "text_similarity": 0.5038953423500061,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer only matches the qualitative relation ('after') but the timestamps are substantially incorrect (both anchor and target times differ by >10s and the target is placed much earlier than in the reference), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises taking books on management, when does he mention keeping one's wife happy at home?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3701.248,
        "end": 3706.6
      },
      "pred_interval": {
        "start": 3760.0,
        "end": 3770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.75199999999995,
        "end": 63.40000000000009,
        "average": 61.07600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.5213931798934937,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but the provided timestamps are substantially different from the ground truth (off by tens of seconds), so the anchor/target timing is inaccurate and not aligned with the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises lawyers on how to handle client pleas, when does he suggest setting out facts in the client's presence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.2,
        "end": 3750.22
      },
      "pred_interval": {
        "start": 3805.0,
        "end": 3815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.80000000000018,
        "end": 64.7800000000002,
        "average": 59.79000000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.7395684719085693,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship between the advice and the suggestion, but the provided timestamps are significantly incorrect and the target event duration is inconsistent with the ground truth, so key factual timing is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that a judge would have three drafts of his judgment, when does he elaborate on what the first draft is for?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.51,
        "end": 3750.56
      },
      "pred_interval": {
        "start": 3850.0,
        "end": 3860.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.48999999999978,
        "end": 109.44000000000005,
        "average": 104.46499999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.37681159420289856,
        "text_similarity": 0.8028630018234253,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation, but the timestamps and durations disagree substantially with the reference (off by ~90\u2013110s and mismatched segment boundaries), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Palkiwala's initial lack of work, when does he refer to Justice Chawla's memoir?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3903.539,
        "end": 3917.722
      },
      "pred_interval": {
        "start": 3900.0,
        "end": 3910.0
      },
      "iou": 0.36457510439000734,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5390000000002146,
        "end": 7.722000000000207,
        "average": 5.630500000000211
      },
      "rationale_metrics": {
        "rouge_l": 0.31250000000000006,
        "text_similarity": 0.851065456867218,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted target interval roughly overlaps the true E2 but has incorrect start/end times and is shorter, the anchor time is substantially off (~22s earlier than correct), and the stated relationship ('after') contradicts the ground truth (anchor actually lies within the target)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Make it a habit', when does he mention biting the nail?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3936.731,
        "end": 3942.304
      },
      "pred_interval": {
        "start": 3972.0,
        "end": 3975.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.26899999999978,
        "end": 32.69599999999991,
        "average": 33.982499999999845
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.777296781539917,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the target occurs after the anchor and identifies the nail-biting mention, but the timestamps are substantially incorrect (~33\u201336s off) and it misses that the target occurs directly immediately after the anchor, so key factual details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker finishes requesting the first speaker to talk about staying fit, when does the first speaker repeat 'How to stay fit, physically fit, sir'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3986.204,
        "end": 3987.967
      },
      "pred_interval": {
        "start": 4045.0,
        "end": 4047.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.79599999999982,
        "end": 59.0329999999999,
        "average": 58.91449999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.7794370651245117,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (target occurs after the anchor) but the timestamps are substantially incorrect (off by ~64s) and it omits that the target occurs immediately after the anchor's full completion, so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that 'fitness is absolutely essential in any profession', when does he explain that 'our work transcends the entire waking hours of our time'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4056.898,
        "end": 4064.789
      },
      "pred_interval": {
        "start": 4075.0,
        "end": 4078.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.10199999999986,
        "end": 13.210999999999785,
        "average": 15.656499999999824
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7956596612930298,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the semantic content and the 'after' relationship correct, but the provided timestamps are substantially off from the ground truth (\u224830s difference) and it omits the precise intervals and the GT's note that the target occurs directly after the anchor, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the judge smiling for the first time, when does he explain why the judge smiled?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4157.778,
        "end": 4164.121
      },
      "pred_interval": {
        "start": 4152.0,
        "end": 4160.0
      },
      "iou": 0.18331820806861937,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.778000000000247,
        "end": 4.121000000000095,
        "average": 4.949500000000171
      },
      "rationale_metrics": {
        "rouge_l": 0.31428571428571433,
        "text_similarity": 0.7500211000442505,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after'), but the event timestamps are notably inaccurate (anchor start time differs, anchor end is omitted, and the target start/end times are several seconds off and end earlier than the true completion), so it is incomplete and partially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker uses a cricket analogy involving Kumble, when does he advise the audience to 'Go and observe'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4289.867,
        "end": 4291.509
      },
      "pred_interval": {
        "start": 4185.0,
        "end": 4192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.86700000000019,
        "end": 99.50900000000001,
        "average": 102.1880000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.83577561378479,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the reported timestamps are substantially incorrect (off by ~90\u2013100s) and the anchor's end time from the ground truth is not matched, so the prediction is factually inaccurate despite the right ordering."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker recommends reading Dale Carnegie's book, when does he mention other books on management techniques?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4206.04,
        "end": 4211.851
      },
      "pred_interval": {
        "start": 4210.0,
        "end": 4218.0
      },
      "iou": 0.15476588628759635,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9600000000000364,
        "end": 6.149000000000342,
        "average": 5.054500000000189
      },
      "rationale_metrics": {
        "rouge_l": 0.5063291139240506,
        "text_similarity": 0.898593544960022,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'once' relationship, but it omits the anchor end time and the event timestamps are noticeably misaligned (target starts and ends several seconds later than the ground truth), so it is not precise."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises lawyers not to sit in the canteen, when does he instruct them to sit in a court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4301.616,
        "end": 4305.419
      },
      "pred_interval": {
        "start": 4350.0,
        "end": 4365.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.384000000000015,
        "end": 59.58100000000013,
        "average": 53.98250000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.41666666666666663,
        "text_similarity": 0.7056604027748108,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction grossly mis-localizes both events (timestamps off by tens of seconds and incorrect durations) and does not match the precise E1 end (4301.413s) or E2 start (4301.616s\u20134305.419s); it only correctly indicates a general 'after' relation but is otherwise factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nitika finishes asking her question on YouTube, when does the speaker ask for the question to be repeated?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4378.889,
        "end": 4380.233
      },
      "pred_interval": {
        "start": 4400.0,
        "end": 4405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.110999999999876,
        "end": 24.766999999999825,
        "average": 22.93899999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.7368627190589905,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relationship correct ('after') but the timestamps are substantially inaccurate (E1 predicted ~4395s vs 4377.273s; E2 predicted ~4400\u20134405s vs 4378.889\u20134380.233s), so it fails to correctly locate the events."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'Then which are the sections?' related to the Contract Act, when does he begin describing the illustration about a property purchase agreement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4437.734,
        "end": 4450.995
      },
      "pred_interval": {
        "start": 4440.0,
        "end": 4455.0
      },
      "iou": 0.6368006486737016,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2659999999996217,
        "end": 4.005000000000109,
        "average": 3.1354999999998654
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.6616483926773071,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies that the illustration begins after the question and gives a close start time for E2 (4440s vs 4437.734s), but it significantly misplaces E1 (4430s vs 4402.161s) and slightly overestimates E2's end time, so it is mostly accurate but contains a notable timing error."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he closes his case, when does he explain why?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4475.036,
        "end": 4480.501
      },
      "pred_interval": {
        "start": 4500.5,
        "end": 4515.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.463999999999942,
        "end": 34.498999999999796,
        "average": 29.98149999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.6706783771514893,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the event timestamps are substantially different from the reference and it omits the key detail that the speaker cites the bank statement as the reason, so it is factually and temporally inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises lawyers to practice in trial court, when does he explain the main skill acquired there?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4559.082,
        "end": 4593.185
      },
      "pred_interval": {
        "start": 4550.0,
        "end": 4565.0
      },
      "iou": 0.1370383234919442,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.082000000000335,
        "end": 28.1850000000004,
        "average": 18.633500000000367
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.6521519422531128,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the anchor/start times overlap the ground truth; the predicted target interval encompasses the true target span. Minor boundary inaccuracies (no anchor end given and an earlier predicted target start) prevent a perfect score."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that 'habit would be derived of five words', when does he proceed to list and explain those words?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4629.048,
        "end": 4640.287
      },
      "pred_interval": {
        "start": 4600.0,
        "end": 4620.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.047999999999774,
        "end": 20.287000000000262,
        "average": 24.667500000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.3733333333333333,
        "text_similarity": 0.7396446466445923,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misaligns the timestamps (E1 start ~4s early, E2 starts ~29s earlier and ends ~20s earlier than the reference), omits E1's end time, and uses a different relation label, so it does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking about working parallelly in High Court and Trial Court, when does another speaker give an affirmative answer?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4668.869,
        "end": 4673.473
      },
      "pred_interval": {
        "start": 4675.0,
        "end": 4680.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.131000000000313,
        "end": 6.527000000000044,
        "average": 6.329000000000178
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7941691875457764,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor roughly within the reference E1 interval and the relation ('immediately after' / once_finished) is similar, but the predicted E2 timing and duration do not match the reference (it misplaces and extends the affirmative response), so key temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains how a youngster can gain practical experience in a smaller trial court office, when does he state this would not be possible in a larger office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4722.411,
        "end": 4727.419
      },
      "pred_interval": {
        "start": 4720.0,
        "end": 4725.0
      },
      "iou": 0.3489688637282636,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.411000000000058,
        "end": 2.418999999999869,
        "average": 2.4149999999999636
      },
      "rationale_metrics": {
        "rouge_l": 0.2790697674418605,
        "text_similarity": 0.6948493123054504,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events and the same 'after' relation, with timestamps close to the reference; however it omits the anchor's end time and has small offsets in the target interval compared with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes sharing the Japanese quote about a frog in a well, when does another speaker interject with a related Sanskrit saying?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4760.624,
        "end": 4763.847
      },
      "pred_interval": {
        "start": 4850.0,
        "end": 4855.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.3760000000002,
        "end": 91.15300000000025,
        "average": 90.26450000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.773377537727356,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the order (E2 follows E1) but the timestamps are substantially incorrect (off by ~90s) and the event durations differ, so the factual timing information is largely wrong despite the relation being roughly 'after.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks how a new associate creates space in a big office with many existing associates, when does the host add that the guest also has a very big office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4866.42,
        "end": 4872.823
      },
      "pred_interval": {
        "start": 4835.0,
        "end": 4840.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.420000000000073,
        "end": 32.82300000000032,
        "average": 32.1215000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.7263431549072266,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives substantially incorrect timestamps and durations for both anchor and target (off by ~35+ seconds and mismatched end times), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (guest) finishes explaining that a junior lawyer catches the senior's eye by preparing research and synopses, when does the speaker pose a rhetorical question about how a junior lawyer could get attention by arriving late?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4940.902,
        "end": 4951.577
      },
      "pred_interval": {
        "start": 4905.0,
        "end": 4910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.902000000000044,
        "end": 41.577000000000226,
        "average": 38.739500000000135
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.846703052520752,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but the estimated time intervals are significantly incorrect and do not match the reference spans, so it fails to accurately localize the events."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (guest) emphasizes that hard work is the most important thing in the legal profession, when does the speaker begin to describe a scenario where a client observes their lawyer arguing a case diligently?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4985.39,
        "end": 4996.139
      },
      "pred_interval": {
        "start": 4960.0,
        "end": 4965.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.390000000000327,
        "end": 31.139000000000124,
        "average": 28.264500000000226
      },
      "rationale_metrics": {
        "rouge_l": 0.2711864406779661,
        "text_similarity": 0.9070987701416016,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relation, but the timestamps are substantially off (anchor ~8s earlier and target ~25s earlier than the ground truth), so it does not match the correct temporal intervals."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker emphasizes that youngsters must do hard work, when does the second speaker agree with the point about hard work?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5024.62,
        "end": 5033.21
      },
      "pred_interval": {
        "start": 5070.0,
        "end": 5080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.38000000000011,
        "end": 46.789999999999964,
        "average": 46.085000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.7424229383468628,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the timestamps are significantly off from the ground truth (predicted ~5060\u20135080s vs. actual ~5024\u20135033s) and it adds an unsupported quoted phrase; key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks how to master lower court procedures without much practice, when does the first speaker suggest sitting in courts to learn?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5044.49,
        "end": 5051.81
      },
      "pred_interval": {
        "start": 5120.0,
        "end": 5130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.51000000000022,
        "end": 78.1899999999996,
        "average": 76.84999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.7108207941055298,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the provided timestamps are substantially inaccurate (off by ~70\u201380 seconds) compared to the reference, so it fails on critical temporal correctness."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker asks if one should have High Court experience before practicing in the Supreme Court, when does the first speaker suggest that one is better off going to the trial court first?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5126.422,
        "end": 5141.99
      },
      "pred_interval": {
        "start": 5180.0,
        "end": 5190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.57800000000043,
        "end": 48.01000000000022,
        "average": 50.794000000000324
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.6835920810699463,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps are significantly displaced from the ground truth (off by ~80\u201390 seconds) and thus do not match the anchor/target intervals given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the difference between 'learn' and 'earn', when does he say, 'So thank you everyone'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5198.8,
        "end": 5199.7
      },
      "pred_interval": {
        "start": 5225.0,
        "end": 5227.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.199999999999818,
        "end": 27.300000000000182,
        "average": 26.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6221622824668884,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event ordering and relation correct (immediately after), but the timestamps and durations are substantially off (~26s later and longer), so the temporal details do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker thanks 'Mr. Hola' and mentions it's always a pleasure connecting, when does the second speaker say 'Thank you very much'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5219.7,
        "end": 5221.2
      },
      "pred_interval": {
        "start": 5210.0,
        "end": 5212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.699999999999818,
        "end": 9.199999999999818,
        "average": 9.449999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.3692307692307692,
        "text_similarity": 0.7251043319702148,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the speakers, the utterance 'Thank you very much', and that it follows the main speaker, but the timestamps are off by about 10 seconds and durations differ, and the relation was changed to 'immediately after' instead of the simple 'after', so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker says 'You are the educator of lawyers', when is the next time the first speaker says 'Thank you'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5224.9,
        "end": 5226.9
      },
      "pred_interval": {
        "start": 5229.0,
        "end": 5230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.100000000000364,
        "end": 3.100000000000364,
        "average": 3.600000000000364
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.6937686204910278,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies a subsequent 'Thank you' by the first speaker but gives incorrect timestamps (off by ~4\u20136s) and wrongly states it occurs 'immediately after,' failing to note the intervening second speaker's 'Thank you' at 5223.8s described in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After Trivikram thanks Mr. Vikas Chaturvedi, when does he extend a warm welcome to Mr. Udaya Holla?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.288,
        "end": 166.858
      },
      "pred_interval": {
        "start": 156.0,
        "end": 158.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.288000000000011,
        "end": 8.358000000000004,
        "average": 7.8230000000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.6809846758842468,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the event timestamps are substantially incorrect and inconsistent with the reference (welcome actually occurs ~163\u2013166s, not 156\u2013158.5s), so it fails on factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "During his discussion of strategies for a civil dispute, when does Mr. Udaya Holla state that preparation is what counts the most?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.8,
        "end": 254.67
      },
      "pred_interval": {
        "start": 230.0,
        "end": 232.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.80000000000001,
        "end": 22.669999999999987,
        "average": 22.235
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.6287431120872498,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the statement as part of the broader discussion but gives substantially incorrect timestamps for E2 (230\u2013232s vs the true 251.8\u2013254.67s), so the temporal localization is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After Vikas Chatrath finishes explaining the difference between 'learn' and 'earn', when does he thank everyone and wish them to stay safe and blessed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5198.086,
        "end": 5203.11
      },
      "pred_interval": {
        "start": 5190.0,
        "end": 5195.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.08600000000024,
        "end": 8.109999999999673,
        "average": 8.097999999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6445908546447754,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the reference times and order: it places the thanking segment earlier (5190\u20135195s) and labels the relationship as 'immediate', whereas the correct timing has the thank-you occurring after the explanation (5198.086\u20135203.11s); key factual timing and order are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the announcement of the next session, when does Vikas Chatrath mention Mr. Shingar Murali?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5207.213,
        "end": 5209.213
      },
      "pred_interval": {
        "start": 5215.0,
        "end": 5220.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.787000000000262,
        "end": 10.787000000000262,
        "average": 9.287000000000262
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7683910131454468,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the mention occurs during the announcement but gives substantially incorrect timestamps (5215\u20135220s vs. ground-truth 5207.213\u20135209.213), misplacing the event by ~8\u201311 seconds, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes telling everyone to stay safe and blessed, when does he say it's always a pleasure connecting with Mr. Hola?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5201.609,
        "end": 5204.971
      },
      "pred_interval": {
        "start": 5195.0,
        "end": 5200.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.609000000000378,
        "end": 4.970999999999549,
        "average": 5.789999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413793,
        "text_similarity": 0.5867915153503418,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterance but gives timestamps that conflict with the reference (predicted E2 occurs before the stated E1 finish and far earlier than the true E2 window) and omits the detail about 'and Thrikram and associates', so the timing and relationship are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the prosecutor finishes introducing his opening statement, when does he explain why he gets to go first?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.329,
        "end": 50.118
      },
      "pred_interval": {
        "start": 10.0,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.329,
        "end": 35.118,
        "average": 34.2235
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.7472736835479736,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the reference (5.0s/10.0\u201315.0s vs 41.646s/43.329\u201350.118s) and it fails to state that the target immediately follows the anchor, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the prosecutor mentions the bartender, John Thomas, and the server, Roberta Jones, when does John observe Mr. Miller pass a small glass bottle?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 150.298,
        "end": 158.469
      },
      "pred_interval": {
        "start": 45.0,
        "end": 50.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.298,
        "end": 108.469,
        "average": 106.8835
      },
      "rationale_metrics": {
        "rouge_l": 0.2933333333333334,
        "text_similarity": 0.6924172043800354,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives entirely incorrect anchor and target timestamps/durations compared to the reference, so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once John observes the unknown man pull a gun on the defendant, when does the gunman pull out his shield and shout 'Police! You are under arrest!'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 176.477,
        "end": 185.645
      },
      "pred_interval": {
        "start": 100.0,
        "end": 105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 76.477,
        "end": 80.64500000000001,
        "average": 78.561
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.814897894859314,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct, but the timestamps and durations are wildly incorrect\u2014both E1 and E2 times differ substantially from the reference (174.915s and 176.477\u2013185.645s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "001",
      "question": "After John observes Mr. Miller pass a small glass bottle with white powder, when does John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.6,
        "end": 166.8
      },
      "pred_interval": {
        "start": 210.0,
        "end": 215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.400000000000006,
        "end": 48.19999999999999,
        "average": 47.3
      },
      "rationale_metrics": {
        "rouge_l": 0.37209302325581395,
        "text_similarity": 0.8287758827209473,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relation ('after') but gives substantially incorrect event timestamps/durations and adds an unfounded visual cue (hallucination), so it only partially matches the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the officer gives chase and the defendant pushes him, when does the officer trip and hit his head, opening a big gash on his forehead?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 216.0,
        "end": 226.824
      },
      "pred_interval": {
        "start": 250.0,
        "end": 255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 28.175999999999988,
        "average": 31.087999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.31111111111111117,
        "text_similarity": 0.8628767132759094,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative order and general action (running/tripping/falling) but the timestamps are substantially wrong and it omits the key factual detail about the large forehead gash described in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes sees the defendant exiting the saloon like a bat out of hell, when does she decide she should get back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 343.5
      },
      "pred_interval": {
        "start": 300.0,
        "end": 305.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 38.5,
        "average": 36.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3423423423423423,
        "text_similarity": 0.8666350841522217,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and the character's decision to return, but both anchor and target timestamps are substantially wrong and the predicted event boundaries do not match the ground truth segments, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Reyes finishes thinking 'something seems wrong', when does she think 'I should get back to that bar'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.3,
        "end": 340.5
      },
      "pred_interval": {
        "start": 335.0,
        "end": 337.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3000000000000114,
        "end": 3.5,
        "average": 2.9000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.46511627906976744,
        "text_similarity": 0.8431771993637085,
        "llm_judge_score": 4,
        "llm_judge_justification": "The anchor timing is close to the ground truth, and the relation 'after' matches 'once_finished', but the predicted target interval is incorrectly earlier (overlaps with the anchor and ends before the true E2 start), so the key temporal placement is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the car was seized, when does he mention a forensic technician finding cocaine residue?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.1,
        "end": 428.2
      },
      "pred_interval": {
        "start": 510.0,
        "end": 512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.89999999999998,
        "end": 83.80000000000001,
        "average": 86.85
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.9347237348556519,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') and identifies the correct event content, but the timestamps are substantially wrong (off by ~90s) and do not match the reference intervals, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker specifies the time and date of the incident, when does he mention the car being seized?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 415.9,
        "end": 419.1
      },
      "pred_interval": {
        "start": 415.0,
        "end": 417.0
      },
      "iou": 0.26829268292683334,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8999999999999773,
        "end": 2.1000000000000227,
        "average": 1.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3555555555555555,
        "text_similarity": 0.9219558238983154,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relation, with E1 and E2 times very close to the reference ranges; minor discrepancies in the exact E2 timing and omission of the quoted phrasing warrant a small deduction."
      }
    },
    {
      "question_id": "001",
      "question": "After the prosecution states that the defendant Carl Miller was meeting with his business clients, when does the evidence show that the defendant was in the saloon at 3:55 p.m.?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 510.41,
        "end": 510.45
      },
      "pred_interval": {
        "start": 515.0,
        "end": 525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.589999999999975,
        "end": 14.550000000000011,
        "average": 9.569999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.44680851063829785,
        "text_similarity": 0.7841530442237854,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor content and the temporal relation ('after'), but the timestamps are substantially incorrect\u2014E1's time is slightly off and E2's interval (515.0\u2013525.0s) contradicts the ground truth (510.41\u2013510.45s), so the key factual timing is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once John observed Mr. Miller pass a small glass bottle with a white powder in it to his buddy, when did John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 631.0,
        "end": 638.07
      },
      "pred_interval": {
        "start": 575.0,
        "end": 585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.0,
        "end": 53.07000000000005,
        "average": 54.535000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.4935064935064935,
        "text_similarity": 0.8274325728416443,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the temporal order ('after') but is factually incorrect about both event timings (anchor and target times are off by ~45\u201360s and the anchor end time is omitted), so it fails to match the correct intervals."
      }
    },
    {
      "question_id": "003",
      "question": "Once the gunman shouted, 'Police! You are under arrest!', when did the defendant immediately take off running towards the exit?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 673.544,
        "end": 683.655
      },
      "pred_interval": {
        "start": 605.0,
        "end": 615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.54399999999998,
        "end": 68.65499999999997,
        "average": 68.59949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.8165022730827332,
        "llm_judge_score": 2,
        "llm_judge_justification": "Both predicted timestamps for E1 and E2 are incorrect (off by about 44\u201368 seconds) and thus the events are mislocalized; while the predicted temporal relation 'after' matches the ground truth relation, it is based on wrong timings, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Dr. Reyes sees the defendant exiting the saloon, when does she wonder if something is wrong?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.4,
        "end": 751.6
      },
      "pred_interval": {
        "start": 720.0,
        "end": 735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.399999999999977,
        "end": 16.600000000000023,
        "average": 21.5
      },
      "rationale_metrics": {
        "rouge_l": 0.55,
        "text_similarity": 0.9296678304672241,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same events but the timestamps are substantially incorrect and the temporal alignment is wrong: the reference has E1 ending at 739.0s and E2 occurring after at ~746.4\u2013751.6s, while the prediction places E2 earlier (720\u2013735s) and misaligns E1, so it fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the defendant jumped into a convertible, when does he look again at Dr. Reyes while starting the car?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.3,
        "end": 773.5
      },
      "pred_interval": {
        "start": 750.0,
        "end": 765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.299999999999955,
        "end": 8.5,
        "average": 13.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.6666666666666667,
        "text_similarity": 0.9580267071723938,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the 'after' relation, it gives incorrect event boundaries and timestamps (mislabeling E1 as starting at 745.0s instead of completing at 761.2s, and shifting E2's start/end far earlier than 768.3\u2013773.5s), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes writes down the Mustang's plate number, when does she decide to go back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 797.9,
        "end": 808.8
      },
      "pred_interval": {
        "start": 780.0,
        "end": 795.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.899999999999977,
        "end": 13.799999999999955,
        "average": 15.849999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.40476190476190477,
        "text_similarity": 0.8344457149505615,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives substantially different timestamps and ordering: it places E2 (decision to return) earlier (780\u2013795s) than the ground truth (starts 797.9s, ends 808.8s) and misstates E1 timing/completion, contradicting the correct temporal alignment despite both labeling the relation 'after'."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"You will not hear one single person,\" when does the speaker mention the date \"September 8th, 2020\"?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 882.8,
        "end": 885.6
      },
      "pred_interval": {
        "start": 874.0,
        "end": 878.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.799999999999955,
        "end": 7.600000000000023,
        "average": 8.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.6674761176109314,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the event timestamps are substantially incorrect: the anchor time is mis-specified (single point at 872.0s vs 873.7\u2013877.9s) and the target is placed much earlier (874.0\u2013878.0s vs 882.8\u2013885.6s), so key timing information is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that the car was seized and taken to the crime lab, when does the speaker begin describing the finding of cocaine residue by a forensic technician?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 891.0,
        "end": 904.4
      },
      "pred_interval": {
        "start": 884.0,
        "end": 888.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 16.399999999999977,
        "average": 11.699999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.380952380952381,
        "text_similarity": 0.751914918422699,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('immediately after') but the provided timestamps are substantially incorrect compared to the reference (predicted ~884.0\u2013888.0s vs correct 890.9\u2013904.4s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's statement about finding the defendant Carl Miller guilty of felonies, when is \"fleeing and eluding\" specifically mentioned?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 930.8,
        "end": 947.9
      },
      "pred_interval": {
        "start": 935.0,
        "end": 937.0
      },
      "iou": 0.11695906432748522,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.2000000000000455,
        "end": 10.899999999999977,
        "average": 7.550000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.7904868125915527,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relationship and places the target within the anchor, but the timestamp estimates are notably inaccurate: the anchor start is shifted ~10s later than ground truth and the target span is shortened and offset (predicted 935.0\u2013937.0s vs true 930.8\u2013947.9s), so it omits substantial portions of the true interval."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks the witness to state her name and spell her last name, when does the witness spell out her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.561,
        "end": 36.805
      },
      "pred_interval": {
        "start": 15.0,
        "end": 18.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.561,
        "end": 18.305,
        "average": 18.433
      },
      "rationale_metrics": {
        "rouge_l": 0.3513513513513513,
        "text_similarity": 0.7090733051300049,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are significantly different from the ground truth, so the timing information is factually incorrect and not aligned with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks the witness if she was working as a plumber at a construction site on the evening of October 27th, 2020, when does the witness describe finding the broken window of her vehicle?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 69.075,
        "end": 74.643
      },
      "pred_interval": {
        "start": 45.0,
        "end": 50.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.075000000000003,
        "end": 24.643,
        "average": 24.359
      },
      "rationale_metrics": {
        "rouge_l": 0.3513513513513513,
        "text_similarity": 0.723129153251648,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' but gives substantially incorrect timestamps for both events (E1 predicted at ~40s vs 54.536\u201360.183s, E2 predicted at ~45\u201350s vs 69.075\u201374.643s), so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker asks the witness to describe the area where she parked her car and if she was worried about its safety, when does the witness explain that she was not worried because she always parked near a bar?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.14,
        "end": 128.77
      },
      "pred_interval": {
        "start": 75.0,
        "end": 80.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.14,
        "end": 48.77000000000001,
        "average": 41.955000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6850854754447937,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the predicted answer correctly identifies the relation ('after') and the content of E2, its timestamps are completely mismatched with the ground truth, so it fails to provide the correct timing information."
      }
    },
    {
      "question_id": "001",
      "question": "After Ms. Mendoza reports that her tablet and ice skates were stolen, when does the lawyer ask her if she made any attempt to look for the perpetrators or immediately contacted the police?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 164.191,
        "end": 172.955
      },
      "pred_interval": {
        "start": 152.5,
        "end": 155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.691000000000003,
        "end": 17.955000000000013,
        "average": 14.823000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.7181264162063599,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and the anchor start time but gives substantially incorrect timestamps for E2 (and omits E1 end time), producing a temporal placement that contradicts the correct 'after' ordering; thus it largely fails on temporal accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza explains that she noticed a suspicious man around her car while on the phone with the operator, when does the lawyer ask her what the officer did?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 261.353,
        "end": 262.466
      },
      "pred_interval": {
        "start": 180.0,
        "end": 182.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.35300000000001,
        "end": 79.96600000000001,
        "average": 80.65950000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317073,
        "text_similarity": 0.7419468760490417,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but the timestamps are wildly incorrect (predicted ~178\u2013182s vs ground truth ~229.6\u2013262.5s), so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Ms. Mendoza states that the thief got out of the car and started running down the street, when does she describe the thief punching the arresting officer?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 321.13,
        "end": 331.68
      },
      "pred_interval": {
        "start": 210.0,
        "end": 213.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.13,
        "end": 118.68,
        "average": 114.905
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.6406635046005249,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', its timestamps for both E1 and E2 are grossly incorrect compared to the reference (off by ~90\u2013110 seconds) and thus fails to match the key temporal boundaries described in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza to point out the man who assaulted the officer, when does Ms. Mendoza describe him?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 349.986,
        "end": 355.253
      },
      "pred_interval": {
        "start": 375.0,
        "end": 380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.01400000000001,
        "end": 24.747000000000014,
        "average": 24.880500000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903614,
        "text_similarity": 0.7642039060592651,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the relational order (after) and that the lawyer asks and Ms. Mendoza then describes the man, but the provided timestamps are significantly off from the reference and it omits the key descriptive content ('skinny and with gray hair'), reducing completeness and factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking if the man settled down and cooperated after being put in the patrol car, when does Ms. Mendoza reply that he did not?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 457.548,
        "end": 460.233
      },
      "pred_interval": {
        "start": 410.0,
        "end": 415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.548,
        "end": 45.233000000000004,
        "average": 46.3905
      },
      "rationale_metrics": {
        "rouge_l": 0.24390243902439027,
        "text_similarity": 0.666366696357727,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that Ms. Mendoza replied 'he did not' after the lawyer's question, but the provided timestamps are substantially different from the reference (predicted ~410s vs reference ~457.5s) and the relation label is less precise, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the new lawyer says 'Good morning', when does he ask Ms. Mendoza to state and spell her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 502.241,
        "end": 504.904
      },
      "pred_interval": {
        "start": 500.0,
        "end": 505.0
      },
      "iou": 0.5326000000000022,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2409999999999854,
        "end": 0.09600000000000364,
        "average": 1.1684999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.4556962025316456,
        "text_similarity": 0.802004337310791,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relation, but the timestamps differ from the reference (E1 about 6.3s earlier, E2 about 2.24s earlier) and it omits that the reference times are relative to the 330.0s segment."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza if she prefers to testify with an interpreter, when does Ms. Mendoza confirm her preference?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.733,
        "end": 528.876
      },
      "pred_interval": {
        "start": 514.0,
        "end": 520.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.732999999999947,
        "end": 8.875999999999976,
        "average": 10.304499999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.2807017543859649,
        "text_similarity": 0.7105984687805176,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the relation as 'after', its timestamps are substantially incorrect and inconsistent with the ground truth (predicted events occur much earlier and contradict the true timing), so the key factual elements are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza states that she saw someone inside her car through the broken window, when does the lawyer acknowledge her statement?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 559.358,
        "end": 561.878
      },
      "pred_interval": {
        "start": 540.0,
        "end": 545.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.357999999999947,
        "end": 16.878000000000043,
        "average": 18.117999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.7808030247688293,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the event timestamps are significantly different from the reference (both start and end times are incorrect), so it fails to match key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the lawyer asks Ms. Mendoza if anything was removed from her vehicle, when does Ms. Mendoza list the stolen items?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.801,
        "end": 634.921
      },
      "pred_interval": {
        "start": 700.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.19899999999996,
        "end": 75.07899999999995,
        "average": 76.13899999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.8050002455711365,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation as 'after' but the timestamps are substantially incorrect (shifted by ~78s and use start times instead of the correct finish time) and it omits the note about the initial 'Por supuesto', so it is largely inaccurate. "
      }
    },
    {
      "question_id": "001",
      "question": "After the witness finishes describing the man looking through the car window, when does the lawyer state that a deputy was arriving?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 711.914,
        "end": 714.633
      },
      "pred_interval": {
        "start": 715.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0860000000000127,
        "end": 5.366999999999962,
        "average": 4.226499999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7428760528564453,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are significantly different from the ground truth (each timestamp is off by several seconds), so it contradicts key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking what the officer did, when does the witness describe the officer speaking into his radio and telling her to stay put?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.233,
        "end": 766.142
      },
      "pred_interval": {
        "start": 745.0,
        "end": 750.0
      },
      "iou": 0.209126270442092,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7670000000000528,
        "end": 16.142000000000053,
        "average": 9.454500000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.7062888145446777,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct ordering (E2 occurs after E1) but timestamps are substantially off (E1 ~5s late, E2 start ~2.8s late and end ~16s early) and the relation label ('after' vs. 'once_finished') does not match the expected precise relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the witness finishes stating she is absolutely sure about identifying the defendant, when does the lawyer ask about the suspect's behavior after being cuffed?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 851.101,
        "end": 861.493
      },
      "pred_interval": {
        "start": 865.0,
        "end": 870.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.899000000000001,
        "end": 8.506999999999948,
        "average": 11.202999999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.7610486149787903,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') and the events, but the provided timestamps are substantially different from the ground truth (off by ~13\u201316s for E1/E2 start times and ~9s for E2 end), so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the male lawyer asks what happened when they got to the patrol car, when does Ms. Mendoza describe the officers searching the suspect?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 885.63,
        "end": 894.463
      },
      "pred_interval": {
        "start": 888.0,
        "end": 895.0
      },
      "iou": 0.6897545357523972,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3700000000000045,
        "end": 0.5370000000000346,
        "average": 1.4535000000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.6335443258285522,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, with start/end times close to the reference; however it omits the anchor's end time and shifts the target interval by a few seconds and does not note that the description completes the list of found items."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ms. Mendoza finishes explaining she got closer to see what was happening, when does she state that's why she remembers it well?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 919.062,
        "end": 923.688
      },
      "pred_interval": {
        "start": 914.0,
        "end": 917.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.062000000000012,
        "end": 6.687999999999988,
        "average": 5.875
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.7972267270088196,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the two events and a similar temporal relation ('after'), but the timestamps are substantially incorrect (E1 start misaligned and missing end; E2 interval is earlier and non-overlapping with the ground truth), so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male lawyer finishes asking if the suspect settled down and cooperated in the patrol car, when does Ms. Mendoza state he did not cooperate?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 937.892,
        "end": 940.207
      },
      "pred_interval": {
        "start": 937.0,
        "end": 942.0
      },
      "iou": 0.4629999999999882,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8920000000000528,
        "end": 1.7930000000000064,
        "average": 1.3425000000000296
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.6383810043334961,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the same two events and the temporal ordering (E2 occurs after E1) with overlapping timestamps, but the provided start/end times differ modestly from the reference and the relation label 'after' is less precise than 'once_finished.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Good evening, friends,' when does he mention that their journey started in 2020 during the COVID times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 5.453,
        "end": 8.514
      },
      "pred_interval": {
        "start": 14.0,
        "end": 18.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.547,
        "end": 9.486,
        "average": 9.0165
      },
      "rationale_metrics": {
        "rouge_l": 0.32142857142857145,
        "text_similarity": 0.6200028657913208,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the 'after' relationship, but the anchor and target timestamps are significantly incorrect compared to the reference, so the answer is factually inaccurate/incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker (Vikas Chatrath) says 'I think there's some issue with the net,' when does the screen go black with his name displayed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.726,
        "end": 75.546
      },
      "pred_interval": {
        "start": 60.0,
        "end": 63.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.725999999999999,
        "end": 12.546000000000006,
        "average": 10.136000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7162190079689026,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and ordering contradict the ground truth: the prediction places the black-screen name at 60\u201363s (and the anchor starting at 58s), whereas the reference specifies the anchor ends at 63.456s and the target starts at 67.726s, so the timing and relation are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath says 'Over to you, sir,' when does Mr. R.S. Cheema begin speaking?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.165,
        "end": 175.752
      },
      "pred_interval": {
        "start": 200.0,
        "end": 205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.835000000000008,
        "end": 29.24799999999999,
        "average": 30.0415
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7124773859977722,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps contradict the ground truth (anchor/target occur around 167\u2013175s in the reference vs ~195\u2013205s in the prediction) and the timing/relationship ('after' vs 'immediately after') is incorrect, so the prediction is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chatrath finishes his speech and invites Mr. Cheema, when does Mr. Cheema describe the topic as very generic and vast?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.048,
        "end": 204.229
      },
      "pred_interval": {
        "start": 153.0,
        "end": 156.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.048,
        "end": 48.22900000000001,
        "average": 47.63850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.6617366671562195,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the 'after' relation and paraphrases Cheema's line, but the provided timestamps are wildly incorrect (150s/153s vs. ~15s and ~50s in the reference), so the temporal localization is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After Mr. Cheema says he agreed with the choice of the broad topic, when does he explain the reason for his agreement?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 229.669,
        "end": 242.242
      },
      "pred_interval": {
        "start": 170.0,
        "end": 175.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.66900000000001,
        "end": 67.24199999999999,
        "average": 63.4555
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164376,
        "text_similarity": 0.730614185333252,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps for both the anchor and the explanation are far off from the ground truth (predicted ~165\u2013175s vs actual ~224.6\u2013242.2s), so it fails to correctly locate when the reason is given."
      }
    },
    {
      "question_id": "003",
      "question": "During Mr. Cheema's description of going into division bench courts to hear idols and icons, when does he mention that the judges are not in a hurry?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.317,
        "end": 313.619
      },
      "pred_interval": {
        "start": 320.0,
        "end": 325.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.682999999999993,
        "end": 11.380999999999972,
        "average": 13.031999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.4556962025316456,
        "text_similarity": 0.8756446838378906,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer states the same relation but the timestamps for both anchor and target are incorrect and inconsistent with each other (they fall outside the ground-truth intervals and contradict the 'during' relation), so it fails temporal alignment despite matching the content."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions he has been appearing in three High Courts, when does he state that criminal appeals have not been argued for a long time in the Punjab and Haryana High Court?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 379.0,
        "end": 385.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 15.0,
        "average": 17.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2816901408450704,
        "text_similarity": 0.7001278400421143,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and correctly labels the relation as 'after', but the provided timestamps are significantly incorrect (E1 and E2 times differ substantially from the ground truth), so it does not accurately match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions there is very little hearing in appeal, when does he state that the most 'scaring part' is the flood of outstanding litigation causing the hearing to get a beating?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 408.915,
        "end": 418.887
      },
      "pred_interval": {
        "start": 400.0,
        "end": 410.0
      },
      "iou": 0.05744692116270342,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.91500000000002,
        "end": 8.887,
        "average": 8.90100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.276923076923077,
        "text_similarity": 0.74402916431427,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but both event timestamps are substantially inaccurate: E1 is placed ~15s too early and E2 is shifted earlier and ends ~9s before the true end, resulting in only ~1s overlap with the correct E2\u2014thus the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says, 'Let's first also define what the appeals are', when does he begin to define criminal appeals as generally understood?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.605,
        "end": 487.757
      },
      "pred_interval": {
        "start": 500.0,
        "end": 510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.394999999999982,
        "end": 22.242999999999995,
        "average": 24.31899999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424246,
        "text_similarity": 0.7669231295585632,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general order (anchor then target) but the timestamps are significantly off (both start and end times differ greatly from the reference) and the relation is imprecise compared to 'once_finished', so it fails to match the ground truth timing and relation accurately."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes stating that the real purpose of this exercise is to scuttle the path of the litigant, when does he discuss today's purpose?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 562.2,
        "end": 567.6
      },
      "pred_interval": {
        "start": 540.0,
        "end": 550.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.200000000000045,
        "end": 17.600000000000023,
        "average": 19.900000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7085753083229065,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the order (E1 then E2) and theme right but the timestamps are substantially incorrect and it fails to capture that E2 immediately follows E1 (it only says 'after'), so key temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is listing the second category of appeals, when does he mention the 'Essential Commodities Act'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 595.2,
        "end": 601.662
      },
      "pred_interval": {
        "start": 590.0,
        "end": 600.0
      },
      "iou": 0.41159320871205113,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.2000000000000455,
        "end": 1.6620000000000346,
        "average": 3.43100000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.772578775882721,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the two events but gives incorrect start times (E1 10s early, E2 ~5s early) and incorrectly states the relationship as 'after' whereas the reference places E2 during E1; these are substantial factual errors."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating he will confine today's discussion to the Indian Penal Code, when does he explain that he doesn't want to just touch and go?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.624,
        "end": 635.538
      },
      "pred_interval": {
        "start": 665.0,
        "end": 675.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.375999999999976,
        "end": 39.46199999999999,
        "average": 37.91899999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2926829268292683,
        "text_similarity": 0.6427164673805237,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events but gives timestamps that are far off (E1/E2 ~34\u201336s later than ground truth) and states a generic 'after' relation rather than the immediate-follow relation; thus it is largely incorrect despite naming the right phrases."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that their judgments are not very clear, when does he begin to explain that, unlike IPC, other acts *do* have provisions for vicarious liability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.063,
        "end": 753.951
      },
      "pred_interval": {
        "start": 690.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.06299999999999,
        "end": 43.95100000000002,
        "average": 50.007000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481928,
        "text_similarity": 0.7863823771476746,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and event ordering are wholly incorrect (both events placed at 690s and ending at 710s) and contradict the reference times (around 742\u2013754s); the stated relationship ('immediately after') also does not match the correct 'after' placement, so the prediction fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the first clause of the provisions, when does he explain that an individual is a 'deemed accused' by virtue of their office?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 772.927,
        "end": 777.037
      },
      "pred_interval": {
        "start": 740.0,
        "end": 750.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.92700000000002,
        "end": 27.037000000000035,
        "average": 29.982000000000028
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.6988950967788696,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the coarse ordering (E2 occurs after E1) but the timestamps are substantially incorrect and inconsistent with the reference (E1 at 771.695s vs 740.0s predicted; E2 should start at 772.927s and end at 777.037s but predicted 740.0\u2013750.0s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that the 'second part is most important', when does he begin describing this second part of the provisions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 786.039,
        "end": 799.875
      },
      "pred_interval": {
        "start": 770.0,
        "end": 790.0
      },
      "iou": 0.13258577405857783,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.038999999999987,
        "end": 9.875,
        "average": 12.956999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526316,
        "text_similarity": 0.7345600128173828,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and relation contradict the reference: it misplaces E1 and E2 start/end times (770.0s vs 786.019s/786.039s) and gives the wrong temporal relation, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking if people are vicariously liable, when does he state that the answer is 'yes and no both'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.653,
        "end": 886.655
      },
      "pred_interval": {
        "start": 884.0,
        "end": 888.0
      },
      "iou": 0.5004999999999882,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.65300000000002,
        "end": 1.3450000000000273,
        "average": 0.9990000000000236
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.7261593341827393,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct ordering (E2 follows E1) and roughly locates the utterances, but it gives inaccurate and inconsistent timestamps (E1 start too early, missing E1 end; E2 start/end off by ~0.65s and ~1.35s) and uses a less precise relation ('after' vs 'once_finished')."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the Food Safety and Security Act allows a company to appoint a competent person as a nominee, when does he mention the Environment Act?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 954.035,
        "end": 958.702
      },
      "pred_interval": {
        "start": 950.0,
        "end": 960.0
      },
      "iou": 0.466700000000003,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.034999999999968,
        "end": 1.2980000000000018,
        "average": 2.666499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.42424242424242425,
        "text_similarity": 0.7689908742904663,
        "llm_judge_score": 6,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') and gives approximate intervals for E1 and E2, but the timestamps differ noticeably from the reference (E1 is placed ~9s earlier and E2 start/end are off by several seconds), so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes that the rest of the appeals would always be technical, when does he introduce the topic of drafting an appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1052.092,
        "end": 1055.115
      },
      "pred_interval": {
        "start": 1020.0,
        "end": 1025.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.0920000000001,
        "end": 30.11500000000001,
        "average": 31.103500000000054
      },
      "rationale_metrics": {
        "rouge_l": 0.45714285714285713,
        "text_similarity": 0.6823970079421997,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the provided timestamps for E1 and E2 are substantially different from the reference (off by ~24\u201332 seconds), so the core temporal information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions looking into the nuances of drafting, when does he state what he found regarding drafting and appeals?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1054.8,
        "end": 1058.3
      },
      "pred_interval": {
        "start": 1080.0,
        "end": 1086.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.200000000000045,
        "end": 27.700000000000045,
        "average": 26.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.8313788771629333,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but it mislocates both events by large margins (timestamps differ substantially from the ground truth), so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that criminal proceedings are not adversarial, when does he discuss how formal errors and oversights can be overcome?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1120.624,
        "end": 1125.731
      },
      "pred_interval": {
        "start": 1165.0,
        "end": 1175.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.375999999999976,
        "end": 49.269000000000005,
        "average": 46.82249999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23188405797101447,
        "text_similarity": 0.8704630136489868,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target relationship ('after') but the provided timestamps are substantially misaligned (\u224840s later) from the ground truth, so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that the 313 was not correctly recorded in a Ropar case, when does he recall the accused saying he was made to sign a blank paper?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1194.217,
        "end": 1254.6
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1240.0
      },
      "iou": 0.16560952585992797,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.7829999999999,
        "end": 14.599999999999909,
        "average": 25.191499999999905
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.8054221868515015,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') but the provided timestamps and durations diverge substantially from the ground truth (both anchor and target times are shifted and E2 duration is greatly inflated), so the answer is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what they should do, when does he ask if an application needs to be filed for additional evidence in the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1251.721,
        "end": 1266.539
      },
      "pred_interval": {
        "start": 1252.5,
        "end": 1264.0
      },
      "iou": 0.7760831421244441,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.7789999999999964,
        "end": 2.5389999999999873,
        "average": 1.6589999999999918
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.7804975509643555,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction identifies the same two utterances and that E2 follows E1, but the E1 timestamp is substantially off (predicted 1252.5s vs correct spanning ~1230.2\u20131251.0s) and the relation is tightened to 'immediate following' while E2/end times also differ slightly; overall order is correct but timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the court made a mistake by not formally proving a cassette recording, when does he explain that they had to make an application for evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.0,
        "end": 1294.14
      },
      "pred_interval": {
        "start": 1335.0,
        "end": 1347.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.0,
        "end": 52.8599999999999,
        "average": 48.42999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.7374146580696106,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly captures the semantic relation (the application for evidence follows the mistake) but the time intervals are substantially misaligned (predicted ~1335s\u20131347s vs. reference ~1290.54s\u20131294.14s) and the event boundaries/durations do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that first reactions may never come again if not noted down, when does he state that he made a practice of noting his reactions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1394.269,
        "end": 1401.682
      },
      "pred_interval": {
        "start": 1405.0,
        "end": 1415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.730999999999995,
        "end": 13.317999999999984,
        "average": 12.024499999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.7963599562644958,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the order (E2 follows E1) and the quoted content, but the timestamps are significantly offset from the reference and the relation is changed to 'immediate following' rather than the referenced 'after', so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says one should not put the detail in an appeal, when does he suggest that subtle points may be hinted at when drafting grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1444.582,
        "end": 1452.109
      },
      "pred_interval": {
        "start": 1450.0,
        "end": 1460.0
      },
      "iou": 0.13678816967180815,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.417999999999893,
        "end": 7.891000000000076,
        "average": 6.6544999999999845
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.785668134689331,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship but the timestamp placements are substantially incorrect (anchor moved from 1416s to 1440s and target times are shifted/extended), so it fails to align with the reference timings and contains factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'What is the key question?', when does he compare a trial or appeal to writing a novel?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1536.909,
        "end": 1545.661
      },
      "pred_interval": {
        "start": 1530.0,
        "end": 1545.0
      },
      "iou": 0.5166336760104632,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.9090000000001055,
        "end": 0.6610000000000582,
        "average": 3.785000000000082
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7269750833511353,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the event times (E2 start 1530.0 vs correct 1536.909 and gives an incorrect E1 timing), and describes the relation merely as 'after' instead of the direct, immediate follow specified; only the general ordering is roughly preserved."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions keeping the core issue in mind, when does he advise that the first reading of an appeal should be relaxed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1594.74,
        "end": 1607.579
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1610.0
      },
      "iou": 0.49665792922673363,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.259999999999991,
        "end": 2.421000000000049,
        "average": 3.84050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.8376784324645996,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that the advice comes after the anchor, but the timestamps and which anchor boundary is referenced are inaccurate (E1 timing and E2 start/end differ notably from the ground truth)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says that the first reading of an appeal should be relaxed, when does he compare it to watching a soap opera, reading a novel, or listening to music?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1608.151,
        "end": 1615.0
      },
      "pred_interval": {
        "start": 1602.0,
        "end": 1608.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.151000000000067,
        "end": 7.0,
        "average": 6.575500000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6463052034378052,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events (relaxed first reading and comparison to relaxing activities) but gives significantly different start/end timestamps and duration boundaries and labels the relation as 'immediately after' rather than the specified 'once_finished', so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises being as neutral as possible, when does he explain that this enables objectivity and unbiased case knowledge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1638.375,
        "end": 1647.567
      },
      "pred_interval": {
        "start": 1635.0,
        "end": 1645.0
      },
      "iou": 0.5271743455080764,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.375,
        "end": 2.5670000000000073,
        "average": 2.9710000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131148,
        "text_similarity": 0.6399370431900024,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target events and the 'after' relation, but the predicted time boundaries are slightly offset (E1 starts earlier and E2 starts/ends a few seconds different) and E1's end time is omitted."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if he has a good case, when does he follow up by asking if he has a bad case or one in between?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1674.0,
        "end": 1681.0
      },
      "pred_interval": {
        "start": 1675.0,
        "end": 1682.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 1.0,
        "average": 1.0
      },
      "rationale_metrics": {
        "rouge_l": 0.39436619718309857,
        "text_similarity": 0.6894617080688477,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction matches the reference closely\u2014timings for E1 and E2 are within ~1s and the relation ('immediately after') is equivalent to 'next'. Minor omissions: the predicted answer omits E1's end time and shifts E2 end by ~1s, so not a perfect match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that three to four things would matter when preparing an appeal, what is the first thing he suggests?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1824.432,
        "end": 1828.85
      },
      "pred_interval": {
        "start": 1775.0,
        "end": 1785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.432000000000016,
        "end": 43.84999999999991,
        "average": 46.64099999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.7610020041465759,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase but provides a wrong target segment (incorrect timestamps and quoted phrase) and an incorrect temporal relation\u2014contradicting the ground-truth where the target immediately follows the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker emphasizes that stages in an appeal should be clear in one's mind, when does he start talking about the 'finest lawyers'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1890.082,
        "end": 1904.09
      },
      "pred_interval": {
        "start": 1810.0,
        "end": 1820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.08200000000011,
        "end": 84.08999999999992,
        "average": 82.08600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7053133249282837,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the qualitative relationship ('after') but fails on key factual elements: it omits a timestamp for E1 and gives completely incorrect E2 timestamps (1810\u20131820s vs. ~1890s in the reference), so it does not align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes his point about preparation, when does he announce moving to the next phase of the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1921.825,
        "end": 1924.428
      },
      "pred_interval": {
        "start": 1950.0,
        "end": 1960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.174999999999955,
        "end": 35.57199999999989,
        "average": 31.873499999999922
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7671048641204834,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are significantly different from the reference (1950\u20131960s vs 1918\u20131924s) and it misstates the temporal relationship ('after' instead of 'immediately follows'), so it does not match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a judge will eventually respond to an appeal, when does he mention being placed before a newly transferred judge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1984.778,
        "end": 1991.706
      },
      "pred_interval": {
        "start": 1997.0,
        "end": 2007.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.22199999999998,
        "end": 15.294000000000096,
        "average": 13.758000000000038
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481927,
        "text_similarity": 0.8217613697052002,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the predicted time boundaries are largely incorrect (misplacing both the anchor and target spans by many seconds), so it fails to correctly identify the specified segments."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes how he handles a client's ticklish case by listening to their story, when does he next explain that judges would analyze in a similar way and evolve their style?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2000.528,
        "end": 2006.954
      },
      "pred_interval": {
        "start": 2085.0,
        "end": 2095.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 84.47199999999998,
        "end": 88.04600000000005,
        "average": 86.25900000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.7433191537857056,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely mislocates both events (timestamps are off by ~60s+) and misstates the temporal relation ('after' vs the correct 'next'), so it fails to match the correct answer's timing and relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a diligent lawyer would form and correct their opinion about the bench every day, when does he next say that this practice saves the court's time?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2071.617,
        "end": 2074.641
      },
      "pred_interval": {
        "start": 2135.0,
        "end": 2145.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.38299999999981,
        "end": 70.35899999999992,
        "average": 66.87099999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7485284805297852,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps for both anchor and target are substantially different from the reference and the relation 'after' does not match the reference 'next' (immediate succession), so it is largely incorrect despite referencing the same event."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what makes a good lawyer, when does he begin to answer by describing a man who has prepared his brief fully?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.444,
        "end": 2200.017
      },
      "pred_interval": {
        "start": 2155.0,
        "end": 2165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.44399999999996,
        "end": 35.016999999999825,
        "average": 36.23049999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2535211267605634,
        "text_similarity": 0.7688313722610474,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives times that are far off from the reference (\u224840\u201350s earlier) and omits the correct E1 end and E2 completion time; only the 'after' relationship matches, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a fresh judge saying 'no reading of evidence', when does he express his opinion that it's illegal and unconstitutional?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2237.752,
        "end": 2243.727
      },
      "pred_interval": {
        "start": 2220.0,
        "end": 2230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.751999999999953,
        "end": 13.726999999999862,
        "average": 15.739499999999907
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7563674449920654,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the correct ordering (anchor before target) but the timestamps are substantially off (anchor start and both target boundaries differ by ~13\u201322s) and the anchor end is omitted, so it does not match the ground truth timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to make the judge read Section 54 of the Evidence Act, when does he explain its content regarding the previous character of an accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2302.58,
        "end": 2310.354
      },
      "pred_interval": {
        "start": 2280.0,
        "end": 2290.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.579999999999927,
        "end": 20.353999999999814,
        "average": 21.46699999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7694101333618164,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the qualitative relation (that the explanation follows the instruction) but the reported start/end times are substantially and incorrectly shifted compared to the ground truth, with wrong boundaries and missing E1 end, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of the second question as a heinous crime, when does he describe specific examples like 'chopped off the hand'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2354.498,
        "end": 2357.601
      },
      "pred_interval": {
        "start": 2360.0,
        "end": 2372.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.501999999999953,
        "end": 14.398999999999887,
        "average": 9.95049999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.8954253196716309,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct relation ('during') and that specific examples are mentioned, but the anchor and target timestamps are significantly off (\u224811\u201315s later) compared to the ground truth, so the key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states 'this is the second kind of roadblock', when does he introduce the 'Third kind of roadblock'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2414.579,
        "end": 2418.664
      },
      "pred_interval": {
        "start": 2440.0,
        "end": 2455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.42099999999982,
        "end": 36.335999999999785,
        "average": 30.878499999999804
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666667,
        "text_similarity": 0.7849282026290894,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the Third roadblock follows the second, but the anchor/target timestamps and durations are substantially wrong and the relation is mischaracterized as 'after' rather than the immediate 'next', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes explaining 'these are the questions which are put to you', when does he transition to discussing myths about 'a dying declaration'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2462.739,
        "end": 2468.126
      },
      "pred_interval": {
        "start": 2475.0,
        "end": 2490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.260999999999967,
        "end": 21.873999999999796,
        "average": 17.06749999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7857306599617004,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the 'after' relationship, but the timestamp values are substantially off (by ~12\u201318s), and it omits E1's start time while misaligning E2's boundaries, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker extensively discusses the 1925 Lahore Bakshish Singh judgment, when does he bring up the Lakshmi versus Om Prakash case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2549.448,
        "end": 2555.294
      },
      "pred_interval": {
        "start": 2550.0,
        "end": 2568.0
      },
      "iou": 0.28536006899524746,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5520000000001346,
        "end": 12.706000000000131,
        "average": 6.629000000000133
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.8145396113395691,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' and locates both events around 2550s, but the provided timestamps are substantially inaccurate (E1 end and E2 end differ markedly from the reference) and the predicted E1/E2 boundaries do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying, 'These are the cases when you have opened the door of a hearing', when does he state, 'These are situations which we must know'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2605.712,
        "end": 2610.678
      },
      "pred_interval": {
        "start": 2630.0,
        "end": 2638.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.28800000000001,
        "end": 27.322000000000116,
        "average": 25.805000000000064
      },
      "rationale_metrics": {
        "rouge_l": 0.2921348314606741,
        "text_similarity": 0.6555179357528687,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative order roughly but misstates all timestamps (off by ~25s), collapses the small gap (E2 starts exactly at E1 end) and labels the relation as 'after' instead of the correct 'once_finished', so key temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises, 'But when you go to this, don't offend the judge', when does he describe making notes in 'three phases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2647.594,
        "end": 2653.382
      },
      "pred_interval": {
        "start": 2660.0,
        "end": 2675.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.405999999999949,
        "end": 21.617999999999938,
        "average": 17.011999999999944
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526316,
        "text_similarity": 0.7503266334533691,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly captures the temporal relation ('after') but the timestamps for E1 and E2 are substantially shifted and the E2 duration is incorrect compared to the reference, so it does not accurately align with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises a lawyer to stop if their best argument isn't selling, when does he suggest the lawyer might say, 'Your lordship may hear me on other issues'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2687.877,
        "end": 2690.499
      },
      "pred_interval": {
        "start": 2700.0,
        "end": 2720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.123000000000047,
        "end": 29.501000000000204,
        "average": 20.812000000000126
      },
      "rationale_metrics": {
        "rouge_l": 0.2784810126582279,
        "text_similarity": 0.7936697006225586,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but the provided timestamps are substantially inaccurate (anchor timing and target start/end deviate by many seconds and the anchor finish time is omitted), so it fails to match the ground-truth intervals."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how language helps with flexibility and playing cool in arguments, when does he introduce 'sense of humor' as an important quality?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2718.862,
        "end": 2725.929
      },
      "pred_interval": {
        "start": 2775.0,
        "end": 2790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.13799999999992,
        "end": 64.07099999999991,
        "average": 60.104499999999916
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.8707157969474792,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misstates the timestamps for both E1 and E2 (off by tens of seconds) and gives an incorrect target interval; while it correctly indicates a subsequent relationship ('after'), it fails to capture that E2 follows immediately after E1 and thus contains incorrect/hallucinated timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker identifies 'trap cases' and 'DA cases' as the majority of Corruption Act cases, when does he introduce 'scam cases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2786.395,
        "end": 2789.04
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 2865.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.60500000000002,
        "end": 75.96000000000004,
        "average": 69.78250000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.8760722279548645,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordinal relationship (E2 occurs after E1) but the timestamps are substantially incorrect and it fails to reflect that E2 immediately follows E1; thus the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if a busy or young lawyer needs a register of important judgments, when does he directly ask if it is a necessary tool?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2898.935,
        "end": 2905.299
      },
      "pred_interval": {
        "start": 2915.0,
        "end": 2920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.065000000000055,
        "end": 14.701000000000022,
        "average": 15.383000000000038
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324323,
        "text_similarity": 0.8006422519683838,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relation ('after') and identifies the correct turn types (anchor and target), but both anchor and target timestamps are significantly incorrect and do not match the reference intervals, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes emphasizing the need to know, read, and re-read foundational judgments, when does he state he will recount a few of them?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.268,
        "end": 2935.553
      },
      "pred_interval": {
        "start": 3005.0,
        "end": 3010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.73199999999997,
        "end": 74.44700000000012,
        "average": 74.58950000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.7725454568862915,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative order ('after') but the timestamp values are substantially incorrect compared with the reference, omitting accurate timing details and thus failing key factual elements."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing the question about a false defense or wrong answer given by the accused, when does he explicitly state his suggestion and request for foundational judgments?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3029.268,
        "end": 3033.995
      },
      "pred_interval": {
        "start": 3050.0,
        "end": 3055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.73199999999997,
        "end": 21.00500000000011,
        "average": 20.86850000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.7841930389404297,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but the reported timestamps diverge substantially from the ground truth (E1 and E2 are off by tens of seconds and E2's end time doesn't match), so the answer is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'Virsar Singh's judgment, 1958', when does he mention 'Vivian Bose's judgment, a classic'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3045.106,
        "end": 3052.317
      },
      "pred_interval": {
        "start": 3075.0,
        "end": 3085.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.893999999999778,
        "end": 32.68299999999999,
        "average": 31.288499999999885
      },
      "rationale_metrics": {
        "rouge_l": 0.3673469387755102,
        "text_similarity": 0.34337058663368225,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation and identifies both events, but the provided timestamps deviate substantially (by ~20\u201333 seconds) from the reference, so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker refers to the case 'Prabhu versus the Emperor, 1941', when does he ask if the initial onus on the prosecution transfers itself?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.51,
        "end": 3129.351
      },
      "pred_interval": {
        "start": 3145.0,
        "end": 3155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.48999999999978,
        "end": 25.648999999999887,
        "average": 25.069499999999834
      },
      "rationale_metrics": {
        "rouge_l": 0.36,
        "text_similarity": 0.5235682725906372,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer preserves the correct relation ('after') but the anchor and target timestamps are substantially shifted (~15\u201325s later) and do not match the precise spans given, so the temporal localization is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying '3.', when does he explain that the court has a 'broader picture' with the accused's evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3158.541,
        "end": 3164.576
      },
      "pred_interval": {
        "start": 3190.0,
        "end": 3200.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.458999999999833,
        "end": 35.42399999999998,
        "average": 33.441499999999905
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.758318305015564,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps deviate substantially from the reference (by ~20\u201335s) and the relation is changed from an immediate 'once_finished' elaboration to a non-immediate 'after', so it contradicts the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 1954 Supreme Court case, when does he detail the specific allegation and offense?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3228.663,
        "end": 3239.981
      },
      "pred_interval": {
        "start": 3245.0,
        "end": 3260.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.33699999999999,
        "end": 20.018999999999778,
        "average": 18.177999999999884
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.8892459273338318,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation 'after' correct but the timestamps are substantially off: E1 start is earlier and lacks the end time, and E2 is shifted much later (and likely misses the described offense), so it fails to match the reference timing and coverage."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying that Tanu Bedi is present, when does he begin describing the case involving an Afghan Airlines pilot?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3267.18,
        "end": 3280.178
      },
      "pred_interval": {
        "start": 3365.0,
        "end": 3380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.82000000000016,
        "end": 99.82200000000012,
        "average": 98.82100000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.32500000000000007,
        "text_similarity": 0.8659528493881226,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering (E2 after E1) but the timestamps are off by ~100 seconds and the temporal relation label ('after') does not match the specific 'once_finished' adjacency; thus the answer is largely incorrect despite correct ordering."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if they can continue for 10 minutes, when does someone respond with 'Yes, of course, of course'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3407.405,
        "end": 3409.588
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3410.0
      },
      "iou": 0.43659999999999854,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4050000000002,
        "end": 0.4119999999998072,
        "average": 1.4085000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.35443037974683544,
        "text_similarity": 0.8971692323684692,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct utterances but misstates the anchor timestamp significantly and slightly misaligns the target span, and it labels the relation as 'after' rather than the correct 'once_finished', so key temporal and relational details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how long they can continue, when does he ask if they can go on for 10 minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3405.21,
        "end": 3408.57
      },
      "pred_interval": {
        "start": 3398.5,
        "end": 3401.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.710000000000036,
        "end": 7.570000000000164,
        "average": 7.1400000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.1875,
        "text_similarity": 0.7023149728775024,
        "llm_judge_score": 2,
        "llm_judge_justification": "The anchor timing is close to the ground truth, but the predicted target is placed ~7s earlier than the correct interval and thus mislocates the event (the predicted times also do not support a clear 'after' relation), so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions the judge saying 'no, you are convicted', when does he recall playing basketball?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3490.94,
        "end": 3501.65
      },
      "pred_interval": {
        "start": 3456.0,
        "end": 3460.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.940000000000055,
        "end": 41.65000000000009,
        "average": 38.29500000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7946025133132935,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the reported timestamps are substantially incorrect (off by ~27\u201342 seconds) and the anchor is given as a single time rather than the correct range, so it fails to match the reference events accurately."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises that 'Your case must bother you', when does he begin telling another interesting case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3552.29,
        "end": 3555.63
      },
      "pred_interval": {
        "start": 3550.5,
        "end": 3554.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.7899999999999636,
        "end": 1.6300000000001091,
        "average": 1.7100000000000364
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.8277860283851624,
        "llm_judge_score": 8,
        "llm_judge_justification": "The predicted anchor timestamp lies within the reference anchor interval and the predicted target overlaps the reference target, though it starts ~1.8s earlier and ends slightly earlier; the temporal relation 'after' is correctly identified."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the detailed injury on the forehead, when does he state that the benefit of doubt went to the accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3636.678,
        "end": 3640.261
      },
      "pred_interval": {
        "start": 3632.0,
        "end": 3642.0
      },
      "iou": 0.3583000000000084,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.677999999999884,
        "end": 1.7390000000000327,
        "average": 3.208499999999958
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.7958806753158569,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' and roughly locates the E2 utterance, but it omits the precise E1 timestamp and gives an inaccurate E2 start time (3632.0s vs 3636.678s), making it incomplete and partially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's description of the trickster character from Maupassant's story and his famous acts, when does he specifically mention the trickster showing different tricks to people?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3732.928,
        "end": 3736.294
      },
      "pred_interval": {
        "start": 3725.0,
        "end": 3735.0
      },
      "iou": 0.18346024437755803,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.927999999999884,
        "end": 1.293999999999869,
        "average": 4.610999999999876
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.6864436268806458,
        "llm_judge_score": 0,
        "llm_judge_justification": "Error parsing LLM response: Expecting ',' delimiter: line 3 column 291 (char 306)"
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing that there are 'two more cases', when does he name the location of the first of these new cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3665.138,
        "end": 3668.806
      },
      "pred_interval": {
        "start": 3665.0,
        "end": 3675.0
      },
      "iou": 0.366800000000012,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.13799999999991996,
        "end": 6.19399999999996,
        "average": 3.16599999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322037,
        "text_similarity": 0.7002907395362854,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies E1 and that E2 names Kurukshetra and gives an approximate start time, but it labels the relation as 'after' (not the immediate 'once_finished') and specifies E2 starting at 3665.0s, which slightly contradicts the annotated E1 end (3665.098s), indicating a minor timing/order mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "Once the main speaker finishes stating that 'So that persuaded the court', when does he begin talking about other interesting cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.541,
        "end": 3838.467
      },
      "pred_interval": {
        "start": 3787.5,
        "end": 3792.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.04100000000017,
        "end": 46.4670000000001,
        "average": 46.25400000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384614,
        "text_similarity": 0.7746360301971436,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misaligns the timestamps (off by ~20\u201345s), gives inconsistent timing (anchor end equals target start) and labels the relation as 'after' instead of the correct 'once_finished', so it fails to match the ground truth despite indicating a following event."
      }
    },
    {
      "question_id": "002",
      "question": "After the host says 'Thank you, sir', when does he describe how the session shifted to English?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3905.664,
        "end": 3912.762
      },
      "pred_interval": {
        "start": 3895.0,
        "end": 3902.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.664000000000215,
        "end": 10.762000000000171,
        "average": 10.713000000000193
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7703373432159424,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the anchor and target start/end times are substantially incorrect compared to the ground truth (E1 true end 3874.0s vs predicted 3895.0s; E2 true start 3905.664s vs predicted 3895.0s), so it fails to match the correct timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "After the host announces that there are questions received on the chat, when does he ask the first question about supplementary grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3949.503,
        "end": 3955.411
      },
      "pred_interval": {
        "start": 3935.0,
        "end": 3940.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.503000000000156,
        "end": 15.411000000000058,
        "average": 14.957000000000107
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.7747207283973694,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misstates the event timings (E1 end and E2 start/end are off by ~9\u201315s), omits E1's start time, and labels the relation as 'after' instead of the direct-followup 'next', so it does not match key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they can combine the two questions, when does he state that all questions of fact can be raised?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3972.976,
        "end": 3975.039
      },
      "pred_interval": {
        "start": 3960.0,
        "end": 3970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.976000000000113,
        "end": 5.039000000000215,
        "average": 9.007500000000164
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7478931546211243,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and that the target occurs after the anchor, but the provided timestamps are significantly off from the ground truth (anchor ~10s early, target ~12\u201313s early) and the target event's end time is also inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that supplementary grounds are not needed in a normal case, when does he explain that all questions of fact and law are open in a normal case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4030.938,
        "end": 4036.242
      },
      "pred_interval": {
        "start": 4030.0,
        "end": 4040.0
      },
      "iou": 0.5304000000000088,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9380000000001019,
        "end": 3.757999999999811,
        "average": 2.3479999999999563
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6820887327194214,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the ordering (the target follows the anchor) but the timestamps are notably off \u2014 the anchor finish (4028.0s) and the target start/end (4030.0s\u20134040.0s) conflict with the reference (anchor 4030.096\u20134030.898, target 4030.938\u20134036.242), so the timing and duration are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining how a long note was required in the Sajan Kumar case, when does he describe the judge quoting his written note 80 times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4130.358,
        "end": 4139.64
      },
      "pred_interval": {
        "start": 4100.0,
        "end": 4110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.358000000000175,
        "end": 29.640000000000327,
        "average": 29.99900000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.358974358974359,
        "text_similarity": 0.8202560544013977,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct temporal relation ('after') but the reported timestamps are substantially incorrect for both the anchor and target intervals (off by ~30s), failing to match the ground-truth segments."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking about making an opening statement to impress the court, when does the main speaker begin explaining that a prepared lawyer can present their case in two minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4186.419,
        "end": 4209.102
      },
      "pred_interval": {
        "start": 4150.0,
        "end": 4160.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.41899999999987,
        "end": 49.10199999999986,
        "average": 42.760499999999865
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.657703161239624,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gives substantially incorrect timestamps (off by ~36\u201338 seconds) and omits E2's end time; although it matches the 'after' relation, the timing and completeness are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the main speaker's narration of the Churchill story, when does he say that Churchill responded he needed '15 days before' to prepare for a two-minute speech?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4275.956,
        "end": 4281.618
      },
      "pred_interval": {
        "start": 4210.0,
        "end": 4218.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.95600000000013,
        "end": 63.61800000000039,
        "average": 64.78700000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.7865508794784546,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly labels the relation as 'during', it gives incorrect timestamps (E2 at 4210\u20134218s) that do not fall within the correct E1 interval (4265.1\u20134299.124s), so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the main speaker finishes explaining that the less one has to speak, the more deeply one has to prepare, when does he begin defining a summary as a test of a lawyer's ability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4306.932,
        "end": 4319.831
      },
      "pred_interval": {
        "start": 4280.0,
        "end": 4290.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.93199999999979,
        "end": 29.83100000000013,
        "average": 28.38149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962025,
        "text_similarity": 0.6699883341789246,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but the timestamps are substantially off (\u224825\u201330s earlier than the reference) and the start/end times for E1/E2 do not match the ground truth, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks about the most challenging situations, when does the guest start explaining the amount of preparation needed for short vs long speeches?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4289.354,
        "end": 4303.141
      },
      "pred_interval": {
        "start": 4370.0,
        "end": 4392.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.64599999999973,
        "end": 88.85900000000038,
        "average": 84.75250000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.27499999999999997,
        "text_similarity": 0.7952523827552795,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and the 'after' relationship, but the reported start/end times are substantially incorrect (off by ~80\u201390 seconds) and the answer adds an unverified quoted phrase; key temporal facts are therefore wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes rephrasing the question about the impact of oral advocacy versus drafting, when does the guest immediately state that oral advocacy is more impactful?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4347.622,
        "end": 4350.188
      },
      "pred_interval": {
        "start": 4410.0,
        "end": 4425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.3779999999997,
        "end": 74.8119999999999,
        "average": 68.5949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.8273900747299194,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly captures the 'immediately after' relationship, but its timestamps for E2 (4410\u20134425s) are far off from the ground truth (4347.622\u20134350.188s) and thus misalign the events; timings and duration are the main inaccuracies."
      }
    },
    {
      "question_id": "003",
      "question": "After the guest confirms that the framing of questions of law is 'all important', when does he state that 'manifest injustice' is the category for most cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4407.975,
        "end": 4412.261
      },
      "pred_interval": {
        "start": 4450.0,
        "end": 4465.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.024999999999636,
        "end": 52.73899999999958,
        "average": 47.38199999999961
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7960585951805115,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relationship, but the reported E2 timestamps (4450.0\u20134465.0s) are significantly later than the ground truth (4407.975\u20134412.261s), a major factual timing error."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interviewer finishes asking how the speaker's different fields and readings affect their argument style, when does the speaker respond by saying 'Sir, you're absolutely right'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4537.667,
        "end": 4541.776
      },
      "pred_interval": {
        "start": 4545.0,
        "end": 4548.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.332999999999629,
        "end": 6.22400000000016,
        "average": 6.7784999999998945
      },
      "rationale_metrics": {
        "rouge_l": 0.2181818181818182,
        "text_similarity": 0.762662947177887,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the quoted response and the claimed immediacy, but it gives a start time (4545.0s) that contradicts the ground truth (E2 starts at 4537.667s and ends at 4541.776s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that 'Advocacy is a communicative skill', when does he elaborate that 'One part of a communicative skill is language'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4562.343,
        "end": 4567.786
      },
      "pred_interval": {
        "start": 4580.0,
        "end": 4583.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.657000000000153,
        "end": 15.213999999999942,
        "average": 16.435500000000047
      },
      "rationale_metrics": {
        "rouge_l": 0.3571428571428571,
        "text_similarity": 0.809842586517334,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase but gives a wrong start time for E2 (4580.0s) and an incorrect 'after' relation; the correct E2 occurs earlier (4562.343\u20134567.786) and directly elaborates on E1 (4563.201\u20134564.942)."
      }
    },
    {
      "question_id": "003",
      "question": "After the interviewer mentions watching Justice Muralidhar's YouTube, when does the interviewer give an example from him about being asked 'did you murder?'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4617.635,
        "end": 4624.683
      },
      "pred_interval": {
        "start": 4615.0,
        "end": 4618.0
      },
      "iou": 0.03769492925743902,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6350000000002183,
        "end": 6.682999999999993,
        "average": 4.6590000000001055
      },
      "rationale_metrics": {
        "rouge_l": 0.4827586206896552,
        "text_similarity": 0.7501674890518188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the segments conceptually and the 'after' relationship, but it gives an incorrect start time for E2 (4615.0s) that contradicts the ground-truth timeline (E1 ends at 4617.595s) and omits the E2 end time, so the timing is factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines the appeal as an 'extension of the trial', when does he explain that notes can 'awaken you to the subtleties of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4673.51,
        "end": 4680.899
      },
      "pred_interval": {
        "start": 4710.0,
        "end": 4725.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.48999999999978,
        "end": 44.10099999999966,
        "average": 40.29549999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7213969230651855,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the relative order (target after anchor) but the reported timestamps are substantially off from the ground-truth times and the durations/intervals do not match, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising how to objectively approach an appeal, when does he state that 'You have to absorb all the witnesses of the surrounding environment'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4746.348,
        "end": 4752.661
      },
      "pred_interval": {
        "start": 4745.0,
        "end": 4760.0
      },
      "iou": 0.42086666666667344,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3479999999999563,
        "end": 7.338999999999942,
        "average": 4.343499999999949
      },
      "rationale_metrics": {
        "rouge_l": 0.1764705882352941,
        "text_similarity": 0.6901065111160278,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the target follows the anchor and includes the quoted phrase, but the timestamps are imprecise (anchor time is approximate and target start/end deviate from the reference, with the predicted end notably extended), so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'then I become a lawyer', when does he describe the 'story of the lawyer of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4805.697,
        "end": 4824.178
      },
      "pred_interval": {
        "start": 4780.0,
        "end": 4795.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.697000000000116,
        "end": 29.177999999999884,
        "average": 27.4375
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.6199020743370056,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase, the target content, and their 'after' relationship, but the provided timestamps are substantially different from the ground truth, so the answer is largely temporally incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the 'art of communication' as the 'foundation of law', when does he state that its effectiveness depends on the 'quality of your preparation'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4848.488,
        "end": 4862.369
      },
      "pred_interval": {
        "start": 4880.0,
        "end": 4890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.511999999999716,
        "end": 27.631000000000313,
        "average": 29.571500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.41666666666666663,
        "text_similarity": 0.8894227743148804,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the 'after' relation, but it omits the anchor timestamps and gives target timestamps that are significantly later than the ground truth, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the initial speaker says 'Thank you, sir', when does the second speaker begin apologizing for not taking all questions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4960.598,
        "end": 4970.611
      },
      "pred_interval": {
        "start": 4960.0,
        "end": 4970.0
      },
      "iou": 0.8860616341532517,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5979999999999563,
        "end": 0.6109999999998763,
        "average": 0.6044999999999163
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8815809488296509,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the relation ('after') and approximate E2 interval right, but it misstates E1 timing (starts ~1s late and omits E1's end) and shifts E2 boundaries slightly; these omissions and timing inaccuracies reduce accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the host refers to 'Q and Q, quality and quantity insurance', when does he mention the three important judgments: Prabhu, Rishikesh, and Vijay?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4996.141,
        "end": 5009.076
      },
      "pred_interval": {
        "start": 5010.0,
        "end": 5020.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.859000000000378,
        "end": 10.923999999999978,
        "average": 12.391500000000178
      },
      "rationale_metrics": {
        "rouge_l": 0.5142857142857143,
        "text_similarity": 0.8704830408096313,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' but gives incorrect event timings and misrepresents E1 (saying it starts rather than finishes) and places E2 much later (5010\u20135020s vs the correct 4996.141\u20135009.076s), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes quoting 'abhi na jao chhod ke, dil abhi bhara nahi', when does he state 'That's what the common sense is'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5019.2,
        "end": 5022.6
      },
      "pred_interval": {
        "start": 5025.0,
        "end": 5029.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.800000000000182,
        "end": 6.399999999999636,
        "average": 6.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.5121951219512195,
        "text_similarity": 0.7378976941108704,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly matches E1 timing but places E2 several seconds later than the ground truth and mislabels the temporal relation ('after' vs. 'once_finished'), so it fails to capture the correct timing and relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker expresses surprise about people increasing viewership on YouTube, when does he mention everybody was viewed in the Zoom chat?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5030.3,
        "end": 5032.8
      },
      "pred_interval": {
        "start": 5042.0,
        "end": 5045.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.699999999999818,
        "end": 12.199999999999818,
        "average": 11.949999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.8125982284545898,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation, but both event time ranges are substantially later (\u22489\u201312s) than the ground truth, so the temporal boundaries are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Thank you, sir' for the first time, when does he specifically thank Tanu Bedi?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5046.2,
        "end": 5049.1
      },
      "pred_interval": {
        "start": 5048.0,
        "end": 5051.0
      },
      "iou": 0.22916666666673377,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.800000000000182,
        "end": 1.8999999999996362,
        "average": 1.849999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.3548387096774193,
        "text_similarity": 0.7961198091506958,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation and gives approximate timestamps for both events, but the reported times are slightly shifted from the reference (E1 ~0.6\u20131.1s late; E2 start/end ~1.8\u20132.0s late), so minor temporal inaccuracies exist."
      }
    },
    {
      "question_id": "001",
      "question": "Once Alex finishes asking about the key challenges facing individuals who are called to give evidence, when does Paul begin to talk about nervousness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.237,
        "end": 36.762
      },
      "pred_interval": {
        "start": 29.0,
        "end": 30.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.237000000000002,
        "end": 6.2620000000000005,
        "average": 5.249500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.6897642612457275,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relationship direction right ('after') but the timestamps are substantially incorrect (anchor 28.5s vs 33.216s; target 29.0\u201330.5s vs 33.237\u201336.762s) and it omits the precise 'immediately after' timing\u2014constituting major factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "Once Alex finishes asking what makes the actual process of giving evidence so difficult, when does Paul begin to describe the cross-examination process?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.92,
        "end": 92.191
      },
      "pred_interval": {
        "start": 73.0,
        "end": 75.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.920000000000002,
        "end": 16.691000000000003,
        "average": 14.305500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.7536036968231201,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative order ('after') right but the timestamps are significantly incorrect (E1: 72.5s vs 83.718s; E2 start: 73.0s vs 84.92s; E2 end: 75.5s vs 92.191s), so it fails to match the correct event boundaries."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes describing the anecdote about the expert witness losing credibility, when does he start describing the other example of a fact witness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 172.683,
        "end": 180.236
      },
      "pred_interval": {
        "start": 185.0,
        "end": 187.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.317000000000007,
        "end": 6.76400000000001,
        "average": 9.540500000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.8189973831176758,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly labels the relation as 'after' but the reported timestamps are substantially different from the ground truth (off by ~12\u201314s), so it contradicts the factual timing and is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker describes the sarcastic comments made about expert witness answers, when does he recount the witness's emotional reaction?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.3,
        "end": 164.2
      },
      "pred_interval": {
        "start": 180.0,
        "end": 190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.69999999999999,
        "end": 25.80000000000001,
        "average": 24.25
      },
      "rationale_metrics": {
        "rouge_l": 0.5263157894736842,
        "text_similarity": 0.9314640760421753,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer matches the relation ('after'), it gives incorrect and inconsistent timestamps for both anchor and target (and omits E1 end), so it fails to align with the key temporal details in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker asks what witness familiarisation actually means, when does the first speaker begin to define the term?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 197.7,
        "end": 204.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 220.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.300000000000011,
        "end": 16.0,
        "average": 14.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.8012286424636841,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general ordering (definition follows the question) but the timestamps are substantially different and the anchor/target timings are incorrect, so it contradicts key factual details of the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker finishes describing the benefits of witness familiarisation for individuals, when does he next outline the benefits for instructing solicitors?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 300.4,
        "end": 308.0
      },
      "pred_interval": {
        "start": 240.0,
        "end": 250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.39999999999998,
        "end": 58.0,
        "average": 59.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7915797233581543,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps for both the anchor and target are substantially incorrect and contradict the ground truth (240s/250s vs 299.0s/300.4\u2013308.0s), and the relation label ('after') does not match the specified 'next', so it fails to align with the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker explains why they don't jump straight into cross-examination, when does he start talking about the need for theory?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 356.6,
        "end": 365.0
      },
      "pred_interval": {
        "start": 340.0,
        "end": 345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.600000000000023,
        "end": 20.0,
        "average": 18.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.31746031746031744,
        "text_similarity": 0.7765384912490845,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same events but the timestamps for both E1 and E2 are substantially incorrect (and E2's predicted times contradict the ground-truth ordering), so key factual elements are wrong despite the matching 'after' label."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the first speaker is discussing the differences in virtual hearings, when does he mention judges requiring witnesses to demonstrate they are alone?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 394.0,
        "end": 398.0
      },
      "pred_interval": {
        "start": 370.0,
        "end": 375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 23.0,
        "average": 23.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4193548387096774,
        "text_similarity": 0.7831249237060547,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the overall relation ('during') and topic (virtual hearings) but the time spans are substantially incorrect: the anchor start is 15.5s early and the target interval (370.0\u2013375.0s) does not match or overlap the correct 394.0\u2013398.0s window, so the answer is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the second speaker finishes thanking Paul, when does he mention sharing some of his insights?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 435.4,
        "end": 437.5
      },
      "pred_interval": {
        "start": 445.0,
        "end": 450.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.600000000000023,
        "end": 12.5,
        "average": 11.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.4406779661016949,
        "text_similarity": 0.8043927550315857,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the correct temporal relation ('once'), but both event timestamps substantially differ from the reference (E1: 440.0s vs 420.0s; E2: 445.0\u2013450.0s vs 435.4\u2013437.5s), so the prediction is factually incorrect on key timing details."
      }
    }
  ]
}