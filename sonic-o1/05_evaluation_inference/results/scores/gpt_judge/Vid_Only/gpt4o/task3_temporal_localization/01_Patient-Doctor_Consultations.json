{
  "topic_id": 1,
  "topic_name": "Patient-Doctor Consultations",
  "num_evaluated": 269,
  "aggregated_metrics": {
    "mean_iou": 0.047025634652034055,
    "std_iou": 0.14289115049639595,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.05947955390334572,
      "count": 16,
      "total": 269
    },
    "R@0.5": {
      "recall": 0.03345724907063197,
      "count": 9,
      "total": 269
    },
    "R@0.7": {
      "recall": 0.011152416356877323,
      "count": 3,
      "total": 269
    },
    "mae": {
      "start_mean": 33.11740892193309,
      "end_mean": 3509.862423791822,
      "average_mean": 1771.4899163568773
    },
    "rationale": {
      "rouge_l_mean": 0.3459023232948279,
      "rouge_l_std": 0.09127552956660848,
      "text_similarity_mean": 0.7177910921077303,
      "text_similarity_std": 0.09103322359157741,
      "llm_judge_score_mean": 2.970260223048327,
      "llm_judge_score_std": 1.6448207343869101
    },
    "rationale_cider": 0.5190187880519358
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the speaker welcomes viewers and introduces himself as 'Karma Medic', when does he state that he is a 'final year medical student'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 35.0,
        "end": 36.62
      },
      "pred_interval": {
        "start": 36.0,
        "end": 37.6
      },
      "iou": 0.23846153846153734,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 0.980000000000004,
        "average": 0.990000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.7200000000000001,
        "text_similarity": 0.8613682389259338,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target and their 'after' relationship, but the timestamps are inaccurate (anchor start given as 0.0s vs 3.54s in reference; target shifted ~1s later and end ~1s later), so it's partially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'Now with that lovely disclaimer out of the way, let's get right into it', when does the text 'before the history' appear on screen?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.06,
        "end": 57.06
      },
      "pred_interval": {
        "start": 95.5,
        "end": 97.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.44,
        "end": 39.94,
        "average": 39.69
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.8411535620689392,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's timestamps (E1 at 94.5s, E2 95.5\u201397.0s) directly contradict the ground truth (~56.03s and 56.06\u201357.06s) and it gives the relationship as 'after' instead of the correct 'once_finished' (immediate appearance), so it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'So before starting the history, there's generally two things that I try and keep in mind', when does he begin describing 'washing your hands'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 206.36,
        "end": 207.36
      },
      "pred_interval": {
        "start": 105.0,
        "end": 106.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.36000000000001,
        "end": 100.86000000000001,
        "average": 101.11000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.8780031204223633,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relationship 'after' correct but both anchor and target timestamps (and target phrase timing) are incorrect compared to the ground truth, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the acronym 'ICE', when does he explain what it stands for?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 155.7,
        "end": 158.7
      },
      "pred_interval": {
        "start": 156.0,
        "end": 159.0
      },
      "iou": 0.8181818181818119,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.30000000000001137,
        "end": 0.30000000000001137,
        "average": 0.30000000000001137
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.5540623664855957,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction matches the reference closely: the mention and explanation times are within ~0.3s, the explanation content matches, and the relation 'after' is preserved (predicted as 'immediately after'). Minor timing offsets and added visual-cue detail do not meaningfully contradict the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes listing the components of the WIPER acronym, when does he start elaborating on 'washing your hands'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.0,
        "end": 212.0
      },
      "pred_interval": {
        "start": 214.0,
        "end": 219.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 7.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5070422535211268,
        "text_similarity": 0.7484608292579651,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and a similar temporal relation, but the timestamps are uniformly off by about 9 seconds (E1/E2 times and E2 duration disagree with the reference), so the answer is factually inaccurate despite matching the event types."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what brought the patient in, when does he explain what the 'history of presenting complaint' is about?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 346.0,
        "end": 351.0
      },
      "pred_interval": {
        "start": 373.0,
        "end": 390.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.0,
        "end": 39.0,
        "average": 33.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.7421977519989014,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the events and their temporal relation ('after'), but the provided timestamps deviate substantially from the ground truth (both E1 and E2 are moved much later), so the key factual timing information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "How long after the speaker says he'll put a picture of all possible questions does the \"REVIEW OF SYSTEMS\" checklist first appear on screen?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 539.8,
        "end": 543.7
      },
      "pred_interval": {
        "start": 535.0,
        "end": 540.0
      },
      "iou": 0.022988505747131543,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.7999999999999545,
        "end": 3.7000000000000455,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2531645569620253,
        "text_similarity": 0.6331979036331177,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets E1 nearly correct and the relation ('shortly after') aligns, but E2 timestamps are incorrect and inconsistent with the reference (wrong start time and a fabricated end time), omitting the correct 29.8\u201333.7s interval."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is giving examples of systems review questions, when does he ask about \"tummy pain\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 565.74,
        "end": 566.422
      },
      "pred_interval": {
        "start": 560.0,
        "end": 561.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.740000000000009,
        "end": 5.4220000000000255,
        "average": 5.581000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.6755647659301758,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially different timestamps and an incorrect temporal relation ('after' vs correct 'during'); the time intervals for both E1 and E2 are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions the \"JAM THREADS\" mnemonic, when does he say the name \"Sketchy Medical\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 699.531
      },
      "pred_interval": {
        "start": 600.0,
        "end": 602.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.0,
        "end": 97.53099999999995,
        "average": 96.76549999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7182076573371887,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but the timestamps are substantially incorrect and the relation 'immediately after' contradicts the correct temporal span (speaker says 'Sketchy Medical' much later). Major factual timing errors warrant a low score."
      }
    },
    {
      "question_id": "001",
      "question": "While the speaker describes Sketchy Medical, when does he mention drugs' mechanism of action and side effects?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 701.0,
        "end": 703.982
      },
      "pred_interval": {
        "start": 732.0,
        "end": 740.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.0,
        "end": 36.01800000000003,
        "average": 33.509000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7693418860435486,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'during' relationship, its timestamps are substantially off (anchor predicted ~730s vs ground truth 697.5s; target predicted 732\u2013740s vs ground truth 701.0\u2013703.98s), so the temporal information is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks a general question about family health, when does he suggest being specific about asthma, diabetes, and hypertension?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.914,
        "end": 745.914
      },
      "pred_interval": {
        "start": 812.0,
        "end": 820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.08600000000001,
        "end": 74.08600000000001,
        "average": 71.58600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24691358024691357,
        "text_similarity": 0.6035330295562744,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps conflict substantially with the reference (predicted ~810\u2013820s vs. ground truth ~730.7s and 742.9\u2013745.9s), so the key factual timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining the importance of signposting, when does he ask if the patient uses any recreational drugs?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 811.123,
        "end": 812.664
      },
      "pred_interval": {
        "start": 870.0,
        "end": 880.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.87699999999995,
        "end": 67.33600000000001,
        "average": 63.10649999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324326,
        "text_similarity": 0.6534125208854675,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the target occurs after the anchor, but the timestamps are substantially inaccurate (off by ~65+ seconds) and the predicted duration is much longer; it also fails to reflect that the question 'immediately follows' the explanation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"concerns from ICE\", when does he start saying \"Just generally, if you're feeling stuck\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 880.187,
        "end": 883.471
      },
      "pred_interval": {
        "start": 878.0,
        "end": 880.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.187000000000012,
        "end": 3.4710000000000036,
        "average": 2.8290000000000077
      },
      "rationale_metrics": {
        "rouge_l": 0.5294117647058824,
        "text_similarity": 0.7311176657676697,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') but gives incorrect and inconsistent timestamps (both ~2s early), sets E2 start equal to E1 end rather than after, and omits the E2 end time."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"golden rulebook\", when does he open both hands outwards in a gesture?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 895.8,
        "end": 897.5
      },
      "pred_interval": {
        "start": 895.0,
        "end": 897.0
      },
      "iou": 0.4800000000000182,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7999999999999545,
        "end": 0.5,
        "average": 0.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7236168384552002,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the relation and approximate timing correct, but the anchor time is ~1.15s off, the target start is ~0.8s early, and the predicted answer omits the target end time, so it is incomplete and slightly inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I hope you find this video useful\", when does he say \"Peace\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 910.148,
        "end": 910.609
      },
      "pred_interval": {
        "start": 915.0,
        "end": 917.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.851999999999975,
        "end": 6.390999999999963,
        "average": 5.621499999999969
      },
      "rationale_metrics": {
        "rouge_l": 0.46875000000000006,
        "text_similarity": 0.7450891733169556,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the general ordering ('after') but is factually inaccurate: it gives incorrect and much later timestamps (915.0s vs 909.546\u2013910.609s), sets anchor and target to the same time (contradicting the correct slight delay), and omits the target end time\u2014so it is not a precise match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying he has an appointment at 10 am, when does the green text 'Sure, what's your name?' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 6.1,
        "end": 8.2
      },
      "pred_interval": {
        "start": 3.5,
        "end": 5.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5999999999999996,
        "end": 3.1999999999999993,
        "average": 2.8999999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.7894012331962585,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly captures the qualitative relation (immediate/once_finished), its timestamps are substantially incorrect for both the anchor (3.5s vs 5.9s) and target (3.5\u20135.0s vs 6.1\u20138.2s), so it does not align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes stating his name, when does the green text 'Thank you, Lucas. Please take a seat...' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 11.9,
        "end": 19.0
      },
      "pred_interval": {
        "start": 8.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9000000000000004,
        "end": 9.0,
        "average": 6.45
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.6696960926055908,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misreports all timestamps (E1 is 10.6s, not 8.0s; E2 is 11.9\u201319.0s, not 8.0\u201310.0s) and gives a different relation; it only vaguely matches the fact that the text follows the name, but is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks 'How long is the wait?', when does the green text 'About 10 minutes. Would you like some water while you wait?' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 22.1,
        "end": 25.3
      },
      "pred_interval": {
        "start": 15.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.100000000000001,
        "end": 8.3,
        "average": 7.700000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.18750000000000003,
        "text_similarity": 0.7836064696311951,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps (off by several seconds for both anchor and target) and incorrectly labels the relation as immediate rather than after with a slight pause, so it contradicts the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the video explains the 'we're a team' approach with animated graphics, when does the speaker appear at his desk looking at a computer?",
      "video_id": "WR5dACe-spg",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 59.0
      },
      "gt_interval": {
        "start": 34.6,
        "end": 36.0
      },
      "pred_interval": {
        "start": 38.0,
        "end": 39.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.3999999999999986,
        "end": 3.0,
        "average": 3.1999999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481928,
        "text_similarity": 0.7702423334121704,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same events but significantly misstates timings and their relation: E1 actually ends ~34.6s and E2 starts ~34.0\u201334.6s (overlapping), whereas the prediction shifts E1/E2 later (36s and 38\u201339s) and incorrectly labels E2 as 'after', contradicting the reference."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker says 'take that extra bit of time to listen', when does the 'OK' hand gesture emoji appear?",
      "video_id": "WR5dACe-spg",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 59.0
      },
      "gt_interval": {
        "start": 44.0,
        "end": 45.5
      },
      "pred_interval": {
        "start": 50.0,
        "end": 51.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 5.5,
        "average": 5.75
      },
      "rationale_metrics": {
        "rouge_l": 0.5675675675675675,
        "text_similarity": 0.88276207447052,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the emoji appears during the spoken phrase, but the timestamps are substantially incorrect (predicted ~50.0\u201351.0s vs. ground truth 42.8\u201345.5s), so the answer is largely factually wrong on timing."
      }
    },
    {
      "question_id": "001",
      "question": "After Nurse Kim mentions graduating as a registered nurse, when does she talk about working for many different pharmaceutical companies?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.0,
        "end": 50.475
      },
      "pred_interval": {
        "start": 37.0,
        "end": 42.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 8.475000000000001,
        "average": 7.237500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131148,
        "text_similarity": 0.6815681457519531,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct semantic relation ('after') and correctly identifies the nursing remark, but the timestamp for E1 is slightly off and the E2 interval is substantially mislocalized (starts and ends several seconds earlier than the ground truth), so the answer is largely incorrect on crucial timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nurse Kim finishes describing her background as an 'incredible journey', when does she mention training side-by-side with Dr. Jugenberg for five years?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 149.87,
        "end": 153.25
      },
      "pred_interval": {
        "start": 80.0,
        "end": 85.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.87,
        "end": 68.25,
        "average": 69.06
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.5906416773796082,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'once finished' relation, but the timestamps are substantially incorrect compared to the ground truth (E1 off by ~33s and E2 off by ~30s), a major factual mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "While Nurse Kim explains options and possible outcomes, when does she begin examining the patient's stomach?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 157.5,
        "end": 160.5
      },
      "pred_interval": {
        "start": 153.0,
        "end": 156.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.5,
        "end": 4.5,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.6835752725601196,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('while') right but the timestamps are significantly incorrect (both start times are several seconds earlier and the predicted exam ends before the correct interval), so it misrepresents the factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "After Nurse Kim finishes discussing the benefits, risks, and possible complications of the procedure, when does she start talking about asymmetry?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 169.7,
        "end": 172.0
      },
      "pred_interval": {
        "start": 162.0,
        "end": 165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.699999999999989,
        "end": 7.0,
        "average": 7.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6939707398414612,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates that the asymmetry discussion occurs after the risks/benefits discussion, but the timestamps are substantially incorrect (off by ~8\u201310s) and it fails to reflect that the target occurs immediately after the anchor; it also adds an unjustified end time for E2."
      }
    },
    {
      "question_id": "003",
      "question": "Once Nurse Kim finishes explaining that the one-hour consultation cannot provide everything you need to know, when does she mention that they are always available?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 201.5,
        "end": 203.71
      },
      "pred_interval": {
        "start": 210.0,
        "end": 214.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.5,
        "end": 10.289999999999992,
        "average": 9.394999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.17142857142857143,
        "text_similarity": 0.4871600270271301,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes availability is mentioned after the consultation comment but gives significantly different timestamps (208.0s/210.0s vs. ground truth 201.5s) and misses the immediate transition, so it is factually incorrect on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces himself and the topic, when does the slide change to 'Objectives for today's lesson'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 24.379,
        "end": 24.5
      },
      "pred_interval": {
        "start": 0.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.379,
        "end": 4.5,
        "average": 14.4395
      },
      "rationale_metrics": {
        "rouge_l": 0.3846153846153846,
        "text_similarity": 0.8024465441703796,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but both event timestamps are inaccurate (E1 wrongly starts at 0.0s and omits its end time; E2 is given as 20.0s vs the correct 24.379s), so key temporal details are incorrect or missing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes listing the objectives for the lesson, when does the slide change to 'Brain storming time'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 46.529,
        "end": 47.0
      },
      "pred_interval": {
        "start": 40.0,
        "end": 60.0
      },
      "iou": 0.023549999999999828,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.5290000000000035,
        "end": 13.0,
        "average": 9.764500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.30434782608695654,
        "text_similarity": 0.65507572889328,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it labels E1 as the start of listing rather than the finish and gives E2 at 60.0s instead of 46.529s, and the relation 'after' is less precise than 'once_finished'; key timing facts are contradicted."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes defining communication as the successful passage of a message from one person to another, when does he start explaining how good communication manifests in medical practice by informing patients of their diagnosis?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.0,
        "end": 177.0
      },
      "pred_interval": {
        "start": 160.0,
        "end": 172.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 5.0,
        "average": 6.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.6983981132507324,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the target immediately follows the anchor, but the timestamps are notably off\u2014the anchor/target are shifted ~7s late and the predicted target ends ~5s early\u2014omitting portions of the labeled target interval."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the 'Importance of communication' slide, when does he begin discussing that good doctor-patient communication has been linked to improved patient satisfaction?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 190.0,
        "end": 198.0
      },
      "pred_interval": {
        "start": 200.0,
        "end": 215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 17.0,
        "average": 13.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7599313259124756,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timings contradict the ground truth: the anchor is given as 177.5\u2013179.5s and the target as 190.0\u2013198.0s, but the prediction places the anchor at 198.0s and the target at 200.0\u2013215.0s, so the timestamps and durations are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker starts talking about how a lot of malpractice lawsuits have been documented, when does he explicitly advise being aware of communication's importance to avoid lawsuits?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 226.0,
        "end": 271.0
      },
      "pred_interval": {
        "start": 230.0,
        "end": 240.0
      },
      "iou": 0.2222222222222222,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 31.0,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2702702702702703,
        "text_similarity": 0.819003701210022,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the advice follows the lawsuits comment, but it misidentifies the anchor timing (198\u2013212s vs predicted 225s) and only partially covers the target (226\u2013271s vs predicted 230\u2013240s), so the timestamps are largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the initial slide 'Communication is not just talking' is displayed, when does the speaker mention that physicians can improve health outcomes?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.28,
        "end": 346.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 350.0
      },
      "iou": 0.093283582089552,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.720000000000027,
        "end": 4.0,
        "average": 4.860000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.8020029664039612,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the 'after' relationship and the target start falls within the true event window, but the predicted start is ~5.7s later than the reference and the predicted end extends ~4s past the true end, so timings are slightly inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the slide showing two images (bored girl vs. smiling doctor/patient), when does the speaker describe the first image as depicting a 'horribly bored' lady?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 354.8,
        "end": 359.0
      },
      "pred_interval": {
        "start": 410.0,
        "end": 415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.19999999999999,
        "end": 56.0,
        "average": 55.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.7688009738922119,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces both events: the reference E1 is 347.8\u2013410.7s and E2 is 354.8\u2013359.0s, while the prediction gives E1 starting at 405.0s and E2 at 410\u2013415s. This shifts the description to mostly after the slide ends, contradicting the ground truth despite also claiming a 'during' relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker defines verbal communication as 'using spoken words', when is the next time they define non-verbal communication?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 428.87,
        "end": 433.596
      },
      "pred_interval": {
        "start": 500.0,
        "end": 505.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.13,
        "end": 71.404,
        "average": 71.267
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.7881827354431152,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates that the non-verbal definition comes after the verbal one, but it gives substantially different timestamps and durations (and uses a vague 'after' instead of the reference's 'immediately follows'), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'golden minute', when does he describe the patient's hypothetical response?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 613.818,
        "end": 630.0
      },
      "pred_interval": {
        "start": 512.0,
        "end": 516.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.81799999999998,
        "end": 114.0,
        "average": 107.90899999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4594594594594595,
        "text_similarity": 0.8741514682769775,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relation as 'after', both anchor and target timestamps are substantially incorrect (off by many seconds) and the predicted target duration/location contradicts the ground truth, making it factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'Checking facts', when does he mention the next essential element of listening?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 641.157,
        "end": 642.461
      },
      "pred_interval": {
        "start": 518.0,
        "end": 522.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.15700000000004,
        "end": 120.46100000000001,
        "average": 121.80900000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.8678514957427979,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the next element ('Checking feelings') and the 'after' relationship, but the provided timestamps are substantially different from the ground-truth times and do not reflect the immediate succession indicated in the reference, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Before the speaker says 'So, for example, we have three main types of reflective listening', when does he explain what reflective listening involves?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 667.457,
        "end": 687.051
      },
      "pred_interval": {
        "start": 530.0,
        "end": 535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.457,
        "end": 152.05100000000004,
        "average": 144.75400000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.7838444709777832,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it provides completely different timestamps and swaps anchor/target roles, misidentifies the span where reflective listening is defined, and thus fails to match the ground truth despite both stating a 'before' relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the three main types of reflective listening, when does he start explaining the 'Repeating' example?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 710.0,
        "end": 737.0
      },
      "pred_interval": {
        "start": 702.0,
        "end": 708.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 29.0,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.6629631519317627,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation right ('after') but the reported timestamps for both E1 and E2 deviate substantially from the reference (E1 off by ~6s, E2 start off by ~8s and end off by ~29s), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining the 'Repeating' example, when does he introduce 'Rephrasing'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 720.0,
        "end": 720.4
      },
      "pred_interval": {
        "start": 718.0,
        "end": 725.0
      },
      "iou": 0.057142857142853894,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 4.600000000000023,
        "average": 3.3000000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.31746031746031744,
        "text_similarity": 0.6110893487930298,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that 'Rephrasing' follows the 'Repeating' example and roughly when it is introduced, but it misstates the anchor end time (708s vs 698s), gives a slightly different start for E2 (718s vs 720s) and adds an end time not present in the reference, so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing 'Reflection of feeling by showing empathy', when does the 'Non-verbal' slide appear?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 780.0,
        "end": 821.5
      },
      "pred_interval": {
        "start": 880.0,
        "end": 885.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.0,
        "end": 63.5,
        "average": 81.75
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.6712459325790405,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the ordering (Non-verbal comes after the empathy discussion) but the timestamps are substantially different from the reference (778.5s/780.0s vs 875.0s/880.0\u2013885.0s), and it adds an unsupported end time and a looser 'after' relation rather than the immediate 'once_finished'."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises to smile, when does he mention checking for signs of pain?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 873.045,
        "end": 882.0
      },
      "pred_interval": {
        "start": 880.0,
        "end": 885.0
      },
      "iou": 0.16729401923881165,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.955000000000041,
        "end": 3.0,
        "average": 4.9775000000000205
      },
      "rationale_metrics": {
        "rouge_l": 0.4242424242424243,
        "text_similarity": 0.6847142577171326,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relationship, but the timestamps differ noticeably from the reference (E1 ~4.1s late, E2 start ~6.96s late and E2 end ~3s late), so it is directionally correct but temporally inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses the cultural interpretations of folding arms, when does he advise to avoid folding arms?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 932.0,
        "end": 936009.0
      },
      "pred_interval": {
        "start": 895.0,
        "end": 900.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 935109.0,
        "average": 467573.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4776119402985075,
        "text_similarity": 0.7338184118270874,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but gives substantially incorrect time intervals (about 25\u201340 seconds early) and wrong span for E2; although it asserts an 'after' relation, the temporal localizations do not match the ground truth, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to introduce yourself to the patient, when does he advise to explain your role as a student or intern?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 985.0,
        "end": 990.853
      },
      "pred_interval": {
        "start": 910.0,
        "end": 915.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.0,
        "end": 75.85299999999995,
        "average": 75.42649999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.6467891335487366,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order right ('after') but the reported start/end times for both E1 and E2 are substantially incorrect and do not match the ground-truth intervals, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"if you're in the hospital\", when does he refer to \"inpatient patients\"?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1059.6,
        "end": 1059.8
      },
      "pred_interval": {
        "start": 1060.0,
        "end": 1065.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.40000000000009095,
        "end": 5.2000000000000455,
        "average": 2.800000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540983,
        "text_similarity": 0.6233739256858826,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target and that the target occurs after the anchor, but the reported times are imprecise: the anchor is shifted ~2.5s late and the target's end time is substantially overstated compared to the brief 1059.6\u20131059.8s reference."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is explaining how to start a consultation, when does he give the example \"how can I help you today?\"",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1069.0,
        "end": 1070.0
      },
      "pred_interval": {
        "start": 1075.0,
        "end": 1080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 10.0,
        "average": 8.0
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.7840363383293152,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly identifies the same events but misplaces both timestamps by ~5\u201310 seconds and gives an incorrect temporal relation (places E2 much later and labels it 'during'), so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining the 'golden minute', when does he announce the end of the lecture?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1090.0,
        "end": 1094.0
      },
      "pred_interval": {
        "start": 1115.0,
        "end": 1121.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.0,
        "end": 27.0,
        "average": 26.0
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.7913225889205933,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the temporal relation ('after') but gives substantially incorrect timestamps and durations (off by ~21\u201327 seconds) compared to the reference, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "While Raquel is talking about the hospital providing opportunities for nurses, when is she shown smiling and opening a package?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 2.0,
        "end": 4.5
      },
      "pred_interval": {
        "start": 10.0,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 10.5,
        "average": 9.25
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.6375792026519775,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and the 'during' relationship, but it omits the specific timestamps and precise timing details (E1 0.031\u20135.0s, E2 2.0\u20134.5s) given in the correct answer, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once Maria finishes saying that new nurses will be nudged to become lifelong learners, when does Precious state that the teamwork is strong?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 14.321,
        "end": 16.486
      },
      "pred_interval": {
        "start": 25.0,
        "end": 30.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.679,
        "end": 13.514,
        "average": 12.096499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.6340382099151611,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the relative sequence (Precious speaks immediately after Maria and the relation 'once'), but it omits the precise timestamps (14.301s, 14.321s\u201316.486s) given in the correct answer, thus missing key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After Reny states that the hospital does things up to a magnet level, when does Raquel say her values align with the hospital's values?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 42.854,
        "end": 50.692
      },
      "pred_interval": {
        "start": 40.0,
        "end": 45.0
      },
      "iou": 0.200710811821923,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.853999999999999,
        "end": 5.692,
        "average": 4.273
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131145,
        "text_similarity": 0.5489926338195801,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target and that Raquel speaks after Reny, but it omits the precise timestamps and detailed timing (start 42.854s, end 50.692s) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that healthcare in Siem Reap is not the best, when is the Royal Angkor International Hospital first shown on screen?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 94.0,
        "end": 99.1
      },
      "pred_interval": {
        "start": 85.0,
        "end": 90.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 9.099999999999994,
        "average": 9.049999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.275,
        "text_similarity": 0.7003433704376221,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction reasonably approximates E1 timing, but it gives a significantly incorrect E2 start (85.0s vs 94.0s) and omits the speaker's description start at 99.1s, so it is largely inaccurate on key facts."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing the Neak Tep Hospital, when does he begin describing the Ly Sreyvyna II Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 180.289,
        "end": 185.074
      },
      "pred_interval": {
        "start": 175.0,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.288999999999987,
        "end": 5.074000000000012,
        "average": 5.1815
      },
      "rationale_metrics": {
        "rouge_l": 0.37681159420289856,
        "text_similarity": 0.7703801393508911,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the temporal relationship ('after') but gives substantially incorrect timestamps (E1 ~173s vs 165.611s; E2 start 175s vs 180.289s) and omits the E2 end time, so it is incomplete and factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he visited a clinic for chest congestion, when does he mention the Paschern Dental Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 209.8,
        "end": 211.4
      },
      "pred_interval": {
        "start": 190.0,
        "end": 200.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.80000000000001,
        "end": 11.400000000000006,
        "average": 15.600000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.7512322664260864,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the provided timestamps are substantially different from the reference (off by ~20\u201330s), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing the Neak Tep Hospital, when does he introduce the Ly Sreyvyna II Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 184.0,
        "end": 184.8
      },
      "pred_interval": {
        "start": 210.0,
        "end": 220.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 35.19999999999999,
        "average": 30.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.49230769230769234,
        "text_similarity": 0.7523416876792908,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but it gives substantially incorrect timestamps and durations for both events (E1: 182.0s vs 205.0s; E2: 184.0\u2013184.8s vs 210.0\u2013220.0s), omitting the precise timing in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker introduces the Cigna International Health Policy, when is the insurance quote form displayed with personal information?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 351.0,
        "end": 360.0
      },
      "pred_interval": {
        "start": 350.0,
        "end": 360.0
      },
      "iou": 0.9,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 0.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837845,
        "text_similarity": 0.7772325277328491,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the form and its end time correctly but misstates the anchor timing (saying E1 starts at ~340s and omitting its end vs. 350.0\u2013352.8s), slightly shifts E2 start (350s vs 351s), and gives the wrong relation ('after' instead of 'once_finished')."
      }
    },
    {
      "question_id": "002",
      "question": "After the voiceover states that the Cigna policy is \"fairly typical of policies of this type\", when does the Cigna website display the form for inputting personal details to get a quote?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 456.0
      },
      "gt_interval": {
        "start": 352.9,
        "end": 358.0
      },
      "pred_interval": {
        "start": 372.0,
        "end": 375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.100000000000023,
        "end": 17.0,
        "average": 18.05000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3243243243243243,
        "text_similarity": 0.6138659715652466,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relationship, but the timestamps are off by ~17\u201319 seconds (anchor 351.0s\u2192368.0s; target 352.9\u2013358.0s\u2192372.0\u2013375.0s) and the predicted end time contradicts the ground truth, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the voiceover mentions \"evacuation service, also part of Cigna plan\", when is the Global Rescue website displayed on screen?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 456.0
      },
      "gt_interval": {
        "start": 384.0,
        "end": 431.0
      },
      "pred_interval": {
        "start": 438.0,
        "end": 441.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.0,
        "end": 10.0,
        "average": 32.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.813758909702301,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order and labels right (anchor then target) but the timestamps and duration are significantly incorrect (shifted by ~56s and vastly shorter E2 span), so it does not match the correct answer's key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the host concludes his introduction about the fight in modern healthcare, when does he introduce Sarah?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 19.4,
        "end": 22.0
      },
      "pred_interval": {
        "start": 64.0,
        "end": 67.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.6,
        "end": 45.0,
        "average": 44.8
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6891167759895325,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation correct ('after') but the event timestamps are substantially incorrect (off by ~45s) and the descriptions diverge, so it fails to match the reference timing and details."
      }
    },
    {
      "question_id": "002",
      "question": "While Sarah is introducing herself and her genetic condition, when does she mention having her very first surgery?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.08,
        "end": 108.8
      },
      "pred_interval": {
        "start": 88.0,
        "end": 92.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.08,
        "end": 16.799999999999997,
        "average": 16.439999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.7657526135444641,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor (Sarah's introduction) and the 'during' relation, but it mislocates the temporal boundaries: ground truth E2 is 104.08\u2013108.08s (and E1 begins at 95.0s) while the prediction places E2 at 88.0\u201392.0s, a significant mismatch. This large timing error makes the answer mostly incorrect despite the right relation label."
      }
    },
    {
      "question_id": "001",
      "question": "Once Sarah finishes describing her role as a volunteer patient representative for a non-profit organization, when does the static image showing her behind a 'CHILDREN'S TUMOR FOUNDATION' table appear?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 185.0,
        "end": 190.0
      },
      "pred_interval": {
        "start": 192.0,
        "end": 208.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 18.0,
        "average": 12.5
      },
      "rationale_metrics": {
        "rouge_l": 0.45783132530120485,
        "text_similarity": 0.8233934640884399,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives completely different timestamps and durations (E1 at 192s vs 150s; E2 192\u2013208s vs 185\u2013190s) and thus is factually incorrect despite matching the stated relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once Sarah finishes explaining the purpose of the 'Shine a Light Walk' to raise money and awareness, when does the video clip showing children running at an outdoor event play?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 189.0,
        "end": 192.0
      },
      "pred_interval": {
        "start": 240.0,
        "end": 250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.0,
        "end": 58.0,
        "average": 54.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7195842266082764,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relative order ('immediately after') but the timestamps are materially incorrect and contradict the ground truth (179.0s vs 240.0s for E1; 189.0\u2013192.0s vs 240.0\u2013250.0s for E2), so it fails on factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Steve asks if the 'Shine a Light Walk' goes throughout the world, when does Sarah begin to explain that the walks do not?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 253.2,
        "end": 258.88
      },
      "pred_interval": {
        "start": 300.0,
        "end": 310.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.80000000000001,
        "end": 51.120000000000005,
        "average": 48.96000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.74090975522995,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly locates both events by ~47 seconds (E1 at 300.0s vs 252.5s; E2 start at 300.0s vs 253.2s) and wrongly makes the response simultaneous rather than shortly after; it only correctly captures the notion of an immediate response, so overall largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes asking Sarah what things in miscommunication can lead to delays or misdiagnosis, when does the woman start responding?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 362.48,
        "end": 365.44
      },
      "pred_interval": {
        "start": 354.0,
        "end": 356.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.480000000000018,
        "end": 9.439999999999998,
        "average": 8.960000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6503918170928955,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly captures the qualitative relation ('immediately after'), it gives timestamps that contradict the ground truth by roughly 8 seconds (353.8/354.0s vs 361.920/362.480s), so key factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman gives the example of writing 'hyperthyroid instead of hypothyroid', when does the man respond with 'That that's pretty bad'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 389.2,
        "end": 432.5
      },
      "pred_interval": {
        "start": 422.0,
        "end": 424.0
      },
      "iou": 0.046189376443418,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.80000000000001,
        "end": 8.5,
        "average": 20.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.23376623376623376,
        "text_similarity": 0.635077714920044,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the man's line follows the woman's example, but the timestamps are substantially incorrect (off by ~33s) and it overstates the timing as 'immediately after' rather than the short pause indicated in the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the man says he tried researching miscommunication problems, when does he state his finding about thousands of preventable deaths?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 446.56,
        "end": 535.68
      },
      "pred_interval": {
        "start": 508.0,
        "end": 510.0
      },
      "iou": 0.022441651705565543,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.44,
        "end": 25.67999999999995,
        "average": 43.559999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.1643835616438356,
        "text_similarity": 0.4541357159614563,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the statement about deaths follows the research comment, but the timestamps are substantially different from the reference (507.8\u2013508.0s vs. 435.0\u2013451.68s range) and it wrongly characterizes the relation as 'immediately after' despite the reference showing a several-second separation, so the answer is largely temporally inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What's in my budget to fix it?\", when does she start asking, \"How important is it to me to fix this issue?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 518.66,
        "end": 522.26
      },
      "pred_interval": {
        "start": 514.2,
        "end": 519.0
      },
      "iou": 0.042183622828788354,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.459999999999923,
        "end": 3.259999999999991,
        "average": 3.859999999999957
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.594373881816864,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same utterances but the timestamps substantially disagree with the reference (anchor/target times off by several seconds and target end shorter), and it claims immediate succession contrary to the referenced timing, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man finishes saying, \"not continuing medical bills,\" when does he start asking, \"So, what does successful self-advocacy look like?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 643.04,
        "end": 646.32
      },
      "pred_interval": {
        "start": 652.5,
        "end": 656.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.460000000000036,
        "end": 9.67999999999995,
        "average": 9.569999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.6600435376167297,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but the timestamps are substantially off (anchor and target ~9\u201318s later than ground truth) and it incorrectly labels their relationship as 'immediate succession' despite the correct answer showing a gap; thus it is largely factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes explaining what a doctor's follow-up might entail, when does the woman start asking, \"Or will I actually be able to get into your office in two weeks?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 679.0,
        "end": 683.92
      },
      "pred_interval": {
        "start": 694.0,
        "end": 699.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 15.080000000000041,
        "average": 15.04000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.6761410236358643,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the woman's question immediately follows the man's explanation, but it gives substantially incorrect timestamps and durations (off by ~16\u201317 seconds and different span), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Immediately after the woman asks if she should follow up if she is still experiencing symptoms, when does the man ask what if the symptoms go away?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 699.38,
        "end": 707.15
      },
      "pred_interval": {
        "start": 711.0,
        "end": 715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.620000000000005,
        "end": 7.850000000000023,
        "average": 9.735000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.4324324324324324,
        "text_similarity": 0.8446673154830933,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and claims they are 'immediately after', but the provided timestamps are substantially different from the ground truth (off by several seconds) and do not preserve the immediate-follow relationship indicated in the reference, so the timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying to voice symptoms and concerns clearly, when does he give an example about shoulder pain?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 734.59,
        "end": 737.0
      },
      "pred_interval": {
        "start": 730.0,
        "end": 735.0
      },
      "iou": 0.05857142857142402,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.590000000000032,
        "end": 2.0,
        "average": 3.295000000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7999400496482849,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives timestamps that substantially deviate from the reference (E1 end 725.0s vs 733.68s; E2 730.0\u2013735.0s vs 734.59\u2013737.0s) and thus misalign the events; while it conveys a post-finish relationship, it fails to capture the immediate adjacency and accurate times."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes warning not to try putting a hand in an electrical outlet, when does the woman agree and say not to try that?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 810.0,
        "end": 812.0
      },
      "pred_interval": {
        "start": 780.0,
        "end": 783.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 29.0,
        "average": 29.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.753344714641571,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the woman replying immediately after the man, but the timestamps are significantly off (predicted 776/780\u2013783s vs correct 808/810\u2013812s), so the temporal facts are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes saying to assume benevolence of your doctor, when does the man begin to speak?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 878.9,
        "end": 879.1
      },
      "pred_interval": {
        "start": 880.0,
        "end": 882.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1000000000000227,
        "end": 2.8999999999999773,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.39393939393939387,
        "text_similarity": 0.6662514805793762,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction preserves the sequence and relation (woman finishes then man speaks) and is a reasonable paraphrase of 'once_finished' as 'immediately after', but the reported timestamps differ from the reference by about 1\u20131.5 seconds, so it is slightly temporally inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man asks about trying non-surgical options first, when does the woman reply 'Yes'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 899.7,
        "end": 900.1
      },
      "pred_interval": {
        "start": 920.0,
        "end": 922.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.299999999999955,
        "end": 21.899999999999977,
        "average": 21.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.6706702709197998,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the ordering (woman replies right after the man) but gives timestamps ~20 seconds later than the reference and thus fails to match the key factual timing; the relation wording is a minor variant."
      }
    },
    {
      "question_id": "003",
      "question": "After the man concludes his statement about how to ask for another opinion, when does the woman respond that asking for another opinion is definitely valid?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 982.0,
        "end": 988.72
      },
      "pred_interval": {
        "start": 1015.0,
        "end": 1017.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 28.279999999999973,
        "average": 30.639999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.6160634756088257,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct pair of events (woman responds after the man) but the timestamps differ substantially from the reference (off by ~33\u201338s) and the relation 'immediately after' contradicts the reference 'after', so it is only a partial match."
      }
    },
    {
      "question_id": "001",
      "question": "After the man suggests bringing someone along if you're not feeling safe, when does the woman agree that it's advisable?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1127.0,
        "end": 1130.0
      },
      "pred_interval": {
        "start": 1094.0,
        "end": 1106.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 24.0,
        "average": 28.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.6850109696388245,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the man\u2019s suggestion, the woman\u2019s agreement, and the \u2018after\u2019 relation, but the event timestamps are substantially earlier than the reference (off by ~40s and ~33s) and the woman's quoted wording differs, so the timing/alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman talks about a doctor not trusting a patient's pain because they don't act like they're in pain, when does she give an example of a loved one vouching for the patient?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1167.68,
        "end": 1174.48
      },
      "pred_interval": {
        "start": 1215.0,
        "end": 1228.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.319999999999936,
        "end": 53.51999999999998,
        "average": 50.41999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.33707865168539325,
        "text_similarity": 0.6874061822891235,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the qualitative relationship ('after') but the timestamps are substantially incorrect compared with the ground truth (E1 1161\u20131166 vs 1200.0; E2 1167.68\u20131174.48 vs 1215.0\u20131228.0), so the factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks if it is legal to be given your own medical records, when does the woman confirm that it is?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1268.6,
        "end": 1270.7
      },
      "pred_interval": {
        "start": 1238.0,
        "end": 1240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.59999999999991,
        "end": 30.700000000000045,
        "average": 30.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.6540008783340454,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation (confirmation follows the question) but gives incorrect timestamps for the target (1238\u20131240s vs correct 1268.6\u20131270.7s) and fails to provide the anchor timing, so the temporal localization is substantially wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman mentions that things have changed a lot with electronic medical records, when does the man state that bureaucracy reminds him of common barriers?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1333.0,
        "end": 1339.5
      },
      "pred_interval": {
        "start": 1285.0,
        "end": 1288.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.0,
        "end": 51.5,
        "average": 49.75
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.8512368202209473,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly labels the relation as 'after' but provides incorrect/contradictory timestamps for the target (1285.0\u20131288.0 vs the correct 1333.0\u20131339.5) and omits the anchor timestamps, so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks about common barriers and how to overcome them, when does the woman share her fear of ants?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1377.36,
        "end": 1383.7
      },
      "pred_interval": {
        "start": 1390.0,
        "end": 1393.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.6400000000001,
        "end": 9.299999999999955,
        "average": 10.970000000000027
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.7897951006889343,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor (the man's question) and the temporal relation ('after'), but it gives substantially incorrect timestamps for the target event (1390\u20131393s vs. the correct 1377.36\u20131383.7s), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says to write things down on paper and give it to the doctor, when does he mention a doctor refusing to look at the paper?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1484.96,
        "end": 1490.0
      },
      "pred_interval": {
        "start": 1535.0,
        "end": 1544.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.039999999999964,
        "end": 54.0,
        "average": 52.01999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.38,
        "text_similarity": 0.8582979440689087,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and their 'after' relationship, but the provided timestamps are substantially and incorrectly shifted (much later) compared to the ground-truth times, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "While the woman discusses prioritizing cognition, when does she state that she would rather be in pain than have her mental capacity harmed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1534.64,
        "end": 1542.24
      },
      "pred_interval": {
        "start": 1582.0,
        "end": 1588.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.3599999999999,
        "end": 45.75999999999999,
        "average": 46.559999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.45833333333333337,
        "text_similarity": 0.8276317119598389,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the semantic relation and the quoted phrase, but it gives an incorrect time window for E2 (off by ~48 seconds) and fails to provide the anchor timestamps specified in the correct answer, so the key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man asks 'Nord, what is that?', when does the woman state what NORD stands for?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1613.4,
        "end": 1615.4
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1625.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.599999999999909,
        "end": 9.599999999999909,
        "average": 8.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.27777777777777773,
        "text_similarity": 0.7960594892501831,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the woman states what NORD stands for and that it occurs after the man's question, but the provided timestamps are substantially later than the ground truth and it fails to capture the key detail that the target immediately follows the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says 'I read that I need to start this at 30', when does she explain why she needs the doctor to order it?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1692.24,
        "end": 1711.28
      },
      "pred_interval": {
        "start": 1660.0,
        "end": 1665.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.24000000000001,
        "end": 46.27999999999997,
        "average": 39.25999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.40860215053763443,
        "text_similarity": 0.6662851572036743,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterance and relation ('after') but the timestamps for both the anchor and target are substantially incorrect and do not match the ground-truth segments, so it fails on factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman explains how to mirror a planned course of action, when does she suggest asking the doctor what they heard?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1797.0,
        "end": 1799.8
      },
      "pred_interval": {
        "start": 1818.0,
        "end": 1822.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 22.200000000000045,
        "average": 21.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.7890787124633789,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and that the target follows the anchor, but the provided timestamps substantially contradict the ground-truth times (anchor/target start and end times are incorrect), so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the man advises to 'just dig' and not use a medical dictionary, when does he ask if medical language can be 'dumbed down'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1836.56,
        "end": 1841.52
      },
      "pred_interval": {
        "start": 1884.0,
        "end": 1888.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.440000000000055,
        "end": 46.48000000000002,
        "average": 46.960000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.6630844473838806,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering and identifies the same utterances, but the timestamps are substantially incorrect (off by ~47\u201368 seconds) and it omits the correct contextual note about the intervening discussion of complex terminology."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks what to do when doctors look rushed, when does the woman describe slowing down and capturing their attention?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1965.6,
        "end": 1973.5
      },
      "pred_interval": {
        "start": 1980.0,
        "end": 2005.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.400000000000091,
        "end": 31.5,
        "average": 22.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.6046810150146484,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relation ('after') and event descriptions, but the reported timestamps are substantially shifted later (E1 ~+21s, E2 start ~+14s and end ~+32s) and the E2 duration is inflated, so the timing does not align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes suggesting a doctor might be having a bad day, when does the man humorously ask if doctors have bad days?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2002.5,
        "end": 2004.0
      },
      "pred_interval": {
        "start": 2070.0,
        "end": 2090.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.5,
        "end": 86.0,
        "average": 76.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7947725653648376,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives times that are substantially offset and durations that are incorrect, and it labels the relation as generic 'after' rather than the immediate succession stated in the correct answer, so it does not match the reference timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the man introduces the 'five practical tips to advocate for yourself', when does the woman begin talking about writing down questions?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2195.28,
        "end": 2199.7
      },
      "pred_interval": {
        "start": 2180.0,
        "end": 2189.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.2800000000002,
        "end": 10.699999999999818,
        "average": 12.990000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.1643835616438356,
        "text_similarity": 0.6513105630874634,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation correct ('after') but both anchor and target timestamps are substantially earlier than the reference ranges (anchor off by ~39s, target off by ~15s), so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the man's explanation about preparing beforehand, when does he demonstrate by pointing to his neck?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2235.0,
        "end": 2237.0
      },
      "pred_interval": {
        "start": 2265.0,
        "end": 2272.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 35.0,
        "average": 32.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7907280921936035,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly identifies the 'during' relationship, its anchor and target timestamps are substantially off from the reference (anchor 2240.0s vs 2225s; target 2265\u20132272s vs 2235\u20132237s), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man describes getting dizzy when walking up and down stairs, when does the woman mention repeating back what was heard?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2316.0,
        "end": 2317.0
      },
      "pred_interval": {
        "start": 2385.0,
        "end": 2390.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.0,
        "end": 73.0,
        "average": 71.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.7441328763961792,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the provided timestamps are significantly offset (~70 seconds later) from the ground truth, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman expresses her inability to distract herself from the pain, when does the man advise her to be specific?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2368.7,
        "end": 2369.5
      },
      "pred_interval": {
        "start": 2465.0,
        "end": 2470.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.30000000000018,
        "end": 100.5,
        "average": 98.40000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.7689840793609619,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the provided timestamps are substantially off (\u2248100s difference) from the ground truth, so the temporal anchoring is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says 'document everything', when does the woman affirm the advice and tell viewers to take notes?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2504.5,
        "end": 2506.0
      },
      "pred_interval": {
        "start": 2498.0,
        "end": 2505.0
      },
      "iou": 0.0625,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.5,
        "end": 1.0,
        "average": 3.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3823529411764706,
        "text_similarity": 0.6538007259368896,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same utterances and the 'after' relation, but the timestamps are noticeably off (E1: predicted 2495.0s vs 2499.9s; E2: predicted 2498.0\u20132505.0s vs 2504.5\u20132506.0s), so it is a partial but imperfect match."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes asking if one should ask permission before recording their doctor, when does the woman respond?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2531.6,
        "end": 2533.5
      },
      "pred_interval": {
        "start": 2570.0,
        "end": 2578.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.40000000000009,
        "end": 44.5,
        "average": 41.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.6588212847709656,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative order and paraphrases the woman's reply, but the provided timestamps are substantially incorrect (off by ~36\u201345 seconds) and the event durations differ, so the temporal localization is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman begins explaining the hope that doctors will focus more on patients with AI recording, when does she explain why she almost always checks her online appointment notes?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2566.0,
        "end": 2579.0
      },
      "pred_interval": {
        "start": 2635.0,
        "end": 2645.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.0,
        "end": 66.0,
        "average": 67.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7678655982017517,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer preserves the correct temporal relation ('after') and identifies the same events, but the provided timestamps are substantially shifted (off by ~68s) and the E2 duration differs from the ground truth, so the prediction is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man asks if one should be assertive, when does he introduce the topic of emotional intelligence?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2701.0,
        "end": 2710.0
      },
      "pred_interval": {
        "start": 2695.0,
        "end": 2702.0
      },
      "iou": 0.06666666666666667,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 8.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7077591419219971,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events and the temporal order with timestamps within ~1s of the reference; it only omits the nuance that the introduction occurs immediately after a short pause and gives slightly rounded times."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man says 'You wanna learn some breathing control', when does he start describing box breathing?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2740.0,
        "end": 2747.0
      },
      "pred_interval": {
        "start": 2760.0,
        "end": 2765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 18.0,
        "average": 19.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4411764705882353,
        "text_similarity": 0.6895685195922852,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the description occurs after the prompt, but the absolute timestamps are incorrect (predicted events are ~30s later than the reference), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "While the man is saying 'If you want, share your story in the comments', when is the 'COMMENT BELOW' graphic displayed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2850.0,
        "end": 2992.0
      },
      "gt_interval": {
        "start": 2920.0,
        "end": 2923.0
      },
      "pred_interval": {
        "start": 2905.0,
        "end": 2907.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 16.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413793,
        "text_similarity": 0.5874769687652588,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events as simultaneous but is largely incorrect: both event timestamps and the graphic duration are misplaced by ~15 seconds and the predicted end time vastly shortens the actual display, so it fails on factual timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the thumbs up icon appears on screen, when is the next graphic ('COMMENT BELOW') displayed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2850.0,
        "end": 2992.0
      },
      "gt_interval": {
        "start": 2920.0,
        "end": 2923.0
      },
      "pred_interval": {
        "start": 2905.0,
        "end": 2907.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 16.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3880597014925374,
        "text_similarity": 0.7776684761047363,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the next graphic ('COMMENT BELOW') and that it follows the thumbs-up, but the reported times (2905.0\u20132907.0) conflict with the ground truth (2920.0\u20132923.0) and the prediction omits the anchor's exact timestamp (2862.0), so the timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Marissa Fourie introduces herself, when does she mention cross-cultural communication?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 34.2,
        "end": 36.5
      },
      "pred_interval": {
        "start": 5.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.200000000000003,
        "end": 26.5,
        "average": 27.85
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363637,
        "text_similarity": 0.7611582279205322,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the timestamps are substantially incorrect (E1: 2.0s vs 8.1s; E2: 5.0\u201310.0s vs 34.2\u201336.5s), so it fails on factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "After mentioning cross-cultural communication, when does Marissa Fourie next mention personality-specific communication skills?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 37.0,
        "end": 39.0
      },
      "pred_interval": {
        "start": 15.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.0,
        "end": 19.0,
        "average": 20.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4838709677419355,
        "text_similarity": 0.7162564992904663,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction preserves a general ordering (E2 after E1) but is largely incorrect: the reported start/end times for both events differ substantially from the ground truth, and the relation label 'after' does not match the ground-truth 'next' (immediate follow)."
      }
    },
    {
      "question_id": "003",
      "question": "After encouraging viewers to join PhysioPlus, when does Marissa Fourie say 'See you there!'?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 62.9,
        "end": 63.7
      },
      "pred_interval": {
        "start": 65.0,
        "end": 68.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.1000000000000014,
        "end": 4.299999999999997,
        "average": 3.1999999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3934426229508197,
        "text_similarity": 0.7051920890808105,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and the 'after' relation, but the provided timestamps are notably inaccurate (E1 is ~11s late; E2 start/end are off by ~2\u20134s), so it fails to match the precise timing in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes mentioning \"the dosage in each area\", when does the woman in blue gloves point to the glabella area of the patient's forehead?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 4.469,
        "end": 4.8
      },
      "pred_interval": {
        "start": 10.0,
        "end": 12.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.531,
        "end": 7.2,
        "average": 6.3655
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.663159966468811,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation as 'after' but the timestamps are substantially incorrect (predicted ~9.5\u201312.0s vs ground truth 4.161\u20134.469/4.800s) and it invents a longer pointer duration, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining the dosage for the brow lift, when does the woman in blue gloves point to the patient's upper lip?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 12.121,
        "end": 12.5
      },
      "pred_interval": {
        "start": 15.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8789999999999996,
        "end": 4.5,
        "average": 3.6895
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.6975328326225281,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order (E2 occurs after E1) but the timestamps and duration are substantially incorrect (predicted ~14.5\u201317.0s vs ground truth ~12.08\u201312.50s) and the relation label ('after') is less precise than 'once_finished'."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining the dosage for the lip flip, when does the text \"TIME TO INJECT!\" appear on screen?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 18.291,
        "end": 21.0
      },
      "pred_interval": {
        "start": 19.0,
        "end": 20.0
      },
      "iou": 0.369139904023625,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7089999999999996,
        "end": 1.0,
        "average": 0.8544999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.6885465383529663,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely disagrees with the reference: it misstates E1 timing (18.5s vs 15.067s), gives an imprecise E2 start and an incorrect finite end (19.0\u201320.0s vs 18.291s until end), and the relationship label is less precise\u2014so it is mostly incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the host welcomes Rich, when does Rich begin his response?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.015,
        "end": 34.078
      },
      "pred_interval": {
        "start": 4.5,
        "end": 6.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.515,
        "end": 28.078000000000003,
        "average": 28.2965
      },
      "rationale_metrics": {
        "rouge_l": 0.2909090909090909,
        "text_similarity": 0.8466137647628784,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the temporal relation right ('after'), but the absolute timestamps for both the host welcome and Rich's response are incorrect and it omits the correct precise timing (33.015s) \u2014 key factual elements are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "While Rich is explaining how medicine may have let relationships with patients deteriorate, when does he say that scientific facts will protect us?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 89.0,
        "end": 93.76
      },
      "pred_interval": {
        "start": 65.0,
        "end": 67.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 26.760000000000005,
        "average": 25.380000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.5916810035705566,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps contradict the ground truth\u2014E1 was given as starting at 73.611s and E2 at 89.0\u201393.76s, whereas the prediction places them around 60\u201367s\u2014so the phrase is mislocalized; although it notes a 'during' relationship, the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the host asks what trust looks like in the future with intermediaries, when does Rich first discuss the stethoscope in relation to technology in medicine?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 112.0,
        "end": 113.0
      },
      "pred_interval": {
        "start": 120.0,
        "end": 122.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 9.0,
        "average": 8.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7003626823425293,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps are significantly off (predicted E1 ~115s vs reference 106.718s; predicted E2 120.0s vs reference 112.700s) and it adds an end time not specified in the ground truth, so the answer is inaccurate despite getting the order right."
      }
    },
    {
      "question_id": "001",
      "question": "After the man in glasses finishes describing the giant TV screen in a new hospital exam room, when does the video show a patient interacting with a screen in a hospital bed?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 167.6,
        "end": 177.6
      },
      "pred_interval": {
        "start": 180.0,
        "end": 185.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.400000000000006,
        "end": 7.400000000000006,
        "average": 9.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.5063291139240506,
        "text_similarity": 0.9110738039016724,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly labels the relationship as 'after', but it omits the anchor timestamp and gives significantly incorrect start/end times for the target interval (180.0\u2013185.0s vs. the correct 167.6\u2013177.6s), so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "While the interviewer asks if technology can bring doctors and patients closer together, when is he holding a small white 'Trust tv' card?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 178.0,
        "end": 183.5
      },
      "pred_interval": {
        "start": 200.0,
        "end": 205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.0,
        "end": 21.5,
        "average": 21.75
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.7397996187210083,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the card is held 'while' the question is asked, but gives completely different timestamps (200\u2013205s) that do not overlap the ground-truth interval (178\u2013183.5s) and fails to match the required temporal grounding."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer thanks Rich and says viewers learned a lot, when does Rich respond 'It's really a pleasure'?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 210.3,
        "end": 212.1
      },
      "pred_interval": {
        "start": 220.0,
        "end": 222.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.699999999999989,
        "end": 9.900000000000006,
        "average": 9.799999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.46153846153846156,
        "text_similarity": 0.8303260207176208,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct anchor and target utterances and a similar 'once' relation, but the start/end timestamps are off by ~10 seconds and do not reflect the immediate succession stated in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions learning about 'patient rapport', when does he discuss charting and interacting with other healthcare providers?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 2.075,
        "end": 9.55
      },
      "pred_interval": {
        "start": 1.0,
        "end": 12.5
      },
      "iou": 0.65,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0750000000000002,
        "end": 2.9499999999999993,
        "average": 2.0124999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3561643835616438,
        "text_similarity": 0.7456303834915161,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct ordering and approximate start times (E1 ~1.0s, E2 ~2.5s) but omits E1's end, gives an incorrect/extended E2 end (12.5s vs 9.55s), and uses a less precise relation ('after' vs immediate 'once_finished')."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker talks about developing skills like putting an IV, when does he mention getting a patient discharged?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 15.42,
        "end": 24.583
      },
      "pred_interval": {
        "start": 17.0,
        "end": 21.0
      },
      "iou": 0.43653825166430216,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.58,
        "end": 3.5829999999999984,
        "average": 2.5814999999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6988165974617004,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct event types (putting an IV and patient discharge) but misrepresents both timestamps and the temporal relation\u2014the anchor/target times contradict the ground truth and the relation should be 'once_finished' (immediate) rather than 'after'."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Make their problem, your problem', when does he introduce the importance of self-care?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 45.009,
        "end": 48.396
      },
      "pred_interval": {
        "start": 30.0,
        "end": 45.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.009,
        "end": 3.396000000000001,
        "average": 9.2025
      },
      "rationale_metrics": {
        "rouge_l": 0.325,
        "text_similarity": 0.6454527974128723,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation 'after' correct but misstates both event time spans (E1 and E2 times differ substantially from the reference), omitting the correct timestamps and introducing incorrect boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of herself, when does she mention specializing in wounds?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 22.605,
        "end": 26.329
      },
      "pred_interval": {
        "start": 0.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.605,
        "end": 16.329,
        "average": 19.467
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.3698287606239319,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events (introduction and mention of specializing in wounds) and the 'during' relation, but the provided timestamps are far from the ground-truth intervals and thus factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the topic of 'getting the most out of your GP consultation', when does she mention that GP practices are getting a huge injection of funding?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.82,
        "end": 75.533
      },
      "pred_interval": {
        "start": 10.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.81999999999999,
        "end": 55.533,
        "average": 56.6765
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.5483641624450684,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the timestamps (around 10\u201320s) are far from the ground-truth intervals (\u224862\u201376s), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "While the slide titled 'Appointments are precious' is on screen, when does the speaker mention that GP practices are moving back towards face-to-face appointments?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 123.0,
        "end": 129.0
      },
      "pred_interval": {
        "start": 70.0,
        "end": 80.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.0,
        "end": 49.0,
        "average": 51.0
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.670379638671875,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timings for both the slide (70s vs ~100.74s) and the speaker mention (75\u201380s vs 123\u2013129s) are substantially incorrect, so the claimed overlap ('while') is unsupported by the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that GP practices are very different places now, when does she begin listing the specific roles in a GP practice?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 203.0,
        "end": 204.0
      },
      "pred_interval": {
        "start": 180.0,
        "end": 190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.0,
        "end": 14.0,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.646973192691803,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', the anchor and target timestamps are substantially incorrect (off by ~25s and ~23s respectively) and the predicted target span does not match the ground truth event, so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide displays the question 'Does it need to be a GP?', when does the speaker mention that paramedics work in primary care?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "pred_interval": {
        "start": 200.0,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.0,
        "end": 30.0,
        "average": 32.5
      },
      "rationale_metrics": {
        "rouge_l": 0.37142857142857144,
        "text_similarity": 0.7447556257247925,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation right (the mention occurs after the slide), but the timestamps are substantially incorrect (anchor off by ~10s and the paramedic mention placed ~25\u201335s earlier than the reference), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker talks about paramedics working in primary care, when does she begin to explain the role of Advanced Clinical Practitioners?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 241.0,
        "end": 249.0
      },
      "pred_interval": {
        "start": 220.0,
        "end": 230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 19.0,
        "average": 20.0
      },
      "rationale_metrics": {
        "rouge_l": 0.32727272727272727,
        "text_similarity": 0.7323059439659119,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both state the same 'after' relationship, the predicted timestamps are substantially incorrect\u2014predicted E1/E2 (210\u2013230s) occur well before the ground-truth E1 end (238.5s) and E2 start (241.0s), so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the problem of a wound on your foot, when does she strongly advise mentioning if you are diabetic?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.875,
        "end": 343.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 340.0
      },
      "iou": 0.16346153846153846,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.875,
        "end": 3.0,
        "average": 5.4375
      },
      "rationale_metrics": {
        "rouge_l": 0.31428571428571433,
        "text_similarity": 0.6950868368148804,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states the advice occurs immediately after the problem introduction, but the provided timestamps are shifted earlier by ~2\u20135 seconds, omit the anchor end time, and therefore do not match the reference boundaries precisely."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses having a new wound on your leg, when does she suggest going to a local pharmacist for simple dressings?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 363.968,
        "end": 366.552
      },
      "pred_interval": {
        "start": 355.0,
        "end": 365.0
      },
      "iou": 0.08933518005539996,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.968000000000018,
        "end": 1.552000000000021,
        "average": 5.260000000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.813809871673584,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the general topic, but the timestamp values are inaccurate and inconsistent with the reference (anchor off by ~6.7s and the target interval is completely wrong/fabricated), so it fails to match the ground truth timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker explains that a nurse's appointment is needed for long-standing wounds, when does she advise to clearly state how long the wound has been there?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 409.579,
        "end": 439.62
      },
      "pred_interval": {
        "start": 380.0,
        "end": 390.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.579000000000008,
        "end": 49.620000000000005,
        "average": 39.599500000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.795483410358429,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') and the content sequence, but the provided timestamps are substantially different from the reference (off by ~45\u201365 seconds), so it fails to match the key factual timings."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if you feel more short of breath, when does she state that a GP or nurse practitioner might be needed the same day?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 522.783,
        "end": 525.113
      },
      "pred_interval": {
        "start": 512.0,
        "end": 518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.783000000000015,
        "end": 7.113000000000056,
        "average": 8.948000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.24691358024691357,
        "text_similarity": 0.6345059871673584,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general 'after' relationship and the content about seeing a GP/nurse, but the timestamps are substantially incorrect and the predicted target does not match the correct segment that follows the discussion of serious new leg swelling, so it fails on factual alignment and timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises to measure your ankle and calf, when does she give an example of a calf measurement that would 'perk up more interest'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 583.623,
        "end": 586.297
      },
      "pred_interval": {
        "start": 550.0,
        "end": 560.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.62300000000005,
        "end": 26.297000000000025,
        "average": 29.960000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324326,
        "text_similarity": 0.7220032215118408,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') but the reported timestamps are substantially incorrect\u2014both anchor and target times deviate greatly from the ground truth and the target interval does not match the correct 583.6\u2013586.3s window."
      }
    },
    {
      "question_id": "003",
      "question": "Once the slide changes to 'Photography', when does the speaker advise to 'expect to be asked for a photo'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 670.384,
        "end": 672.807
      },
      "pred_interval": {
        "start": 605.0,
        "end": 610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.38400000000001,
        "end": 62.807000000000016,
        "average": 64.09550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.8358806371688843,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relationship ('once') and the semantic event ordering right but the timestamps are substantially incorrect (predicted E1 ~600s vs ground-truth 650.676s; predicted E2 605\u2013610s vs ground-truth 670.384\u2013672.807s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions some GP practices use video consultations, when does she state that a good quality photograph is better than a video?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.278,
        "end": 717.251
      },
      "pred_interval": {
        "start": 740.0,
        "end": 745.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.72199999999998,
        "end": 27.749000000000024,
        "average": 26.735500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.6224216222763062,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but it omits the crucial numeric timestamps (708.98s and 714.278s) required by the correct answer, so key factual details are missing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the slide changes to 'Photography tips', when does the speaker begin discussing taking a close-up and further-away picture?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 738.601,
        "end": 740.91
      },
      "pred_interval": {
        "start": 760.0,
        "end": 765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.399,
        "end": 24.090000000000032,
        "average": 22.744500000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.3859649122807017,
        "text_similarity": 0.6394698619842529,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events and relation directionally, but it omits the precise timestamps (736.057s and 738.601s) and the exact relation label ('once_finished'), and it adds an unsupported remark about the segment ending shortly after."
      }
    },
    {
      "question_id": "003",
      "question": "After the slide changes to 'General top tips- face to face appointments', when does the speaker advise to 'Go suitably dressed'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 860.136,
        "end": 860.846
      },
      "pred_interval": {
        "start": 780.0,
        "end": 785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.13599999999997,
        "end": 75.846,
        "average": 77.99099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.6531661748886108,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and the 'after' relation, but it omits the required exact timestamps for E1 and E2 and is vague about the timing, making it incomplete compared to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises not to wear tight socks, trousers, or wellies, when does she suggest wearing something with quick access to lower limbs?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 873.0,
        "end": 877.5
      },
      "pred_interval": {
        "start": 910.0,
        "end": 920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 42.5,
        "average": 39.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.700749397277832,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the events and their 'after' relation, but the provided timestamps are significantly off (~30s later than reference) and the predicted answer omits the end time for E2, so it is partially but not fully accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising not to make chit-chat about the weather, when does she advise not to dodge the real problem?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 893.0,
        "end": 894.5
      },
      "pred_interval": {
        "start": 950.0,
        "end": 960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.0,
        "end": 65.5,
        "average": 61.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4324324324324324,
        "text_similarity": 0.6432170271873474,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same events and the 'once' relation (ordering) but the timestamps are substantially incorrect (predicted ~940\u2013950s vs ground truth ~888\u2013894s) and the temporal gap/durations do not match, making it factually misleading."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises to take a list of the medications you are actually taking, when does she advise against describing tablets by their appearance?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 948.0,
        "end": 969.0
      },
      "pred_interval": {
        "start": 1000.0,
        "end": 1010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.0,
        "end": 41.0,
        "average": 46.5
      },
      "rationale_metrics": {
        "rouge_l": 0.40506329113924044,
        "text_similarity": 0.690748929977417,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relationship, but the reported timestamps are substantially off (about 55\u201360 seconds later) compared to the ground truth, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises speaking to the practice in advance about a relative, when does she explain the reason for this advance arrangement due to confidentiality?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1065.0,
        "end": 1095.0
      },
      "pred_interval": {
        "start": 1092.0,
        "end": 1100.0
      },
      "iou": 0.08571428571428572,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.0,
        "end": 5.0,
        "average": 16.0
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.6471307277679443,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering (E1 before E2) but substantially misaligns both event timestamps (E1/E2 are placed much later than ground truth) and weakens the relation ('after' vs. the specified immediate 'once_finished'), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker suggests writing things down before an appointment to help structure what you say, when does she first ask 'How did it start?' regarding the leg problem?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1130.415,
        "end": 1131.738
      },
      "pred_interval": {
        "start": 1140.0,
        "end": 1146.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.585000000000036,
        "end": 14.261999999999944,
        "average": 11.92349999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7111524939537048,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mentions the same events but gives substantially incorrect timestamps (E1/E2 shifted and E1 labeled as a start not an end) and an incorrect relation ('after' vs immediate 'once_finished'), so it is largely misaligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes advising to ask to be referred to a specialist service, when does she start introducing the referrals examples?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1248.105,
        "end": 1249.385
      },
      "pred_interval": {
        "start": 1260.0,
        "end": 1275.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.894999999999982,
        "end": 25.61500000000001,
        "average": 18.754999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.4715999960899353,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation and matches the content (introducing referral examples), but it omits the explicit timestamps (1236.741s and 1248.105\u20131249.385s) provided in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that lymphoedema services can be patchy, when does she first advise writing to your MP?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1377.0,
        "end": 1378.0
      },
      "pred_interval": {
        "start": 1340.0,
        "end": 1355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 23.0,
        "average": 30.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.7274225950241089,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation and quotes the advising phrase, but it omits the key absolute timestamps (1335.096s and 1377.0\u20131378.0s) given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions that a GP will assess new leg swelling for onward referral, when does she explain there are many different causes?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1429.846,
        "end": 1432.0
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1435.0
      },
      "iou": 0.14359999999999976,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.846000000000004,
        "end": 3.0,
        "average": 6.423000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3950617283950617,
        "text_similarity": 0.6927224397659302,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both events and the temporal relation ('after') and quotes the phrasing for E2, but it omits the precise timestamps provided in the reference, so it is incomplete relative to the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what information you could take with you, when does she suggest looking up the National Wound Care Strategy Lower Limb Recommendations?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1465.0,
        "end": 1469.5
      },
      "pred_interval": {
        "start": 1445.0,
        "end": 1455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 14.5,
        "average": 17.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3209876543209877,
        "text_similarity": 0.6906598806381226,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the timestamps are substantially inaccurate (both E1 and E2 are shifted earlier by ~10\u201320s and E2's duration does not match), so it only partially aligns with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions escalating concerns to the practice manager, when does she mention escalating concerns to your MP?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1523.6,
        "end": 1525.7
      },
      "pred_interval": {
        "start": 1500.0,
        "end": 1510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.59999999999991,
        "end": 15.700000000000045,
        "average": 19.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.6758788824081421,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies both events but the timestamps are substantially incorrect and the temporal relation ('after' vs. correct 'next') does not match; key factual timing and relation details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says 'I'll stop sharing', when does she start reading the first question from a viewer?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1574.5,
        "end": 1578.5
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.5,
        "end": 36.5,
        "average": 33.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3947368421052631,
        "text_similarity": 0.7568036317825317,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct event order (E1 before E2) but the timestamps and durations differ substantially from the ground truth and the relation 'after' is less precise than the correct 'once_finished', so key factual details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker initially suggests the mum needs compression hosiery, when does she mention asking for an appointment with the nurse for stronger compression?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1654.942,
        "end": 1664.2
      },
      "pred_interval": {
        "start": 1610.0,
        "end": 1618.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.94200000000001,
        "end": 46.200000000000045,
        "average": 45.571000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.5063291139240507,
        "text_similarity": 0.8618836402893066,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('after') and roughly locates the anchor, but the target timestamps are significantly incorrect (off by ~45s from the reference) and the predicted span/end times do not match the ground truth, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'That is such a good question', when does she state that self-diagnosis via the internet is never a good idea?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1757.815,
        "end": 1762.821
      },
      "pred_interval": {
        "start": 1645.0,
        "end": 1653.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 112.81500000000005,
        "end": 109.82099999999991,
        "average": 111.31799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.5679012345679013,
        "text_similarity": 0.8407988548278809,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the 'after' relation, but the anchor and target timestamps are far from the ground-truth intervals (major temporal mismatches), so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that approaching a GP is about framing the conversation, when does she tell the viewer not to worry about being labeled a difficult patient?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1795.335,
        "end": 1798.383
      },
      "pred_interval": {
        "start": 1680.0,
        "end": 1687.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 115.33500000000004,
        "end": 111.38300000000004,
        "average": 113.35900000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.6847032904624939,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation and phrasing right but the timestamps are substantially incorrect (off by ~100+ seconds for both events) and the target duration mismatches, so it is not a correct localization."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker first says, 'Please don't worry about things like that', when does she next advise not to worry about being labelled as a difficult patient?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.66,
        "end": 1831.19
      },
      "pred_interval": {
        "start": 1850.0,
        "end": 1865.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.339999999999918,
        "end": 33.809999999999945,
        "average": 28.074999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.12195121951219513,
        "text_similarity": 0.6414110660552979,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only preserves the relative 'after' relationship but the timestamps differ substantially from the reference (predicted anchor ~1820s vs 1787.02s; predicted target 1850\u20131865s vs 1827.66\u20131831.19s) and it adds unsupported visual/audio cues, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, 'What can I do to maintain healthy legs or feet so I don't get any problems?', when does she start listing actions like 'walk' and 'legs up'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1865.412,
        "end": 1883.383
      },
      "pred_interval": {
        "start": 1900.0,
        "end": 1920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.587999999999965,
        "end": 36.61699999999996,
        "average": 35.602499999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.202020202020202,
        "text_similarity": 0.6418049335479736,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative relation ('after') and the content (listing actions) right but the reported timestamps are substantially incorrect (off by ~35\u201340s) and it adds unsupported audio/visual cues, so it fails to match the correct timing and contains extra hallucinated details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks how much is in the GP curriculum, when does she say 'I don't know'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1983.7,
        "end": 1984.201
      },
      "pred_interval": {
        "start": 2050.5,
        "end": 2052.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.79999999999995,
        "end": 67.79899999999998,
        "average": 67.29949999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.6199843883514404,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct relative order ('after') but the timestamps are substantially off (~66\u201369s later) and it fails to mark that the target immediately follows the anchor, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'I think it is something that Legs Matter can help with', when does she discuss Legs Matter influencing GP curriculums?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2004.063,
        "end": 2009.063
      },
      "pred_interval": {
        "start": 2100.0,
        "end": 2110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 95.9369999999999,
        "end": 100.9369999999999,
        "average": 98.4369999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21212121212121213,
        "text_similarity": 0.5919145345687866,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') but the reported timestamps for both the anchor and target are substantially different (\u2248106s later) than the ground truth intervals, so the timings are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if seeing a nurse practitioner is appropriate, when does she state that nurse practitioners are 'extremely experienced clinicians'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2062.584,
        "end": 2066.851
      },
      "pred_interval": {
        "start": 2150.0,
        "end": 2155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.41600000000017,
        "end": 88.14899999999989,
        "average": 87.78250000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.1818181818181818,
        "text_similarity": 0.5046254396438599,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relational order ('after') but the provided timestamps are substantially off from the reference (\u224889s later), so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"I understand the issue of smartphones and taking pictures too\", when does she first ask \"is there somebody who can help you?\"",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2320.0
      },
      "gt_interval": {
        "start": 2174.0,
        "end": 2176.0
      },
      "pred_interval": {
        "start": 2145.0,
        "end": 2146.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.0,
        "end": 29.5,
        "average": 29.25
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.5115017294883728,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the quoted utterance and correctly labels the temporal relation as 'after', but the provided timestamps are substantially earlier (~20\u201330s) than the ground truth, so the timing is factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the speaker discusses the importance of planning phone calls to the GP, when does she ask, \"What am I feeling?\"",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2320.0
      },
      "gt_interval": {
        "start": 2197.721,
        "end": 2198.663
      },
      "pred_interval": {
        "start": 2250.0,
        "end": 2251.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.278999999999996,
        "end": 52.83699999999999,
        "average": 52.55799999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.6745713353157043,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are significantly different from the ground truth (both anchor and target are shifted later), so the target is not within the true anchor interval; although it labels the relation as 'during', the timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Angelos finishes introducing Dr. Tolchin, when does Dr. Tolchin begin speaking?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.128,
        "end": 109.393
      },
      "pred_interval": {
        "start": 15.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.128,
        "end": 89.393,
        "average": 89.76050000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.7008329629898071,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the timestamps are far off (15.0s/20.0s vs. 100.128s/105.128s) and it omits the E2 duration; key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Dr. Angelos describes Dr. Tolchin's research on crisis standards of care, when does he describe his research on functional neurological disorders and epilepsy?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.426,
        "end": 116.456
      },
      "pred_interval": {
        "start": 90.0,
        "end": 95.0
      },
      "iou": 0.08329168748958854,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.574,
        "end": 21.456000000000003,
        "average": 27.515
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.7784653902053833,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer is largely incorrect: both event timestamps and durations disagree substantially with the ground truth, though it correctly states that E2 occurs after E1."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes stating the second learning objective, when does he start explaining the third learning objective?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 167.0,
        "end": 181.0
      },
      "pred_interval": {
        "start": 180.0,
        "end": 185.0
      },
      "iou": 0.05555555555555555,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 4.0,
        "average": 8.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4333333333333333,
        "text_similarity": 0.7419849634170532,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives completely incorrect absolute timestamps (175s/180s vs 16.4s/17.0s) and omits the third objective's span (17.0\u201331.0), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "While the slide titled 'Why conduct clinical ethics consultations?' is displayed, when does the speaker discuss moral distress among clinicians and staff?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 285.4,
        "end": 304.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 220.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.39999999999998,
        "end": 84.0,
        "average": 79.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4615384615384615,
        "text_similarity": 0.7729578018188477,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'during' relation but misstates critical timestamps (E1 start given as 205.0s vs 181.7s, and E2 incorrectly placed at 210.0s rather than 285.4\u2013304.0s) and omits E1 end time, so it is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that clinical ethics consultations were helpful, when does he state that they were more likely to achieve consensus in clinical decisions?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 350.2,
        "end": 357.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.800000000000011,
        "end": 13.0,
        "average": 11.400000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.6158221960067749,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are substantially off compared to the reference (E1 predicted ~350s vs true finish 337.0s; E2 predicted 360\u2013370s vs true 350.2\u2013357.0s), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the topic of resource utilization, when does he specifically state that there was a reduced length of stay?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 438.9,
        "end": 450.3
      },
      "pred_interval": {
        "start": 420.0,
        "end": 430.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.899999999999977,
        "end": 20.30000000000001,
        "average": 19.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3934426229508197,
        "text_similarity": 0.7524183988571167,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('after') and mentions both events, its timestamps are substantially different from the ground truth (E1 should finish at 369.0s; E2 should be 438.9\u2013450.3s), so it is factually incorrect on the key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'to look at disparities', when does he begin to introduce Ellen Fox's team and their survey?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 493.5,
        "end": 499.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 520.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.5,
        "end": 21.0,
        "average": 18.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4126984126984127,
        "text_similarity": 0.7215527892112732,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general ordering (target occurs after the anchor) but the timestamps diverge substantially from the reference (500.0 vs 393.0 for E1; 510.0\u2013520.0 vs 493.5\u2013499.0 for E2) and the relation label ('after') does not match the more specific 'once_finished'. These factual/time discrepancies make the answer largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says 'hospitals with less than 400 beds', when does he mention 'little or no growth over that two decade period'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 527.809,
        "end": 530.91
      },
      "pred_interval": {
        "start": 525.0,
        "end": 530.0
      },
      "iou": 0.37072758037225767,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.808999999999969,
        "end": 0.9099999999999682,
        "average": 1.8594999999999686
      },
      "rationale_metrics": {
        "rouge_l": 0.46341463414634143,
        "text_similarity": 0.8314239978790283,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the order ('after') and mentions both phrases, but the timestamps are notably inaccurate (anchor start ~4.8s early, target start ~2.8s early, target end ~0.9s early) and the anchor end time is omitted, so it fails to match the reference precisely."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide titled 'Prior Healthcare System Ethics Committees' is fully displayed, when do the images of the six hospitals with their bed counts appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 551.7,
        "end": 552.0
      },
      "pred_interval": {
        "start": 550.0,
        "end": 555.0
      },
      "iou": 0.05999999999999091,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.7000000000000455,
        "end": 3.0,
        "average": 2.3500000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.358974358974359,
        "text_similarity": 0.7338679432868958,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after'), but both event timestamps are inaccurate: the anchor is off by ~8.8s (545.0s vs 536.2s) and the target timing/duration (550.0\u2013555.0s) does not match the precise appearance (551.7s\u2013552.0s). These significant timing errors reduce correctness despite the correct relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states the number of ethics consults at Yale New Haven Hospital increased from 50 to 239, when does he describe this as 'approximately a five-fold increase in consult volume'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.7,
        "end": 624.7
      },
      "pred_interval": {
        "start": 680.0,
        "end": 685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.299999999999955,
        "end": 60.299999999999955,
        "average": 58.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.4583333333333333,
        "text_similarity": 0.7212004661560059,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right (target occurs after the anchor) but the timestamps are substantially incorrect and it fails to match the correct temporal relation of 'directly follows' and the given anchor/target intervals."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker initially mentions the 'Community Bioethics Forum', when does he start describing its community members?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 887.216,
        "end": 905.918
      },
      "pred_interval": {
        "start": 920.0,
        "end": 930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.78399999999999,
        "end": 24.081999999999994,
        "average": 28.432999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.8956617116928101,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the temporal relation ('after') correct but both anchor and target timestamps are substantially wrong (predicted ~910\u2013930s vs true 882.782\u2013905.918s), omitting the correct time ranges and thus failing on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that the primary focus of the Center for Clinical Ethics has been ethics education, when does he start listing 'Systemwide Ethics Forum and Newsletter'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1055.54,
        "end": 1069.28
      },
      "pred_interval": {
        "start": 950.0,
        "end": 960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.53999999999996,
        "end": 109.27999999999997,
        "average": 107.40999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164387,
        "text_similarity": 0.9013708829879761,
        "llm_judge_score": 3,
        "llm_judge_justification": "The anchor time and the 'after' relation are correct, but the predicted target timing (950\u2013960s) significantly conflicts with the ground truth (1055.54\u20131069.28s), so a key factual element is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker lists 'ICU Walk Rounds', when does he mention 'HEC-C Certification'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1048.0,
        "end": 1052.0
      },
      "pred_interval": {
        "start": 1020.0,
        "end": 1030.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.0,
        "end": 22.0,
        "average": 25.0
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.8798847198486328,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two phrases but gives substantially incorrect timestamps for both anchor and target (off by ~30\u201340s) and uses a vague 'after' instead of the specified 'next' relation, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"ethics consultation services,\" when does he start talking about collecting feedback?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1240.8,
        "end": 1249.8
      },
      "pred_interval": {
        "start": 1265.0,
        "end": 1270.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.200000000000045,
        "end": 20.200000000000045,
        "average": 22.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.6582197546958923,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the ordinal relation (E2 occurs after E1) but the timestamps are substantially incorrect (off by ~21\u201324s) and it adds an unsupported end time; it also uses a vague 'after' rather than the specific 'once_finished' relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that participant satisfaction is not the \"be-all and end-all,\" when does he say they have begun the survey process with clinicians?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.3,
        "end": 1282.8
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1325.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.700000000000045,
        "end": 42.200000000000045,
        "average": 41.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.4057971014492754,
        "text_similarity": 0.6241722106933594,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering (E2 occurs after E1) but the timestamps are substantially off (\u224840s later than reference) and the relation 'after' misses the immediate 'once finished' timing; it also adds an unnecessary end time for E2."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes describing the first pie chart about helpful advice/guidance, when does the second pie chart about communication appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1367.5,
        "end": 1367.9
      },
      "pred_interval": {
        "start": 1380.0,
        "end": 1385.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.5,
        "end": 17.09999999999991,
        "average": 14.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.5245901639344263,
        "text_similarity": 0.7822929620742798,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the order correct (second chart appears after the first) but the timestamps differ substantially (E1 predicted 1375.0s vs correct 1356.0s; E2 predicted 1380.0s vs correct 1376.5s) and the relation 'after' is a weaker label than the required 'once_finished'; the extra end time for E2 is unnecessary."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he wants to turn to some of the organizational ethics consultation work, when does the slide showing the 'Organizational ethics consultations' table appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1472.0,
        "end": 1472.5
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.0,
        "end": 47.5,
        "average": 49.75
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.5991066694259644,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels E1 as the speaker's introduction but gives incorrect timestamps for E2 (1420\u20131425s vs ground-truth 1472s) and thus contradicts the true temporal order; major factual timing error. "
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining that organizational ethics work is new to them, when do they state that it began during the COVID pandemic?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1469.5,
        "end": 1472.0
      },
      "pred_interval": {
        "start": 1435.0,
        "end": 1440.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.5,
        "end": 32.0,
        "average": 33.25
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.589377760887146,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and their immediate-after relation, but provides incorrect timestamps (E2 listed at 1435\u20131440s vs correct 1469.5\u20131472.0s) and omits a precise end time for E1, so the temporal information is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "During the display of the 'Organizational ethics consultations' table, when does the speaker mention the 'Blood products scarcity protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1510.0,
        "end": 1513.0
      },
      "pred_interval": {
        "start": 1450.0,
        "end": 1455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.0,
        "end": 58.0,
        "average": 59.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4545454545454545,
        "text_similarity": 0.718126118183136,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives wrong timestamps for the mention (1450\u20131455s vs correct 1510\u20131513s) and thus contradicts the true temporal relationship with the table display (1474\u20131573s), so the 'during' claim is inconsistent."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the 'sequential organ failure assessment or SOFA score', when does he begin to explain what it is?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1647.6,
        "end": 1697.0
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.59999999999991,
        "end": 82.0,
        "average": 64.79999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.5660069584846497,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the anchor timestamp (1600.0s vs correct 1621.0s), provides no precise end times, and incorrectly claims the explanation begins 'immediately' whereas the reference shows a ~18.6s gap; thus it is largely inaccurate. "
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that '70% of publicly available crisis standards of care used either the SOFA score or a modified version', when does he mention the SOFA score being used in Alaska?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1726.0,
        "end": 1733.0
      },
      "pred_interval": {
        "start": 1660.0,
        "end": 1670.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.0,
        "end": 63.0,
        "average": 64.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.6300290822982788,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly locates both the anchor and target timestamps (1650/1660s vs. correct 1705\u20131712s and 1726\u20131733s), so it is factually wrong, though it correctly captures that the Alaska example follows shortly after the general statement."
      }
    },
    {
      "question_id": "003",
      "question": "Once the 'SOFA Disparities' slide appears, when does the speaker begin discussing concerns about the score's accuracy and contributions to disparities?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1770.0,
        "end": 1776.606
      },
      "pred_interval": {
        "start": 1750.0,
        "end": 1765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 11.605999999999995,
        "average": 15.802999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131148,
        "text_similarity": 0.7586007118225098,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures that the speaker addresses the topic immediately, but it gives an incorrect anchor timestamp (1750.0s vs correct 1762.0s) and omits the precise target timestamps (should be 1770.0\u20131776.606s), so it is partially correct but incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that the center was able to test the triage protocol before it was used, when does he state that they developed a SOFA calculation system?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1799.553,
        "end": 1807.997
      },
      "pred_interval": {
        "start": 1780.0,
        "end": 1790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.55300000000011,
        "end": 17.99700000000007,
        "average": 18.77500000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837845,
        "text_similarity": 0.8749305605888367,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') but the provided timestamps are substantially different from the reference (E1 off by ~20s and E2 start/end off by ~19\u201318s), so the temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the retrospective cohort study, when does he detail the demographic breakdown of the patients?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1846.122,
        "end": 1858.077
      },
      "pred_interval": {
        "start": 1800.0,
        "end": 1810.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.12200000000007,
        "end": 48.077,
        "average": 47.099500000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.6934823989868164,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the demographic details occur after the study introduction, but the reported timestamps are substantially incorrect (predicted E1/E2 times differ markedly from the reference and the predicted E2 is much earlier than the ground truth), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker highlights that non-Hispanic Black patients had greater odds of an elevated SOFA score, when does he state that no significant difference by race in mortality was found when controlling for other factors?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1873.642,
        "end": 1879.694
      },
      "pred_interval": {
        "start": 1820.0,
        "end": 1830.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.64200000000005,
        "end": 49.69399999999996,
        "average": 51.668000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.34090909090909094,
        "text_similarity": 0.7823261618614197,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct qualitative relationship ('after') but the provided timestamps are substantially incorrect compared to the reference (events are ~50\u201360s earlier and durations mismatched), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the early small cohort out of Wuhan, China, when does he state that subsequent larger cohorts in the United States did not show such high accuracy rates?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1959.0,
        "end": 1966.5
      },
      "pred_interval": {
        "start": 1980.5,
        "end": 1995.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.5,
        "end": 28.5,
        "average": 25.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.7289512157440186,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the qualitative relation ('after') but the timestamps are inaccurate: the anchor is slightly off and the target interval is substantially later than the reference, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'This graph here is a calibration curve', when does he explain that the diagonal line shows a perfectly calibrated predictor of mortality?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2014.0,
        "end": 2020.0
      },
      "pred_interval": {
        "start": 2010.0,
        "end": 2020.0
      },
      "iou": 0.6,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 0.0,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3291139240506329,
        "text_similarity": 0.8278825879096985,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly indicates the explanation occurs after the graph introduction and gives an approximate target interval (2010\u20132020 vs correct 2014\u20132020), but it misreports the anchor start time (2000.0s vs correct 58.1s), omits the anchor end, and thus only partially matches the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that SOFA predicted mortality with less accuracy than age in their own COVID cohort, when does he mention that SOFA predicted mortality with better accuracy than age in the pre-COVID eICU cohort?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2066.0,
        "end": 2069.0
      },
      "pred_interval": {
        "start": 2040.0,
        "end": 2055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 14.0,
        "average": 20.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7970309257507324,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the contrast and that the target occurs after the anchor, but the provided timestamps are significantly off from the reference (anchor ~32s late, target start ~26s early and end ~14s early), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Omicron surge increasing, when does he talk about working with the healthcare system's legal team?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2153.6,
        "end": 2174.93
      },
      "pred_interval": {
        "start": 2150.0,
        "end": 2160.0
      },
      "iou": 0.25671881267549673,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.599999999999909,
        "end": 14.929999999999836,
        "average": 9.264999999999873
      },
      "rationale_metrics": {
        "rouge_l": 0.3943661971830986,
        "text_similarity": 0.6977392435073853,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and gives approximate times for E2, but it misstates E1's timing (saying it starts at ~2140s rather than finishing around 2132s) and E2's start/end times differ notably from the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating the policy was active until late February of 2022, when does the first 'Scope of protocol' slide appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2194.0,
        "end": 2234.0
      },
      "pred_interval": {
        "start": 2200.0,
        "end": 2210.0
      },
      "iou": 0.25,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 24.0,
        "average": 15.0
      },
      "rationale_metrics": {
        "rouge_l": 0.37142857142857144,
        "text_similarity": 0.7803592681884766,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the order/relationship and has a close anchor time, but the target interval times are inaccurate (starts ~6s late and ends ~24s early) and omit the full span given in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the second 'Scope of protocol' slide appears, when does the speaker mention 'renal replacement therapy'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2263.679,
        "end": 2254.733
      },
      "pred_interval": {
        "start": 2250.0,
        "end": 2260.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.679000000000087,
        "end": 5.266999999999825,
        "average": 9.472999999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.4054054054054054,
        "text_similarity": 0.8130685091018677,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the ordering and roughly the same time window for the target mention, but the anchor time is off by ~10s and the predicted target interval only partially overlaps the reference (start ~4.7s early, end ~3.7s early), so the times are imprecise."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that goals of care discussions significantly changed, when does the speaker mention that patients were more likely to choose limited life-sustaining interventions?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2320.0,
        "end": 2327.0
      },
      "pred_interval": {
        "start": 2390.0,
        "end": 2395.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.0,
        "end": 68.0,
        "average": 69.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4507042253521127,
        "text_similarity": 0.6276289224624634,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events and the 'after' relation, but the timestamps are substantially incorrect (off by ~67\u201370s), so it fails to provide the correct timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states he wants to highlight some takeaway points, when does the first takeaway point appear on the screen?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2395.0,
        "end": 2400.0
      },
      "pred_interval": {
        "start": 2450.0,
        "end": 2455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.0,
        "end": 55.0,
        "average": 55.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2711864406779661,
        "text_similarity": 0.6948043704032898,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event ordering right (the first takeaway appears after the speaker's mention) and provides timestamps, but those timestamps are significantly shifted (~53s later) from the ground truth and the anchor event end is omitted, so the answer is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"I'll stop and take questions,\" when does an audience member begin speaking?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2541.6,
        "end": 2544.0
      },
      "pred_interval": {
        "start": 2500.5,
        "end": 2502.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.09999999999991,
        "end": 42.0,
        "average": 41.549999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.5142857142857142,
        "text_similarity": 0.687149703502655,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relation, but both event timestamps are substantially inaccurate (E1 ~20s early, E2 ~41s early) and the predicted E2 duration differs from the ground truth, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the audience member finishes complimenting the center, when does he ask a specific question about local hospital ethics committees?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2571.5,
        "end": 2580.5
      },
      "pred_interval": {
        "start": 2515.0,
        "end": 2516.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.5,
        "end": 64.0,
        "average": 60.25
      },
      "rationale_metrics": {
        "rouge_l": 0.49275362318840576,
        "text_similarity": 0.7266618609428406,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order (E2 occurs after E1) but is otherwise inaccurate: both event timestamps and the duration of E2 differ substantially from the correct times, and the relation label ('after' vs 'once_finished') is a weaker mismatch. These factual timing errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the audience member mentions the low numbers of ethics consultations, when does the speaker begin to answer the question?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2624.0,
        "end": 2634.8
      },
      "pred_interval": {
        "start": 2530.0,
        "end": 2532.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 94.0,
        "end": 102.80000000000018,
        "average": 98.40000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.5405405405405405,
        "text_similarity": 0.662212073802948,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the qualitative relation ('after') right, the timestamps for both E1 and E2 and the E2 duration are substantially incorrect (off by ~93s and much shorter than the reference), so it fails on factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the listener asks about assessing the quality of care across the system, when does the speaker respond by calling it a 'great question'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2744.1,
        "end": 2745.7
      },
      "pred_interval": {
        "start": 2685.0,
        "end": 2688.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.09999999999991,
        "end": 57.69999999999982,
        "average": 58.399999999999864
      },
      "rationale_metrics": {
        "rouge_l": 0.3943661971830986,
        "text_similarity": 0.6218023896217346,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the reported timestamps are substantially incorrect and do not match the reference event times or durations, so key factual timing information is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions starting to survey clinicians for feedback, when does he mention planning to survey patients and families?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.8,
        "end": 2821.6
      },
      "pred_interval": {
        "start": 2780.0,
        "end": 2785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.800000000000182,
        "end": 36.59999999999991,
        "average": 32.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.31111111111111106,
        "text_similarity": 0.5904408097267151,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the event timestamps and durations are substantially incorrect (both E1 and E2 are shifted earlier and E2 is much shorter than the reference), so it fails accurate temporal localization."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that hospitals in the healthcare system can join together, when does he state that they will preferentially present cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2854.49,
        "end": 2856.13
      },
      "pred_interval": {
        "start": 2905.0,
        "end": 2912.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.51000000000022,
        "end": 55.86999999999989,
        "average": 53.190000000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.7916042804718018,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') between the anchor and target, but the provided timestamps do not match the ground truth (they are substantially offset), so the answer is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces 'a third method of feedback', when does he describe it as 'formal needs assessments'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2877.53,
        "end": 2879.53
      },
      "pred_interval": {
        "start": 2975.0,
        "end": 2982.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.4699999999998,
        "end": 102.4699999999998,
        "average": 99.9699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.8111143112182617,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the qualitative relationship right ('after') and identifies the same utterances, the timestamp spans are substantially incorrect and do not match the reference (off by ~80\u2013100s and using different start/end points), so key factual elements are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'the overwhelming response was number one', when does he specify the first response as 'a lack of ethics education'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2901.56,
        "end": 2903.46
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3035.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 128.44000000000005,
        "end": 131.53999999999996,
        "average": 129.99
      },
      "rationale_metrics": {
        "rouge_l": 0.48000000000000004,
        "text_similarity": 0.8559497594833374,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their temporal relation ('after'), but the provided timestamps are wildly off from the ground truth (predicted ~3025\u20133035s vs. actual ~2900\u20132903s), so it is factually incorrect on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"The more medically complex cases tend to transfer,\" when does he start listing examples of such cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3044.3,
        "end": 3048.2
      },
      "pred_interval": {
        "start": 3070.0,
        "end": 3085.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.699999999999818,
        "end": 36.80000000000018,
        "average": 31.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.5985746383666992,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content (he lists examples like 'cardiac cases' and says they occur immediately after), but the timestamps are substantially incorrect and it even misstates the start alignment (anchor and target both at 3070.0s), contradicting the correct 3038.7\u20133048.2s timings."
      }
    },
    {
      "question_id": "002",
      "question": "After the questioner asks about the 'escalation of care policy', when does the slide titled 'Escalation of Care Protocol' appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3114.8,
        "end": 3117.8
      },
      "pred_interval": {
        "start": 3150.0,
        "end": 3165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.19999999999982,
        "end": 47.19999999999982,
        "average": 41.19999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.7061694264411926,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the slide appears after the question, but the reported timestamps are substantially off (\u224835\u201336s later) and the predicted end time conflicts with the reference, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions \"boarding 190 patients in the emergency department\", when does he discuss concerns about the level of care?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3154.983,
        "end": 3143.945
      },
      "pred_interval": {
        "start": 3210.0,
        "end": 3225.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.016999999999825,
        "end": 81.05499999999984,
        "average": 68.03599999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7443552017211914,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps deviate substantially from the reference (3210s vs ~3150s), it incorrectly sets E2 to start simultaneously with E1 rather than immediately after the anchor, and introduces an unsupported end time\u2014so it fails to match the correct timing relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker mentions 'in all 26 of those cases', when does he then talk about 'many more cases'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3214.9,
        "end": 3215.4
      },
      "pred_interval": {
        "start": 3280.0,
        "end": 3285.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.09999999999991,
        "end": 69.59999999999991,
        "average": 67.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.5161290322580645,
        "text_similarity": 0.6047468781471252,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') correct but the timestamps are substantially inaccurate (off by ~60\u201370s) and the E1 end time from the reference is not provided; key temporal details are therefore incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker states that the 'escalation of care protocol' was nice, when does he mention a 'SOFA-based protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3246.0,
        "end": 3249.0
      },
      "pred_interval": {
        "start": 3320.0,
        "end": 3325.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.0,
        "end": 76.0,
        "average": 75.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5074626865671642,
        "text_similarity": 0.709449052810669,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but the event timestamps are substantially incorrect (off by ~79s) and the end time for E1 is omitted, so key factual details do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker says 'SOFA is horrendous', when does he mention 'SOFA's AUC goes up'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3322.32,
        "end": 3324.71
      },
      "pred_interval": {
        "start": 3380.0,
        "end": 3385.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.679999999999836,
        "end": 60.289999999999964,
        "average": 58.9849999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.44776119402985076,
        "text_similarity": 0.7335226535797119,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation ('after') and event labels right, but the provided timestamps are substantially incorrect (predicted starts ~3370s/3380s vs. ground truth ~3320s/3322s, ~50\u201360s off) and durations do not match, so it fails to align temporally with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the question about equity monitoring is asked, when does the speaker begin explaining the logging process for patient cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3401.583,
        "end": 3406.09
      },
      "pred_interval": {
        "start": 3400.0,
        "end": 3408.0
      },
      "iou": 0.5633750000000077,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5830000000000837,
        "end": 1.9099999999998545,
        "average": 1.746499999999969
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.7798489928245544,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the same 'after' relation, but the timestamps are substantially off (E1 placed ~8.5s earlier than the reference) and the predicted event order implied by the times contradicts the correct timing, so it is largely incorrect on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the 'Escalation of Care Protocol', when does the 'Conscientious Practice Policy' slide appear on screen?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3429.8,
        "end": 3430.5
      },
      "pred_interval": {
        "start": 3450.0,
        "end": 3455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.199999999999818,
        "end": 24.5,
        "average": 22.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.3947368421052631,
        "text_similarity": 0.7747281789779663,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives times that are ~24s (E1) and ~20s (E2) later than the ground truth and weakens the relation from 'once_finished' to a vague 'after', so key factual timing and relation are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Conscientious Practice Policy' slide appears, when does the speaker mention tracking outcomes and looking back retrospectively for this policy?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3444.0,
        "end": 3492.0
      },
      "pred_interval": {
        "start": 3470.0,
        "end": 3478.0
      },
      "iou": 0.16666666666666666,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 14.0,
        "average": 20.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5454545454545455,
        "text_similarity": 0.7840209603309631,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the relation right but the timestamps are notably off (E1 3450s vs 3434s; E2 start 3470s vs 3444s) and it omits the E2 end time (3492s), so it is imprecise and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions an increasing disparity over time, when does he discuss how they can provide support to all hospitals?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 707.399,
        "end": 742.972
      },
      "pred_interval": {
        "start": 765.0,
        "end": 770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.601,
        "end": 27.02800000000002,
        "average": 42.31450000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32911392405063294,
        "text_similarity": 0.8519034385681152,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer preserves the topical relation ('after') and the shift to solutions, but it misplaces both anchor and target timestamps by ~60+ seconds and gives incorrect target duration, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "While the organizational chart for the Center for Clinical Ethics is displayed, when does the speaker describe the Ethics Education program?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 769.177,
        "end": 786.763
      },
      "pred_interval": {
        "start": 810.0,
        "end": 820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.82299999999998,
        "end": 33.236999999999966,
        "average": 37.02999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.43920883536338806,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'while' relationship and gesture cue, but the timestamps are largely incorrect \u2014 the anchor and target times do not match the reference (predicted times occur after or outside the correct 749\u2013810s slide and 769.177\u2013786.763s description)."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says he will go into depth on the programs, when does he first mention the Yale Interdisciplinary Center for Bioethics?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 837.605,
        "end": 845.26
      },
      "pred_interval": {
        "start": 870.0,
        "end": 875.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.39499999999998,
        "end": 29.74000000000001,
        "average": 31.067499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.4123711340206186,
        "text_similarity": 0.7654151916503906,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation right ('after') but the provided timestamps for both the anchor and target are substantially off from the reference (anchor ~60s late, target ~30s late), so the answer is factually incorrect on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states the title 'Systemwide Ethics Forum and Newsletter', when does he describe it as a hybrid meeting?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1070.5,
        "end": 1076.5
      },
      "pred_interval": {
        "start": 1080.0,
        "end": 1085.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.5,
        "end": 8.5,
        "average": 9.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.6651122570037842,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies both utterances and their relation ('after') with equivalent wording; timestamp differences are minor and not required to be exact, so the semantic match is complete and accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes explaining that they looked through the 26 specific patient cases individually, when does the slide transition to 'Scope of protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3425.8,
        "end": 3429.0
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3412.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.800000000000182,
        "end": 17.0,
        "average": 18.90000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.6930103898048401,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event timestamps (off by ~12\u201321s) and the temporal relation (claims 'immediate' vs. correct 'after' with an ~8s gap), so it largely contradicts the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the 'Scope of protocol' slide finishes being displayed, when does the 'Conscientious Practice Policy' slide appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3429.0,
        "end": 3519.5
      },
      "pred_interval": {
        "start": 3440.0,
        "end": 3447.0
      },
      "iou": 0.07734806629834254,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 72.5,
        "average": 41.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666666,
        "text_similarity": 0.7691773772239685,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the slide change occurs when the first ends, but the timestamps are significantly off (3440.0s vs 3429.0s) and it wrongly shortens the Conscientious Practice Policy slide duration (ends at 3447.0s vs 3519.5s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes discussing the tracking of equity, socioeconomic status, and other demographic characteristics, when is the presentation window minimized?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3530.0,
        "end": 3531.0
      },
      "pred_interval": {
        "start": 3530.0,
        "end": 3533.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.0,
        "end": 2.0,
        "average": 1.0
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111105,
        "text_similarity": 0.7389158010482788,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the E2 start time; it misstates E1 end (3530.0s vs 3508.5s), E2 end (3533.0s vs 3531.0s), and the relation ('immediate' vs 'after'), and adds unsupported detail about a task shift."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states the audience will be on mute, when does he mention that the live event can be paused?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 38.524,
        "end": 43.729
      },
      "pred_interval": {
        "start": 25.0,
        "end": 30.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.524000000000001,
        "end": 13.729,
        "average": 13.6265
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7685387134552002,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation 'after' and the identification of the mute/ pause events align semantically, the predicted timestamps and target span are substantially incorrect (off by ~13+ seconds) and thus fail to match the ground truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses changing the speed of presentations and speakers, when does he advise on what to do if Wi-Fi or connection is lost?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 55.563,
        "end": 59.787
      },
      "pred_interval": {
        "start": 55.0,
        "end": 60.0
      },
      "iou": 0.8447999999999993,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5630000000000024,
        "end": 0.21300000000000097,
        "average": 0.3880000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.8030076026916504,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the semantic content and the 'after' relation, and E2's timing is very close to the reference; however E1's start time is off by ~5 seconds from the ground truth, a noteworthy temporal inaccuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the presenter mentions Tom Gardner in the background, when does he mention Stephanie Fraser joining in place of Jane Preston?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 168.258,
        "end": 171.201
      },
      "pred_interval": {
        "start": 210.0,
        "end": 215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.74199999999999,
        "end": 43.79900000000001,
        "average": 42.7705
      },
      "rationale_metrics": {
        "rouge_l": 0.5135135135135135,
        "text_similarity": 0.9027472138404846,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the 'after' relation, but it omits the anchor timestamp and gives grossly incorrect target timestamps (210\u2013215s vs ~18.8\u201321.6s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the male presenter finishes introducing Stephanie Fraser, when does Stephanie Fraser begin speaking?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 223.86,
        "end": 224.8
      },
      "pred_interval": {
        "start": 217.0,
        "end": 220.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.860000000000014,
        "end": 4.800000000000011,
        "average": 5.8300000000000125
      },
      "rationale_metrics": {
        "rouge_l": 0.48387096774193544,
        "text_similarity": 0.8237970471382141,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but gives incorrect and conflicting timestamps for the target (217.0\u2013220.0 vs. correct 223.86\u2013224.8) and omits the anchor's precise end time (222.0), so key factual elements are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "While the speaker is discussing the recent research undertaken by the Neurological Alliance of Scotland, when does she state that 57% of respondents reported not being able to access a face-to-face appointment?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 433.0,
        "end": 434.9
      },
      "pred_interval": {
        "start": 360.0,
        "end": 366.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.0,
        "end": 68.89999999999998,
        "average": 70.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.630622386932373,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer identifies the same statement but gives timestamps that are substantially different from the reference (target predicted at ~360\u2013366s vs correct ~433\u2013435s), so the temporal alignment is incorrect despite matching content."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that nearly two-thirds of respondents had not had a video appointment, when does she state that telephone appointments were the most common way to access care?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 447.8,
        "end": 452.9
      },
      "pred_interval": {
        "start": 372.0,
        "end": 378.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.80000000000001,
        "end": 74.89999999999998,
        "average": 75.35
      },
      "rationale_metrics": {
        "rouge_l": 0.368421052631579,
        "text_similarity": 0.7787091732025146,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates that the target occurs after the anchor, but the timestamps are substantially incorrect (do not match the reference) and it misses the immediacy/adjacency stated in the correct answer, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the blue slide with the speaker's title disappears, when does the speaker begin to mention what factors clinicians should consider for appointment formats?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 479.3,
        "end": 480.3
      },
      "pred_interval": {
        "start": 390.0,
        "end": 396.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.30000000000001,
        "end": 84.30000000000001,
        "average": 86.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.770422637462616,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted times are substantially wrong (anchor 384.0s vs 476.3s; target 390.0s vs 479.3s), so it fails to match the ground-truth temporal markers\u2014only the relative 'after' ordering aligns; it also adds an unwarranted end time."
      }
    },
    {
      "question_id": "001",
      "question": "Once Stephanie finishes speaking and hands over to Mark, when does Mark begin to speak?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 606.5,
        "end": 607.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 515.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.5,
        "end": 92.0,
        "average": 94.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3137254901960784,
        "text_similarity": 0.7028608322143555,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates Mark speaks after Stephanie, but the timestamps are substantially wrong (510/515s vs 593.7-607s) and the temporal gap/precision differ; key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mark finishes introducing Calum Duncan, when does Calum Duncan start speaking?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 638.3,
        "end": 639.3
      },
      "pred_interval": {
        "start": 570.0,
        "end": 573.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.29999999999995,
        "end": 66.29999999999995,
        "average": 67.29999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.7045979499816895,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the order (Mark then Calum) but gives times that are off by ~66 seconds and uses a generic 'after' relation instead of the specified 'once_finished', so it is largely incorrect and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once Calum Duncan says 'Next slide please', when does the second presentation slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 685.7,
        "end": 686.0
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.70000000000005,
        "end": 81.0,
        "average": 83.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3508771929824561,
        "text_similarity": 0.6916987895965576,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (E1 before E2) but gives completely incorrect timestamps (off by ~84s) and uses a vague 'after' instead of the specific 'once_finished' relation; key timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that 'near me is what we're going to focus on today', when does he describe it as 'internet-based'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 702.7,
        "end": 703.5
      },
      "pred_interval": {
        "start": 706.5,
        "end": 708.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.7999999999999545,
        "end": 4.5,
        "average": 4.149999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.6510998010635376,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and their temporal order, but the timestamps are offset by roughly 5 seconds from the ground truth and it adds an \"immediately after\" qualifier not specified in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states there were '330 consultations per week' before the pandemic, when does he mention it went up to '10,000'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 739.0
      },
      "pred_interval": {
        "start": 738.0,
        "end": 740.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 1.0,
        "average": 1.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.43228209018707275,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation and accurately locates the '10,000' mention (overlapping the gold span), but the anchor timestamp for '330 consultations' is shifted ~2\u20133.5s later than the reference, a minor timing discrepancy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Next slide, please' for the first time, when does he point to the map on the screen?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 767.0,
        "end": 767.5
      },
      "pred_interval": {
        "start": 750.0,
        "end": 752.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 15.5,
        "average": 16.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3461538461538462,
        "text_similarity": 0.6319551467895508,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the correct qualitative relation (the pointing happens after the utterance) but the timestamps are substantially off (anchor ~11s early and target ~15\u201317s early compared to ground truth), so it is largely incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says 'go back to the next slide', when does the slide titled 'Video consulting using near me via attend anywhere platform' appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 874.0,
        "end": 874.1
      },
      "pred_interval": {
        "start": 885.0,
        "end": 892.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 17.899999999999977,
        "average": 14.449999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.6896548271179199,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps deviate substantially from the ground truth (873.91/874.0 vs 885.0/892.0) and incorrectly indicates the slide appears ~7s after the instruction rather than immediately, so it is factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that 'Stephanie Fraser has talked about' the survey, when does he then say 'Back to next slide, Mark, please'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 883.0,
        "end": 884.0
      },
      "pred_interval": {
        "start": 945.0,
        "end": 950.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.0,
        "end": 66.0,
        "average": 64.0
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.6402071714401245,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after'), but it mislabels the event IDs and gives timestamps that differ substantially (~60s later) from the ground truth, so the factual timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says 'Next slide, please' at the 42-second mark, when does the slide titled 'Clinician and patient experience - Scotland' appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 913.0,
        "end": 913.1
      },
      "pred_interval": {
        "start": 1042.0,
        "end": 1048.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.0,
        "end": 134.89999999999998,
        "average": 131.95
      },
      "rationale_metrics": {
        "rouge_l": 0.36734693877551017,
        "text_similarity": 0.5937639474868774,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives completely different timestamps (1042s/1048s vs 912s/913s) and a 6s delay rather than 'immediately after'; while it notes a slide transition, the core temporal facts contradict the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "During the discussion of what works well with video calls, when does the speaker express finding it much easier to interact with groups on a video call than on the telephone?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.0,
        "end": 1062.5
      },
      "pred_interval": {
        "start": 1055.0,
        "end": 1065.0
      },
      "iou": 0.625,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 2.5,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.38961038961038963,
        "text_similarity": 0.7704349756240845,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'during' relation with E1 matching exactly; however E2's start and end times are slightly shifted later (\u2248+2s start, +2.5s end) compared to the reference, a minor temporal discrepancy."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions technical issues with patient bandwidth, when does he advise to choose patients correctly to avoid those difficulties?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1134.0,
        "end": 1135.5
      },
      "pred_interval": {
        "start": 1095.0,
        "end": 1105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.0,
        "end": 30.5,
        "average": 34.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.6138340830802917,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same semantic events and the 'after' relation, but the predicted start/end times and durations differ substantially from the reference (both events placed much earlier), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Next slide, please' to introduce the smart phone camera, when does he specifically point out his wife's iPhone on the screen?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1213.0,
        "end": 1215.0
      },
      "pred_interval": {
        "start": 1130.0,
        "end": 1135.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 83.0,
        "end": 80.0,
        "average": 81.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4477611940298507,
        "text_similarity": 0.7407556772232056,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and correct 'after' relation, but the timestamps are substantially incorrect (off by ~78 seconds for both E1 and E2) and the E2 interval/duration does not match the reference, so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'Next slide please', when does the 'Sharing content' slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1248.574,
        "end": 1249.574
      },
      "pred_interval": {
        "start": 1260.0,
        "end": 1262.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.42599999999993,
        "end": 12.92599999999993,
        "average": 12.17599999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.7952076196670532,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives timestamps that are off by ~11 seconds for both the anchor and target and adds an incorrect target end time; although it correctly states the target follows the anchor, the temporal values substantially contradict the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'You can share things', when does he point towards the screen showing the brain scan?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1252.25,
        "end": 1252.85
      },
      "pred_interval": {
        "start": 1275.0,
        "end": 1276.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.75,
        "end": 23.65000000000009,
        "average": 23.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6097197532653809,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the event order (pointing occurs after the utterance) but the absolute timestamps are significantly off (~22\u201325s later) and the pointing interval duration differs from the reference, so the answer is mostly incorrect despite correct relative ordering."
      }
    },
    {
      "question_id": "003",
      "question": "During the discussion about poor picture quality, when does the speaker suggest clearing browser history?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1313.823,
        "end": 1315.286
      },
      "pred_interval": {
        "start": 1305.0,
        "end": 1307.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.823000000000093,
        "end": 8.286000000000058,
        "average": 8.554500000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.41269841269841273,
        "text_similarity": 0.8728162050247192,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives timestamps that are significantly off (anchor ~3.8s early; target ~8.8s early and not overlapping the true 1313.823\u20131315.286s interval), so the temporal relation is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying \"Thank you very much for that\", when does he state he is handing over to Jane?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1428.837,
        "end": 1430.682
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.836999999999989,
        "end": 5.682000000000016,
        "average": 7.259500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.4507042253521127,
        "text_similarity": 0.8116077184677124,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their order, but the timestamps are substantially off from the ground truth (both E1 and E2 times differ by many seconds) and the relation is labeled 'after' instead of the precise 'once_finished', so the answer is largely incorrect despite matching content."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman (Jane) describes the challenges of managing patients over the telephone, when does she mention that they had a pilot of 'Near Me' even prior to Covid?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1609.855,
        "end": 1624.692
      },
      "pred_interval": {
        "start": 1530.0,
        "end": 1535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.85500000000002,
        "end": 89.69200000000001,
        "average": 84.77350000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3414634146341463,
        "text_similarity": 0.691286563873291,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct utterance and relation ('after') but the timestamps are substantially off (predicted ~1525\u20131535s vs ground truth ~1604.9\u20131624.7s), so timing is incorrect despite matching content."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that using 'Near Me' felt quite adventurous, when does she state that its use became vital to their whole service?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1643.0
      },
      "pred_interval": {
        "start": 1622.0,
        "end": 1628.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.0,
        "end": 15.0,
        "average": 14.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3614457831325301,
        "text_similarity": 0.7923241853713989,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces both the anchor and target timestamps by roughly 16\u201330 seconds and provides an incorrect target span and an extra detail ('during the lockdown') not present in the reference; only the temporal relation ('after') matches."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes asking Mark to go back to the previous slide, when does she say 'Thank you'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1676.54,
        "end": 1678.02
      },
      "pred_interval": {
        "start": 1785.0,
        "end": 1787.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.46000000000004,
        "end": 108.98000000000002,
        "average": 108.72000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767127,
        "text_similarity": 0.7454094290733337,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the relative ordering ('after') but the reported timestamps and span far mismatch the reference (off by ~100+ seconds) and thus do not align with the correct event times; key temporal details are incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "After the 'Training and preparation' slide appears, when does the speaker mention the 'Level 1' training?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1791.0,
        "end": 1791.5
      },
      "pred_interval": {
        "start": 1790.0,
        "end": 1795.0
      },
      "iou": 0.1,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 3.5,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.645683228969574,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the speaker's mention of Level 1 (E2), and E2's interval covers the ground-truth time; however E1's time is off by ~4.4s and E2's interval is broader/shifted, so there are minor timing inaccuracies."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing tele-swallowing partners as 'our eyes and our hands and our ears', when does she start talking about preparing the clinical room?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1897.0,
        "end": 1901.0
      },
      "pred_interval": {
        "start": 1820.0,
        "end": 1825.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.0,
        "end": 76.0,
        "average": 76.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356165,
        "text_similarity": 0.6031995415687561,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same events but the timestamps are off by ~80 seconds, the relation ('after') conflicts with the correct 'once_finished' (immediate follow), and it adds an unsupported end time for E2, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker discusses tele-swallowing partners preparing the clinical room, when does she next talk about them providing reassurance to patients?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1906.0,
        "end": 1910.0
      },
      "pred_interval": {
        "start": 1845.0,
        "end": 1850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.0,
        "end": 60.0,
        "average": 60.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.6844791173934937,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two event types (preparing the clinical room and providing reassurance) but gives substantially incorrect time spans and a different relation ('after' vs the required immediate 'next'), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning emergency procedures in place onsite, when does the slide change to 'Technology/equipment'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1971.6,
        "end": 1972.0
      },
      "pred_interval": {
        "start": 1980.0,
        "end": 1985.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.400000000000091,
        "end": 13.0,
        "average": 10.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.8250551223754883,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer provides substantially different timestamps and durations (1980\u20131985s vs the reference 1962.607\u20131972.0s) and a conflicting temporal relation ('immediate' vs 'once_finished'), so it contradicts key factual elements of the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "During the time the 'Technology/equipment' slide is displayed, when does the speaker discuss the need for a device with a webcam and microphone?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2024.079,
        "end": 2026.579
      },
      "pred_interval": {
        "start": 1985.0,
        "end": 1990.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.07899999999995,
        "end": 36.57899999999995,
        "average": 37.82899999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.31707317073170727,
        "text_similarity": 0.8320664763450623,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly labels the relationship as 'during' and gives an anchor start near the true slide onset, but the target interval (1985\u20131990s) does not match the ground-truth target (2024.079\u20132026.579s), a substantive timing error."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker introduces the general category of 'certain resources' for teleswallow sessions, when does she mention 'appropriate diet and fluid consistencies'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2058.952,
        "end": 2061.952
      },
      "pred_interval": {
        "start": 2005.0,
        "end": 2010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.952000000000226,
        "end": 51.952000000000226,
        "average": 52.952000000000226
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7189090251922607,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative ordering ('after'/'next') right but incorrectly identifies both anchor and target time spans (off by ~50s) and misplaces the resource timing; these major factual/time errors warrant a low score."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that remote swallowing assessments are not intended to fully replace face-to-face assessments, when does she mention that they are a very useful addition?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2159.677,
        "end": 2162.619
      },
      "pred_interval": {
        "start": 2145.0,
        "end": 2150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.677000000000135,
        "end": 12.619000000000142,
        "average": 13.648000000000138
      },
      "rationale_metrics": {
        "rouge_l": 0.45333333333333337,
        "text_similarity": 0.7310593724250793,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the qualitative relationship right ('immediately after') but both event timestamps are substantially off (~15\u201316 seconds earlier) compared to the correct times, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes mentioning gathering feedback from those who completed the training, when does she start talking about evaluating quantitative data?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2164.643,
        "end": 2186.427
      },
      "pred_interval": {
        "start": 2180.0,
        "end": 2185.0
      },
      "iou": 0.22952625780389166,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.356999999999971,
        "end": 1.4270000000001346,
        "average": 8.392000000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.2388059701492537,
        "text_similarity": 0.6774197816848755,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the sequence ('immediately after') but the timestamps are substantially off (predicted E2 start 2180.0s vs reference 2186.427s and E1 end mismatched by ~6.4s), so it is not temporally accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the first speaker finishes her presentation by saying 'thank you very much for listening', when does the video visually transition to the male presenter?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2257.0,
        "end": 2258.0
      },
      "pred_interval": {
        "start": 2325.0,
        "end": 2330.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.0,
        "end": 72.0,
        "average": 70.0
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.7800730466842651,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relationship correct ('immediately after') but the absolute timestamps are substantially off (2324/2325s vs. 2256/2257s), a major factual error; the extra end time is unnecessary but not harmful."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that picking up cues is difficult, when does she start talking about 'points to consider' for virtual technology?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2491.8,
        "end": 2498.2
      },
      "pred_interval": {
        "start": 2570.0,
        "end": 2585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.19999999999982,
        "end": 86.80000000000018,
        "average": 82.5
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324323,
        "text_similarity": 0.613919734954834,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and their immediate-after relationship, but it gives substantially incorrect timestamps (2570.0s vs the reference 2491.8s), failing to match the key temporal annotations."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions conducting a 'sprint audit' with patients, when does she state that 'most were very satisfied' with the virtual appointments?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2515.0,
        "end": 2516.0
      },
      "pred_interval": {
        "start": 2515.0,
        "end": 2530.0
      },
      "iou": 0.06666666666666667,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.0,
        "end": 14.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.6712309122085571,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly locates E2 but misplaces E1 (should be 2509.5s, not ~2515.0s) and thus misstates the temporal relation\u2014calling it 'immediately after' or implying simultaneity contradicts the reference ordering and timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying that patients found virtual technology 'more acceptable', when does she say 'So moving on to the next slide'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2638.0,
        "end": 2639.3
      },
      "pred_interval": {
        "start": 2590.0,
        "end": 2595.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.0,
        "end": 44.30000000000018,
        "average": 46.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.39436619718309857,
        "text_similarity": 0.7208261489868164,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation roughly right but the timestamps are significantly off (predicts ~2590.0s vs ground truth 2637.6\u20132639.3s) and omits E2's end time, so the key factual timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes discussing confidentiality, when does she begin to mention the subtlety of the therapeutic relationship?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2693.583,
        "end": 2697.126
      },
      "pred_interval": {
        "start": 2710.0,
        "end": 2715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.416999999999916,
        "end": 17.873999999999796,
        "average": 17.145499999999856
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.7712869048118591,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target events and their temporal relationship ('after'), but the provided timestamps are substantially later (~16\u201318 seconds off) from the ground truth, so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'It all comes down to Wi-Fi', when does she state that 'delivery of remote therapy is very, very difficult'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2727.0,
        "end": 2729.0
      },
      "pred_interval": {
        "start": 2735.0,
        "end": 2740.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 11.0,
        "average": 9.5
      },
      "rationale_metrics": {
        "rouge_l": 0.5185185185185186,
        "text_similarity": 0.8602014780044556,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies both utterances and that the target occurs after the anchor, but the provided timestamps are noticeably shifted (about 8+ seconds later) and the target interval duration is incorrect, so it is only partially accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'So next slide', when does the slide visually change to 'Practical considerations'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2884.0,
        "end": 2884.2
      },
      "pred_interval": {
        "start": 2890.0,
        "end": 2892.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 7.800000000000182,
        "average": 6.900000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.4776119402985075,
        "text_similarity": 0.7213467359542847,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes the slide change occurs after the verbal cue and identifies the visual cue, but both timestamps are off by about 5\u20136 seconds (predicted 2888/2890 vs ground-truth 2883/2884), so it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing 'Practical considerations', when does she first mention 'increasing reflective feedback'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2913.483,
        "end": 2916.268
      },
      "pred_interval": {
        "start": 2902.0,
        "end": 2905.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.483000000000175,
        "end": 11.268000000000029,
        "average": 11.375500000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8251328468322754,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the event relation ('during') and the mention of 'reflective feedback', but both timestamps are inaccurate (anchor is ~40s late and the target ~11.5s early) compared to the reference, so only partial credit is warranted."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"for the patients\", when does the slide change to \"WHERE WE ARE NOW\"?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3067.769,
        "end": 3068.2
      },
      "pred_interval": {
        "start": 3098.5,
        "end": 3100.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.731000000000222,
        "end": 31.800000000000182,
        "average": 31.265500000000202
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.6818429231643677,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the event timestamps are off by ~30 seconds compared to the ground truth and it omits the 'fully visible by 3068.2s' detail, so it's largely incorrect on key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "After the man says \"open up for some discussion\", when does the discussion slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3163.435,
        "end": 3163.7
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3222.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.565000000000055,
        "end": 58.80000000000018,
        "average": 57.68250000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.4727272727272727,
        "text_similarity": 0.6986222863197327,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation 'after' matches, the timestamps are substantially incorrect (E1 predicted ~3219s vs reference 3120s, E2 predicted ~3220s vs reference ~3163.4s), so key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first male speaker asks about attendees' experience with Near Me, when does the second male speaker begin talking about starting to use NearMe?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3268.9,
        "end": 3312.0
      },
      "pred_interval": {
        "start": 3245.0,
        "end": 3255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.90000000000009,
        "end": 57.0,
        "average": 40.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.41791044776119407,
        "text_similarity": 0.6890397667884827,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') right but the timestamps are incorrect and inconsistent with the reference (misses E1 end time and places E2 start ~24s earlier than the correct 3268.9s), so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second male speaker finishes stating the advantages and utility of NearMe, when does he mention supplementing normal activities?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3288.4,
        "end": 3293.32
      },
      "pred_interval": {
        "start": 3285.0,
        "end": 3295.0
      },
      "iou": 0.49200000000000726,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.400000000000091,
        "end": 1.6799999999998363,
        "average": 2.5399999999999636
      },
      "rationale_metrics": {
        "rouge_l": 0.3548387096774193,
        "text_similarity": 0.5962883234024048,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the ordering (E2 occurs after E1) and roughly the same time window, but the timestamps differ by ~3\u20134 seconds, the relation uses a weaker label ('after' vs. 'once_finished'), and it introduces an unsupported end time for E2, so it is only partially accurate and less precise."
      }
    },
    {
      "question_id": "001",
      "question": "After the first man finishes reading Jenny's chat message, when does he ask the audience if they would find guidance helpful?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3411.0,
        "end": 3415.0
      },
      "pred_interval": {
        "start": 3450.0,
        "end": 3455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.0,
        "end": 40.0,
        "average": 39.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.7220814824104309,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation but the timestamps are substantially off (correct E1 ~3407.6s and E2 around 3411\u20133415s versus predicted E1 3430s and E2 3450\u20133455s), so the key temporal facts are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first man finishes reading John Hogan's comment about clinical interviewing, when does he state he was quite skeptical?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3434.9,
        "end": 3437.7
      },
      "pred_interval": {
        "start": 3500.0,
        "end": 3505.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.09999999999991,
        "end": 67.30000000000018,
        "average": 66.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.7823961973190308,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation direction ('after') but the event timestamps/spans are substantially incorrect (off by ~66s) and do not match the required target span, so it fails on factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the second woman mentions neuropsychology bringing out guidance, when is the next time a woman speaks about professional guidance?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3511.043,
        "end": 3528.447
      },
      "pred_interval": {
        "start": 3550.0,
        "end": 3555.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.95699999999988,
        "end": 26.552999999999884,
        "average": 32.75499999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.8376694321632385,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the high-level event types (second woman mentions neuropsychology, another woman later speaks about guidance) but the timestamps are substantially incorrect and the target span/end times do not match the reference; the relation label is also less precise than the gold 'next.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that 36 people joined the session, when does he talk about taking the next steps with Richard and the team?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3574.7,
        "end": 3576.5
      },
      "pred_interval": {
        "start": 3580.0,
        "end": 3588.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.300000000000182,
        "end": 11.5,
        "average": 8.400000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.25974025974025977,
        "text_similarity": 0.7905133962631226,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation right ('after') but both event timestamps are notably inaccurate compared to the reference (anchor is placed ~2\u20135s late and the target is shifted ~5s later and extended), so it fails to match the correct time spans."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker makes a plea to fill in the survey, when does he ask if listeners would like to engage with the advisory committee?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3592.9,
        "end": 3594.1
      },
      "pred_interval": {
        "start": 3595.0,
        "end": 3602.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.099999999999909,
        "end": 7.900000000000091,
        "average": 5.0
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.7887328267097473,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the correct events and relative 'after' relationship and roughly locates them, but the timestamps are notably off (anchor ~2\u20133s early/late, target start ~2s late) and the predicted target end is much longer than the ground truth, introducing inaccurate duration."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes thanking everyone for joining the session today, when does he mention that the session will be recorded and resources will be provided?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3599.8,
        "end": 3603.2
      },
      "pred_interval": {
        "start": 3605.0,
        "end": 3612.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.199999999999818,
        "end": 8.800000000000182,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.8174247741699219,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction roughly locates the anchor (\u22483600s) but the target timings are significantly off (predicted 3605.0\u20133612.0 vs. actual 3599.8\u20133603.2) and the relation 'after' fails to capture that the target immediately follows the anchor; key temporal details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks 'where did we start?', when does she mention considering moving to Near Me for patient contacts?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2332.719,
        "end": 2336.344
      },
      "pred_interval": {
        "start": 2325.0,
        "end": 2335.0
      },
      "iou": 0.20107545839209615,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.719000000000051,
        "end": 1.344000000000051,
        "average": 4.531500000000051
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.709296703338623,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target events and their ordering, but the timestamps are notably off (anchor ~10s early; target start ~7.7s early) and the relation is given as the less specific 'after' rather than a direct follow-up."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says the pandemic came along, when does she mention adopting Near Me as their default for routine people?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.217,
        "end": 2412.045
      },
      "pred_interval": {
        "start": 2345.0,
        "end": 2355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.2170000000001,
        "end": 57.04500000000007,
        "average": 39.631000000000085
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7613111734390259,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship but both anchor and target timestamps are substantially incorrect (anchor given as 2310.0s vs 2349.476\u20132350.756s; target given as 2345.0\u20132355.0s vs 2367.217\u20132412.045s), so the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker describes the results of the focus groups for the qualitative study, when does she introduce the quotes from the participants?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2511.0,
        "end": 2512.0
      },
      "pred_interval": {
        "start": 2400.0,
        "end": 2410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.0,
        "end": 102.0,
        "average": 106.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4411764705882353,
        "text_similarity": 0.7623064517974854,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') but the event timings are substantially incorrect (E2 is placed ~100+ seconds earlier than the reference) and the anchor's end time is omitted/misaligned, so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks to fill in the survey, when does he ask if listeners want to engage with the advisory committee?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3591.7,
        "end": 3595.8
      },
      "pred_interval": {
        "start": 3590.5,
        "end": 3595.0
      },
      "iou": 0.6226415094339752,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.199999999999818,
        "end": 0.8000000000001819,
        "average": 1.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5116279069767442,
        "text_similarity": 0.8487311601638794,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct temporal relation ('after') and locates both events roughly, but it omits the anchor's end time and the reported timestamps differ noticeably from the ground truth (anchor start is earlier and both target start/end are shifted), so it is incomplete/partially inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Before the speaker thanks the speakers for their expertise, when does he mention the session will be recorded and resources will be provided?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3599.9,
        "end": 3603.7
      },
      "pred_interval": {
        "start": 3580.0,
        "end": 3583.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.90000000000009,
        "end": 20.699999999999818,
        "average": 20.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.32941176470588235,
        "text_similarity": 0.8603893518447876,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly states a 'before' relationship but it swaps the E1/E2 labels and gives substantially incorrect timestamps (3580\u20133588s vs the correct ~3599.9\u20133605s), so it does not match the reference events."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker initially thanks the audience for joining, when does he deliver his final 'thank you very much' for the session?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3614.6,
        "end": 3615.4
      },
      "pred_interval": {
        "start": 3610.0,
        "end": 3615.0
      },
      "iou": 0.07407407407408967,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.599999999999909,
        "end": 0.40000000000009095,
        "average": 2.5
      },
      "rationale_metrics": {
        "rouge_l": 0.35416666666666663,
        "text_similarity": 0.7763740420341492,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the ordinal relation correct (target after anchor) but the anchor timestamp is substantially incorrect and missing an end time, and the target timing is offset (starts ~4.6s early and ends slightly early), so key temporal details are not aligned with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After Mark introduces Dr. John Mckeown and Dr. Naomi Dow, when does he ask Dr. Dow to describe how they've been using Near Me?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 31.48,
        "end": 34.4
      },
      "pred_interval": {
        "start": 15.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.48,
        "end": 14.399999999999999,
        "average": 15.44
      },
      "rationale_metrics": {
        "rouge_l": 0.4285714285714286,
        "text_similarity": 0.7074686884880066,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both answers agree the relationship is 'after', the predicted timestamps are substantially incorrect (both E1 and E2 times differ markedly from the ground truth and E2 is placed much earlier), omitting the correct event boundaries and timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Dr. Naomi Dow finishes explaining how students take part in consultations, when does Mark ask Dr. Mckeown about the impact on the teaching team?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 118.96,
        "end": 124.4
      },
      "pred_interval": {
        "start": 100.0,
        "end": 105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.959999999999994,
        "end": 19.400000000000006,
        "average": 19.18
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.7286132574081421,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly gives both event timestamps (90.0s vs 117.60s and 100.0\u2013105.0s vs 118.96\u2013124.4s) and labels the relation as 'after' rather than the more precise 'once_finished'; only the vague ordering is preserved, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker introduces the concept of emotions in the session, when does the female speaker first mention 'real patients'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 201.9,
        "end": 202.6
      },
      "pred_interval": {
        "start": 170.0,
        "end": 172.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.900000000000006,
        "end": 30.099999999999994,
        "average": 31.0
      },
      "rationale_metrics": {
        "rouge_l": 0.38709677419354843,
        "text_similarity": 0.7868070602416992,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer preserves the correct temporal relation ('after'), it mislabels both event timings\u2014especially E2, which is off by ~32 seconds and has an incorrect span\u2014so it fails to match the key factual timestamps in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer finishes asking the question about comparing models, when does the female speaker finish explaining the advantages of 'Near Me' regarding real patients and capacity?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 198.7,
        "end": 306.9
      },
      "pred_interval": {
        "start": 250.0,
        "end": 255.0
      },
      "iou": 0.04621072088724584,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.30000000000001,
        "end": 51.89999999999998,
        "average": 51.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.7286972999572754,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the speaker finishes after the interviewer, but both reported timestamps are substantially wrong (E1 and E2 times do not match the reference) and the predicted answer omits the female speaker's start time and the correct end time, so it is largely factually incorrect and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "During the time the man is speaking on screen, when does he mention 'Near Me'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 342.0,
        "end": 344.0
      },
      "pred_interval": {
        "start": 333.0,
        "end": 334.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 9.5,
        "average": 9.25
      },
      "rationale_metrics": {
        "rouge_l": 0.21538461538461537,
        "text_similarity": 0.7461367845535278,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the speaker and that 'Near Me' is mentioned, but the times are substantially incorrect (both E1 and E2 timings differ from the reference), the E1 end time is omitted, and the temporal relation is wrongly labeled as 'after' instead of 'during'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying 'Thank you and goodbye', when do the 'NHS Scotland' and 'Near Me' logos appear with text links?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 351.0,
        "end": 360.0
      },
      "pred_interval": {
        "start": 358.0,
        "end": 360.0
      },
      "iou": 0.2222222222222222,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 0.0,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7340363264083862,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the logos appear after the man finishes, but the timestamps are substantially incorrect (E1 predicted end 357.5s vs ground-truth 350.7s; E2 start 358.0s vs 351.0s), so key timing information is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the initial voiceover concludes with 'patient that day', when does the man on screen begin to say 'Thanks very much John and Amy'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 336.4,
        "end": 341.6
      },
      "pred_interval": {
        "start": 335.0,
        "end": 336.5
      },
      "iou": 0.015151515151518545,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3999999999999773,
        "end": 5.100000000000023,
        "average": 3.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4383561643835616,
        "text_similarity": 0.7807009220123291,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the utterance and the 'after' relation, but the timestamps are substantially off (E1 ~0.4s early, E2 start ~1.4s early and E2 end ~5.1s early), so the timing is not accurate enough."
      }
    }
  ]
}