{
  "model": "videollama",
  "experiment_name": "Vid_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.1310590411300558,
            "rouge_l_std": 0.02124078071823752,
            "text_similarity_mean": 0.47945084795355797,
            "text_similarity_std": 0.09954961448665985,
            "llm_judge_score_mean": 2.25,
            "llm_judge_score_std": 1.14564392373896
          },
          "short": {
            "rouge_l_mean": 0.12481231422730744,
            "rouge_l_std": 0.03276727195503216,
            "text_similarity_mean": 0.45693658804520965,
            "text_similarity_std": 0.14465194391465983,
            "llm_judge_score_mean": 2.0625,
            "llm_judge_score_std": 0.8267972847076845
          },
          "cider": {
            "cider_detailed": 0.0001247881617706645,
            "cider_short": 0.01580229498246703
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.1306180876472842,
            "rouge_l_std": 0.02660296756124312,
            "text_similarity_mean": 0.44557404589085353,
            "text_similarity_std": 0.1654899744024618,
            "llm_judge_score_mean": 2.4761904761904763,
            "llm_judge_score_std": 1.4677176197545183
          },
          "short": {
            "rouge_l_mean": 0.1388944580287746,
            "rouge_l_std": 0.0541041193059824,
            "text_similarity_mean": 0.41304876939171836,
            "text_similarity_std": 0.18922790452983032,
            "llm_judge_score_mean": 2.5714285714285716,
            "llm_judge_score_std": 2.2375885951372823
          },
          "cider": {
            "cider_detailed": 5.479316343644258e-06,
            "cider_short": 0.003301433430912807
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.12221867756076461,
            "rouge_l_std": 0.03006298606115045,
            "text_similarity_mean": 0.30144648941663593,
            "text_similarity_std": 0.1175856678564443,
            "llm_judge_score_mean": 1.0,
            "llm_judge_score_std": 0.7844645405527362
          },
          "short": {
            "rouge_l_mean": 0.07561963418623292,
            "rouge_l_std": 0.03795399867277571,
            "text_similarity_mean": 0.24370980721253616,
            "text_similarity_std": 0.1250276663013831,
            "llm_judge_score_mean": 0.6923076923076923,
            "llm_judge_score_std": 0.7216024245882199
          },
          "cider": {
            "cider_detailed": 3.894699041447598e-05,
            "cider_short": 0.00036440177105298544
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.1279652687793682,
          "text_similarity_mean": 0.40882379442034916,
          "llm_judge_score_mean": 1.9087301587301588
        },
        "short": {
          "rouge_l_mean": 0.1131088021474383,
          "text_similarity_mean": 0.37123172154982137,
          "llm_judge_score_mean": 1.7754120879120878
        },
        "cider": {
          "cider_detailed_mean": 5.640482284292824e-05,
          "cider_short_mean": 0.006489376728144275
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.5588235294117647,
          "correct": 57,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.22408402102550826,
            "rouge_l_std": 0.07361388966307196,
            "text_similarity_mean": 0.6513061506637171,
            "text_similarity_std": 0.16908007557958976,
            "llm_judge_score_mean": 5.637254901960785,
            "llm_judge_score_std": 3.8112043685620636
          },
          "rationale_cider": 0.07027599842814766
        },
        "02_Job_Interviews": {
          "accuracy": 0.62,
          "correct": 62,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.21724209220233218,
            "rouge_l_std": 0.06478169842027247,
            "text_similarity_mean": 0.6342522448673844,
            "text_similarity_std": 0.16566140452028624,
            "llm_judge_score_mean": 6.81,
            "llm_judge_score_std": 3.554419783874718
          },
          "rationale_cider": 0.10489518593198963
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.46956521739130436,
          "correct": 54,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.19663750741001265,
            "rouge_l_std": 0.07963050379867247,
            "text_similarity_mean": 0.5981155295937282,
            "text_similarity_std": 0.1838915376140582,
            "llm_judge_score_mean": 4.547826086956522,
            "llm_judge_score_std": 3.9458144654609915
          },
          "rationale_cider": 0.04320863306189865
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.549462915601023,
        "rationale": {
          "rouge_l_mean": 0.21265454021261768,
          "text_similarity_mean": 0.6278913083749432,
          "llm_judge_score_mean": 5.665026996305769
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.03940291939748842,
          "std_iou": 0.0962203654582832,
          "median_iou": 0.004761904761904762,
          "R@0.3": {
            "recall": 0.03383458646616541,
            "count": 9,
            "total": 266
          },
          "R@0.5": {
            "recall": 0.018796992481203006,
            "count": 5,
            "total": 266
          },
          "R@0.7": {
            "recall": 0.0037593984962406013,
            "count": 1,
            "total": 266
          },
          "mae": {
            "start_mean": 170.01943233082704,
            "end_mean": 3724.0714812030074,
            "average_mean": 1947.0454567669171
          },
          "rationale": {
            "rouge_l_mean": 0.2328729385077344,
            "rouge_l_std": 0.11288352576780235,
            "text_similarity_mean": 0.4552626983801785,
            "text_similarity_std": 0.19847592530939134,
            "llm_judge_score_mean": 2.601503759398496,
            "llm_judge_score_std": 2.1882799563307938
          },
          "rationale_cider": 0.26596667247401795
        },
        "02_Job_Interviews": {
          "mean_iou": 0.039155035081843224,
          "std_iou": 0.08091401830266971,
          "median_iou": 0.013447619047619376,
          "R@0.3": {
            "recall": 0.011857707509881422,
            "count": 3,
            "total": 253
          },
          "R@0.5": {
            "recall": 0.003952569169960474,
            "count": 1,
            "total": 253
          },
          "R@0.7": {
            "recall": 0.003952569169960474,
            "count": 1,
            "total": 253
          },
          "mae": {
            "start_mean": 122.41057707509881,
            "end_mean": 152.36122529644265,
            "average_mean": 137.38590118577076
          },
          "rationale": {
            "rouge_l_mean": 0.21612260964556212,
            "rouge_l_std": 0.10250207053794232,
            "text_similarity_mean": 0.41471585609283845,
            "text_similarity_std": 0.1915190316360527,
            "llm_judge_score_mean": 3.1185770750988144,
            "llm_judge_score_std": 2.435283044921383
          },
          "rationale_cider": 0.26993597230299254
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.03247753370564074,
          "std_iou": 0.06925346975263057,
          "median_iou": 0.008571428571429437,
          "R@0.3": {
            "recall": 0.011661807580174927,
            "count": 4,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.0029154518950437317,
            "count": 1,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0029154518950437317,
            "count": 1,
            "total": 343
          },
          "mae": {
            "start_mean": 168.47562973760932,
            "end_mean": 213.45146647230322,
            "average_mean": 190.9635481049563
          },
          "rationale": {
            "rouge_l_mean": 0.21729344630470301,
            "rouge_l_std": 0.11497238926869263,
            "text_similarity_mean": 0.41571963335992906,
            "text_similarity_std": 0.2100229547523211,
            "llm_judge_score_mean": 2.935860058309038,
            "llm_judge_score_std": 2.478237006953142
          },
          "rationale_cider": 0.25001319542365197
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.03701182939499079,
        "mae_average": 758.4649686858814,
        "R@0.3": 0.01911803385207392,
        "R@0.5": 0.008555004515402405,
        "R@0.7": 0.0035424731870816024,
        "rationale": {
          "rouge_l_mean": 0.22209633148599983,
          "text_similarity_mean": 0.428566062610982,
          "llm_judge_score_mean": 2.8853136309354497
        }
      }
    }
  }
}