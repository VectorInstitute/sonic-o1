{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 13,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.12221867756076461,
      "rouge_l_std": 0.03006298606115045,
      "text_similarity_mean": 0.30144648941663593,
      "text_similarity_std": 0.1175856678564443,
      "llm_judge_score_mean": 1.0,
      "llm_judge_score_std": 0.7844645405527362
    },
    "short": {
      "rouge_l_mean": 0.07561963418623292,
      "rouge_l_std": 0.03795399867277571,
      "text_similarity_mean": 0.24370980721253616,
      "text_similarity_std": 0.1250276663013831,
      "llm_judge_score_mean": 0.6923076923076923,
      "llm_judge_score_std": 0.7216024245882199
    },
    "cider": {
      "cider_detailed": 3.894699041447598e-05,
      "cider_short": 0.00036440177105298544
    }
  },
  "per_entry_results": [
    {
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.13450292397660818,
        "text_similarity": 0.4960743188858032,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only describes generic courtroom visuals that loosely match the video but omits all key factual elements (sentencing hearing, attorney Iworski, Frank's disorderly conduct and bail charge being dropped, his protest, and the censorship/promotion segment) and incorrectly asserts the topic is cybercrime, which is a clear factual mismatch."
      },
      "short": {
        "rouge_l": 0.05755395683453237,
        "text_similarity": 0.3210873007774353,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only vaguely notes a courtroom setting, which matches the video scene, but it omits almost all key details (YouTube/ODYSSEY promotion, attorney/defendant names, charges, protests, First Amendment claims) and introduces an incorrect topic (cybercrime)."
      }
    },
    {
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.13513513513513511,
        "text_similarity": 0.279046893119812,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives only a superficial visual description of a courtroom scene and tense atmosphere, but omits nearly all substantive facts from the correct answer (defendant's crimes and history, sentencing recommendation, victim impact statements, Skolman's statements, and the judge's response)."
      },
      "short": {
        "rouge_l": 0.029197080291970805,
        "text_similarity": 0.20868541300296783,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction captures only a vague courtroom scene (someone speaking, judge present) but omits nearly all factual content from the reference\u2014criminal history, victim impact statements, Skolman's admissions/denials, and the judge's moral response\u2014and adds an unsupported detail about police outside."
      }
    },
    {
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.13262599469496023,
        "text_similarity": 0.39154767990112305,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only superficial visual elements (news anchor, courtroom shots) but omits all key factual content from the correct answer\u2014no mention of the guilty verdict, counts, evidence, legal actions, officials' statements, or family reactions\u2014and includes irrelevant/unverified details like a masked man."
      },
      "short": {
        "rouge_l": 0.1188118811881188,
        "text_similarity": 0.332864373922348,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only describes brief visual elements and generically notes a murder trial, but it omits nearly all key factual content from the correct answer (verdict, charges, evidence, officials' statements, and sentencing details)."
      }
    },
    {
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.1602209944751381,
        "text_similarity": 0.44463130831718445,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary: it describes different people and topics (Air Force, Supreme Court support, unrelated images) and omits the core AI-avatar courtroom incident, judges' reactions, and the discussion of AI's legal implications."
      },
      "short": {
        "rouge_l": 0.10294117647058823,
        "text_similarity": 0.4141971468925476,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only mentions a man and a judge-like figure and omits the core facts about the AI-generated lawyer avatar, the judge stopping the video, the man's admission and promotion, and the ethical/legal discussion; it also incorrectly implies a 'Supreme Court Justice.'"
      }
    },
    {
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.07228915662650602,
        "text_similarity": 0.2551991939544678,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes a man testifying in a courtroom and the serious tone, but it omits the core factual content\u2014that the testimony is Lyle Menendez alleging sexual abuse by his father\u2014and thus fails to capture the summary's main subject and implications."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": 0.2278856784105301,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only describes a man in a courtroom and his demeanor, failing to mention that the video is Lyle Menendez's testimony about alleged sexual abuse or its relation to the Menendez Brothers case, omitting key factual elements."
      }
    },
    {
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.06289308176100629,
        "text_similarity": 0.11603449285030365,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the reference: it describes generic participants in a video conference and omits all substantive legal details, parties, arguments, and case law discussed in the correct summary."
      },
      "short": {
        "rouge_l": 0.07179487179487179,
        "text_similarity": 0.1775687336921692,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is generic and unrelated, omitting all substantive legal points about Hothi v. Musk and even introducing incorrect details (e.g., cameras on heads), so it fails to match the reference."
      }
    },
    {
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.1032258064516129,
        "text_similarity": 0.4585855007171631,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction notes limited visual details and correctly identifies Judge Jackson, but it omits the central substantive content\u2014Senator Ted Cruz's line of questioning about legal standing and self-identification, the hypothetical scenarios, and Judge Jackson's responses\u2014so it fails to match the correct answer's core meaning."
      },
      "short": {
        "rouge_l": 0.144,
        "text_similarity": 0.5405113697052002,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only mentions the judge's identity and an incidental visual detail, but omits virtually all key content from the reference (Cruz's questioning, hypotheticals about gender/race, legal standing, and Jackson's refusal and judicial reasoning), and thus fails to capture the summary."
      }
    },
    {
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.16348773841961853,
        "text_similarity": 0.33370524644851685,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only gives vague visual descriptions and fails to convey the key factual elements (cross-examination, Pettis admitting coercion, Lankford's accusation and contempt, and motive), so it does not match the detailed substantive summary; only the final recess note is roughly mentioned."
      },
      "short": {
        "rouge_l": 0.1076923076923077,
        "text_similarity": 0.2196069359779358,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only gives generic visual descriptions of people speaking in a courtroom and omits all key factual details from the correct answer (cross-examination, admission of threatening the witness, who ordered it, and the judge's contempt/recess)."
      }
    },
    {
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.09154929577464789,
        "text_similarity": 0.22384612262248993,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect and highly incomplete: it describes generic visual details and tone while omitting virtually all substantive content about civil litigation, settlement advice, research, courtroom training, and professional guidance present in the correct answer, and includes likely hallucinated elements."
      },
      "short": {
        "rouge_l": 0.09420289855072464,
        "text_similarity": 0.13190850615501404,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary: it describes a man offering help and a closing screen, whereas the correct answer details extensive legal advice on civil litigation, pleadings, preparation, settlements, and professional development; none of those key elements are present or matched."
      }
    },
    {
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.14754098360655737,
        "text_similarity": 0.22670301795005798,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary, describing a document about exam translation rather than the trial narrative; it omits all key factual elements and contradicts the video's content."
      },
      "short": {
        "rouge_l": 0.039473684210526314,
        "text_similarity": 0.16330988705158234,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary\u2014it describes exam/instruction text instead of the prosecutor's case about Carl Miller, omitting all key facts."
      }
    },
    {
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.13654618473895583,
        "text_similarity": 0.10452266037464142,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the reference: it describes a clock image and interpreter practice scripts rather than the theft, identification, arrest, and resistance involving Walter Merchant, omitting all key facts and adding unrelated content."
      },
      "short": {
        "rouge_l": 0.05405405405405405,
        "text_similarity": 0.06610338389873505,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, describing unrelated visuals and scripts; it omits all key facts about the vandalism, theft, the suspect (Merchant), his resistance/arrest, items found, and Mendoza's identification."
      }
    },
    {
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.12612612612612611,
        "text_similarity": 0.292678564786911,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely generic and unrelated, focusing on abstract notions of law, justice, and gestures, while omitting almost all key specifics from the correct summary (appeal types, drafting and Section 313 error, appellate strategy, courtroom advocacy, judge psychology, and the cited case anecdotes)."
      },
      "short": {
        "rouge_l": 0.08333333333333333,
        "text_similarity": 0.24400220811367035,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated and much more generic/ideological, omitting all key factual points from the correct answer (case details, categories of appeals, advocacy techniques) and introducing unsupported statements not present in the reference."
      }
    },
    {
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.12269938650306747,
        "text_similarity": 0.29622936248779297,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only describes generic visuals and the Bond Solon logo, missing virtually all substantive content of the interview (witness preparation, challenges like nervousness and cross-examination, and the described training methodology), so it fails to capture key facts though it correctly notes the branding."
      },
      "short": {
        "rouge_l": 0.08,
        "text_similarity": 0.12049655616283417,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer only describes visual details (a man, pointing, and a logo) and is semantically unrelated to the video's substantive content about witness preparation, training, and its impact on trials, omitting all key factual elements."
      }
    }
  ]
}