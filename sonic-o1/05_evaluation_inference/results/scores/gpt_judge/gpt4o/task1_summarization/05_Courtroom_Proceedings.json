{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 13,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.17233116526263645,
      "rouge_l_std": 0.06667314641329872,
      "text_similarity_mean": 0.382079865783453,
      "text_similarity_std": 0.30422573581484674,
      "llm_judge_score_mean": 2.769230769230769,
      "llm_judge_score_std": 3.400313246969284
    },
    "short": {
      "rouge_l_mean": 0.11653953421404226,
      "rouge_l_std": 0.08137452763347958,
      "text_similarity_mean": 0.2883607823974811,
      "text_similarity_std": 0.320428836396047,
      "llm_judge_score_mean": 2.0,
      "llm_judge_score_std": 2.4806946917841692
    },
    "cider": {
      "cider_detailed": 0.019601671693466844,
      "cider_short": 2.9088754360451685e-06
    }
  },
  "per_entry_results": [
    {
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.12745098039215688,
        "text_similarity": 0.09720359742641449,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the reference: it discusses renewable energy rather than the courtroom scene and censorship/promotional content described in the correct answer, omitting all key factual elements and introducing unrelated content."
      },
      "short": {
        "rouge_l": 0.12790697674418605,
        "text_similarity": 0.16494445502758026,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated and contradicts the correct answer: it discusses renewable energy rather than the courtroom/censorship content, omitting all key facts from the reference."
      }
    },
    {
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.1388888888888889,
        "text_similarity": 0.07960332185029984,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, describing a climate change video instead of the courtroom victim impact statements and defendant's remarks; it omits all key facts and introduces unrelated content."
      },
      "short": {
        "rouge_l": 0.09302325581395349,
        "text_similarity": 0.042385198175907135,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, discussing climate change instead of the criminal case, victims' statements, and defendant remarks, thus omitting all key facts and contradicting the content."
      }
    },
    {
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.12903225806451615,
        "text_similarity": 0.018405143171548843,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated, discussing renewable energy rather than the Chandler Halderson murder trial; it omits all key facts from the correct summary and introduces wholly different content."
      },
      "short": {
        "rouge_l": 0.05405405405405405,
        "text_similarity": -0.012393567711114883,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct answer, discussing renewable energy instead of the Chandler Halderson trial; it omits all key facts and introduces irrelevant content."
      }
    },
    {
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.2548076923076923,
        "text_similarity": 0.7937425374984741,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction accurately captures the core incident, the judge's displeasure, and the video's ethical concerns about AI in courts, but it omits specific details about AI's current uses, the projected 90% adoption by 2030, and courts' disclosure/training measures mentioned in the correct answer."
      },
      "short": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.8273446559906006,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the main themes (an AI video used in court, transparency, and ethical questions) but omits key facts (the man admitted generating it, was promoting his AI startup, the judge stopped the video) and adds an unsupported claim about AI making justice faster."
      }
    },
    {
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.1408450704225352,
        "text_similarity": 0.17466512322425842,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated\u2014discussing climate change rather than Lyle Menendez's testimony about sexual abuse\u2014thus omitting all key facts and contradicting the correct summary."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": 0.058019235730171204,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary\u2014discussing climate change rather than Lyle Menendez's testimony and its context\u2014so it omits all key factual elements and contradicts the topic."
      }
    },
    {
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.09200968523002422,
        "text_similarity": 0.251735657453537,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary\u2014it's about renewable energy rather than the court proceedings in Hothi v. Musk\u2014so it fails to capture any factual elements of the reference."
      },
      "short": {
        "rouge_l": 0.054644808743169404,
        "text_similarity": 0.10566042363643646,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, omitting all key facts about the Hothi v. Musk oral argument and instead introducing irrelevant content about renewable energy."
      }
    },
    {
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.11793611793611791,
        "text_similarity": 0.09862510859966278,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated\u2014it's a product promotional summary, while the correct answer describes a Supreme Court confirmation hearing about Senator Cruz questioning Judge Ketanji Brown Jackson on legal standing and self-identification; it omits and contradicts all key facts."
      },
      "short": {
        "rouge_l": 0.08130081300813008,
        "text_similarity": -0.01579192839562893,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated\u2014a product advertisement\u2014while the correct answer summarizes a judicial hearing about Senator Cruz questioning Judge Jackson on self-identification and legal standing; it omits all key facts and contradicts the content."
      }
    },
    {
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.28116710875331563,
        "text_similarity": 0.7360600233078003,
        "llm_judge_score": 7,
        "llm_judge_justification": "The predicted answer captures the core events (Pettis threatening Gloria under orders, the career-promotion promise, identification of Detective Lankford, and adjournment) but omits that Mickey Haller was conducting the cross-examination, fails to mention the instructing detective's stated belief about Menendez's guilt and the 'some pros' testimony, misspells Lankford, and does not report that the judge formally held Lankford in contempt."
      },
      "short": {
        "rouge_l": 0.26548672566371684,
        "text_similarity": 0.7839222550392151,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the core event (Pettis testifying that she was pressured and naming another detective) and the court being halted, but it omits the defense attorney's role, misstates the judge's sanction (warns vs held in contempt/angry outburst), misspells the detective's name, and adds an unsupported claim about a promised career advancement."
      }
    },
    {
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.12398921832884097,
        "text_similarity": 0.1302652359008789,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated\u2014covering renewable energy\u2014while the correct answer summarizes a talk on civil litigation, legal practice, and career advice; it omits all key elements and introduces unrelated content."
      },
      "short": {
        "rouge_l": 0.01593625498007968,
        "text_similarity": 0.024453088641166687,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is entirely unrelated to the correct answer: the reference summarizes civil litigation practice and courtroom advice, while the prediction discusses renewable energy, omitting and contradicting all key elements."
      }
    },
    {
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.1981424148606811,
        "text_similarity": 0.5813096165657043,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the core criminal events (date, location, defendant, alleged offenses, witness and forensic evidence) but incorrectly frames the video as an exam practice and adds unfounded details (exam format, underlined scoring units, defense argument). It also omits several specific factual elements from the correct answer (e.g., witness names, license plate, exact address), so it includes both omissions and hallucinations."
      },
      "short": {
        "rouge_l": 0.08750000000000002,
        "text_similarity": 0.23300151526927948,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is a vague, generic summary that mentions a drug case and witness testimony but omits nearly all key specifics (names, bottle transfer, flight and battery, license plate, car location, forensic match) and even introduces an unrelated 'NCSC Court Interpreter' detail."
      }
    },
    {
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.1443298969072165,
        "text_similarity": 0.4157659411430359,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely mischaracterizes the video as an interpreter practice/courtroom training and adds hallucinated details (e.g., underlined scoring units), while only broadly referencing Mendoza recounting the incident; it omits key factual elements from the correct answer such as the date, suspect identity, the assault on the deputy, items found on the suspect, and the specifics of his arrest."
      },
      "short": {
        "rouge_l": 0.10714285714285715,
        "text_similarity": 0.1322980523109436,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely mischaracterizes the video as a court interpreter practice/script and omits almost all key facts (names, date, 911 call, theft, arrest, assault on an officer, items found, and courtroom ID), only vaguely mentioning a car vandalism theme."
      }
    },
    {
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.17827298050139276,
        "text_similarity": 0.6994315385818481,
        "llm_judge_score": 6,
        "llm_judge_justification": "The predicted answer captures the general theme\u2014an appellate legal discussion, categories of appeals, drafting tips, witness-reading strategies, and anecdotes\u2014but omits many specific points from the reference (R.S. Cheema as the speaker, emphasis on oral advocacy and judge psychology, the Section 313/Ropar example, the 306/304B suicide illustration, and named case anecdotes), so it is partially complete but lacks key details."
      },
      "short": {
        "rouge_l": 0.15929203539823011,
        "text_similarity": 0.619997501373291,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures several high-level themes (appeal categories, drafting/reading appeals, strategic questioning, and citation of judgments) but omits many key specifics from the correct answer\u2014such as the emphasis on oral advocacy, judge\u2011style/communication, vicarious liability in special statutes, concrete case examples, and the call for deep, evidence\u2011based preparation."
      }
    },
    {
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.31343283582089554,
        "text_similarity": 0.8902254104614258,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction accurately captures the main points about witness nervousness, cross-examination risks, the purpose of witness familiarisation, and the theory-then-practice training approach, but it adds unverified details (Alex's specific role, virtual hearing logistics, and a closing invitation) and omits Gilbert's designation as a solicitor advocate."
      },
      "short": {
        "rouge_l": 0.23188405797101447,
        "text_similarity": 0.7848492860794067,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction captures the core points that familiarisation training reduces nervousness and includes theory plus mock cross-examinations, but it omits that unprepared witnesses can negatively impact legal cases and the explicit goal of helping witnesses understand their role and the lawyer's objectives; it also adds an unsupported point about virtual hearings."
      }
    }
  ]
}