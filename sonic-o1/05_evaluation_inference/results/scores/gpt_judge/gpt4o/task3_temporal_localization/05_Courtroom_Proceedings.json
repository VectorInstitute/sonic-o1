{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 343,
  "aggregated_metrics": {
    "mean_iou": 0.04548413113610914,
    "std_iou": 0.12062949301063076,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.06705539358600583,
      "count": 23,
      "total": 343
    },
    "R@0.5": {
      "recall": 0.02040816326530612,
      "count": 7,
      "total": 343
    },
    "R@0.7": {
      "recall": 0.0,
      "count": 0,
      "total": 343
    },
    "mae": {
      "start_mean": 36.243384839650155,
      "end_mean": 36.73593294460643,
      "average_mean": 36.4896588921283
    },
    "rationale": {
      "rouge_l_mean": 0.3268960745906418,
      "rouge_l_std": 0.08288641821332576,
      "text_similarity_mean": 0.7490067421173563,
      "text_similarity_std": 0.076860834808838,
      "llm_judge_score_mean": 2.795918367346939,
      "llm_judge_score_std": 1.6604771923924893
    },
    "rationale_cider": 0.3171677728036678
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the attorney states they will only proceed with the sentencing hearing for disorderly conduct and not the breach of bail, when does Frank ask if the breach of bail charge is being dropped?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.292,
        "end": 41.433
      },
      "pred_interval": {
        "start": 20.5,
        "end": 21.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.792,
        "end": 20.133,
        "average": 19.9625
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.5956190824508667,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the correct relation ('after') and the event descriptions, it gives substantially incorrect timestamps for both E1 and E2 compared to the ground truth, which is a key factual error for a timing-based question."
      }
    },
    {
      "question_id": "002",
      "question": "After Frank states he is being persecuted for exercising his First Amendment rights, when does he say he was found guilty for being loud due to a disability?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 133.165,
        "end": 141.734
      },
      "pred_interval": {
        "start": 55.0,
        "end": 58.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.16499999999999,
        "end": 83.23400000000001,
        "average": 80.6995
      },
      "rationale_metrics": {
        "rouge_l": 0.2531645569620253,
        "text_similarity": 0.7137417793273926,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the predicted answer correctly labels the relationship as 'after', it gives substantially incorrect timestamps for both E1 and E2 compared to the reference, failing to match the key factual timing information."
      }
    },
    {
      "question_id": "003",
      "question": "After the attorney asks Frank why he is getting angry, when does Frank respond that he is angry because he is there for nothing?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 119.983,
        "end": 123.427
      },
      "pred_interval": {
        "start": 42.0,
        "end": 44.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.983,
        "end": 78.927,
        "average": 78.45500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.738349199295044,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events and the same 'after' relation, but the provided timestamps are substantially different from the ground truth, so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'Frank, you're in a courthouse,' when does the man respond by telling her to arrest him?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 173.35,
        "end": 176.25
      },
      "pred_interval": {
        "start": 159.0,
        "end": 160.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.349999999999994,
        "end": 15.75,
        "average": 15.049999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.6024637818336487,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterances and the 'after' relation, but the reported timestamps for both E1 and E2 are substantially incorrect (off by ~10\u201316s) and E2's duration/start do not match the ground truth, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the text 'Libraries already protected more than one video that YouTube took down' finishes being described by the narrator, when does the text describing YouTube's strike appear?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 169.23,
        "end": 175.0
      },
      "pred_interval": {
        "start": 225.0,
        "end": 227.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.77000000000001,
        "end": 52.0,
        "average": 53.885000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2888888888888889,
        "text_similarity": 0.6291385293006897,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer misstates all timestamps by a large margin (E1 end and E2 start/end differ significantly from the reference), though it correctly indicates a sequential/after relationship; therefore it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the defendant was convicted of three counts of homicide by intoxicated use of a motor vehicle, when does he mention a count of injury by intoxicated use of a motor vehicle?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 16.341,
        "end": 19.565
      },
      "pred_interval": {
        "start": 9.0,
        "end": 11.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.341000000000001,
        "end": 8.565000000000001,
        "average": 7.953000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7432094812393188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('once finished') and event ordering right but the timestamps are substantially incorrect compared to the ground truth (E1/E2 times differ by many seconds), so it fails factual accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the defendant was serving a significant prison sentence for those convictions, when does he specify the total sentence length?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.5,
        "end": 46.7
      },
      "pred_interval": {
        "start": 29.0,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.5,
        "end": 14.700000000000003,
        "average": 13.600000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7299805879592896,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation and even the content ('serving a significant prison sentence' and '51 years'), but the provided timestamps are substantially different from the reference (wrong start/end times and missing E1 end), so it fails on the key temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker laments that Tim had a long life ahead of him, when does he state that rehabilitation is one of the reasons people are sent to prison?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.786,
        "end": 207.069
      },
      "pred_interval": {
        "start": 95.0,
        "end": 97.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.786,
        "end": 110.06899999999999,
        "average": 109.4275
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7266804575920105,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterances and labels the relation as 'after', but the timestamps are grossly incorrect and it misrepresents the sizeable gap between the events described in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male attorney finishes his address to the court, when does the judge ask if there are representatives of the victim?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 304.214,
        "end": 307.942
      },
      "pred_interval": {
        "start": 178.0,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 126.214,
        "end": 127.94200000000001,
        "average": 127.078
      },
      "rationale_metrics": {
        "rouge_l": 0.47457627118644063,
        "text_similarity": 0.7808177471160889,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly captures the immediate/once_finished relationship, but the timestamps are drastically incorrect (predicted ~177\u2013178s vs. ground truth ~300\u2013307s), so it fails to match the key factual temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge states that the person has to use the microphone, when does the man in the white t-shirt (victim) begin moving to the microphone?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 356.0
      },
      "pred_interval": {
        "start": 183.0,
        "end": 186.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 169.0,
        "end": 170.0,
        "average": 169.5
      },
      "rationale_metrics": {
        "rouge_l": 0.37142857142857144,
        "text_similarity": 0.7012391686439514,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship but is factually inaccurate: it gives substantially different timestamps for both events (182s/183s vs 349.7\u2013351.2s and 352\u2013356s) and omits the target's completion time."
      }
    },
    {
      "question_id": "003",
      "question": "During the man's speech to the court, when does he say, 'You killed somebody that meant a lot'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 401.276,
        "end": 403.024
      },
      "pred_interval": {
        "start": 205.0,
        "end": 207.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 196.276,
        "end": 196.024,
        "average": 196.15
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068966,
        "text_similarity": 0.7309014797210693,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relationship ('during') correct but is factually wrong about both the speech start and phrase timestamps (200s/205s vs. 368.0s/401.276-403.024s), so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge warns the man in the white shirt about interrupting, when does the judge get up and leave the bench?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.13,
        "end": 331.15
      },
      "pred_interval": {
        "start": 330.0,
        "end": 345.0
      },
      "iou": 0.0013333333333321207,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1299999999999955,
        "end": 13.850000000000023,
        "average": 7.490000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.7996521592140198,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states that the judge leaves shortly after the warning (matching the anchor\u2192target order), but it omits the precise timestamps and exact timing details given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes asking the man if he wishes to address him, when does the man respond?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.38,
        "end": 331.39
      },
      "pred_interval": {
        "start": 375.0,
        "end": 380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.620000000000005,
        "end": 48.610000000000014,
        "average": 46.11500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7148289680480957,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the man's response is immediate after the judge's question, but it gives a significantly incorrect timestamp for the anchor event (\u2248375s vs correct \u2248331.36s), so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the man in the white shirt states his child's birth date, when does he describe being happy and proud about becoming a parent?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.55,
        "end": 331.58
      },
      "pred_interval": {
        "start": 400.0,
        "end": 415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.44999999999999,
        "end": 83.42000000000002,
        "average": 75.935
      },
      "rationale_metrics": {
        "rouge_l": 0.3823529411764706,
        "text_similarity": 0.7981411218643188,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the happiness statement follows the birth-date remark (immediate/after), but it gives a significantly incorrect timestamp for E1 (~400s vs ~331.4s) and thus fails on the key factual timing detail."
      }
    },
    {
      "question_id": "001",
      "question": "After the man finishes his emotional statement, when does the woman with long dark hair start walking towards the speaker's table?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.096,
        "end": 512.12
      },
      "pred_interval": {
        "start": 560.0,
        "end": 570.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.903999999999996,
        "end": 57.879999999999995,
        "average": 52.891999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7405325174331665,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but the timestamps are substantially wrong (predicted ~560\u2013570s vs. ground-truth ~511.56\u2013512.12s) and the actual E2 start time is not accurately provided, omitting key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman with long dark hair finishes sitting down at the table, when does she say 'Good afternoon'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.245,
        "end": 512.259
      },
      "pred_interval": {
        "start": 580.0,
        "end": 582.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.755,
        "end": 69.74099999999999,
        "average": 68.74799999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4262295081967213,
        "text_similarity": 0.7439651489257812,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the event order (E2 occurs after E1) but gives significantly incorrect timestamps and a much longer/incorrect duration for E2, and uses a less precise relation ('after' vs 'once_finished'), so it is largely factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman says 'my son did not deserve this', when does she list his family relationships?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 513.109,
        "end": 513.197
      },
      "pred_interval": {
        "start": 610.0,
        "end": 615.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.89099999999996,
        "end": 101.803,
        "average": 99.34699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.6222187280654907,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates E2 occurs after E1, but the reported timestamps are substantially off (~100s difference) and it fails to note the short crying pause; thus it largely mismatches the reference timing and detail."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states she misses the things that irritated her about her son, when does she request life without the possibility of parole?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 779.1,
        "end": 786.0
      },
      "pred_interval": {
        "start": 720.0,
        "end": 726.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.10000000000002,
        "end": 60.0,
        "average": 59.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.6780381202697754,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance content but fails to provide the correct anchor times and gives incorrect target timestamps (720\u2013726s vs correct 779.1\u2013786.0s), and the stated temporal relation is inconsistent with its own timestamps; hence largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first woman finishes her statement saying 'That's it', when does attorney Koenig begin describing the defendant's difficult upbringing?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 829.7,
        "end": 831.0
      },
      "pred_interval": {
        "start": 736.0,
        "end": 742.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.70000000000005,
        "end": 89.0,
        "average": 91.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.47500000000000003,
        "text_similarity": 0.7583886384963989,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer misstates both event timestamps (E1 given as ~736.0s vs 791.6s in the reference; E2 given as 736.0\u2013742.0s vs 829.7\u2013831.0s) and wrongly characterizes the relation as 'immediately after.' Only the general direction (E2 occurs after E1) aligns, but the timing and adjacency claims are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After attorney Koenig explains that the defendant did not pursue an NGI defense, when does she state that the NGI defense actually came from her and attorney Zawada?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 900.0
      },
      "pred_interval": {
        "start": 860.0,
        "end": 870.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.0,
        "end": 30.0,
        "average": 31.0
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.6125425100326538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor segment semantically and labels the relation as 'after', but it majorly misplaces the target timestamps (860\u2013870s vs. the correct 892\u2013900s) and thus conflicts with the ground-truth timing (and with its own timestamps the relation would be incorrect)."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"mental illness\", when does she state that Joshua Skolman needs treatment in the institution?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.573,
        "end": 922.798
      },
      "pred_interval": {
        "start": 875.0,
        "end": 880.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.57299999999998,
        "end": 42.798,
        "average": 44.18549999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.6914249658584595,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but gives significantly incorrect timestamps (and single time points rather than the referenced ranges) that contradict the ground truth, so key factual details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks Joshua Skolman if he has anything to tell him, when does Skolman explicitly deny having mental illness?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1001.283,
        "end": 1002.784
      },
      "pred_interval": {
        "start": 920.0,
        "end": 925.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.28300000000002,
        "end": 77.78399999999999,
        "average": 79.5335
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.7235180139541626,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the denial occurs after the judge's question, but it gives incorrect and inconsistent timestamps for both anchor and target (920.0s vs ~987s and ~1001s in the ground truth) and even lists the same start time for both events, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After Joshua Skolman states \"I don't have mental illness\", when does he next explicitly claim that \"you guys\" have the mental illness or defect?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1006.129,
        "end": 1009.331
      },
      "pred_interval": {
        "start": 925.0,
        "end": 930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.12900000000002,
        "end": 79.33100000000002,
        "average": 80.23000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.8337548971176147,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the directionality ('after') but the anchor and target timestamps are substantially incorrect (off by ~80+ seconds) and it fails to identify the immediate next occurrence as specified in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the Judge says, 'Thank you, Mr. Scolman', when does he state that only God creates life?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1149.0,
        "end": 1151.0
      },
      "pred_interval": {
        "start": 1090.0,
        "end": 1095.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.0,
        "end": 56.0,
        "average": 57.5
      },
      "rationale_metrics": {
        "rouge_l": 0.44067796610169496,
        "text_similarity": 0.6746261119842529,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct (target after anchor) but both event timestamps are substantially incorrect compared to the reference (anchor off by ~27s and target off by ~59\u201361s), so the localization is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the clerk finishes commanding silence, when does the deputy say 'Be seated'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1109.8,
        "end": 1110.5
      },
      "pred_interval": {
        "start": 1060.0,
        "end": 1065.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.799999999999955,
        "end": 45.5,
        "average": 47.64999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32142857142857145,
        "text_similarity": 0.6817448735237122,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('once') but gives timestamps that are substantially incorrect (about 50+ seconds earlier) compared to the ground truth, so the key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the Judge states 'Every life has value', when does he begin speaking about humanity's 'terrible, terrible dark side'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1164.5,
        "end": 1169.5
      },
      "pred_interval": {
        "start": 1110.0,
        "end": 1115.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.5,
        "end": 54.5,
        "average": 54.5
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.5991861820220947,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation right (target after anchor) but the timestamps are significantly incorrect (predicted ~1110\u20131115s vs correct ~1156\u20131169s), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states he doesn't think it's a mental illness, when does he define a thoughtful conviction?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.453,
        "end": 1238.14
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1235.0
      },
      "iou": 0.19004914004914084,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4529999999999745,
        "end": 3.1400000000001,
        "average": 3.2965000000000373
      },
      "rationale_metrics": {
        "rouge_l": 0.3692307692307692,
        "text_similarity": 0.7233726978302002,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the ordering (target occurs after the anchor) and roughly the same time region, but the timestamps are noticeably inaccurate (different start/end times) and the anchor end time is omitted, so it lacks precise alignment with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if he needs to recite historical crimes, when does he state that it causes one to pause and lose their breath?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1259.682,
        "end": 1264.588
      },
      "pred_interval": {
        "start": 1240.0,
        "end": 1245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.682000000000016,
        "end": 19.587999999999965,
        "average": 19.63499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7807442545890808,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives substantially incorrect start/end times for both the anchor and target compared to the ground truth, so it fails major factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge declares that civilized society values life, when does he state that taking a life is the highest crime?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1362.484,
        "end": 1366.753
      },
      "pred_interval": {
        "start": 1260.0,
        "end": 1265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 102.48399999999992,
        "end": 101.75299999999993,
        "average": 102.11849999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.5617680549621582,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the provided timestamps are far off from the reference (\u224890\u2013100s earlier) and thus contradict the key factual timing; major timestamp inaccuracies warrant a low score."
      }
    },
    {
      "question_id": "001",
      "question": "After the inmate first looks down at the paper handed to him, when is he handed paper again by an officer?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1603.0,
        "end": 1603.4
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1607.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 3.599999999999909,
        "average": 2.7999999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.8314080238342285,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the events and that E2 occurs after E1 and adds a plausible visual cue, but the timestamps are notably inaccurate (E1 predicted ~1600.0s vs actual 1590.8\u20131590.9s, E2 predicted 1605.0s vs actual 1603.0\u20131603.4s), reducing precision."
      }
    },
    {
      "question_id": "002",
      "question": "After the inmate turns his head to the side, when does he start walking away, escorted by the officers?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1626.0,
        "end": 1627.0
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1622.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 5.0,
        "average": 5.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.8721809983253479,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('after') and the visual cue correct, but both event timestamps are substantially wrong (E1 is ~18s off and E2 ~6s earlier than the ground truth), failing to match key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the distinct sound of a door or gate opening, when does the inmate walk through the door?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1637.0
      },
      "pred_interval": {
        "start": 1635.0,
        "end": 1637.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 0.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.28205128205128205,
        "text_similarity": 0.8903408646583557,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly identifies the event types (door sound then inmate walking) and the 'after' relationship, it gives substantially incorrect timestamps\u2014especially E1 (1633.0s vs 1603.2s)\u2014and misrepresents the large temporal gap, so key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states, \"You will never be released from the state prison system,\" when does he mention a \"compass evaluation\" as part of the extended supervision conditions?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1433.0,
        "end": 1436.0
      },
      "pred_interval": {
        "start": 1435.0,
        "end": 1440.0
      },
      "iou": 0.14285714285714285,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 4.0,
        "average": 3.0
      },
      "rationale_metrics": {
        "rouge_l": 0.35,
        "text_similarity": 0.7725144624710083,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the target as occurring after the anchor and places E2 start within the reference window, but it mislocates E1 (1425s vs correct 1418s) and extends E2 end too late (1440s vs 1436s), so only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge mentions that the 25-year sentence for Count 2 will be consecutive to any sentence the defendant is now serving, when does the camera cut to a shot of the back of the defendant's head?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.8,
        "end": 1440.5
      },
      "pred_interval": {
        "start": 1450.0,
        "end": 1455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.200000000000045,
        "end": 14.5,
        "average": 12.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.7980228066444397,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces both event timings by ~10\u201320 seconds and mislabels E1 boundary (says E1 starts at 1445s vs correct E1 ends at 1429.5s), though it correctly preserves the temporal 'after' relation; overall the timestamps are substantially incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge orders the $5,000 paid by the Compensation Panel to be part of the restitution order, when does the defendant stand up in the courtroom?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1539.0,
        "end": 1542.0
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.0,
        "end": 63.0,
        "average": 62.0
      },
      "rationale_metrics": {
        "rouge_l": 0.42105263157894735,
        "text_similarity": 0.8074291944503784,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the defendant stands after the judge's restitution order, but the timestamps are substantially incorrect (off by over a minute and a half) and do not match the ground-truth start/end times, so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the man looks up and turns his head to his left, when do the man and deputies open the door and exit the room?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1642.992
      },
      "gt_interval": {
        "start": 1631.0,
        "end": 1634.0
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1635.0
      },
      "iou": 0.2,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 1.0,
        "average": 6.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4158415841584158,
        "text_similarity": 0.7718282341957092,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and visual cues but gives substantially incorrect timestamps (E1 at 1620 vs 1625\u20131627; E2 starting at 1625 vs 1631) and thus contradicts the correct temporal relation and durations, though it roughly captures the exit action."
      }
    },
    {
      "question_id": "001",
      "question": "While the male anchor announces a jury has reached a verdict in the Chandler Halderson case, when does the on-screen text 'JURY REACHES VERDICT' first appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 4.6,
        "end": 22.8
      },
      "pred_interval": {
        "start": 8.0,
        "end": 10.0
      },
      "iou": 0.10989010989010987,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4000000000000004,
        "end": 12.8,
        "average": 8.100000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.8786364793777466,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes an anchor announcement but grossly misstates when the on-screen text first appears (predicts 8.0s\u201310.0s vs correct 4.6s\u201322.8s) and thus incorrectly labels the relationship as 'immediately after' instead of 'during', omitting key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female anchor states Halderson faces eight charges, when does the on-screen graphic detailing the charges appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.7,
        "end": 35.8
      },
      "pred_interval": {
        "start": 45.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.3,
        "end": 11.200000000000003,
        "average": 16.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2807017543859649,
        "text_similarity": 0.8220438957214355,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timings contradict the reference: it places the anchor statement and graphic much later and with different durations, including incorrect start/finish relations, so it is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male anchor says 'Let's actually go to the courtroom now live', when does the judge begin speaking?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.7,
        "end": 204.9
      },
      "pred_interval": {
        "start": 190.0,
        "end": 192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.699999999999989,
        "end": 12.900000000000006,
        "average": 13.299999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.829599142074585,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps (judge at 190.0s vs correct 203.7s and wrong end time) and incorrect timing of the anchor event, so it fails to match the factual timing despite preserving the order of events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking if there is anything from the state, when does the state reply?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.02,
        "end": 151.03
      },
      "pred_interval": {
        "start": 248.5,
        "end": 249.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.47999999999999,
        "end": 98.16999999999999,
        "average": 97.82499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7495194673538208,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the ordering and relation ('immediately after') but the timestamps are substantially incorrect (248s vs ~151s) and it adds an unverified reply ('No'), so it fails on factual alignment and includes hallucinated detail."
      }
    },
    {
      "question_id": "002",
      "question": "After the female reporter states the jury deliberated for just over two hours, when does the male reporter comment on the quick verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.45,
        "end": 152.5
      },
      "pred_interval": {
        "start": 309.5,
        "end": 311.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 157.05,
        "end": 158.5,
        "average": 157.775
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.5681641101837158,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the male comment follows the female reporter, but the timestamps are significantly different from the ground truth and it incorrectly labels the relation as 'immediately after' despite the ground truth noting intervening discussion."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes asking if the jury reached a verdict on all counts, when does the jury foreman respond?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.2,
        "end": 153.2
      },
      "pred_interval": {
        "start": 351.0,
        "end": 351.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 197.8,
        "end": 198.60000000000002,
        "average": 198.20000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.21875000000000003,
        "text_similarity": 0.669672966003418,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence and relation (foreman responds immediately after the judge and even quotes the reply), but the temporal localization is significantly wrong (350.7/351.0s vs correct 153.16/153.2s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the foreperson states that the jury reached a verdict, when does the court staff receive the verdict folder?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.2,
        "end": 357.9
      },
      "pred_interval": {
        "start": 340.0,
        "end": 344.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.19999999999999,
        "end": 13.899999999999977,
        "average": 15.549999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.41463414634146334,
        "text_similarity": 0.6837942600250244,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but misstates key facts: the event timings differ significantly from the reference and it misattributes the instruction (saying the foreperson gives the folder to 'Randy' rather than the judge instructing court staff to take the folder)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes reading the verdict for Count 1, when does he begin reading the verdict for Count 2?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.7,
        "end": 445.2
      },
      "pred_interval": {
        "start": 370.0,
        "end": 374.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.69999999999999,
        "end": 71.19999999999999,
        "average": 71.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.7810544371604919,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same events but gives timestamps that are substantially incorrect (~370s vs 441.7s) and states the relation as 'after' with a 4s gap, contradicting the ground truth that Count 2 begins immediately at 441.7s. This major factual and temporal mismatch warrants a very low score."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge finishes reading the verdict for Count 8, when does he state that 'not guilty' verdict forms were also returned?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 630.9,
        "end": 641.0
      },
      "pred_interval": {
        "start": 500.0,
        "end": 504.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 130.89999999999998,
        "end": 137.0,
        "average": 133.95
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837834,
        "text_similarity": 0.7879716157913208,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the 'after' relation, its timestamps for both events are substantially incorrect (\u2248500\u2013504s vs. the correct \u2248629\u2013641s), contradicting key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge finishes reading the final guilty verdict for count 8, when does he begin to inquire if the jury members agreed with the verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 528.9,
        "end": 619.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 550.0
      },
      "iou": 0.19357798165137635,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.899999999999977,
        "end": 69.0,
        "average": 43.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.6663026809692383,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the inquiry follows the verdict reading (right order) but omits all numeric timestamps and wrongly claims it occurs 'immediately after'\u2014the reference shows a ~16s gap (start at 528.9s) and provides an end time (619.0s), so key temporal details are missing or inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the last juror confirms their agreement to the verdicts, when does the judge begin his speech thanking the jury for their service?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 621.0,
        "end": 665.0
      },
      "pred_interval": {
        "start": 580.0,
        "end": 600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 65.0,
        "average": 53.0
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.6316529512405396,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and their temporal relation ('after') but fails to provide the required precise timestamps (start at 621.0s, end at 665.0s) and thus is incomplete. "
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes instructing the defendant's table to be seated, when does Attorney Brown make his motion?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 741.0
      },
      "pred_interval": {
        "start": 660.0,
        "end": 680.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.0,
        "end": 61.0,
        "average": 69.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4074074074074074,
        "text_similarity": 0.7278945446014404,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the temporal relation ('after'), but it omits the key quantitative details in the reference (the specific start and end times 737.0s\u2013741.0s and the anchor time 732.0s), making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking about any pre-sentence request, when does Attorney Brown state that a pre-sentence investigation should be performed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 698.0
      },
      "iou": 0.3125,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 0.5,
        "average": 2.75
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.7234479188919067,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer contradicts the key temporal detail by placing Attorney Brown's start at 690.0s (before the judge finishes at 694.2s) whereas the correct answer states the response begins immediately after (695.0s); while the relationship and end time are roughly similar, the critical start-time alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge orders a pre-sentence investigation, when does he specify that there should not be any recommendations made within the report?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.6,
        "end": 754.5
      },
      "pred_interval": {
        "start": 720.0,
        "end": 728.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.600000000000023,
        "end": 26.5,
        "average": 28.05000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3103448275862069,
        "text_similarity": 0.715850830078125,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('immediately after') right but fails on key factual elements: it omits the E1 timestamp and gives E2 times (720.0\u2013728.0s) that contradict the correct interval (749.6\u2013754.5s), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the news anchor finishes stating the sentencing hearing dates, when does the District Attorney begin his statement to the media?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 935.0,
        "end": 938.5
      },
      "pred_interval": {
        "start": 788.0,
        "end": 792.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 147.0,
        "end": 146.5,
        "average": 146.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3880597014925374,
        "text_similarity": 0.8276761174201965,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives completely incorrect timestamps (788\u2013792s vs reference 935.0\u2013938.5s), omits the anchor finish time (903.8s), and wrongly asserts an 'immediately after' relation, contradicting the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the system does not work without jurors, when does he mention that the jury of 18 sacrificed a month of their life?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 901.4,
        "end": 908.9
      },
      "pred_interval": {
        "start": 880.0,
        "end": 890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.399999999999977,
        "end": 18.899999999999977,
        "average": 20.149999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.6713693141937256,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the same utterances but the timestamps are significantly off (by ~25+ seconds) and do not match the ground-truth anchor/target intervals or their immediate adjacency; thus the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the District Attorney states that the process had to take a whole week off because of the virus, when does he commend people for sacrificing their time and potentially their safety?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 971.4,
        "end": 982.1
      },
      "pred_interval": {
        "start": 930.0,
        "end": 940.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.39999999999998,
        "end": 42.10000000000002,
        "average": 41.75
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7055385112762451,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly reports both start and end times (off by ~40\u201350s) and thus does not match the ground-truth timestamps, though it correctly implies the target speech follows the anchor. Significant factual timing errors warrant a very low score."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer asks for a message to Barton Krista's family, when does the District Attorney state he will speak to them after?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1027.2,
        "end": 1028.7
      },
      "pred_interval": {
        "start": 1000.0,
        "end": 1010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.200000000000045,
        "end": 18.700000000000045,
        "average": 22.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925378,
        "text_similarity": 0.6261393427848816,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the qualitative relationship ('after') right but the timestamps are substantially incorrect and do not match the provided start/end times, so it fails to accurately report the key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the Sheriff's Department could get the case to trial in short order, when does he talk about the professionalism and integrity of law enforcement?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1088.6,
        "end": 1095.4
      },
      "pred_interval": {
        "start": 1085.0,
        "end": 1095.0
      },
      "iou": 0.6153846153846187,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.599999999999909,
        "end": 0.40000000000009095,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.6210908889770508,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction gets the relation ('after') and roughly matches the target end time, but it mislocates the anchor and target start times (1080.0s vs 1086.1s and 1085.0s vs 1088.6s) and omits the specific quoted phrases, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer asks about the historic significance and challenges of the trial, when does the District Attorney confirm the trial was unprecedented?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.2,
        "end": 1202.0
      },
      "pred_interval": {
        "start": 1135.0,
        "end": 1145.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.20000000000005,
        "end": 57.0,
        "average": 61.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4150943396226415,
        "text_similarity": 0.7276155948638916,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same event (DA confirming the trial was unprecedented) but gives substantially incorrect timestamps and the wrong temporal relation ('after' vs correct 'once_finished' immediate response), so it fails to align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the District Attorney says they may never know 'the why' behind the crime, when does the news anchor cut in to summarize this point?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1358.6,
        "end": 1367.8
      },
      "pred_interval": {
        "start": 1220.0,
        "end": 1230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 138.5999999999999,
        "end": 137.79999999999995,
        "average": 138.19999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.8238646388053894,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives entirely different timestamps, a different relation label ('after' vs 'next'), and omits the District Attorney's subsequent remarks about the family/prosecution; only the notion that the anchor summarizes is retained."
      }
    },
    {
      "question_id": "001",
      "question": "Once the news anchor finishes stating that the system worked and the verdict graphic is fully shown, when does the narrator begin listing the specific guilty verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1265.0,
        "end": 1275.0
      },
      "pred_interval": {
        "start": 1235.0,
        "end": 1245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 30.0,
        "average": 30.0
      },
      "rationale_metrics": {
        "rouge_l": 0.410958904109589,
        "text_similarity": 0.7689368724822998,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the anchor and narrator timestamps differ substantially from the reference (predicted ~25\u201330s earlier), and the relation ('after') does not match the reference 'once_finished' timing sequence; key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After reporter Jaymes Langrehr finishes saying that the sheer volume of evidence proved to be overwhelming for the jurors, when does he explain that DNA analysts proved bloodstains belonged to the parents?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1350.0,
        "end": 1364.0
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1330.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 34.0,
        "average": 32.0
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.6879644393920898,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation 'after' correct but the timestamps are substantially off (E1/E2 differ by 20\u201330s from the reference), so it fails to match the key temporal boundaries of the events."
      }
    },
    {
      "question_id": "003",
      "question": "Once reporter Jaymes Langrehr finishes saying there was no one specific piece of evidence that 'really stuck out', when is the next time he mentions bringing in the DNA analysts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1348.0,
        "end": 1352.0
      },
      "pred_interval": {
        "start": 1335.0,
        "end": 1345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 7.0,
        "average": 10.0
      },
      "rationale_metrics": {
        "rouge_l": 0.30985915492957744,
        "text_similarity": 0.7570185661315918,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies a subsequent mention of DNA analysts but the timestamps differ substantially from the reference (E1/E2 off by ~11\u201313s and E2 end off by ~7s) and the relation 'after' fails to convey the immediate 'next' transition specified; therefore it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After MS. NULAND finishes asking \"What's your response to that?\", when does the Sheriff begin responding about his faith in the team?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.47,
        "end": 1430.395
      },
      "pred_interval": {
        "start": 1415.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.470000000000027,
        "end": 5.394999999999982,
        "average": 8.432500000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2985074626865672,
        "text_similarity": 0.7243654727935791,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gives timestamps that are significantly different from the reference (about 10\u201311s earlier) and even makes E2 start simultaneously with E1 rather than after, so the temporal intervals are incorrect despite matching the 'after' label."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Sheriff finishes explaining what he will remember most about the case, when does a reporter ask about the case informing future work?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1491.452,
        "end": 1494.597
      },
      "pred_interval": {
        "start": 1470.0,
        "end": 1475.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.451999999999998,
        "end": 19.59699999999998,
        "average": 20.52449999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705885,
        "text_similarity": 0.7034590244293213,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event timestamps by ~20s, mislabels the speaker, and uses a different relation ('after' vs. the specified 'once_finished'), so it largely contradicts the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the reporter finishes their commentary thanking Sheriff Calvin Baer and his deputies for their hard work, when does the reporter next mention that \"A lot of resources involved for a very long time\"?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1528.402,
        "end": 1530.927
      },
      "pred_interval": {
        "start": 1505.0,
        "end": 1515.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.402000000000044,
        "end": 15.926999999999907,
        "average": 19.664499999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324323,
        "text_similarity": 0.6929576396942139,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially earlier than the ground truth and the temporal relation is mislabeled ('after' vs. correct 'next'), so the prediction does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the main anchor introduces Tahlil Maudeen, when does Tahlil begin reporting on the emotion in the gallery?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1701.902,
        "end": 1708.327
      },
      "pred_interval": {
        "start": 1602.0,
        "end": 1620.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.90200000000004,
        "end": 88.327,
        "average": 94.11450000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.5185185185185185,
        "text_similarity": 0.7581731081008911,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relation as 'after', its anchor and target timestamps are substantially different from the ground truth (incorrect start/end times), making it factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the main anchor asks Tahlil if she talked to Chandler Halderson's defense attorneys, when does the anchor next ask about whom Tahlil interviewed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1765.828,
        "end": 1767.07
      },
      "pred_interval": {
        "start": 1650.0,
        "end": 1662.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 115.82799999999997,
        "end": 105.06999999999994,
        "average": 110.44899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.41269841269841273,
        "text_similarity": 0.8595027923583984,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction matches the content (anchor asks about defense attorneys then about the DA/prosecutors) but the timestamps are significantly different from the reference and the relation labeling ('after' vs 'next') does not reflect the specified immediate succession, so the alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Tahlil Maudeen reports that the defense attorneys were unavailable for interviews, when does she report that the DA seemed pleased with the verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1769.555,
        "end": 1783.597
      },
      "pred_interval": {
        "start": 1662.0,
        "end": 1675.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 107.55500000000006,
        "end": 108.59699999999998,
        "average": 108.07600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4838709677419355,
        "text_similarity": 0.7864634990692139,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation right but the anchor/target timestamps are substantially and incorrectly shifted (1662s vs correct ~1765\u20131784s), misplacing both segments; only the label 'after' aligns."
      }
    },
    {
      "question_id": "001",
      "question": "After the reporter asks if people were surprised by the quick jury return, when does she explain why they were expecting it?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1789.692,
        "end": 1798.408
      },
      "pred_interval": {
        "start": 1777.0,
        "end": 1785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.692000000000007,
        "end": 13.407999999999902,
        "average": 13.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.841422438621521,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the provided timestamps are substantially off from the ground truth (predicted times are ~9\u201313 seconds earlier than the correct intervals), so the key factual timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host says, 'Thank you all,' when does she begin to introduce the website for more information on the case?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1809.891,
        "end": 1815.742
      },
      "pred_interval": {
        "start": 1795.0,
        "end": 1803.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.891000000000076,
        "end": 12.741999999999962,
        "average": 13.816500000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.8311827182769775,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misaligns the timestamps and durations (off by ~15s for both anchor and target and incorrect end time), so it fails to locate the event accurately; it only correctly notes the target occurs after the anchor and involves the website, which is insufficient."
      }
    },
    {
      "question_id": "003",
      "question": "Once the host says, 'Thanks for joining us,' when does she announce the return to regular programming?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1830.005,
        "end": 1831.628
      },
      "pred_interval": {
        "start": 1845.0,
        "end": 1851.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.99499999999989,
        "end": 19.37200000000007,
        "average": 17.18349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.8036226034164429,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but the timestamps are substantially offset (by ~13\u201320s) and the temporal relation is labeled simply 'after' rather than immediately following as in the reference, so it does not align with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the narrator explains that the judges stopped the video, when does the judge in the embedded video actually stop the video and question the man?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 217.92,
        "end": 221.605
      },
      "pred_interval": {
        "start": 4.0,
        "end": 8.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 213.92,
        "end": 213.605,
        "average": 213.7625
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.7372590899467468,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation 'after' correct but the event annotations are largely incorrect: E1 is misidentified (should be the narrator at ~22s, not 0.0s) and E2 timestamps/content are far off (should be ~218\u2013222s). Therefore only minimal credit for the correct temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if the video is counsel for the case, when does the man reply, 'I generated that'?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 224.77,
        "end": 225.951
      },
      "pred_interval": {
        "start": 8.5,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 216.27,
        "end": 215.951,
        "average": 216.1105
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.6459616422653198,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the man's reply occurs after the judge's question, but it gives incorrect absolute timestamps (different time scale) and adds the qualifier 'immediately after'; thus it misaligns the key temporal anchors and timing details from the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes telling the man he is not going to use the courtroom as a business launch, when does she instruct him to stand up for oral argument time?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 323.425,
        "end": 328.018
      },
      "pred_interval": {
        "start": 100.0,
        "end": 104.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 223.425,
        "end": 224.01799999999997,
        "average": 223.7215
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317074,
        "text_similarity": 0.7193755507469177,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation right and references the correct utterances, but the event timestamps are drastically incorrect (off by ~215\u2013225 seconds), so it fails to match the ground truth timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says \"That is not a real person\", when does the judge say it would have been nice to know that information?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 154.321,
        "end": 158.401
      },
      "pred_interval": {
        "start": 155.0,
        "end": 157.0
      },
      "iou": 0.49019607843137103,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.679000000000002,
        "end": 1.4010000000000105,
        "average": 1.0400000000000063
      },
      "rationale_metrics": {
        "rouge_l": 0.2535211267605634,
        "text_similarity": 0.7219549417495728,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') and gives approximate timestamps close to the ground truth, but the anchor time is ~2.3s early, the anchor end is omitted, and the target end timestamp is about 1.4s earlier than the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes describing the man's past verbal conversations with her staff, when does she say she doesn't appreciate being misled?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 177.031,
        "end": 181.351
      },
      "pred_interval": {
        "start": 170.0,
        "end": 173.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.031000000000006,
        "end": 8.350999999999999,
        "average": 7.6910000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7244840860366821,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives timestamps that place the target (170\u2013173s) inside the anchor rather than after it and thus contradicts the reference timings (anchor 168.061\u2013176.491, target 177.031\u2013181.351). The claimed relationship 'after' is inconsistent with the predicted timestamps, so the timing and ordering are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge states that the man is not going to use the courtroom as a launch for his business, when does she command him to \"shut that off\"?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 200.411,
        "end": 201.981
      },
      "pred_interval": {
        "start": 194.0,
        "end": 197.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.411000000000001,
        "end": 4.9809999999999945,
        "average": 5.695999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.7850516438484192,
        "llm_judge_score": 3,
        "llm_judge_justification": "The anchor timing is roughly correct and the predicted relationship 'after' matches the reference, but the predicted target timestamps are substantially earlier (194\u2013197s vs. 200.411\u2013201.981s), misplacing the event and contradicting the precise timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interrogator asks what kind of objects were used, when does the witness name a toothbrush and shaving utensil?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 150.14,
        "end": 150.22
      },
      "pred_interval": {
        "start": 160.0,
        "end": 168.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.860000000000014,
        "end": 17.78,
        "average": 13.820000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.778479814529419,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relationship ('immediately after') correct but the timestamps and durations are substantially incorrect (correct events occur ~150.12\u2013150.22s, prediction places them at 160.0s and 162.0\u2013168.0s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interrogator asks if the abuser decided to use something besides a toothbrush, when does the witness answer 'Yes'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.11,
        "end": 151.12
      },
      "pred_interval": {
        "start": 190.0,
        "end": 192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.889999999999986,
        "end": 40.879999999999995,
        "average": 39.88499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.8189845681190491,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('immediately after') right but the event timestamps and durations are substantially incorrect (off by ~39s for both events and with an inaccurate target duration), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interrogator asks what his mom said, when does the witness recount his mother's full response?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.06,
        "end": 153.23
      },
      "pred_interval": {
        "start": 300.0,
        "end": 315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 146.94,
        "end": 161.77,
        "average": 154.35500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7823679447174072,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('immediately after'), its timestamps are substantially incorrect and inconsistent with the ground truth (majorly different start/end times and duration), so it fails on key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What did your mom say?\", when does the man respond about his mom telling him to stop and that he was exaggerating?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 340.0
      },
      "pred_interval": {
        "start": 330.5,
        "end": 334.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5,
        "end": 6.0,
        "average": 4.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4242424242424242,
        "text_similarity": 0.8366810083389282,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the content and relation ('after') but the timestamps are significantly incorrect (anchor and target start times are several seconds earlier and the target end is 6s earlier than the ground truth), so it misaligns the event timing and omits the correct completion time."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he was afraid to tell anyone because his dad didn't want him to, when does he reveal his dad said it was 'our secret'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 387.0,
        "end": 389.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 365.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.0,
        "end": 24.0,
        "average": 25.5
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.7896282076835632,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct, but it mislocates both the anchor and target timestamps by a large margin compared to the reference, so the timing information is factually incorrect and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes asking, \"What did you do to your brother?\", when does the man describe taking him to the woods and playing with a toothbrush in the same way?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 427.0,
        "end": 438.0
      },
      "pred_interval": {
        "start": 410.0,
        "end": 420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 18.0,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.7756593227386475,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer significantly misreports the time intervals for both anchor and target (off by ~15\u201318s) and shortens the target duration; while the relation 'after' is roughly similar to 'once_finished', the temporal details are incorrect, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once Lyle Menendez removes his hand from his mouth/face, when does he put his fingers back into his mouth while crying?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 515.8,
        "end": 517.7
      },
      "pred_interval": {
        "start": 515.5,
        "end": 517.0
      },
      "iou": 0.5454545454545549,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.2999999999999545,
        "end": 0.7000000000000455,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.339622641509434,
        "text_similarity": 0.6093906760215759,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction roughly locates the E2 interval but gives a substantially incorrect anchor time (514.0s vs 515.7s) and slightly shifted start/end times, and it fails to reflect that E2 begins immediately after E1. Thus it is only partially correct and imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "While the female voice asks, 'It stopped for you with your dad when you were eight,' when is Erik Menendez first shown on screen?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 536.0,
        "end": 579.0
      },
      "pred_interval": {
        "start": 520.0,
        "end": 522.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 57.0,
        "average": 36.5
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.7174480557441711,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timings are largely incorrect and contradict the reference (Erik is actually first shown at 536.0s), and the predicted durations are wrong; only the temporal relation ('after') coincides but that is insufficient."
      }
    },
    {
      "question_id": "003",
      "question": "After Erik Menendez says 'Yes' in response to doing something about it, when does the female voice ask 'What did you do?'",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 560.0,
        "end": 560.8
      },
      "pred_interval": {
        "start": 570.0,
        "end": 572.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 11.200000000000045,
        "average": 10.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.607191801071167,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the temporal relationship ('after') right but the reported timestamps and durations are substantially off (~10s later and different lengths), so it fails to match the correct events precisely."
      }
    },
    {
      "question_id": "001",
      "question": "After the camera shows Lyle Menendez crying and sniffling, when does the female voice ask about the abuse stopping when he was eight?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 533.5,
        "end": 536.5
      },
      "pred_interval": {
        "start": 516.0,
        "end": 518.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.5,
        "end": 18.0,
        "average": 17.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.6857458353042603,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps conflict with the ground truth: E1 should span 510.0\u2013528.0s and E2 533.5\u2013536.5s, but the prediction places E2 at 516.0\u2013518.5s (inside E1). This misstates the temporal boundaries and thus contradicts the correct relation."
      }
    },
    {
      "question_id": "002",
      "question": "During the female voice asking if he thought it might be happening to someone else, when is Erik Menendez shown with a distressed facial expression?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 539.0,
        "end": 545.8
      },
      "pred_interval": {
        "start": 530.0,
        "end": 532.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 13.799999999999955,
        "average": 11.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.7020519971847534,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the relation label 'during' but the timestamps are substantially wrong: the reference places both events from 539.0\u2013545.8s, while the prediction gives much earlier and shorter times (E1 ~528.0s start, E2 530.0\u2013532.0s) and does not reflect that Erik is visible throughout the entire question period."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female voice finishes asking 'And who did you think it was happening to?', when does Erik Menendez answer 'Eric'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 551.0,
        "end": 551.5
      },
      "pred_interval": {
        "start": 538.0,
        "end": 539.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 12.5,
        "average": 12.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3733333333333333,
        "text_similarity": 0.6052564382553101,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct ordering (E2 occurs after E1) but the absolute timestamps are off by ~11 seconds, the answer timing/duration are incorrect, and the relation 'after' fails to convey the immediate 'once_finished' timing\u2014so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the Presiding Justice asks for the appearance of counsel, when does the appellant's counsel introduce himself?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.401,
        "end": 18.83
      },
      "pred_interval": {
        "start": 5.0,
        "end": 7.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.401,
        "end": 11.829999999999998,
        "average": 9.115499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4166666666666667,
        "text_similarity": 0.7155667543411255,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly preserves the qualitative relation that the appellant speaks after the Presiding Justice, but the timestamps are substantially incorrect and inconsistent (predicting simultaneous/overlapping starts and much earlier end times), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "During the Presiding Justice's question about public importance, when does Mr. Lifrak remain silent and attentive?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.5,
        "end": 103.0
      },
      "pred_interval": {
        "start": 40.0,
        "end": 42.0
      },
      "iou": 0.031496062992125984,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 61.0,
        "average": 30.75
      },
      "rationale_metrics": {
        "rouge_l": 0.34920634920634924,
        "text_similarity": 0.7895216345787048,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels the relationship as 'during' and roughly aligns the start time, but it significantly misstates the time spans\u2014E1's end and E2's duration\u2014and wrongly claims Mr. Lifrak resumes speaking at 42.0s, contradicting the ground truth that he remains silent through 103.0s."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Lifrak finishes asking to reserve time for rebuttal, when does the Presiding Justice grant permission?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 109.412,
        "end": 110.2
      },
      "pred_interval": {
        "start": 50.0,
        "end": 52.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.412000000000006,
        "end": 58.2,
        "average": 58.806000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7770147323608398,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the Presiding Justice grants permission after Mr. Lifrak finishes (relation preserved), but the timestamps and durations are substantially incorrect and contradict the correct answer's precise timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (bottom left) mentions Mr. Hothi injected himself into controversies, when does he state that Mr. Hothi said very publicly on Twitter that Mr. Musk and Tesla were lying?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 196.5,
        "end": 201.5
      },
      "pred_interval": {
        "start": 157.0,
        "end": 162.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.5,
        "end": 39.5,
        "average": 39.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.7474209070205688,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but the timestamps are substantially wrong (predicted ~157\u2013162s vs ground truth 188.6\u2013201.5s) and includes an unverified audio-cue detail, so key factual elements are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker (bottom left) is detailing Mr. Hothi's actions, when does he mention Mr. Hothi hitting an employee with a car?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 283.6,
        "end": 285.5
      },
      "pred_interval": {
        "start": 178.0,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.60000000000002,
        "end": 105.5,
        "average": 105.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2666666666666667,
        "text_similarity": 0.6319407224655151,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target relationship ('during') and the anchor concept, but it gives a drastically incorrect time for the target event (178\u2013180s vs. the correct 283.6\u2013285.5s), a major factual error."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge (top left) finishes expressing her doubt about the relevance of Mr. Hothi's actions, when does the speaker (bottom left) begin his response?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 339.9,
        "end": 350.0
      },
      "pred_interval": {
        "start": 209.0,
        "end": 213.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 130.89999999999998,
        "end": 137.0,
        "average": 133.95
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.8914065361022949,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the speaker responds after the judge, but the timestamps are substantially wrong (predicted ~209s vs reference 338.0\u2013339.9s) and it adds unverified details (E2 end time and an audio-cue) not present in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge (top right) asks what if Mr. Musk made an ad hominem attack on Mr. Hothi, when does the lawyer (bottom left) respond by giving examples of unrelated attacks?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 380.5
      },
      "pred_interval": {
        "start": 344.5,
        "end": 360.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.69999999999999,
        "end": 20.5,
        "average": 25.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.8079552054405212,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the judge's query timing approximately, but it misplaces the lawyer's response (predicts 344.5\u2013360.0s vs correct 374.2\u2013380.5s), incorrectly overlaps it with the anchor and wrongly labels the relation as 'immediately after'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge (top center) states that the lawsuit is about Mr. Hothi almost killing Tesla employees, when does the lawyer (bottom left) clarify Mr. Musk's statements regarding Mr. Hothi harassing employees and almost killing one?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 553.0,
        "end": 561.0
      },
      "pred_interval": {
        "start": 410.0,
        "end": 422.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 143.0,
        "end": 139.0,
        "average": 141.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4411764705882353,
        "text_similarity": 0.8717877864837646,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both anchor and target do not match the reference (409\u2013422s vs 479\u2013483.317s and 553\u2013561s), and the temporal relation is incorrect relative to the ground truth, so the prediction is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the lawyer (bottom left) finishes explaining why 'almost killed' is not a false statement of fact using a sidewalk analogy, when does the judge (top center) ask if that just means the lawyer might win the lawsuit?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 586.8
      },
      "pred_interval": {
        "start": 452.0,
        "end": 458.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 132.0,
        "end": 128.79999999999995,
        "average": 130.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285714,
        "text_similarity": 0.8395527601242065,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the relationship ('immediately after') but gives substantially incorrect timestamps for both the anchor and target (451.0\u2013458.0 vs. the correct 564.9\u2013583.5 and 584.0\u2013586.8), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (bottom left) states the problem is that two people would be debating under different rules because Mr. Hothi is protected by anti-SLAPP, when does he state that the target of Mr. Hothi's speech would not have the same protection?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.405,
        "end": 511.559
      },
      "pred_interval": {
        "start": 547.0,
        "end": 553.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.59500000000003,
        "end": 41.440999999999974,
        "average": 38.518
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.7049349546432495,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relation (target follows anchor) but the timestamps are substantially incorrect and the predicted E2 does not actually start immediately after the predicted E1, contradicting the precise temporal alignment in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom right) begins his turn by saying 'I mean, I think that the difficulty, Mr. Lefebvre, that I have is', when does he explain that the argument is based on Mr. Hothi entering the public sphere?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.597,
        "end": 512.074
      },
      "pred_interval": {
        "start": 553.0,
        "end": 558.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.40300000000002,
        "end": 45.926000000000045,
        "average": 43.66450000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2247191011235955,
        "text_similarity": 0.7856709957122803,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the qualitative relation (immediate elaboration) but the timestamps are substantially incorrect and the target duration is wrongly extended; it does not align with the precise anchor/target timings given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker (bottom right) finishes giving the example of Mr. Musk commenting on Mr. Houthi's methodology and data, when does he question the relevance of 'almost killing someone in the parking lot'?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.302,
        "end": 512.387
      },
      "pred_interval": {
        "start": 563.0,
        "end": 568.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.69799999999998,
        "end": 55.613000000000056,
        "average": 53.15550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962025,
        "text_similarity": 0.7834763526916504,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relational label ('directly follows') but the timestamps are substantially incorrect and the target duration is fabricated, missing the precise anchor/target times given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks why this is less attenuated than in the Wilson case, when does he begin explaining the issue?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 703.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 720.0
      },
      "iou": 0.25,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 16.5,
        "average": 11.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.6670503616333008,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: timestamps for both events differ substantially, the predicted E2 starts simultaneously with E1 rather than after the question ends, and the stated relation ('immediate') conflicts with the correct 'once_finished', so it fails to match the key facts."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that their analysis is relevant to 'what he did', when does he give the example of trespassing?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 764.0,
        "end": 768.7
      },
      "pred_interval": {
        "start": 710.0,
        "end": 730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.0,
        "end": 38.700000000000045,
        "average": 46.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3214285714285714,
        "text_similarity": 0.6466765403747559,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is substantially incorrect: all timestamps differ significantly (E1 and E2 are ~53s earlier), the predicted E2 overlaps/starts with E1 whereas the reference has E2 beginning after E1, and the relation ('immediate') contradicts the correct 'after'."
      }
    },
    {
      "question_id": "003",
      "question": "After the presiding justice turns the floor over to the opponent for replies, when does the opponent begin speaking?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 802.5
      },
      "pred_interval": {
        "start": 780.0,
        "end": 790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 12.5,
        "average": 16.25
      },
      "rationale_metrics": {
        "rouge_l": 0.32142857142857145,
        "text_similarity": 0.8054252862930298,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps contradict the reference (E1 and E2 times differ substantially) and the relation is wrong ('immediate' vs correct 'after'), so it is largely incorrect despite providing time values."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mr. Greenspan finishes saying that the purpose was not to monitor or harass employees, when does he begin explaining that the person was counting cars off the production line?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.67,
        "end": 1058.61
      },
      "pred_interval": {
        "start": 1062.0,
        "end": 1066.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.329999999999927,
        "end": 7.8900000000001,
        "average": 8.110000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7776881456375122,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the semantic relation and content (that E2 follows E1 and is about counting cars) but the timestamps are significantly incorrect (predicted 1062.0s vs correct ~1053s) and the event boundaries do not align, so it receives very low credit."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Greenspan finishes asking if there are any specific questions, when does the Presiding Justice respond by asking the panel if they have questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.969,
        "end": 1135.213
      },
      "pred_interval": {
        "start": 1093.0,
        "end": 1095.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.96900000000005,
        "end": 40.212999999999965,
        "average": 39.59100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6806979775428772,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the relational idea (response immediately after) but the timestamps and event boundaries are incorrect and inconsistent with the ground truth (predicted events at 1093.0s vs correct ~1120\u20131135s) and it omits the E1 start and E2 end times."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Liffrec finishes explaining that the Nidal case was about a university building something in a public park and that was the public debate, when does he state that the university then made statements about protesters inciting violence?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1159.099,
        "end": 1164.925
      },
      "pred_interval": {
        "start": 1180.0,
        "end": 1183.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.901000000000067,
        "end": 18.075000000000045,
        "average": 19.488000000000056
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925375,
        "text_similarity": 0.7702905535697937,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('immediately after'/once_finished) but the provided timestamps are significantly different from the reference (predicted 1180.0s vs reference ~1159.09\u20131164.93s), so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's discussion about harassment, when does he mention \"extensive evidence of harassment\"?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1240.5,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 1235.0,
        "end": 1245.0
      },
      "iou": 0.15,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5,
        "end": 3.0,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3728813559322034,
        "text_similarity": 0.8166250586509705,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'during' relationship, but the timestamp boundaries are inaccurate (E1 start and both E2 bounds differ from reference) and the predicted E1 end is omitted, so key temporal details are incorrect or missing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom left) concludes his argument, when does the Presiding Justice (center top) begin asking if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1305.0,
        "end": 1307.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.215999999999894,
        "end": 7.770999999999958,
        "average": 8.493499999999926
      },
      "rationale_metrics": {
        "rouge_l": 0.2985074626865672,
        "text_similarity": 0.7460702657699585,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relationship ('after') but the reported timestamps are substantially off (\u22487\u20139 seconds later) compared to the reference, so it is factually inaccurate on the key timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the Justice (top right) states that the Nadel case came out in 1994, when does he ask if the court would have considered things the same way with the Filmon Supreme Court's decision?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1310.105,
        "end": 1318.758
      },
      "pred_interval": {
        "start": 1323.0,
        "end": 1326.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.894999999999982,
        "end": 7.241999999999962,
        "average": 10.068499999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.672263503074646,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('after') between the two events, but the provided timestamps are substantially different from the ground truth (off by ~10\u201318 seconds), so the key factual timing details are inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'And with that, I'd submit', when does the Presiding Justice ask if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1299.5,
        "end": 1301.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.7159999999998945,
        "end": 1.7709999999999582,
        "average": 2.7434999999999263
      },
      "rationale_metrics": {
        "rouge_l": 0.4054054054054054,
        "text_similarity": 0.7561190128326416,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the general ordering (Presiding Justice speaks after the speaker) but the timestamps are substantially different from the reference (off by ~4\u20135s) and the relation label ('after') is less precise than 'once_finished', so it does not match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the Presiding Justice asks if there are any other questions, when does Justice Sanchez start speaking about the Nadel case?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1300.609,
        "end": 1302.492
      },
      "pred_interval": {
        "start": 1302.0,
        "end": 1303.5
      },
      "iou": 0.17018332756829777,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3910000000000764,
        "end": 1.0080000000000382,
        "average": 1.1995000000000573
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7681235671043396,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and gives start/end times for the events, but the timestamps are off by about 1\u20132 seconds from the ground truth and it omits the brief 'No' utterance noted in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After Senator Cruz asks if the same principle applies to other protected characteristics, when does he ask if he could decide he was an Asian man?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 14.506,
        "end": 16.388
      },
      "pred_interval": {
        "start": 7.3,
        "end": 14.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.206,
        "end": 2.1880000000000024,
        "average": 4.697000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7228348255157471,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mislocates both events\u2014E2 is placed much earlier (7.3\u201314.2s) instead of 14.506\u201316.388s\u2014and the relation ('immediately after') is misleading; only the general 'after' direction is preserved, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Judge Jackson finishes explaining that she is not able to answer hypotheticals, when does Senator Cruz interrupt to re-ask his question about assessing standing?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 30.6,
        "end": 32.439
      },
      "pred_interval": {
        "start": 31.5,
        "end": 34.0
      },
      "iou": 0.2761764705882354,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8999999999999986,
        "end": 1.561,
        "average": 1.2304999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164376,
        "text_similarity": 0.8195469379425049,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the same events and relationship (Cruz immediately following Jackson) and even matches the quoted phrase, but the timestamps are slightly later than the ground truth (E1 +0.3s, E2 start +0.9s, E2 end +1.56s), so it is mostly accurate with minor timing discrepancies."
      }
    },
    {
      "question_id": "003",
      "question": "While Judge Jackson is explaining how she would assess standing, when does she state that she would 'consider the relevant precedents'?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 45.0,
        "end": 47.8
      },
      "pred_interval": {
        "start": 39.0,
        "end": 42.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 5.799999999999997,
        "average": 5.899999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7359595894813538,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation ('during') and roughly locates the explanation, but both time spans are inaccurate\u2014E1 is ~2.4s early and E2 is substantially misaligned (about 6s early), so it fails to mark the correct interval for 'consider the relevant precedents'."
      }
    },
    {
      "question_id": "001",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain why that detective asked her to do it?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 37.335,
        "end": 44.121
      },
      "pred_interval": {
        "start": 21.0,
        "end": 29.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.335,
        "end": 15.121000000000002,
        "average": 15.728000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.6731209754943848,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction fails to match the correct temporal boundaries: it gives no E1 timestamps and places E2 at 21.0\u201329.0s instead of 37.335\u201344.121s, which contradicts the reference, although it correctly labels the relation as 'after'."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mickey Haller finishes stating that the witness pointed to Detective Lee Langford, when does Langford begin his angry outburst?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 66.887,
        "end": 72.795
      },
      "pred_interval": {
        "start": 64.0,
        "end": 66.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8870000000000005,
        "end": 6.795000000000002,
        "average": 4.841000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.6342774033546448,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect event times (E2 at 64.0\u201366.0s vs correct 66.887\u201372.795s) and omits the anchor end time (66.867s), which contradicts the stated 'immediately after' relation despite matching its label."
      }
    },
    {
      "question_id": "003",
      "question": "After Detective Lee Langford's angry outburst about the witness, when does the judge declare the court in recess?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 82.826,
        "end": 85.59
      },
      "pred_interval": {
        "start": 82.0,
        "end": 89.0
      },
      "iou": 0.3948571428571443,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8259999999999934,
        "end": 3.4099999999999966,
        "average": 2.117999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.8168448209762573,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relation ('after') and identifies the judge's intervention around the correct start, but it omits the precise E1 timestamp and gives a notably incorrect E2 end time (89.0s vs 85.59s), so it is only an approximate match."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mickey Haller finishes asking Detective Pettis if she was in the Bonaventure Hotel on October 20th, when does Detective Pettis answer?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 16.239,
        "end": 16.76
      },
      "pred_interval": {
        "start": 9.5,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.739000000000001,
        "end": 6.760000000000002,
        "average": 6.749500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.45614035087719296,
        "text_similarity": 0.7681779861450195,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer matches the qualitative relation ('immediately after'), its timestamps are substantially wrong (anchor 9.5s vs 16.219s; target 10.0s vs 16.239\u201316.76s) and it misstates the target timing/duration, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain her personal motivation for complying?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 46.707,
        "end": 55.417
      },
      "pred_interval": {
        "start": 31.0,
        "end": 33.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.707,
        "end": 22.417,
        "average": 19.062
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.8531960248947144,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction preserves the 'after' relation, both timestamps\u2014particularly the target (predicted 31.0\u201333.0s vs. true 46.707\u201355.417s)\u2014are substantially incorrect, missing the correct interval."
      }
    },
    {
      "question_id": "003",
      "question": "After Mickey Haller asks if the man Detective Pettis made a deal with is in the courtroom, when does she point him out?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 61.92,
        "end": 62.701
      },
      "pred_interval": {
        "start": 46.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.920000000000002,
        "end": 15.701,
        "average": 15.810500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7936508655548096,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted anchor and target timestamps (45\u201347s) conflict with the ground truth (anchor ends at 57.561s; target occurs 61.92\u201362.701s), and the predicted ordering/relationship is incorrect, so the prediction is fundamentally wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chaturved introduces Mr. Uday Hula, when does he state that Mr. Uday Hula has become a popular name pan India and beyond?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.33,
        "end": 43.1
      },
      "pred_interval": {
        "start": 56.0,
        "end": 60.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.670000000000002,
        "end": 16.9,
        "average": 15.785
      },
      "rationale_metrics": {
        "rouge_l": 0.3939393939393939,
        "text_similarity": 0.744029700756073,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the semantic relation and content right (the mention occurs after the introduction) but the timestamps are incorrect/missing: E1 should be at 19.992s and E2 at 41.33\u201343.1s, whereas the prediction gives E2 as 56.0\u201360.0s and omits the precise E1 time."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Vikas Chaturved says 'Over to you Mr. Trikram', when does Mr. Trikram start speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 153.433,
        "end": 154.776
      },
      "pred_interval": {
        "start": 130.0,
        "end": 132.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.432999999999993,
        "end": 22.77600000000001,
        "average": 23.1045
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.7051792144775391,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has substantially incorrect timestamps and duration (off by ~24s versus the ground truth) and thus fails on key factual elements; although the relation 'after' is loosely similar to 'once_finished', the timing and end-time details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Mr. Trikram finishes his welcome, when does Mr. Uday Holla begin his speech?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "pred_interval": {
        "start": 138.0,
        "end": 140.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.0,
        "end": 32.0,
        "average": 31.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4313725490196078,
        "text_similarity": 0.7774043083190918,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives times that substantially contradict the ground truth (138s vs 147.2s for E1 and 140s vs 169\u2013172s for E2), so while the stated relation 'after' matches, the key temporal facts are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker emphasizes that facts are important, when does he state that a lawyer must have the patience of a crane?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 354.8
      },
      "pred_interval": {
        "start": 340.0,
        "end": 350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.0,
        "end": 4.800000000000011,
        "average": 8.400000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.2769230769230769,
        "text_similarity": 0.6962640285491943,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies both events and that E2 occurs after E1, but the provided timestamps are substantially off from the ground truth (differences of ~12\u201315s), so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that appearing for government becomes more difficult, when does he recount his personal experience as Advocate General?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.1,
        "end": 412.9
      },
      "pred_interval": {
        "start": 400.0,
        "end": 420.0
      },
      "iou": 0.4399999999999977,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.100000000000023,
        "end": 7.100000000000023,
        "average": 5.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.6173934936523438,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and that E2 occurs after E1, but the timestamp for E1 is substantially earlier than the ground truth and E2's interval is overly extended, so the timing is imprecise."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a lawyer must know the law to draft better, when does he offer an illustration?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 504.9,
        "end": 506.3
      },
      "pred_interval": {
        "start": 490.0,
        "end": 510.0
      },
      "iou": 0.0700000000000017,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.899999999999977,
        "end": 3.6999999999999886,
        "average": 9.299999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.7116605043411255,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events, that the second is an illustration and that it occurs after the first, but the provided timestamps are substantially inaccurate (E1 and E2 start/end times differ notably from the reference) and the predicted E2 duration is overstated."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that there has been an amendment, when does he state that everment is taken out?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.3,
        "end": 532.0
      },
      "pred_interval": {
        "start": 512.5,
        "end": 515.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.799999999999955,
        "end": 16.799999999999955,
        "average": 16.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.253968253968254,
        "text_similarity": 0.6981515884399414,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both the anchor and target events are incorrect and do not match the reference; the temporal relation is only vaguely correct ('after' vs 'immediately follows') but the major factual errors in timing warrant a very low score."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that readiness and willingness was mandatory prior to 2018, when does he explicitly state that the suit was liable to be dismissed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.568,
        "end": 583.193
      },
      "pred_interval": {
        "start": 523.0,
        "end": 527.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.567999999999984,
        "end": 56.192999999999984,
        "average": 56.380499999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.7387251257896423,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are substantially wrong\u2014the anchor and target times given (518s/523\u2013527s) do not match the ground truth (anchor 533.4\u2013553.9s; target 579.568\u2013583.193s), so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the balance of the purchase amount, when does he explain the necessity for the plaintiff to prove they had the necessary funds?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 634.565,
        "end": 644.056
      },
      "pred_interval": {
        "start": 570.0,
        "end": 575.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.56500000000005,
        "end": 69.05600000000004,
        "average": 66.81050000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.15151515151515152,
        "text_similarity": 0.7041051387786865,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target follows the anchor and paraphrases the content, but it misstates both event timestamps (off by ~60s) and shortens the target duration, so the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a well-drafted plaint helps portray personality to the judge, when does he begin talking about the second benefit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.9,
        "end": 708.7
      },
      "pred_interval": {
        "start": 690.0,
        "end": 698.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.899999999999977,
        "end": 10.700000000000045,
        "average": 10.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.4155844155844156,
        "text_similarity": 0.8061737418174744,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the general transition to the second benefit but gives incorrect timestamps and misaligns the anchor/target boundaries (698.0s vs correct 700.9s for the start), so the key factual timing is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a civil case lingers for a number of years, when does he explain that one becomes a senior by the time the case comes up for evidence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 719.5,
        "end": 724.9
      },
      "pred_interval": {
        "start": 705.0,
        "end": 712.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.5,
        "end": 12.899999999999977,
        "average": 13.699999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.729066014289856,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the semantic consequence ('become virtually a senior') but the timestamps are incorrect and misaligned with the reference (predicted 705s/712s vs correct 714.0\u2013716.5s and 719.5\u2013724.9s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing client satisfaction with plaint length and associated fees, when does he introduce the strategy of putting facts at the beginning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 795.264,
        "end": 805.511
      },
      "pred_interval": {
        "start": 740.0,
        "end": 748.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.26400000000001,
        "end": 57.51099999999997,
        "average": 56.38749999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.6653594970703125,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the strategy is introduced immediately after the prior discussion, but it gives substantially incorrect timestamps (740/748s vs correct 794.0/795.264s) and omits the specific concluding phrase about 'the fee that you ask or quote,' so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Supreme Court, when does he state the specific paragraph number of the judgment?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 925.404,
        "end": 950.0
      },
      "pred_interval": {
        "start": 885.0,
        "end": 890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.403999999999996,
        "end": 60.0,
        "average": 50.202
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.8546007871627808,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relational label correct (target after anchor) but both anchor and target timestamps are substantially incorrect compared to the ground truth, so it contains major factual errors in timing."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing Justice Sanjay Kishan Kaul's judgment, when does he mention the 'Ren and Martin principle of pressie writing'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.581,
        "end": 990.021
      },
      "pred_interval": {
        "start": 895.0,
        "end": 910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 91.58100000000002,
        "end": 80.02099999999996,
        "average": 85.80099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.8525059819221497,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relative order (the mention occurs after the anchor), but both the anchor and target timestamps are off by around 80\u201390 seconds compared to the reference, so the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using 'short paragraphs, short sentences, direct sentences' in a plaint, when does he warn that a judge will lose patience with lengthy paragraphs?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1005.669,
        "end": 1012.012
      },
      "pred_interval": {
        "start": 960.0,
        "end": 970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.66899999999998,
        "end": 42.011999999999944,
        "average": 43.84049999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.7574637532234192,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but both event timestamps (and the anchor end time) are substantially incorrect compared to the reference, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a civil dispute is always before a civil court, when does he emphasize that the lawyer must be thorough with the civil procedure code?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1077.5,
        "end": 1083.7
      },
      "pred_interval": {
        "start": 1065.0,
        "end": 1072.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.5,
        "end": 11.700000000000045,
        "average": 12.100000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.7167983055114746,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the relationship as 'after', both anchor and target timestamps are substantially incorrect and inconsistent (the predicted target even overlaps the anchor yet is labeled 'after'), so it fails to match the reference timings."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that settlement will not benefit in the short term, when does he explain how it will benefit in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1250.671,
        "end": 1254.734
      },
      "pred_interval": {
        "start": 1225.0,
        "end": 1240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.67100000000005,
        "end": 14.733999999999924,
        "average": 20.202499999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.8112026453018188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (target occurs after the anchor and explains long-term benefits), but the provided timestamps are substantially incorrect compared to the reference, so the answer is largely factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that the entire procedure for civil disputes is in the civil procedure code, when does he mention that states also have civil rules of practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1098.4,
        "end": 1101.7
      },
      "pred_interval": {
        "start": 1072.0,
        "end": 1080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.40000000000009,
        "end": 21.700000000000045,
        "average": 24.050000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7450043559074402,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the anchor and target timestamps/spans are substantially wrong compared to the ground truth, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how settlement will benefit you, when does he state that it will definitely help in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1238.9,
        "end": 1241.9
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1242.0
      },
      "iou": 0.25,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.900000000000091,
        "end": 0.09999999999990905,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.7348836660385132,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the general events and quoted phrasing but has incorrect/missing timing (anchor start at 1230.0 vs 1232.9\u20131235.8 and no anchor end; target starts too early at 1236.0 vs 1238.9) and misstates the relationship, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker first mentions there's a saying in Kannada, when does he explain the saying about winners and losers in court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1273.6,
        "end": 1278.4
      },
      "pred_interval": {
        "start": 1255.0,
        "end": 1274.0
      },
      "iou": 0.017094017094020916,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.59999999999991,
        "end": 4.400000000000091,
        "average": 11.5
      },
      "rationale_metrics": {
        "rouge_l": 0.15625,
        "text_similarity": 0.6820514798164368,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but both anchor and target timestamps are substantially off (anchor ~19s early, target start ~18s early and end ~4s short) and it omits the detail that the target follows a repeated mention, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker begins discussing the drafting of the plaint, when does he advise to draft it in a professional manner?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1319.385,
        "end": 1344.757
      },
      "pred_interval": {
        "start": 1305.0,
        "end": 1315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.384999999999991,
        "end": 29.757000000000062,
        "average": 22.071000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.8409479856491089,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the qualitative relationship and advice but the timestamps for both E1 and E2 are substantially incorrect and do not align with the reference intervals, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states 'You have order two, rule two, which specifies that', when does he elaborate on what it specifies?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.147,
        "end": 1451.484
      },
      "pred_interval": {
        "start": 1422.0,
        "end": 1431.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.146999999999935,
        "end": 20.483999999999924,
        "average": 18.81549999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.6934404373168945,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the high-level relation ('after') but the anchor/target timestamps and durations differ substantially from the ground truth and it introduces an unsupported quote, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'life of the lawyer becomes easier', when does he list Order Six, Seven, and Eight?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1460.096,
        "end": 1410.578
      },
      "pred_interval": {
        "start": 1434.0,
        "end": 1440.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.096000000000004,
        "end": 29.422000000000025,
        "average": 27.759000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.38805970149253727,
        "text_similarity": 0.7193850874900818,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially earlier and inconsistent with the ground truth (off by ~27s), so the timing and event alignment do not match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Then comes the stage of evidence', when does he explain that a lawyer must sit with clients to understand what kind of evidence to lead?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1553.662,
        "end": 1566.557
      },
      "pred_interval": {
        "start": 1470.0,
        "end": 1490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 83.66200000000003,
        "end": 76.55700000000002,
        "average": 80.10950000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.6924247741699219,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordinal relationship ('after') correct but the anchor and target timestamps are substantially wrong (both E1 and E2 are off by ~50\u2013100s and spans misaligned), so it fails factual alignment with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the written statement, when does he begin talking about Order 8?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.811,
        "end": 1624.02
      },
      "pred_interval": {
        "start": 1595.0,
        "end": 1599.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.810999999999922,
        "end": 25.019999999999982,
        "average": 22.915499999999952
      },
      "rationale_metrics": {
        "rouge_l": 0.37288135593220334,
        "text_similarity": 0.7734396457672119,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the timestamps for both events are substantially and incorrectly shifted (off by ~10\u201320s), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a general denial is not sufficient, when does he mention the mistake lawyers normally commit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1651.02,
        "end": 1671.11
      },
      "pred_interval": {
        "start": 1603.0,
        "end": 1607.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.01999999999998,
        "end": 64.1099999999999,
        "average": 56.06499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.4166666666666667,
        "text_similarity": 0.7194586396217346,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the temporal relation as 'after', it significantly mislocates both events and their durations (predicted ~1601\u20131607s vs. actual 1650.5\u20131671.11s), so the timestamps are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that a lawyer must know the law, when does he explain the specific areas a lawyer should be thorough with?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.607,
        "end": 1763.816
      },
      "pred_interval": {
        "start": 1646.0,
        "end": 1651.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 112.60699999999997,
        "end": 112.81600000000003,
        "average": 112.7115
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.6674182415008545,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps for both E1 and E2 deviate substantially from the ground truth, so the predicted spans do not match the referenced segments."
      }
    },
    {
      "question_id": "001",
      "question": "When is the next time the speaker mentions an 'Order six, Rule' after mentioning 'Order six, Rule four'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.2,
        "end": 1830.9
      },
      "pred_interval": {
        "start": 1788.0,
        "end": 1793.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.200000000000045,
        "end": 37.90000000000009,
        "average": 38.55000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.27692307692307694,
        "text_similarity": 0.7714917659759521,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the next mentioned rule as 'Order 6, Rule 8', but the timestamps are substantially incorrect (E1 start is slightly early and E2 is ~40s earlier than the ground truth), so it does not accurately match the annotated events."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker explains that fraud and undue influence require specific pleas, when does he state that a general plea is not sufficient?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1802.1,
        "end": 1806.5
      },
      "pred_interval": {
        "start": 1779.0,
        "end": 1785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.09999999999991,
        "end": 21.5,
        "average": 22.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540983,
        "text_similarity": 0.6917595863342285,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct semantic events ('fraud and undue influence' and 'general plea is not sufficient') and the relation, but the timestamps are substantially incorrect and do not align with the ground-truth intervals (E1 and E2 are misplaced and much earlier), so it fails to match the reference temporally."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using short sentences and small paragraphs in a written statement, when does he transition to discussing 'evidence'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1908.4,
        "end": 1914.4
      },
      "pred_interval": {
        "start": 1804.0,
        "end": 1811.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.40000000000009,
        "end": 103.40000000000009,
        "average": 103.90000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.14035087719298248,
        "text_similarity": 0.6439326405525208,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly identifies a topic transition to 'evidence' but the timestamps are wildly incorrect (predicts ~1804\u20131811s vs reference 1908.4\u20131914.4s), so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that leading questions are to be eschewed, when does he advise on how to prepare for examination?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1964.967,
        "end": 1965.937
      },
      "pred_interval": {
        "start": 1960.0,
        "end": 1970.0
      },
      "iou": 0.09699999999997999,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.967000000000098,
        "end": 4.063000000000102,
        "average": 4.5150000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.7030585408210754,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the relation 'after' and the E2 interval broadly covers the true event, but E1 is mislocated (given much earlier and without an end time) and the timestamps are substantially inaccurate, so key factual details are incorrect or imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes how some lawyers approach cross-examination without preparation, when does he explain what a good lawyer always does?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2010.4,
        "end": 2018.651
      },
      "pred_interval": {
        "start": 1985.0,
        "end": 1995.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.40000000000009,
        "end": 23.651000000000067,
        "average": 24.52550000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.7439031600952148,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the general ordering (E2 after E1) but gives substantially incorrect timestamps and a different relation label than the reference; thus it captures only the coarse sequencing and misses key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's explanation of pitfalls when unprepared for cross-examination, when does he mention forgetting to ask relevant questions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2044.393,
        "end": 2049.878
      },
      "pred_interval": {
        "start": 2000.0,
        "end": 2005.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.39300000000003,
        "end": 44.878000000000156,
        "average": 44.63550000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.7152102589607239,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'during' relationship but the timestamps for both the anchor and the mention are significantly incorrect (off by ~40+ seconds), so it fails to accurately locate when the speaker mentions forgetting to ask relevant questions."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that cross-examination is an art by itself, when does he state that watching a good cross-examiner will help enormously?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2187.557,
        "end": 2201.817
      },
      "pred_interval": {
        "start": 2145.0,
        "end": 2150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.55699999999979,
        "end": 51.81700000000001,
        "average": 47.1869999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24719101123595505,
        "text_similarity": 0.8105584979057312,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the basic idea that watching a good cross\u2011examiner helps enormously and that the target follows the anchor, but the timestamps are substantially incorrect and the predicted wording adds details not present in the reference, so it is only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker describes law as a noble profession, when does he mention lawyers dedicating themselves to clients to ensure justice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2222.5,
        "end": 2233.2
      },
      "pred_interval": {
        "start": 2175.0,
        "end": 2185.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.5,
        "end": 48.19999999999982,
        "average": 47.84999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7684850692749023,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic content (mention of dedication to clients) but the timestamps are significantly wrong (~45\u201365 seconds early) and thus fail to match the correct temporal annotations; relationship labeling is imprecise given the misaligned intervals."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that delays are endemic, when does he provide the reason as a call for settlement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2338.838,
        "end": 2346.208
      },
      "pred_interval": {
        "start": 2315.0,
        "end": 2320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.838000000000193,
        "end": 26.208000000000084,
        "average": 25.02300000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.8338802456855774,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an 'after' relationship and that the target is a call for settlement, but the temporal boundaries are substantially incorrect (off by ~20+ seconds) and the predicted quoted wording appears to be hallucinated, so it fails to match the key timing and factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that delays are endemic, when does he ask to ensure settlements occur?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2346.0
      },
      "pred_interval": {
        "start": 2315.0,
        "end": 2325.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.0,
        "end": 21.0,
        "average": 23.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4999999999999999,
        "text_similarity": 0.8579579591751099,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and their 'after' relationship, but the timestamps and durations are substantially and factually incorrect (predicted times are ~20\u201321s earlier and shorter than the ground truth), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker notes it's already 40 minutes, when does he state that he will give some time for questioning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.8,
        "end": 2371.0
      },
      "pred_interval": {
        "start": 2352.0,
        "end": 2360.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.800000000000182,
        "end": 11.0,
        "average": 13.400000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.5454545454545454,
        "text_similarity": 0.8604798913002014,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events and that the target follows the anchor, but the reported timestamps are substantially off from the ground truth (\u224812\u201315s earlier) and the durations do not match, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions postponing because he was busy, when does he thank someone for 'pestering' him to educate the lawyer community?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2391.454,
        "end": 2399.123
      },
      "pred_interval": {
        "start": 2370.0,
        "end": 2380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.45400000000018,
        "end": 19.123000000000047,
        "average": 20.288500000000113
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575343,
        "text_similarity": 0.841766357421875,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events and the relation 'once', but the timestamps are substantially incorrect (off by ~19\u201321s) and the target does not immediately follow the anchor as in the ground truth, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "How long does the speaker talk about the importance of reading entire judgments, not just highlighted portions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2578.041,
        "end": 2584.494
      },
      "pred_interval": {
        "start": 2505.0,
        "end": 2535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.04100000000017,
        "end": 49.49400000000014,
        "average": 61.267500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.36533474922180176,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and relationship contradict the ground truth: the correct interval is 2568.041\u20132578.041 ('during'), whereas the prediction gives 2490.0/2505.0\u20132535.0 and 'after', so it is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes saying he will take questions, when does Mr. Vikas begin speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2614.902,
        "end": 2617.184
      },
      "pred_interval": {
        "start": 2635.0,
        "end": 2640.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.097999999999956,
        "end": 22.815999999999804,
        "average": 21.45699999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7875983119010925,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (anchor before target) but the timestamps are substantially off (E1 and E2 differ by ~20\u201333s from reference) and the end time is incorrect; it also labels the relation as 'after' instead of the direct transition indicated in the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that online law lexicons are not sufficient, when does he directly say 'You must read. A lawyer must read.'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2522.2,
        "end": 2525.3
      },
      "pred_interval": {
        "start": 2495.0,
        "end": 2500.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.199999999999818,
        "end": 25.300000000000182,
        "average": 26.25
      },
      "rationale_metrics": {
        "rouge_l": 0.39473684210526316,
        "text_similarity": 0.7254042625427246,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the target phrase and the 'after' relationship, but the timestamps are significantly off (by ~30 seconds) and do not match the precise ground-truth intervals, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he would fall asleep reading the Civil Procedure Code as it is, when does he explain that he would be enthusiastic if he had a case on hand?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2688.6,
        "end": 2699.0
      },
      "pred_interval": {
        "start": 2710.0,
        "end": 2735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.40000000000009,
        "end": 36.0,
        "average": 28.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.26315789473684204,
        "text_similarity": 0.682247519493103,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two utterances thematically but gives substantially incorrect timestamps, contradicts the correct 'after' relationship by labeling it 'when', and adds an unsupported audio-cue detail; thus it fails on temporal alignment and relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'Specific relief act, what are the sections?', when does he advise to go to the AR manual?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2720.5,
        "end": 2722.3
      },
      "pred_interval": {
        "start": 2760.0,
        "end": 2775.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.5,
        "end": 52.69999999999982,
        "average": 46.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.4050632911392405,
        "text_similarity": 0.8078790307044983,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer preserves the correct 'after' relationship and the instruction to consult the AR manual, but the timestamps for both E1 and E2 are significantly incorrect and the predicted long E2 duration and added 'audio cue' are unsupported/hallucinated, so it fails key factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions he suffered from COVID, when does he start describing a doctor's experience during the peak of COVID?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.119,
        "end": 2850.7
      },
      "pred_interval": {
        "start": 2820.0,
        "end": 2840.0
      },
      "iou": 0.4589155824786065,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.880999999999858,
        "end": 10.699999999999818,
        "average": 11.790499999999838
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.6976320743560791,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the high-level relation ('after') and mentions the doctor's narrative, but both event timestamps and boundaries are substantially incorrect compared to the reference and it adds an unsupported audio cue, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once Vikas Chatrath finishes asking how to structure arguments, when does Udaya Holla begin to explain what one must always remember?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.46,
        "end": 2963.1
      },
      "pred_interval": {
        "start": 2850.5,
        "end": 2852.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.96000000000004,
        "end": 111.09999999999991,
        "average": 88.52999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.6339176893234253,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the immediate relationship and the quoted phrase, but it gives timestamps that differ substantially from the ground truth (2850s vs 2914.7s anchor and 2850.5s vs 2916.46s target), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Udaya Holla suggests preparing a list of dates and synopsis, when does he mention that the High Court has adopted this practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2941.0,
        "end": 2942.8
      },
      "pred_interval": {
        "start": 2863.5,
        "end": 2865.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.5,
        "end": 77.80000000000018,
        "average": 77.65000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.7700501680374146,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation as 'after', but it gives incorrect and contradictory timestamps for both anchor and target compared to the ground truth, omitting the correct anchor finish and wrong target timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes asking how to specifically plead in a summary suit, when does Udaya Holla ask for clarification 'In a summary suit, is it?'",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2999.596,
        "end": 3000.717
      },
      "pred_interval": {
        "start": 2895.5,
        "end": 2897.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.096,
        "end": 103.7170000000001,
        "average": 103.90650000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.360655737704918,
        "text_similarity": 0.7156699299812317,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the interaction as an immediate clarification but gives substantially incorrect timestamps (off by ~104 seconds) and omits the correct E2 end time, so it is factually inaccurate despite matching the relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes lawyers arguing endlessly for hours on end, when does he state that the judge then sleeps?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3046.2,
        "end": 3047.7
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3044.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.199999999999818,
        "end": 3.199999999999818,
        "average": 9.699999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7734376788139343,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction retains the general topic (lawyers arguing and a judge) but is largely incorrect: timestamps differ significantly, the target action is mischaracterized as 'the judge leaves' rather than 'the judge sleeps', and the temporal ordering/timings do not match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker confirms the order number as '37, correct. Sorry.', when does the other speaker (Vikas Chaprath) begin to introduce a common question?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3157.242,
        "end": 3163.028
      },
      "pred_interval": {
        "start": 3120.0,
        "end": 3128.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.24200000000019,
        "end": 35.02799999999979,
        "average": 36.13499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.7238205075263977,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps are significantly shifted (~32 seconds earlier) and event boundaries do not match the ground truth, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that preliminary objections concerning maintainability must appear in the beginning of a written statement, when does he begin describing how lawyers often present their defense?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3301.7,
        "end": 3309.9
      },
      "pred_interval": {
        "start": 3180.0,
        "end": 3195.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 121.69999999999982,
        "end": 114.90000000000009,
        "average": 118.29999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.8030670881271362,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target follows the anchor, but the timestamps are substantially incorrect (off by ~100+ seconds) and do not match the reference event intervals, so key factual elements are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker recommends setting out your own facts in the written statement, when does he mention preliminary objections?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3223.3,
        "end": 3224.548
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3228.0
      },
      "iou": 0.15599999999994907,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.300000000000182,
        "end": 3.4520000000002256,
        "average": 3.3760000000002037
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.6714300513267517,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the time annotations are noticeably off (E1 start and E2 start/end differ from reference) and the predicted answer omits E1's end time, reducing accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions misjoinder or non-joinder of parties, when does he list territorial lack of jurisdiction as an objection?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3254.547,
        "end": 3258.914
      },
      "pred_interval": {
        "start": 3238.0,
        "end": 3244.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.547000000000025,
        "end": 14.914000000000215,
        "average": 15.73050000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.6876680850982666,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same topics but gives substantially different timestamps and a different relation label; the temporal alignment is incorrect and does not match the reference intervals."
      }
    },
    {
      "question_id": "003",
      "question": "After Vikas finishes asking his question about the Advocate General's experience, when does Udaya Holla begin to respond about the State Council's disabilities?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3417.766,
        "end": 3429.231
      },
      "pred_interval": {
        "start": 3345.0,
        "end": 3352.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.76600000000008,
        "end": 77.23100000000022,
        "average": 74.99850000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.4262295081967213,
        "text_similarity": 0.7665785551071167,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relation 'after' is correct, the predicted timestamps are substantially inaccurate (off by ~77 seconds) and misreport E1 timing (gives a start time rather than the correct finish at 3417.525s) and E2 start/end times, so it fails on factual correctness and completeness."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying the colloquial Kannada phrase, when does he translate it to English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.2,
        "end": 3417.7
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.199999999999818,
        "end": 7.699999999999818,
        "average": 7.949999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.7362951636314392,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation right (translation occurs immediately after the Kannada phrase) but the provided timestamps are substantially off (about 7\u20138 seconds earlier) and durations differ, so the timing facts are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes talking about the difficulties faced by state councils in tough situations, when does the second speaker say 'Vikram'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.82,
        "end": 3472.161
      },
      "pred_interval": {
        "start": 3455.0,
        "end": 3456.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.820000000000164,
        "end": 16.161000000000058,
        "average": 16.49050000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.3666666666666667,
        "text_similarity": 0.7166804075241089,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (second speaker speaks after the first) but the timestamps and durations are substantially incorrect (off by ~15+ seconds) and the relation label differs; thus it is largely factually wrong despite matching the general sequence."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises Kannada medium students to acquire more knowledge of English, when does he suggest joining websites for better English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3527.318,
        "end": 3535.0
      },
      "pred_interval": {
        "start": 3505.0,
        "end": 3510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.31800000000021,
        "end": 25.0,
        "average": 23.659000000000106
      },
      "rationale_metrics": {
        "rouge_l": 0.24,
        "text_similarity": 0.5780710577964783,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the suggestion content and the 'after' relation, but the event timestamps are substantially inaccurate (E1 is placed outside the reference interval and E2 is much earlier and shorter than the ground truth), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker talks about drafting in Kannada, when does he emphasize having mastery over the Kannada language?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3590.3,
        "end": 3592.0
      },
      "pred_interval": {
        "start": 3572.0,
        "end": 3585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.300000000000182,
        "end": 7.0,
        "average": 12.650000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.5128205128205128,
        "text_similarity": 0.9048327803611755,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content and that the target follows the anchor, but the provided timestamps are substantially incorrect and internally inconsistent compared to the reference, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks about scheduling a lawyer's day, when does the first speaker refer to the question as a 'multi-million dollar question'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3696.0,
        "end": 3697.2
      },
      "pred_interval": {
        "start": 3640.0,
        "end": 3645.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.0,
        "end": 52.19999999999982,
        "average": 54.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333326,
        "text_similarity": 0.777664065361023,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (target after the anchor) but the timestamps are substantially incorrect and the temporal relation ('immediately after') mischaracterizes the correct gap; key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises taking books on management, when does he mention keeping one's wife happy at home?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3701.248,
        "end": 3706.6
      },
      "pred_interval": {
        "start": 3675.0,
        "end": 3685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.248000000000047,
        "end": 21.59999999999991,
        "average": 23.923999999999978
      },
      "rationale_metrics": {
        "rouge_l": 0.3294117647058824,
        "text_similarity": 0.8327146768569946,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relative ordering (target follows the anchor) and the content, but the provided start/end timestamps for both anchor and target deviate substantially from the reference, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises lawyers on how to handle client pleas, when does he suggest setting out facts in the client's presence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.2,
        "end": 3750.22
      },
      "pred_interval": {
        "start": 3750.0,
        "end": 3765.0
      },
      "iou": 0.0013333333333321207,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.1999999999998181,
        "end": 14.7800000000002,
        "average": 7.490000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.7473862171173096,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and roughly the anchor point, but it substantially mislocates the target event (predicted 3752.0\u20133765.0s vs correct 3750.2\u20133750.22s), providing incorrect temporal boundaries and extra content."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that a judge would have three drafts of his judgment, when does he elaborate on what the first draft is for?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.51,
        "end": 3750.56
      },
      "pred_interval": {
        "start": 3770.0,
        "end": 3780.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.48999999999978,
        "end": 29.440000000000055,
        "average": 24.464999999999918
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767127,
        "text_similarity": 0.7703974843025208,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the relative ordering ('once' he elaborates after stating three drafts) but the provided timestamps are significantly off and do not match the reference intervals, so the answer is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Palkiwala's initial lack of work, when does he refer to Justice Chawla's memoir?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3903.539,
        "end": 3917.722
      },
      "pred_interval": {
        "start": 3885.0,
        "end": 3900.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.539000000000215,
        "end": 17.722000000000207,
        "average": 18.13050000000021
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.8701661825180054,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps differ substantially from the ground truth (anchor ~32s earlier; target range does not match) and the predicted relationship ('after') contradicts the correct timing where the target spans before and through the anchor, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Make it a habit', when does he mention biting the nail?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3936.731,
        "end": 3942.304
      },
      "pred_interval": {
        "start": 3930.0,
        "end": 3940.0
      },
      "iou": 0.2656859557867161,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.731000000000222,
        "end": 2.3040000000000873,
        "average": 4.517500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.8504518270492554,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction gets the correct ordering ('after') and roughly similar timing, but the timestamps are noticeably inaccurate (anchor start is ~5s early, anchor end is omitted, and target start/end differ by ~1.7\u20132.3s) and it fails to state that the target occurs directly after the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker finishes requesting the first speaker to talk about staying fit, when does the first speaker repeat 'How to stay fit, physically fit, sir'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3986.204,
        "end": 3987.967
      },
      "pred_interval": {
        "start": 3960.0,
        "end": 3970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.20400000000018,
        "end": 17.9670000000001,
        "average": 22.08550000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.7758607864379883,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: the reported timestamps and durations for both anchor and target differ substantially from the ground truth, and the temporal relation is only vaguely correct ('after') without matching the ground-truth immediate-following relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that 'fitness is absolutely essential in any profession', when does he explain that 'our work transcends the entire waking hours of our time'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4056.898,
        "end": 4064.789
      },
      "pred_interval": {
        "start": 4000.0,
        "end": 4010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.89800000000014,
        "end": 54.789000000000215,
        "average": 55.843500000000176
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.7897499203681946,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') right but the timestamps are substantially incorrect (off by ~40\u2013 sixty seconds) and do not match the specified anchor/target intervals, so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the judge smiling for the first time, when does he explain why the judge smiled?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4157.778,
        "end": 4164.121
      },
      "pred_interval": {
        "start": 4128.0,
        "end": 4134.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.778000000000247,
        "end": 30.121000000000095,
        "average": 29.94950000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.25352112676056343,
        "text_similarity": 0.7430280447006226,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the temporal relation (E2 after E1) and identifies the two events, but the timestamps are significantly different from the reference (off by ~28\u201334s) and the predicted end time for E2 does not match the correct completion time, so the answer is factually inaccurate on key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker uses a cricket analogy involving Kumble, when does he advise the audience to 'Go and observe'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4289.867,
        "end": 4291.509
      },
      "pred_interval": {
        "start": 4210.0,
        "end": 4216.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.86700000000019,
        "end": 75.50900000000001,
        "average": 77.6880000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.8688001036643982,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation ('after'), its timestamps are substantially incorrect compared to the ground truth (off by ~68+ seconds), so the key factual details about event timings are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker recommends reading Dale Carnegie's book, when does he mention other books on management techniques?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4206.04,
        "end": 4211.851
      },
      "pred_interval": {
        "start": 4150.0,
        "end": 4155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.039999999999964,
        "end": 56.85099999999966,
        "average": 56.44549999999981
      },
      "rationale_metrics": {
        "rouge_l": 0.5128205128205129,
        "text_similarity": 0.9118593335151672,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relationship ('once') correct, but the anchor and target timestamps are substantially different from the reference (off by ~55\u201360 seconds) and the predicted durations/intervals do not align with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises lawyers not to sit in the canteen, when does he instruct them to sit in a court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4301.616,
        "end": 4305.419
      },
      "pred_interval": {
        "start": 4295.0,
        "end": 4305.0
      },
      "iou": 0.32479124676073107,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.6159999999999854,
        "end": 0.41899999999986903,
        "average": 3.5174999999999272
      },
      "rationale_metrics": {
        "rouge_l": 0.4117647058823529,
        "text_similarity": 0.7063741087913513,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events (advice not to sit in the canteen and instruction to sit in court) but gives substantially incorrect and inconsistent timestamps (missing E1 end, wrong E2 start/end) and thus fails to match the precise temporal alignment in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nitika finishes asking her question on YouTube, when does the speaker ask for the question to be repeated?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4378.889,
        "end": 4380.233
      },
      "pred_interval": {
        "start": 4370.0,
        "end": 4375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.889000000000124,
        "end": 5.233000000000175,
        "average": 7.061000000000149
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.762873649597168,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps significantly mismatch the reference (both E1 and E2 are shifted earlier by many seconds) and fail to capture the correct E2 interval; only the qualitative 'after' relation is roughly consistent, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'Then which are the sections?' related to the Contract Act, when does he begin describing the illustration about a property purchase agreement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4437.734,
        "end": 4450.995
      },
      "pred_interval": {
        "start": 4395.0,
        "end": 4405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.73400000000038,
        "end": 45.99499999999989,
        "average": 44.364500000000135
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7342474460601807,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: timestamps and event boundaries do not match (predicted E2 starts/ends much earlier than the correct 4437.734\u20134450.995s) and it reports E1 start instead of the referenced E1 end, so the temporal relation is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he closes his case, when does he explain why?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4475.036,
        "end": 4480.501
      },
      "pred_interval": {
        "start": 4470.0,
        "end": 4492.0
      },
      "iou": 0.24840909090909752,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.036000000000058,
        "end": 11.498999999999796,
        "average": 8.267499999999927
      },
      "rationale_metrics": {
        "rouge_l": 0.3684210526315789,
        "text_similarity": 0.707820475101471,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the explanation follows the closing statement, but it misstates E1/E2 timings, labels the relation as 'immediately after' despite an actual gap, and introduces incorrect details (balance sheet/funds) instead of the cited bank statement."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises lawyers to practice in trial court, when does he explain the main skill acquired there?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4559.082,
        "end": 4593.185
      },
      "pred_interval": {
        "start": 4515.0,
        "end": 4550.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.082000000000335,
        "end": 43.1850000000004,
        "average": 43.63350000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.34375000000000006,
        "text_similarity": 0.7106101512908936,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the events (trial-court advice and cross-examination) and the 'after' relation, but the provided timestamps/boundaries are substantially inaccurate and inconsistent with the reference, so the alignment is unreliable."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that 'habit would be derived of five words', when does he proceed to list and explain those words?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4629.048,
        "end": 4640.287
      },
      "pred_interval": {
        "start": 4557.0,
        "end": 4580.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.04799999999977,
        "end": 60.28700000000026,
        "average": 66.16750000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3733333333333333,
        "text_similarity": 0.714593768119812,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction recognizes the same events and an immediate-following relation, both E1 and E2 timestamps are substantially wrong (and E2 is placed before the correct E1/E2 intervals), so the timing and relation are largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking about working parallelly in High Court and Trial Court, when does another speaker give an affirmative answer?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4668.869,
        "end": 4673.473
      },
      "pred_interval": {
        "start": 4655.0,
        "end": 4660.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.868999999999687,
        "end": 13.472999999999956,
        "average": 13.670999999999822
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.7116929888725281,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies an anchor and a subsequent affirmative response and the general 'after' relation, but the timestamps are significantly off from the reference and the predicted timing/sequence (not immediately after) contradicts the correct temporal alignment; thus only partial credit is warranted."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains how a youngster can gain practical experience in a smaller trial court office, when does he state this would not be possible in a larger office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4722.411,
        "end": 4727.419
      },
      "pred_interval": {
        "start": 4685.0,
        "end": 4692.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.41100000000006,
        "end": 35.41899999999987,
        "average": 36.414999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.24691358024691357,
        "text_similarity": 0.7951440811157227,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the semantic content and the 'after' relation, but the reported timestamps are substantially incorrect (off by ~45\u201350 seconds) compared to the reference, making the answer largely inaccurate for this time-sensitive task."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes sharing the Japanese quote about a frog in a well, when does another speaker interject with a related Sanskrit saying?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4760.624,
        "end": 4763.847
      },
      "pred_interval": {
        "start": 4720.0,
        "end": 4725.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.623999999999796,
        "end": 38.84699999999975,
        "average": 39.735499999999774
      },
      "rationale_metrics": {
        "rouge_l": 0.31428571428571433,
        "text_similarity": 0.8228955864906311,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and the interjection content, but the provided timestamps are substantially incorrect compared to the reference, which is a major factual error."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks how a new associate creates space in a big office with many existing associates, when does the host add that the guest also has a very big office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4866.42,
        "end": 4872.823
      },
      "pred_interval": {
        "start": 4840.0,
        "end": 4850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.420000000000073,
        "end": 22.82300000000032,
        "average": 24.621500000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.2388059701492537,
        "text_similarity": 0.6006665229797363,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relation ('after') and identifies the correct utterance types, but the timestamps and durations are significantly incorrect and do not match the reference events, so it is largely factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (guest) finishes explaining that a junior lawyer catches the senior's eye by preparing research and synopses, when does the speaker pose a rhetorical question about how a junior lawyer could get attention by arriving late?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4940.902,
        "end": 4951.577
      },
      "pred_interval": {
        "start": 4890.0,
        "end": 4900.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.902000000000044,
        "end": 51.577000000000226,
        "average": 51.239500000000135
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322037,
        "text_similarity": 0.6845062375068665,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the content of E1 but gives completely incorrect timestamps for E2 (4890\u20134900s vs. reference 4940.902\u20134951.577s), which contradicts the temporal relation and renders the answer factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (guest) emphasizes that hard work is the most important thing in the legal profession, when does the speaker begin to describe a scenario where a client observes their lawyer arguing a case diligently?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4985.39,
        "end": 4996.139
      },
      "pred_interval": {
        "start": 4920.0,
        "end": 4940.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.39000000000033,
        "end": 56.139000000000124,
        "average": 60.764500000000226
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7133170962333679,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic content but gives timestamps that conflict with the reference (E2 is placed much earlier at 4920\u20134940s vs correct 4985.39s) and thus the temporal relation is incorrect; major factual timing/alignment errors make this largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker emphasizes that youngsters must do hard work, when does the second speaker agree with the point about hard work?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5024.62,
        "end": 5033.21
      },
      "pred_interval": {
        "start": 5010.0,
        "end": 5022.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.61999999999989,
        "end": 11.210000000000036,
        "average": 12.914999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.34375000000000006,
        "text_similarity": 0.7939258813858032,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relationship as 'after', its timestamps are significantly different from the reference (off by ~14\u201315 seconds) and it adds an unsupported quote, so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks how to master lower court procedures without much practice, when does the first speaker suggest sitting in courts to learn?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5044.49,
        "end": 5051.81
      },
      "pred_interval": {
        "start": 5022.0,
        "end": 5045.0
      },
      "iou": 0.017108352901717926,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.48999999999978,
        "end": 6.8100000000004,
        "average": 14.650000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7581703662872314,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps for both the anchor and target are substantially different from the reference (both start/end times are incorrect); only the relative 'after' relationship matches the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker asks if one should have High Court experience before practicing in the Supreme Court, when does the first speaker suggest that one is better off going to the trial court first?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5126.422,
        "end": 5141.99
      },
      "pred_interval": {
        "start": 5045.0,
        "end": 5085.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.42199999999957,
        "end": 56.98999999999978,
        "average": 69.20599999999968
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.736895740032196,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the reported anchor and target time spans are substantially incorrect (off by ~40\u201380 seconds) and do not match the ground-truth intervals, so the answer is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the difference between 'learn' and 'earn', when does he say, 'So thank you everyone'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5198.8,
        "end": 5199.7
      },
      "pred_interval": {
        "start": 5197.5,
        "end": 5199.0
      },
      "iou": 0.09090909090901575,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.300000000000182,
        "end": 0.6999999999998181,
        "average": 1.0
      },
      "rationale_metrics": {
        "rouge_l": 0.40579710144927533,
        "text_similarity": 0.705173134803772,
        "llm_judge_score": 8,
        "llm_judge_justification": "The predicted answer correctly identifies both events and the immediate-after relation, but the timestamps differ from the reference by up to ~1.3 seconds, so it is mostly correct but not exact."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker thanks 'Mr. Hola' and mentions it's always a pleasure connecting, when does the second speaker say 'Thank you very much'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5219.7,
        "end": 5221.2
      },
      "pred_interval": {
        "start": 5222.0,
        "end": 5223.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.300000000000182,
        "end": 2.300000000000182,
        "average": 2.300000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.4776119402985075,
        "text_similarity": 0.7361065149307251,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the order and the utterance 'Thank you very much', but the timestamps differ by about 1.9\u20132.3 seconds from the reference and the relation is labeled 'immediately after' rather than the simple 'after', so it is semantically right but factually imprecise."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker says 'You are the educator of lawyers', when is the next time the first speaker says 'Thank you'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5224.9,
        "end": 5226.9
      },
      "pred_interval": {
        "start": 5232.0,
        "end": 5233.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.100000000000364,
        "end": 6.600000000000364,
        "average": 6.850000000000364
      },
      "rationale_metrics": {
        "rouge_l": 0.3950617283950617,
        "text_similarity": 0.7792855501174927,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has major timestamp discrepancies (off by ~7\u20138s) and does not match the correct timing; it also fails to acknowledge skipping the intervening second speaker's 'Thank you', so it is largely incorrect despite preserving the general order."
      }
    },
    {
      "question_id": "001",
      "question": "After Trivikram thanks Mr. Vikas Chaturvedi, when does he extend a warm welcome to Mr. Udaya Holla?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.288,
        "end": 166.858
      },
      "pred_interval": {
        "start": 150.0,
        "end": 156.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.288000000000011,
        "end": 10.858000000000004,
        "average": 12.073000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.6883164644241333,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and even states the relationship as 'after', but the provided timestamps are substantially incorrect (E2 is placed much earlier and overlaps with E1) and E1's end time is omitted, contradicting the ground truth ordering and timing."
      }
    },
    {
      "question_id": "002",
      "question": "During his discussion of strategies for a civil dispute, when does Mr. Udaya Holla state that preparation is what counts the most?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.8,
        "end": 254.67
      },
      "pred_interval": {
        "start": 190.0,
        "end": 195.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.80000000000001,
        "end": 59.66999999999999,
        "average": 60.735
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.6014599800109863,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('during') right but the reported timecodes are substantially incorrect\u2014both the anchor (150s vs 219.4s) and the target (190\u2013195s vs 251.8\u2013254.67s) do not match the ground truth, so it fails on factual timing and alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After Vikas Chatrath finishes explaining the difference between 'learn' and 'earn', when does he thank everyone and wish them to stay safe and blessed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5198.086,
        "end": 5203.11
      },
      "pred_interval": {
        "start": 5196.0,
        "end": 5201.0
      },
      "iou": 0.4098452883262861,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.08600000000024,
        "end": 2.1099999999996726,
        "average": 2.0979999999999563
      },
      "rationale_metrics": {
        "rouge_l": 0.37333333333333335,
        "text_similarity": 0.8043837547302246,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the explanation and the thanking/wishes segments and their 'after' relationship; however, the timestamps are slightly off by about 1\u20132 seconds compared to the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "During the announcement of the next session, when does Vikas Chatrath mention Mr. Shingar Murali?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5207.213,
        "end": 5209.213
      },
      "pred_interval": {
        "start": 5208.0,
        "end": 5213.0
      },
      "iou": 0.2096077414894908,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7870000000002619,
        "end": 3.787000000000262,
        "average": 2.287000000000262
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7683107852935791,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the announcement as the anchor and that the mention occurs during it, and the predicted start times are close to the reference; however, the predicted end time (5213.0s) significantly overshoots the ground-truth end (5209.213s) and adds extraneous detail, so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes telling everyone to stay safe and blessed, when does he say it's always a pleasure connecting with Mr. Hola?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5201.609,
        "end": 5204.971
      },
      "pred_interval": {
        "start": 5201.0,
        "end": 5206.0
      },
      "iou": 0.6723999999998341,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6090000000003783,
        "end": 1.029000000000451,
        "average": 0.8190000000004147
      },
      "rationale_metrics": {
        "rouge_l": 0.27692307692307694,
        "text_similarity": 0.7630538940429688,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction captures the overall sequence and relation ('once') and has roughly similar timestamps, but it contains minor timing inaccuracies (start/end off by ~0.1\u20131s) and omits the specific phrase 'and Thrikram and associates' mentioned in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the prosecutor finishes introducing his opening statement, when does he explain why he gets to go first?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.329,
        "end": 50.118
      },
      "pred_interval": {
        "start": 5.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.329,
        "end": 40.118,
        "average": 39.2235
      },
      "rationale_metrics": {
        "rouge_l": 0.1639344262295082,
        "text_similarity": 0.7285122871398926,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and anchor location are completely incorrect (0.0 vs 41.646s for the anchor; 5.0\u201310.0s vs 43.329\u201350.118s for the target). Only the vague 'after' relation is somewhat aligned, but it fails to reflect the precise timing and 'immediately follows' relation, so it's largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the prosecutor mentions the bartender, John Thomas, and the server, Roberta Jones, when does John observe Mr. Miller pass a small glass bottle?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 150.298,
        "end": 158.469
      },
      "pred_interval": {
        "start": 30.0,
        "end": 35.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 120.298,
        "end": 123.469,
        "average": 121.8835
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.6551958918571472,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the timestamps and event spans are largely incorrect compared to the reference (134.772s vs 25.0s and 150.298\u2013158.469s vs 30.0\u201335.0s), so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "Once John observes the unknown man pull a gun on the defendant, when does the gunman pull out his shield and shout 'Police! You are under arrest!'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 176.477,
        "end": 185.645
      },
      "pred_interval": {
        "start": 60.0,
        "end": 65.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 116.477,
        "end": 120.64500000000001,
        "average": 118.561
      },
      "rationale_metrics": {
        "rouge_l": 0.37681159420289856,
        "text_similarity": 0.8434315323829651,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps and durations for both events (55\u201365s vs. correct ~174.9\u2013185.6s), so it is largely incorrect; only the coarse 'after' relationship matches the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After John observes Mr. Miller pass a small glass bottle with white powder, when does John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.6,
        "end": 166.8
      },
      "pred_interval": {
        "start": 150.0,
        "end": 155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.599999999999994,
        "end": 11.800000000000011,
        "average": 12.700000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.75396728515625,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal relation ('after'), but it omits the specific start/end timestamps and duration details given in the correct answer, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the officer gives chase and the defendant pushes him, when does the officer trip and hit his head, opening a big gash on his forehead?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 216.0,
        "end": 226.824
      },
      "pred_interval": {
        "start": 160.0,
        "end": 165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.0,
        "end": 61.82400000000001,
        "average": 58.912000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.8331308364868164,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relationship, but it omits the precise timestamps and event boundaries provided in the correct answer (anchor end at 1:08.0 and target start/end at 216.0/226.824), which are key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes sees the defendant exiting the saloon like a bat out of hell, when does she decide she should get back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 343.5
      },
      "pred_interval": {
        "start": 170.0,
        "end": 175.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 164.0,
        "end": 168.5,
        "average": 166.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3146067415730337,
        "text_similarity": 0.7833362817764282,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal relation ('after'), but it omits the specific timestamps and finer details provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Reyes finishes thinking 'something seems wrong', when does she think 'I should get back to that bar'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.3,
        "end": 340.5
      },
      "pred_interval": {
        "start": 340.0,
        "end": 345.0
      },
      "iou": 0.06493506493506503,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6999999999999886,
        "end": 4.5,
        "average": 3.5999999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.38636363636363635,
        "text_similarity": 0.8299996852874756,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor near 335s and that the target is a subsequent thought, but the target timing is shifted (starts later and extends beyond the true end) and the relation label ('immediately after') alters the precise temporal relation compared to the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the car was seized, when does he mention a forensic technician finding cocaine residue?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.1,
        "end": 428.2
      },
      "pred_interval": {
        "start": 420.0,
        "end": 425.0
      },
      "iou": 0.5975609756097542,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.10000000000002274,
        "end": 3.1999999999999886,
        "average": 1.6500000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.3883495145631068,
        "text_similarity": 0.9174139499664307,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal order, but the time spans are slightly off (E1 lacks the correct end time and is ~0.9s early; E2 ends ~3.2s earlier) and it strengthens the relation to 'immediately after' rather than the neutral 'after.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker specifies the time and date of the incident, when does he mention the car being seized?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 415.9,
        "end": 419.1
      },
      "pred_interval": {
        "start": 405.0,
        "end": 410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.899999999999977,
        "end": 9.100000000000023,
        "average": 10.0
      },
      "rationale_metrics": {
        "rouge_l": 0.34408602150537637,
        "text_similarity": 0.8778347969055176,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and their order but both E1 and E2 timestamps are significantly incorrect and the relation 'immediately after' mischaracterizes the timing compared to the reference, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the prosecution states that the defendant Carl Miller was meeting with his business clients, when does the evidence show that the defendant was in the saloon at 3:55 p.m.?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 510.41,
        "end": 510.45
      },
      "pred_interval": {
        "start": 510.0,
        "end": 520.0
      },
      "iou": 0.003999999999996362,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.410000000000025,
        "end": 9.550000000000011,
        "average": 4.980000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.4888888888888889,
        "text_similarity": 0.8876280784606934,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target and the 'after' relation, but it fails to provide the precise timestamps: E1 is left vague and E2 is incorrectly given as 520.0s instead of ~510.41\u2013510.45s, misrepresenting the timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once John observed Mr. Miller pass a small glass bottle with a white powder in it to his buddy, when did John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 631.0,
        "end": 638.07
      },
      "pred_interval": {
        "start": 530.0,
        "end": 540.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.0,
        "end": 98.07000000000005,
        "average": 99.53500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.6,
        "text_similarity": 0.8132777214050293,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates the timing of the target event (540.0s vs correct 631\u2013638.07s), omits the anchor timestamp, and even presents an inconsistent relationship\u2014thus largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the gunman shouted, 'Police! You are under arrest!', when did the defendant immediately take off running towards the exit?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 673.544,
        "end": 683.655
      },
      "pred_interval": {
        "start": 550.0,
        "end": 560.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.54399999999998,
        "end": 123.65499999999997,
        "average": 123.59949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.47457627118644063,
        "text_similarity": 0.7834901809692383,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance content but fails to provide the correct anchor timestamp and gives a wrong target timestamp (560.0s vs correct ~673.5s), producing an inconsistent relationship; this is a factual contradiction and omission of key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Dr. Reyes sees the defendant exiting the saloon, when does she wonder if something is wrong?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.4,
        "end": 751.6
      },
      "pred_interval": {
        "start": 750.0,
        "end": 755.0
      },
      "iou": 0.18604651162790914,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.6000000000000227,
        "end": 3.3999999999999773,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.8731333017349243,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the event order and relation ('after') and identifies when she wonders, but it omits the precise timing details (start/end timestamps) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the defendant jumped into a convertible, when does he look again at Dr. Reyes while starting the car?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.3,
        "end": 773.5
      },
      "pred_interval": {
        "start": 780.0,
        "end": 785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.700000000000045,
        "end": 11.5,
        "average": 11.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.71875,
        "text_similarity": 0.9106360673904419,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation but omits the precise start/end timestamps and durations given in the reference, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes writes down the Mustang's plate number, when does she decide to go back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 797.9,
        "end": 808.8
      },
      "pred_interval": {
        "start": 805.0,
        "end": 810.0
      },
      "iou": 0.3140495867768552,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.100000000000023,
        "end": 1.2000000000000455,
        "average": 4.150000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.4507042253521127,
        "text_similarity": 0.8188978433609009,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but it omits the specific timestamps and the completion/confirmation detail for E1 provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"You will not hear one single person,\" when does the speaker mention the date \"September 8th, 2020\"?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 882.8,
        "end": 885.6
      },
      "pred_interval": {
        "start": 880.0,
        "end": 885.0
      },
      "iou": 0.3928571428571494,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7999999999999545,
        "end": 0.6000000000000227,
        "average": 1.6999999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.8228118419647217,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the 'after' relationship and gives approximate timestamps, but it omits the anchor end time and the target start time is ~2.8s earlier (and target end slightly off) compared to the ground truth, so the timing is imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that the car was seized and taken to the crime lab, when does the speaker begin describing the finding of cocaine residue by a forensic technician?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 891.0,
        "end": 904.4
      },
      "pred_interval": {
        "start": 895.0,
        "end": 900.0
      },
      "iou": 0.3731343283582096,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 4.399999999999977,
        "average": 4.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666666,
        "text_similarity": 0.8437641263008118,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('immediately after') but the timestamps are materially inaccurate (anchor 890.9s vs predicted 895.0s; target should start 891.0s and end 904.4s vs predicted 895.0\u2013900.0), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's statement about finding the defendant Carl Miller guilty of felonies, when is \"fleeing and eluding\" specifically mentioned?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 930.8,
        "end": 947.9
      },
      "pred_interval": {
        "start": 930.0,
        "end": 935.0
      },
      "iou": 0.23463687150838272,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7999999999999545,
        "end": 12.899999999999977,
        "average": 6.849999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.8134210109710693,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly approximates the start of E2 but misstates E1 timing, significantly shortens E2's end (935.0s vs 947.9s) and incorrectly labels the relationship as 'after' instead of E2 occurring within/overlapping the anchor period, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks the witness to state her name and spell her last name, when does the witness spell out her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.561,
        "end": 36.805
      },
      "pred_interval": {
        "start": 4.5,
        "end": 7.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.061,
        "end": 29.805,
        "average": 29.433
      },
      "rationale_metrics": {
        "rouge_l": 0.3513513513513513,
        "text_similarity": 0.7294932007789612,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation and the event types, but the provided timestamps are substantially incorrect (shifted by ~26 seconds) and thus do not match the ground truth; the added spelled name is extraneous but not the main issue."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks the witness if she was working as a plumber at a construction site on the evening of October 27th, 2020, when does the witness describe finding the broken window of her vehicle?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 69.075,
        "end": 74.643
      },
      "pred_interval": {
        "start": 18.0,
        "end": 23.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.075,
        "end": 51.643,
        "average": 51.359
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285715,
        "text_similarity": 0.7112259864807129,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event types and the 'after' relation right but gives completely incorrect timestamps (18.0\u201323.0s vs the correct 69.075\u201374.643s, and E1 times also mismatch), so it fails on the key factual timing detail."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker asks the witness to describe the area where she parked her car and if she was worried about its safety, when does the witness explain that she was not worried because she always parked near a bar?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.14,
        "end": 128.77
      },
      "pred_interval": {
        "start": 35.0,
        "end": 38.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.14,
        "end": 90.77000000000001,
        "average": 82.95500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.676155686378479,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the semantic content and the 'after' relation, but the provided timestamps and durations are significantly inaccurate compared to the reference, so it fails to meet the required temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After Ms. Mendoza reports that her tablet and ice skates were stolen, when does the lawyer ask her if she made any attempt to look for the perpetrators or immediately contacted the police?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 164.191,
        "end": 172.955
      },
      "pred_interval": {
        "start": 151.5,
        "end": 153.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.691000000000003,
        "end": 19.955000000000013,
        "average": 16.323000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.6857072114944458,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the predicted E1 start time matches, E2 timing is significantly incorrect (predicted 151.5\u2013153.0s vs reference 164.191\u2013172.955s), causing a temporal overlap and contradicting the 'after' relation; major factual timing errors make the answer essentially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza explains that she noticed a suspicious man around her car while on the phone with the operator, when does the lawyer ask her what the officer did?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 261.353,
        "end": 262.466
      },
      "pred_interval": {
        "start": 210.0,
        "end": 212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.35300000000001,
        "end": 50.46600000000001,
        "average": 50.90950000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.271604938271605,
        "text_similarity": 0.7294692993164062,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the correct relation, the predicted temporal boundaries are substantially incorrect (off by ~20\u201350s) and omit the key context that E2 follows the officer's arrival and the man being pointed out, so it fails to align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After Ms. Mendoza states that the thief got out of the car and started running down the street, when does she describe the thief punching the arresting officer?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 321.13,
        "end": 331.68
      },
      "pred_interval": {
        "start": 290.0,
        "end": 292.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.129999999999995,
        "end": 39.68000000000001,
        "average": 35.405
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.6788550615310669,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly labels the relation as 'after', but the event timestamps are substantially incorrect and inconsistent with the reference (E1 and E2 times differ by tens of seconds), and the prediction omits the context that the punch occurred after apprehension/resistance."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza to point out the man who assaulted the officer, when does Ms. Mendoza describe him?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 349.986,
        "end": 355.253
      },
      "pred_interval": {
        "start": 352.0,
        "end": 360.0
      },
      "iou": 0.324845216696623,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.01400000000001,
        "end": 4.747000000000014,
        "average": 3.380500000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.44680851063829785,
        "text_similarity": 0.7319088578224182,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures both events, the quoted description (Spanish equivalent) and the 'after' relation; timestamps are slightly shifted but semantically accurate, so only minor timing discrepancies prevent a perfect score."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking if the man settled down and cooperated after being put in the patrol car, when does Ms. Mendoza reply that he did not?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 457.548,
        "end": 460.233
      },
      "pred_interval": {
        "start": 450.0,
        "end": 460.0
      },
      "iou": 0.2396169256327565,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.548000000000002,
        "end": 0.2330000000000041,
        "average": 3.890500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6726133227348328,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the same events and relation (Ms. Mendoza's 'he didn't' reply and that it occurs after the lawyer's question), but the provided timestamps are noticeably earlier than the reference (E1 ~4.8s early; E2 start ~7.5s early) and the E2 span is imprecise, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the new lawyer says 'Good morning', when does he ask Ms. Mendoza to state and spell her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 502.241,
        "end": 504.904
      },
      "pred_interval": {
        "start": 480.0,
        "end": 490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.240999999999985,
        "end": 14.903999999999996,
        "average": 18.57249999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4691358024691358,
        "text_similarity": 0.8079735636711121,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but gives timestamps that are significantly earlier and inconsistent with the reference (predicted ~478\u2013490s vs reference around ~501\u2013502s), so the timing is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza if she prefers to testify with an interpreter, when does Ms. Mendoza confirm her preference?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.733,
        "end": 528.876
      },
      "pred_interval": {
        "start": 514.0,
        "end": 518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.732999999999947,
        "end": 10.875999999999976,
        "average": 11.304499999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.5879669785499573,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the provided timestamps are substantially different from the ground truth (off by ~14\u201316s) and the predicted utterance timing/duration do not match the reference, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza states that she saw someone inside her car through the broken window, when does the lawyer acknowledge her statement?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 559.358,
        "end": 561.878
      },
      "pred_interval": {
        "start": 546.0,
        "end": 550.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.357999999999947,
        "end": 11.878000000000043,
        "average": 12.617999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.36666666666666664,
        "text_similarity": 0.7756850719451904,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the qualitative relation ('after') and identifies the lawyer's 'I see', but the timestamp boundaries are substantially incorrect compared to the reference, so key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the lawyer asks Ms. Mendoza if anything was removed from her vehicle, when does Ms. Mendoza list the stolen items?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.801,
        "end": 634.921
      },
      "pred_interval": {
        "start": 614.0,
        "end": 622.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.801000000000045,
        "end": 12.92100000000005,
        "average": 10.861000000000047
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.7728550434112549,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the reported start/end times differ substantially from the ground truth and the predicted intervals do not align with the correct E1/E2 boundaries (and omit the noted initial 'Por supuesto'), so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the witness finishes describing the man looking through the car window, when does the lawyer state that a deputy was arriving?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 711.914,
        "end": 714.633
      },
      "pred_interval": {
        "start": 705.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.913999999999987,
        "end": 4.633000000000038,
        "average": 5.773500000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.7180755138397217,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation as 'after', but the provided timestamps conflict significantly with the ground truth (E1/E2 times are off by several seconds), so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking what the officer did, when does the witness describe the officer speaking into his radio and telling her to stay put?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.233,
        "end": 766.142
      },
      "pred_interval": {
        "start": 730.0,
        "end": 735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.232999999999947,
        "end": 31.142000000000053,
        "average": 21.6875
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6945537328720093,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly reports all timestamps (substantially earlier and with wrong duration) and thus does not align with the ground-truth intervals; while both state E2 follows E1, the temporal boundaries and relation detail ('once_finished' vs generic 'after') are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the witness finishes stating she is absolutely sure about identifying the defendant, when does the lawyer ask about the suspect's behavior after being cuffed?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 851.101,
        "end": 861.493
      },
      "pred_interval": {
        "start": 860.0,
        "end": 865.0
      },
      "iou": 0.10741779984171895,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.899000000000001,
        "end": 3.506999999999948,
        "average": 6.2029999999999745
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.7229423522949219,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and gives plausible time intervals, but the reported timestamps differ notably from the ground truth (E1 off by ~6.6s, E2 start off by ~8.9s), so it is only partially accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the male lawyer asks what happened when they got to the patrol car, when does Ms. Mendoza describe the officers searching the suspect?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 885.63,
        "end": 894.463
      },
      "pred_interval": {
        "start": 876.0,
        "end": 880.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.629999999999995,
        "end": 14.462999999999965,
        "average": 12.04649999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.6063572764396667,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially incorrect (E1 predicted at ~874s vs reference 882.005\u2013884.029; E2 predicted 876\u2013880s vs reference 885.63\u2013894.463) and the predicted E2 omits the completion of the found-items list; only the stated relation ('after') matches."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ms. Mendoza finishes explaining she got closer to see what was happening, when does she state that's why she remembers it well?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 919.062,
        "end": 923.688
      },
      "pred_interval": {
        "start": 905.0,
        "end": 908.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.062000000000012,
        "end": 15.687999999999988,
        "average": 14.875
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.7399001121520996,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted intervals for E1 and E2 (904\u2013908s range) do not match or overlap the ground-truth timestamps (913.034\u2013923.688s), so the answer is factually incorrect; the relation 'immediately after' is similar in meaning to 'once_finished' but does not compensate for the wrong timings."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male lawyer finishes asking if the suspect settled down and cooperated in the patrol car, when does Ms. Mendoza state he did not cooperate?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 937.892,
        "end": 940.207
      },
      "pred_interval": {
        "start": 930.0,
        "end": 934.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.892000000000053,
        "end": 6.206999999999994,
        "average": 7.049500000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.15151515151515152,
        "text_similarity": 0.631891131401062,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content (Ms. Mendoza saying he did not cooperate) and a similar temporal relation, but the timestamps for both E1 and E2 are substantially incorrect compared to the reference and the precise quoted timing is not matched."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Good evening, friends,' when does he mention that their journey started in 2020 during the COVID times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 5.453,
        "end": 8.514
      },
      "pred_interval": {
        "start": 3.0,
        "end": 6.5
      },
      "iou": 0.18988030467899888,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4530000000000003,
        "end": 2.0139999999999993,
        "average": 2.2335
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.6792614459991455,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relationship, but the timestamps are substantially inaccurate (anchor given as 0.0s vs 3.592s; target 3.0\u20136.5s vs 5.453\u20138.514s), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker (Vikas Chatrath) says 'I think there's some issue with the net,' when does the screen go black with his name displayed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.726,
        "end": 75.546
      },
      "pred_interval": {
        "start": 88.0,
        "end": 89.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.274,
        "end": 13.953999999999994,
        "average": 17.113999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.7348198890686035,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relationship ('immediately after') but the timestamps are wildly incorrect and inconsistent with the ground truth (anchor/target times and durations differ substantially), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath says 'Over to you, sir,' when does Mr. R.S. Cheema begin speaking?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.165,
        "end": 175.752
      },
      "pred_interval": {
        "start": 131.0,
        "end": 134.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.16499999999999,
        "end": 41.75200000000001,
        "average": 39.9585
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.7176908254623413,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps contradict the reference by large margins (predicted anchor/target at ~127\u2013134s vs reference 167.341\u2013175.752s); only the vague \"shortly after\" relationship matches, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chatrath finishes his speech and invites Mr. Cheema, when does Mr. Cheema describe the topic as very generic and vast?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.048,
        "end": 204.229
      },
      "pred_interval": {
        "start": 156.0,
        "end": 164.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.048,
        "end": 40.22900000000001,
        "average": 42.13850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.4333333333333333,
        "text_similarity": 0.5759264826774597,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('after') and anchor identification broadly right, but the target timestamp is grossly incorrect (156.0s vs the correct ~50.0s), so the key temporal localization is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After Mr. Cheema says he agreed with the choice of the broad topic, when does he explain the reason for his agreement?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 229.669,
        "end": 242.242
      },
      "pred_interval": {
        "start": 172.0,
        "end": 184.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.66900000000001,
        "end": 58.24199999999999,
        "average": 57.9555
      },
      "rationale_metrics": {
        "rouge_l": 0.37500000000000006,
        "text_similarity": 0.7827527523040771,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely incorrect temporal locations (E2 at 172\u2013184s vs correct 229.669\u2013242.242s) and omits the correct E1 timestamp, effectively contradicting the true timing despite repeating the 'after' label."
      }
    },
    {
      "question_id": "003",
      "question": "During Mr. Cheema's description of going into division bench courts to hear idols and icons, when does he mention that the judges are not in a hurry?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.317,
        "end": 313.619
      },
      "pred_interval": {
        "start": 236.0,
        "end": 244.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.31700000000001,
        "end": 69.61900000000003,
        "average": 69.46800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3823529411764707,
        "text_similarity": 0.8231655359268188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the 'during' relation, but it gives a wrong timestamp for E2 (236.0s) which does not match the reference target (300.317-313.619) and thus fails the key temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions he has been appearing in three High Courts, when does he state that criminal appeals have not been argued for a long time in the Punjab and Haryana High Court?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 379.0,
        "end": 385.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 30.0,
        "average": 32.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3428571428571428,
        "text_similarity": 0.7086642980575562,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and the event content, but it substantially mislocates both event timestamps (predicting ~340\u2013345s vs. the correct ~365\u2013385s), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions there is very little hearing in appeal, when does he state that the most 'scaring part' is the flood of outstanding litigation causing the hearing to get a beating?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 408.915,
        "end": 418.887
      },
      "pred_interval": {
        "start": 355.0,
        "end": 365.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.91500000000002,
        "end": 53.887,
        "average": 53.90100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3478260869565218,
        "text_similarity": 0.7859538793563843,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but mislocates both events by a large margin (predicted ~352\u2013355s vs correct 405.5\u2013418.9s) and omits end timestamps, so it is largely incorrect despite the correct relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says, 'Let's first also define what the appeals are', when does he begin to define criminal appeals as generally understood?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.605,
        "end": 487.757
      },
      "pred_interval": {
        "start": 395.0,
        "end": 405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.60500000000002,
        "end": 82.757,
        "average": 80.68100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.7758135795593262,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps (395s vs. reference 471\u2013487s) and a different relation ('when' vs. 'once_finished'), so it fails to match the correct temporal spans and relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes stating that the real purpose of this exercise is to scuttle the path of the litigant, when does he discuss today's purpose?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 562.2,
        "end": 567.6
      },
      "pred_interval": {
        "start": 515.0,
        "end": 518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.200000000000045,
        "end": 49.60000000000002,
        "average": 48.400000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.8195200562477112,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the semantic relationship (the target immediately follows the anchor and the quoted phrases match), but it misidentifies the event timings by a large margin (514\u2013515s vs. 548\u2013562s), so it is largely temporally incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is listing the second category of appeals, when does he mention the 'Essential Commodities Act'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 595.2,
        "end": 601.662
      },
      "pred_interval": {
        "start": 542.0,
        "end": 544.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.200000000000045,
        "end": 57.662000000000035,
        "average": 55.43100000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7961238026618958,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies both events but gives timestamps that are substantially different from the ground truth and states an incorrect relationship ('immediately after' versus the reference which places the mention during the anchor); therefore it is largely incorrect despite partial event recognition."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating he will confine today's discussion to the Indian Penal Code, when does he explain that he doesn't want to just touch and go?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.624,
        "end": 635.538
      },
      "pred_interval": {
        "start": 561.0,
        "end": 565.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.62400000000002,
        "end": 70.53800000000001,
        "average": 69.08100000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.6348618268966675,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their immediate-after relationship, but it gives substantially incorrect timestamps for both the anchor and target compared to the reference, so it fails to match the required temporal locations."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that their judgments are not very clear, when does he begin to explain that, unlike IPC, other acts *do* have provisions for vicarious liability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.063,
        "end": 753.951
      },
      "pred_interval": {
        "start": 702.0,
        "end": 714.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.06299999999999,
        "end": 39.95100000000002,
        "average": 42.007000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7520460486412048,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the event timestamps are substantially off (E1 ~40s earlier and E2 ~32s earlier than the reference), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the first clause of the provisions, when does he explain that an individual is a 'deemed accused' by virtue of their office?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 772.927,
        "end": 777.037
      },
      "pred_interval": {
        "start": 762.0,
        "end": 774.0
      },
      "iou": 0.0713573186140837,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.927000000000021,
        "end": 3.0370000000000346,
        "average": 6.982000000000028
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.7810721397399902,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly matches the E2 start time but misplaces E1 by ~10s, omits the E2 end time, and gives the wrong relation ('after' vs 'once_finished'), so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that the 'second part is most important', when does he begin describing this second part of the provisions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 786.039,
        "end": 799.875
      },
      "pred_interval": {
        "start": 798.0,
        "end": 810.0
      },
      "iou": 0.07825215975960932,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.961000000000013,
        "end": 10.125,
        "average": 11.043000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.44776119402985076,
        "text_similarity": 0.740768551826477,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mislocates both events by a large margin (E1 ~798.0s vs reference finishing at 786.019s; E2 at 810.0s vs reference starting 786.039s\u2013799.875s) and uses 'after' rather than the immediate 'once_finished' relation, so it does not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking if people are vicariously liable, when does he state that the answer is 'yes and no both'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.653,
        "end": 886.655
      },
      "pred_interval": {
        "start": 881.0,
        "end": 883.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.65300000000002,
        "end": 3.6549999999999727,
        "average": 3.6539999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.7655636072158813,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their order, but the timestamps are several seconds off (E2 is placed before the true E1 finish) and the temporal relation/timing do not match the ground truth, so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the Food Safety and Security Act allows a company to appoint a competent person as a nominee, when does he mention the Environment Act?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 954.035,
        "end": 958.702
      },
      "pred_interval": {
        "start": 918.0,
        "end": 921.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.03499999999997,
        "end": 37.702,
        "average": 36.86849999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.7623163461685181,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives event times that substantially conflict with the reference (both E1 and E2 are ~35\u201340s earlier than correct) and labels the relation as 'immediately after' rather than the correct 'after', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes that the rest of the appeals would always be technical, when does he introduce the topic of drafting an appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1052.092,
        "end": 1055.115
      },
      "pred_interval": {
        "start": 961.0,
        "end": 963.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 91.0920000000001,
        "end": 92.11500000000001,
        "average": 91.60350000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.410958904109589,
        "text_similarity": 0.6808715462684631,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and that the drafting topic follows, but the timestamps are substantially incorrect (off by ~79\u201396s) and it wrongly asserts they occur 'immediately after' rather than after a ~13s gap, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions looking into the nuances of drafting, when does he state what he found regarding drafting and appeals?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1054.8,
        "end": 1058.3
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1062.0
      },
      "iou": 0.2916666666666667,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.7999999999999545,
        "end": 3.7000000000000455,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.8719716668128967,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target occurs after it, and the reported start times are close; however it omits the anchor end time and the target interval is shifted and extended beyond the reference bounds."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that criminal proceedings are not adversarial, when does he discuss how formal errors and oversights can be overcome?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1120.624,
        "end": 1125.731
      },
      "pred_interval": {
        "start": 1086.0,
        "end": 1102.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.624000000000024,
        "end": 23.730999999999995,
        "average": 29.17750000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.27777777777777773,
        "text_similarity": 0.8473585247993469,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the reported timestamps for E1 and E2 are substantially different from the ground truth (off by ~28\u201334s), so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that the 313 was not correctly recorded in a Ropar case, when does he recall the accused saying he was made to sign a blank paper?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1194.217,
        "end": 1254.6
      },
      "pred_interval": {
        "start": 1188.0,
        "end": 1194.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.217000000000098,
        "end": 60.59999999999991,
        "average": 33.408500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.8275942802429199,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relationship and quotes the target phrase, but the provided timestamps are materially off from the ground truth (several seconds earlier), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what they should do, when does he ask if an application needs to be filed for additional evidence in the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1251.721,
        "end": 1266.539
      },
      "pred_interval": {
        "start": 1235.0,
        "end": 1245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.721000000000004,
        "end": 21.538999999999987,
        "average": 19.129999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.49350649350649345,
        "text_similarity": 0.7889822721481323,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the target question content, but it misplaces the target time span (1235\u20131245s vs correct 1251.7\u20131266.5s), causing an overlap/inconsistency that undermines the claimed 'after' relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the court made a mistake by not formally proving a cassette recording, when does he explain that they had to make an application for evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.0,
        "end": 1294.14
      },
      "pred_interval": {
        "start": 1265.0,
        "end": 1285.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 9.1400000000001,
        "average": 17.57000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111105,
        "text_similarity": 0.6704334020614624,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same two events but mislocates both time spans by ~25\u201330 seconds, collapses their start times (making the order/offset incorrect), and labels the relation as 'once' instead of 'once_finished', so it fails on key temporal and relational details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that first reactions may never come again if not noted down, when does he state that he made a practice of noting his reactions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1394.269,
        "end": 1401.682
      },
      "pred_interval": {
        "start": 1350.0,
        "end": 1370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.269000000000005,
        "end": 31.682000000000016,
        "average": 37.97550000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7734220027923584,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the relevant phrases, but the event timestamps/spans are substantially incorrect and misaligned with the reference (predicted segments start much earlier and do not match the given E1/E2 intervals), so it fails on factual temporal accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says one should not put the detail in an appeal, when does he suggest that subtle points may be hinted at when drafting grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1444.582,
        "end": 1452.109
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1435.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.582000000000107,
        "end": 17.108999999999924,
        "average": 20.845500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322035,
        "text_similarity": 0.7546876668930054,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives incorrect and inconsistent timestamps (mislabels anchor start vs finish, and has a much earlier target start) and omits the target end time, so key factual details are wrong or missing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'What is the key question?', when does he compare a trial or appeal to writing a novel?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1536.909,
        "end": 1545.661
      },
      "pred_interval": {
        "start": 1490.0,
        "end": 1505.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.909000000000106,
        "end": 40.66100000000006,
        "average": 43.78500000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.680335521697998,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps and misstates E1 as starting (not finishing) at 1485s and E2 at 1490s, contradicting the correct immediate follow relationship and exact times around 1536.9\u20131545.661s, thus failing to match key factual elements."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions keeping the core issue in mind, when does he advise that the first reading of an appeal should be relaxed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1594.74,
        "end": 1607.579
      },
      "pred_interval": {
        "start": 1560.0,
        "end": 1580.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.74000000000001,
        "end": 27.57899999999995,
        "average": 31.15949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.834087610244751,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative order ('after') but provides incorrect and inconsistent timestamps (E1/E2 times differ substantially from the reference) and omits the correct end time for E2, so it fails to match the factual timing details. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says that the first reading of an appeal should be relaxed, when does he compare it to watching a soap opera, reading a novel, or listening to music?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1608.151,
        "end": 1615.0
      },
      "pred_interval": {
        "start": 1595.0,
        "end": 1605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.151000000000067,
        "end": 10.0,
        "average": 11.575500000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.5831403732299805,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the speaker's advice and the comparison to relaxing activities, but the event timestamps are substantially wrong and the temporal relation ('when') contradicts the reference ('once_finished'), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises being as neutral as possible, when does he explain that this enables objectivity and unbiased case knowledge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1638.375,
        "end": 1647.567
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1635.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.375,
        "end": 12.567000000000007,
        "average": 15.471000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.28070175438596495,
        "text_similarity": 0.6214714050292969,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation ('after') and the roles (anchor vs. explanation) correct, but both timestamps are significantly inaccurate compared to the reference (predicted E1 at 1610s vs ~1629.7s, predicted E2 1620\u20131635s vs ~1638.4s), omitting key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if he has a good case, when does he follow up by asking if he has a bad case or one in between?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1674.0,
        "end": 1681.0
      },
      "pred_interval": {
        "start": 1650.0,
        "end": 1660.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 21.0,
        "average": 22.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4117647058823529,
        "text_similarity": 0.7366054058074951,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction recognizes an anchor and follow-up question but the timestamps are substantially incorrect (off by ~20\u201330s and E2 even before the true E1) and the relation label ('after') does not match the correct 'next'; thus it fails on factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that three to four things would matter when preparing an appeal, what is the first thing he suggests?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1824.432,
        "end": 1828.85
      },
      "pred_interval": {
        "start": 1775.0,
        "end": 1780.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.432000000000016,
        "end": 48.84999999999991,
        "average": 49.14099999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.7747229337692261,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly references the anchor phrase but gives completely wrong target timestamps and an incorrect temporal relation: the predicted target (1775\u20131780s) is far earlier than the correct target (starts 1824.432s) which immediately follows the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker emphasizes that stages in an appeal should be clear in one's mind, when does he start talking about the 'finest lawyers'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1890.082,
        "end": 1904.09
      },
      "pred_interval": {
        "start": 1850.0,
        "end": 1860.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.08200000000011,
        "end": 44.08999999999992,
        "average": 42.08600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.7243630886077881,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly gives E2 timestamps (1850\u20131860s vs the true ~1890\u20131904s), omits precise anchor timing, and is internally inconsistent (places E2 before the anchor yet labels the relation 'after'); it only correctly identifies the topical phrases."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes his point about preparation, when does he announce moving to the next phase of the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1921.825,
        "end": 1924.428
      },
      "pred_interval": {
        "start": 1925.0,
        "end": 1930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.1749999999999545,
        "end": 5.571999999999889,
        "average": 4.373499999999922
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.8052523732185364,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the general event order but provides no anchor timestamps, gives incorrect E2 start/end times (1925\u20131930s vs. 1921.825\u20131924.428s), and describes the relation merely as 'after' rather than 'immediately follows,' so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a judge will eventually respond to an appeal, when does he mention being placed before a newly transferred judge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1984.778,
        "end": 1991.706
      },
      "pred_interval": {
        "start": 1973.0,
        "end": 1985.0
      },
      "iou": 0.011867849887735546,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.77800000000002,
        "end": 6.705999999999904,
        "average": 9.241999999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.3218390804597701,
        "text_similarity": 0.8086017370223999,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and roughly captures E2's phrasing about being placed before a transferred judge, but the timestamps are substantially inaccurate (E1 is placed ~29s early and E2's window is shifted), so it omits correct temporal boundaries."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes how he handles a client's ticklish case by listening to their story, when does he next explain that judges would analyze in a similar way and evolve their style?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2000.528,
        "end": 2006.954
      },
      "pred_interval": {
        "start": 2007.0,
        "end": 2018.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.47199999999998,
        "end": 11.04600000000005,
        "average": 8.759000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.741967499256134,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the general idea and the 'next' relation but fails on critical details: the anchor/target time boundaries are significantly shifted and the anchor is misidentified (start vs correct end), so the segments do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a diligent lawyer would form and correct their opinion about the bench every day, when does he next say that this practice saves the court's time?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2071.617,
        "end": 2074.641
      },
      "pred_interval": {
        "start": 2074.0,
        "end": 2085.0
      },
      "iou": 0.04789658522006168,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.382999999999811,
        "end": 10.358999999999924,
        "average": 6.370999999999867
      },
      "rationale_metrics": {
        "rouge_l": 0.44117647058823534,
        "text_similarity": 0.8031198978424072,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'next' relation and references the saving phrase, but the anchor and target timestamps are substantially off and the predicted target end includes extra/hallucinated wording ('who understands'), so it is not accurately aligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what makes a good lawyer, when does he begin to answer by describing a man who has prepared his brief fully?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.444,
        "end": 2200.017
      },
      "pred_interval": {
        "start": 2200.0,
        "end": 2205.0
      },
      "iou": 0.0013539343739905482,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.55600000000004,
        "end": 4.983000000000175,
        "average": 6.269500000000107
      },
      "rationale_metrics": {
        "rouge_l": 0.2535211267605634,
        "text_similarity": 0.7811538577079773,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the reported times are substantially wrong: the anchor is placed ~13s later than the ground truth and the target's start/end (2200.0\u20132205.0s) do not match the correct 2192.444\u20132200.017s, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a fresh judge saying 'no reading of evidence', when does he express his opinion that it's illegal and unconstitutional?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2237.752,
        "end": 2243.727
      },
      "pred_interval": {
        "start": 2240.0,
        "end": 2245.0
      },
      "iou": 0.5142108167770195,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2480000000000473,
        "end": 1.2730000000001382,
        "average": 1.7605000000000928
      },
      "rationale_metrics": {
        "rouge_l": 0.2535211267605634,
        "text_similarity": 0.7338918447494507,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction largely matches the correct intervals\u2014E1 start falls within the true E1 span and the predicted E2 lies within the true E2 range\u2014but it omits E1's end time and has small timing offsets (E2 starts ~2.25s late and ends ~1.27s late), so it's partially accurate but incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to make the judge read Section 54 of the Evidence Act, when does he explain its content regarding the previous character of an accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2302.58,
        "end": 2310.354
      },
      "pred_interval": {
        "start": 2285.0,
        "end": 2290.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.579999999999927,
        "end": 20.353999999999814,
        "average": 18.96699999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324326,
        "text_similarity": 0.7686424255371094,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches, the timestamps are substantially incorrect and inconsistent with the ground truth (E2 is placed much earlier and E1 end time is omitted), so it fails to accurately locate the segments."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of the second question as a heinous crime, when does he describe specific examples like 'chopped off the hand'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2354.498,
        "end": 2357.601
      },
      "pred_interval": {
        "start": 2335.0,
        "end": 2345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.498000000000047,
        "end": 12.601000000000113,
        "average": 16.04950000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.8852155208587646,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an anchor, a target containing the quoted phrase, and a similar relation, but the provided timestamps are significantly different from the ground truth (off by ~12\u201315s) and the target interval/end time are incorrect, so the answer is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states 'this is the second kind of roadblock', when does he introduce the 'Third kind of roadblock'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2414.579,
        "end": 2418.664
      },
      "pred_interval": {
        "start": 2390.0,
        "end": 2400.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.57900000000018,
        "end": 18.664000000000215,
        "average": 21.621500000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.3611111111111111,
        "text_similarity": 0.7969233393669128,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the 'Third kind of roadblock' immediately follows the anchor, but the timestamps are substantially different from the ground truth (off by ~20s) and the anchor end time is omitted, so the answer is factually inconsistent with the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes explaining 'these are the questions which are put to you', when does he transition to discussing myths about 'a dying declaration'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2462.739,
        "end": 2468.126
      },
      "pred_interval": {
        "start": 2460.0,
        "end": 2470.0
      },
      "iou": 0.538700000000017,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.7390000000000327,
        "end": 1.8739999999997963,
        "average": 2.3064999999999145
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.8123146891593933,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the transition, content (myths about a dying declaration), and the temporal relation ('after'/'following explanation'); the provided timestamps are slightly shifted (~1\u20133s) from the ground truth but do not contradict it."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker extensively discusses the 1925 Lahore Bakshish Singh judgment, when does he bring up the Lakshmi versus Om Prakash case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2549.448,
        "end": 2555.294
      },
      "pred_interval": {
        "start": 2550.0,
        "end": 2565.0
      },
      "iou": 0.3404063786008117,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5520000000001346,
        "end": 9.706000000000131,
        "average": 5.129000000000133
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111116,
        "text_similarity": 0.7960776090621948,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction identifies both events and their ordering correctly and gives an accurate time for E2, but it misplaces E1 by about 15 seconds (predicting ~2540s vs correct 2506.185\u20132524.919s), so the E1 timing is notably incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying, 'These are the cases when you have opened the door of a hearing', when does he state, 'These are situations which we must know'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2605.712,
        "end": 2610.678
      },
      "pred_interval": {
        "start": 2615.0,
        "end": 2620.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.288000000000011,
        "end": 9.322000000000116,
        "average": 9.305000000000064
      },
      "rationale_metrics": {
        "rouge_l": 0.2380952380952381,
        "text_similarity": 0.6274359226226807,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the immediate succession relationship, but the reported timestamps are significantly off (~9\u201310s later than the reference) and it omits E2's end time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises, 'But when you go to this, don't offend the judge', when does he describe making notes in 'three phases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2647.594,
        "end": 2653.382
      },
      "pred_interval": {
        "start": 2630.0,
        "end": 2640.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.59400000000005,
        "end": 13.382000000000062,
        "average": 15.488000000000056
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111116,
        "text_similarity": 0.7072229385375977,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates E2 follows E1 and repeats the quoted line, but the timestamps are substantially incorrect (off by ~17s) and it mischaracterizes the relation as 'immediate' instead of matching the precise 'after' timing window given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises a lawyer to stop if their best argument isn't selling, when does he suggest the lawyer might say, 'Your lordship may hear me on other issues'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2687.877,
        "end": 2690.499
      },
      "pred_interval": {
        "start": 2670.0,
        "end": 2685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.876999999999953,
        "end": 5.498999999999796,
        "average": 11.687999999999874
      },
      "rationale_metrics": {
        "rouge_l": 0.2769230769230769,
        "text_similarity": 0.8052915334701538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gives timestamps that do not match the reference and are internally inconsistent (its E2 segment actually falls before the referenced E1), though it does state the correct 'after' relationship; major timing errors warrant a low score."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how language helps with flexibility and playing cool in arguments, when does he introduce 'sense of humor' as an important quality?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2718.862,
        "end": 2725.929
      },
      "pred_interval": {
        "start": 2695.0,
        "end": 2705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.86200000000008,
        "end": 20.929000000000087,
        "average": 22.395500000000084
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.8047199249267578,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the speaker introduces 'sense of humor' immediately after the previous segment, but the timestamps are substantially incorrect (off by ~23s) and the predicted end time does not match the reference, so key temporal details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker identifies 'trap cases' and 'DA cases' as the majority of Corruption Act cases, when does he introduce 'scam cases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2786.395,
        "end": 2789.04
      },
      "pred_interval": {
        "start": 2745.0,
        "end": 2760.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.39499999999998,
        "end": 29.039999999999964,
        "average": 35.21749999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.8657235503196716,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the relative 'after' relation but gives substantially incorrect timestamps and durations (off by ~36\u201341s) and does not match the correct immediate-follow timing, so key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if a busy or young lawyer needs a register of important judgments, when does he directly ask if it is a necessary tool?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2898.935,
        "end": 2905.299
      },
      "pred_interval": {
        "start": 2913.0,
        "end": 2920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.065000000000055,
        "end": 14.701000000000022,
        "average": 14.383000000000038
      },
      "rationale_metrics": {
        "rouge_l": 0.20000000000000004,
        "text_similarity": 0.686583399772644,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events semantically (anchor discussing the register and target as the direct question) but gives incorrect timestamps for the target (starts ~15s later than the reference) and an imprecise relation label, so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes emphasizing the need to know, read, and re-read foundational judgments, when does he state he will recount a few of them?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.268,
        "end": 2935.553
      },
      "pred_interval": {
        "start": 2952.0,
        "end": 2957.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.73199999999997,
        "end": 21.447000000000116,
        "average": 21.589500000000044
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.6973884105682373,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the anchor and that the speaker will recount a few, but the timecodes are significantly incorrect and do not match the correct immediate follow-up timing, so it fails on factual accuracy and alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing the question about a false defense or wrong answer given by the accused, when does he explicitly state his suggestion and request for foundational judgments?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3029.268,
        "end": 3033.995
      },
      "pred_interval": {
        "start": 3018.0,
        "end": 3025.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.268000000000029,
        "end": 8.99499999999989,
        "average": 10.13149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.6541093587875366,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but provides incorrect and inconsistent timestamps (E2 start/end differ substantially from reference and E1 finish time is omitted), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'Virsar Singh's judgment, 1958', when does he mention 'Vivian Bose's judgment, a classic'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3045.106,
        "end": 3052.317
      },
      "pred_interval": {
        "start": 3045.0,
        "end": 3047.0
      },
      "iou": 0.25884925515918766,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.10600000000022192,
        "end": 5.317000000000007,
        "average": 2.7115000000001146
      },
      "rationale_metrics": {
        "rouge_l": 0.44897959183673464,
        "text_similarity": 0.5624001026153564,
        "llm_judge_score": 5,
        "llm_judge_justification": "The predicted answer gets the temporal relation right and the target start is close, but it misstates the anchor time by ~5s and the target end by ~5s, omitting the correct completion time\u2014thus only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker refers to the case 'Prabhu versus the Emperor, 1941', when does he ask if the initial onus on the prosecution transfers itself?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.51,
        "end": 3129.351
      },
      "pred_interval": {
        "start": 3108.0,
        "end": 3110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.510000000000218,
        "end": 19.351000000000113,
        "average": 15.930500000000166
      },
      "rationale_metrics": {
        "rouge_l": 0.3773584905660377,
        "text_similarity": 0.5990620851516724,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation 'after' correct but the anchor and target timestamps are substantially earlier (by ~12\u201313s) and the target interval is much shorter than the ground truth, so the timing alignment is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying '3.', when does he explain that the court has a 'broader picture' with the accused's evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3158.541,
        "end": 3164.576
      },
      "pred_interval": {
        "start": 3164.0,
        "end": 3167.0
      },
      "iou": 0.06809315521929699,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.458999999999833,
        "end": 2.423999999999978,
        "average": 3.9414999999999054
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7753921151161194,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies similar events but the timestamps are substantially offset (several seconds) and the temporal relation is labeled 'after' rather than the correct 'once_finished' (immediate elaboration), so it is factually and temporally incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 1954 Supreme Court case, when does he detail the specific allegation and offense?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3228.663,
        "end": 3239.981
      },
      "pred_interval": {
        "start": 3216.5,
        "end": 3228.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.163000000000011,
        "end": 11.981000000000222,
        "average": 12.072000000000116
      },
      "rationale_metrics": {
        "rouge_l": 0.2926829268292683,
        "text_similarity": 0.8377172946929932,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the high-level 'after' relation right but the timestamps are substantially misaligned (both E1 and E2 times differ notably), E2 omits the full offense description and adds an unsupported quote, so it is largely incorrect/incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying that Tanu Bedi is present, when does he begin describing the case involving an Afghan Airlines pilot?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3267.18,
        "end": 3280.178
      },
      "pred_interval": {
        "start": 3285.0,
        "end": 3295.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.820000000000164,
        "end": 14.822000000000116,
        "average": 16.32100000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.3544303797468354,
        "text_similarity": 0.8260207176208496,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misaligns the anchor and target timestamps (off by ~15\u201320s) and gives a different time window for the case description; the relation label is also less precise, so it does not match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if they can continue for 10 minutes, when does someone respond with 'Yes, of course, of course'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3407.405,
        "end": 3409.588
      },
      "pred_interval": {
        "start": 3389.0,
        "end": 3393.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.4050000000002,
        "end": 16.588000000000193,
        "average": 17.496500000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.37974683544303794,
        "text_similarity": 0.874538242816925,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the event labels (anchor and affirmative response) but the timestamps are substantially incorrect (E1 start ~4s early; E2 ~18s earlier and overlapping) and the relation label is wrong ('once' vs 'once_finished'), so it fails to match the correct temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how long they can continue, when does he ask if they can go on for 10 minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3405.21,
        "end": 3408.57
      },
      "pred_interval": {
        "start": 3398.0,
        "end": 3405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.210000000000036,
        "end": 3.5700000000001637,
        "average": 5.3900000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.7226511836051941,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same utterance and the 'after' relation, but the timestamps are substantially misaligned with the ground truth (anchor ~4.8s early; target starts ~7s early and ends ~3.6s early), so the events are not correctly localized."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions the judge saying 'no, you are convicted', when does he recall playing basketball?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3490.94,
        "end": 3501.65
      },
      "pred_interval": {
        "start": 3485.0,
        "end": 3495.0
      },
      "iou": 0.24384384384383923,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.940000000000055,
        "end": 6.650000000000091,
        "average": 6.295000000000073
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.7855572700500488,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and that the basketball recollection occurs after it, and the target interval overlaps the ground truth, but the predicted target start and end times are several seconds off from the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises that 'Your case must bother you', when does he begin telling another interesting case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3552.29,
        "end": 3555.63
      },
      "pred_interval": {
        "start": 3525.0,
        "end": 3535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.289999999999964,
        "end": 20.63000000000011,
        "average": 23.960000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.8035504221916199,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the anchor and target timestamps are substantially different from the ground truth, so the key factual elements (event timings) are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the detailed injury on the forehead, when does he state that the benefit of doubt went to the accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3636.678,
        "end": 3640.261
      },
      "pred_interval": {
        "start": 3640.0,
        "end": 3660.0
      },
      "iou": 0.011191149987135149,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.3220000000001164,
        "end": 19.739000000000033,
        "average": 11.530500000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.29508196721311475,
        "text_similarity": 0.7948240041732788,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' and reasonably locates E2, but misstates E1 timing (places the forehead description around 3620s versus 3594.774\u20133611.21s), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's description of the trickster character from Maupassant's story and his famous acts, when does he specifically mention the trickster showing different tricks to people?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3732.928,
        "end": 3736.294
      },
      "pred_interval": {
        "start": 3690.0,
        "end": 3705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.927999999999884,
        "end": 31.29399999999987,
        "average": 37.110999999999876
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.7295102477073669,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timings and relation contradict the correct answer: it gives entirely different time spans (around 3680\u20133705s vs. 3727.56\u20133741.361s) and labels the relation 'after' rather than 'during', so it fails to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing that there are 'two more cases', when does he name the location of the first of these new cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3665.138,
        "end": 3668.806
      },
      "pred_interval": {
        "start": 3710.0,
        "end": 3720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.86200000000008,
        "end": 51.19399999999996,
        "average": 48.02800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.24561403508771928,
        "text_similarity": 0.7841866612434387,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and that Kurukshetra is named after the anchor, but the timestamps are substantially off (~40\u201345s later) and the relation 'after' is vaguer than the correct 'once_finished' (immediate follow), so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the main speaker finishes stating that 'So that persuaded the court', when does he begin talking about other interesting cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.541,
        "end": 3838.467
      },
      "pred_interval": {
        "start": 3811.0,
        "end": 3815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.541000000000167,
        "end": 23.4670000000001,
        "average": 23.004000000000133
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.7350280284881592,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets E1's end time roughly right but incorrectly places E2 much earlier (3811.0s vs 3833.541s) and asserts an 'immediately after' relation; it therefore misstates the target start time, relation, and adds phrasing not in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the host says 'Thank you, sir', when does he describe how the session shifted to English?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3905.664,
        "end": 3912.762
      },
      "pred_interval": {
        "start": 3880.0,
        "end": 3885.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.664000000000215,
        "end": 27.76200000000017,
        "average": 26.713000000000193
      },
      "rationale_metrics": {
        "rouge_l": 0.3703703703703704,
        "text_similarity": 0.7207344770431519,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates both event timestamps (E1 and E2) and compresses the gap, claiming E2 starts immediately after E1, whereas the ground truth places E2 much later; this contradicts the correct timing and relation. The answer therefore is largely incorrect despite capturing an 'after' relation in spirit."
      }
    },
    {
      "question_id": "003",
      "question": "After the host announces that there are questions received on the chat, when does he ask the first question about supplementary grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3949.503,
        "end": 3955.411
      },
      "pred_interval": {
        "start": 3900.0,
        "end": 3905.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.503000000000156,
        "end": 50.41100000000006,
        "average": 49.95700000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2058823529411765,
        "text_similarity": 0.6858577132225037,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('immediately after' ~ 'next'), but the reported timestamps differ substantially from the reference (off by ~50\u2013100s) and the anchor/target timing details do not match, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they can combine the two questions, when does he state that all questions of fact can be raised?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3972.976,
        "end": 3975.039
      },
      "pred_interval": {
        "start": 3945.0,
        "end": 3950.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.976000000000113,
        "end": 25.039000000000215,
        "average": 26.507500000000164
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7284201383590698,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the provided timestamps are significantly different from the reference (anchor and target times are off by ~28s), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that supplementary grounds are not needed in a normal case, when does he explain that all questions of fact and law are open in a normal case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4030.938,
        "end": 4036.242
      },
      "pred_interval": {
        "start": 4040.0,
        "end": 4045.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.061999999999898,
        "end": 8.75799999999981,
        "average": 8.909999999999854
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6424411535263062,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship (the target follows the anchor) but the provided timestamps are substantially inaccurate (off by ~9 seconds for start/end), so while the ordering is right the factual timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining how a long note was required in the Sajan Kumar case, when does he describe the judge quoting his written note 80 times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4130.358,
        "end": 4139.64
      },
      "pred_interval": {
        "start": 4115.0,
        "end": 4125.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.358000000000175,
        "end": 14.640000000000327,
        "average": 14.999000000000251
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.7882906198501587,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence and content (judge quoted the note and the relation 'after'), but the provided timestamps are significantly incorrect and the event durations differ from the reference, so it is not temporally accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking about making an opening statement to impress the court, when does the main speaker begin explaining that a prepared lawyer can present their case in two minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4186.419,
        "end": 4209.102
      },
      "pred_interval": {
        "start": 4175.0,
        "end": 4185.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.418999999999869,
        "end": 24.10199999999986,
        "average": 17.760499999999865
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315788,
        "text_similarity": 0.714864194393158,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation label right but the timestamps are substantially off: both E1 and E2 are shifted earlier and E2 does not cover the full correct interval (4186.419\u20134209.102), so it fails to capture the complete explanation."
      }
    },
    {
      "question_id": "002",
      "question": "During the main speaker's narration of the Churchill story, when does he say that Churchill responded he needed '15 days before' to prepare for a two-minute speech?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4275.956,
        "end": 4281.618
      },
      "pred_interval": {
        "start": 4200.0,
        "end": 4205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.95600000000013,
        "end": 76.6180000000004,
        "average": 76.28700000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7843114137649536,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the qualitative relation ('during') but gives incorrect timestamps: it places E2 at 4200\u20134205s, which is far outside the correct E1 window (4265.1\u20134299.124s), and fails to align with the reference timings."
      }
    },
    {
      "question_id": "003",
      "question": "After the main speaker finishes explaining that the less one has to speak, the more deeply one has to prepare, when does he begin defining a summary as a test of a lawyer's ability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4306.932,
        "end": 4319.831
      },
      "pred_interval": {
        "start": 4220.0,
        "end": 4230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.93199999999979,
        "end": 89.83100000000013,
        "average": 88.38149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.6873196363449097,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation, but the provided timestamps are substantially incorrect (predicted ~4220s vs ground truth start at 4306.932s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks about the most challenging situations, when does the guest start explaining the amount of preparation needed for short vs long speeches?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4289.354,
        "end": 4303.141
      },
      "pred_interval": {
        "start": 4290.0,
        "end": 4332.0
      },
      "iou": 0.30814144351169404,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6459999999997308,
        "end": 28.85900000000038,
        "average": 14.752500000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.813701331615448,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct 'after' relationship, but both event timestamps are substantially misaligned with the reference (anchor shifted ~1s later; target starts ~12.6s late and ends much later), so the timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes rephrasing the question about the impact of oral advocacy versus drafting, when does the guest immediately state that oral advocacy is more impactful?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4347.622,
        "end": 4350.188
      },
      "pred_interval": {
        "start": 4360.0,
        "end": 4375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.377999999999702,
        "end": 24.811999999999898,
        "average": 18.5949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.8338350057601929,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relationship ('immediately after') but the timestamps and durations are substantially inaccurate (off by ~14s for E1 and E2 and E2 is much longer), so it fails to match the key temporal details in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the guest confirms that the framing of questions of law is 'all important', when does he state that 'manifest injustice' is the category for most cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4407.975,
        "end": 4412.261
      },
      "pred_interval": {
        "start": 4415.0,
        "end": 4430.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.024999999999636,
        "end": 17.738999999999578,
        "average": 12.381999999999607
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.8178920149803162,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event timestamps and their ordering\u2014placing E1 much later and making E2 start simultaneously (and overlap) rather than after E1 as in the reference\u2014and invents an incorrect end time, so it contradicts the correct timeline."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interviewer finishes asking how the speaker's different fields and readings affect their argument style, when does the speaker respond by saying 'Sir, you're absolutely right'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4537.667,
        "end": 4541.776
      },
      "pred_interval": {
        "start": 4552.0,
        "end": 4553.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.332999999999629,
        "end": 11.72400000000016,
        "average": 13.028499999999894
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.8116596937179565,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the qualitative relationship ('immediate response') but the reported start/end times differ substantially (by ~11\u201314 seconds) from the ground truth, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that 'Advocacy is a communicative skill', when does he elaborate that 'One part of a communicative skill is language'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4562.343,
        "end": 4567.786
      },
      "pred_interval": {
        "start": 4562.0,
        "end": 4564.0
      },
      "iou": 0.2863809194607909,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.3429999999998472,
        "end": 3.786000000000058,
        "average": 2.0644999999999527
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.8325238823890686,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events, their content and the 'elaboration' relationship, but the timestamps are inaccurate and incomplete (E1 start is earlier than ground truth and its end is omitted; E2 end is much earlier than the reference), so it is only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "After the interviewer mentions watching Justice Muralidhar's YouTube, when does the interviewer give an example from him about being asked 'did you murder?'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4617.635,
        "end": 4624.683
      },
      "pred_interval": {
        "start": 4590.0,
        "end": 4594.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.63500000000022,
        "end": 30.682999999999993,
        "average": 29.159000000000106
      },
      "rationale_metrics": {
        "rouge_l": 0.37500000000000006,
        "text_similarity": 0.7308220863342285,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the 'did you murder?' example follows the mention, but the provided timestamps are substantially different from the ground truth (off by ~29\u201330 seconds and incorrect end time), so key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines the appeal as an 'extension of the trial', when does he explain that notes can 'awaken you to the subtleties of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4673.51,
        "end": 4680.899
      },
      "pred_interval": {
        "start": 4700.0,
        "end": 4710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.48999999999978,
        "end": 29.100999999999658,
        "average": 27.79549999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.7315531373023987,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relationship right (target after anchor) but the timestamps are significantly off from the reference and the anchor end time is omitted, so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising how to objectively approach an appeal, when does he state that 'You have to absorb all the witnesses of the surrounding environment'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4746.348,
        "end": 4752.661
      },
      "pred_interval": {
        "start": 4720.0,
        "end": 4730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.347999999999956,
        "end": 22.661000000000058,
        "average": 24.504500000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.1971830985915493,
        "text_similarity": 0.7289815545082092,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the immediate-following relationship and content, but the timestamps are substantially incorrect (predicted E1/E2 end/start differ by ~6\u201326 seconds from the reference), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'then I become a lawyer', when does he describe the 'story of the lawyer of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4805.697,
        "end": 4824.178
      },
      "pred_interval": {
        "start": 4750.0,
        "end": 4770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.697000000000116,
        "end": 54.177999999999884,
        "average": 54.9375
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.6888746023178101,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the target speech occurs after the anchor, but the provided timestamps are substantially incorrect compared to the reference (off by ~45\u201355 seconds), so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the 'art of communication' as the 'foundation of law', when does he state that its effectiveness depends on the 'quality of your preparation'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4848.488,
        "end": 4862.369
      },
      "pred_interval": {
        "start": 4850.0,
        "end": 4855.0
      },
      "iou": 0.36020459621066314,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5119999999997162,
        "end": 7.368999999999687,
        "average": 4.440499999999702
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.8301368951797485,
        "llm_judge_score": 6,
        "llm_judge_justification": "Predicted anchor and target timestamps fall within the correct intervals and capture the referenced phrases, but the predicted relation is incorrect ('depends on' vs correct relation 'after'), so the answer is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the initial speaker says 'Thank you, sir', when does the second speaker begin apologizing for not taking all questions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4960.598,
        "end": 4970.611
      },
      "pred_interval": {
        "start": 4970.0,
        "end": 4975.0
      },
      "iou": 0.04242466324120778,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.402000000000044,
        "end": 4.389000000000124,
        "average": 6.895500000000084
      },
      "rationale_metrics": {
        "rouge_l": 0.30000000000000004,
        "text_similarity": 0.8367916345596313,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') correct but the event timestamps are substantially misaligned with the reference (both E1 and E2 start/end times are off by several seconds), so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "003",
      "question": "After the host refers to 'Q and Q, quality and quantity insurance', when does he mention the three important judgments: Prabhu, Rishikesh, and Vijay?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4996.141,
        "end": 5009.076
      },
      "pred_interval": {
        "start": 5000.0,
        "end": 5005.0
      },
      "iou": 0.3865481252415806,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.8590000000003783,
        "end": 4.076000000000022,
        "average": 3.9675000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.5866666666666667,
        "text_similarity": 0.8929040431976318,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the timestamps are noticeably off (anchor ~0.5s late; target start ~3.9s late and end ~4.1s early), so the temporal boundaries are inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes quoting 'abhi na jao chhod ke, dil abhi bhara nahi', when does he state 'That's what the common sense is'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5019.2,
        "end": 5022.6
      },
      "pred_interval": {
        "start": 5015.0,
        "end": 5016.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.199999999999818,
        "end": 6.600000000000364,
        "average": 5.400000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.4705882352941176,
        "text_similarity": 0.7471823692321777,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that E2 follows E1, but the timestamps and durations are substantially different from the reference (E2 is placed ~4s earlier and is much shorter), so the temporal alignment is incorrect despite similar relation wording."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker expresses surprise about people increasing viewership on YouTube, when does he mention everybody was viewed in the Zoom chat?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5030.3,
        "end": 5032.8
      },
      "pred_interval": {
        "start": 5023.0,
        "end": 5024.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.300000000000182,
        "end": 8.800000000000182,
        "average": 8.050000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.8013482093811035,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies similar events but timestamps are substantially incorrect (E2 is placed ~7\u20139s earlier than the reference and E1 timing/end is inconsistent), and the stated relation ('immediately after') contradicts the ground truth ('after' with different timing). These factual timing errors and relation mismatch warrant a low score."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Thank you, sir' for the first time, when does he specifically thank Tanu Bedi?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5046.2,
        "end": 5049.1
      },
      "pred_interval": {
        "start": 5039.0,
        "end": 5040.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.199999999999818,
        "end": 9.100000000000364,
        "average": 8.150000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.8108021020889282,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the provided timestamps for both E1 and E2 are substantially different from the ground truth (each off by ~7\u20139 seconds), so the answer is largely incorrect on the key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "Once Alex finishes asking about the key challenges facing individuals who are called to give evidence, when does Paul begin to talk about nervousness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.237,
        "end": 36.762
      },
      "pred_interval": {
        "start": 12.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.237000000000002,
        "end": 16.762,
        "average": 18.9995
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.6982049345970154,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that Paul speaks about nervousness immediately after the anchor, but the timestamps are substantially wrong (predicted start 12.0s vs correct 33.237s and differing end times), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Alex finishes asking what makes the actual process of giving evidence so difficult, when does Paul begin to describe the cross-examination process?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.92,
        "end": 92.191
      },
      "pred_interval": {
        "start": 42.0,
        "end": 52.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.92,
        "end": 40.191,
        "average": 41.5555
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.7025970220565796,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the qualitative relation (Paul speaks shortly after Alex) but gives wholly incorrect timestamps and event durations compared to the reference (40.5/42.0/52.0s vs 83.718/84.92/92.191s), so key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes describing the anecdote about the expert witness losing credibility, when does he start describing the other example of a fact witness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 172.683,
        "end": 180.236
      },
      "pred_interval": {
        "start": 105.0,
        "end": 115.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.68299999999999,
        "end": 65.23599999999999,
        "average": 66.45949999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4242424242424243,
        "text_similarity": 0.9012845158576965,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'immediately after' relationship, but both E1 and E2 timestamps are substantially incorrect and do not match the reference, so the events are misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker describes the sarcastic comments made about expert witness answers, when does he recount the witness's emotional reaction?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.3,
        "end": 164.2
      },
      "pred_interval": {
        "start": 160.0,
        "end": 170.0
      },
      "iou": 0.33070866141732225,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6999999999999886,
        "end": 5.800000000000011,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.5609756097560975,
        "text_similarity": 0.9358534812927246,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and anchor start, but it omits the anchor end time and gives inaccurate E2 timestamps (starts 2.7s late and ends 5.8s late), so it is partially correct but not precise."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker asks what witness familiarisation actually means, when does the first speaker begin to define the term?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 197.7,
        "end": 204.0
      },
      "pred_interval": {
        "start": 185.0,
        "end": 195.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.699999999999989,
        "end": 9.0,
        "average": 10.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.8088708519935608,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misstates the key timestamps (all start/end times differ by large margins) and gives a different temporal relation ('after' vs 'once_finished'). These factual discrepancies make the answer largely incorrect despite roughly addressing the same event sequence."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker finishes describing the benefits of witness familiarisation for individuals, when does he next outline the benefits for instructing solicitors?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 300.4,
        "end": 308.0
      },
      "pred_interval": {
        "start": 230.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.39999999999998,
        "end": 68.0,
        "average": 69.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.8163946270942688,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect\u2014its anchor and target timestamps (230.0\u2013240.0s) diverge substantially from the correct times (299.0s and 300.4\u2013308.0s); only the relation label is roughly similar, so it fails on key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker explains why they don't jump straight into cross-examination, when does he start talking about the need for theory?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 356.6,
        "end": 365.0
      },
      "pred_interval": {
        "start": 335.0,
        "end": 345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.600000000000023,
        "end": 20.0,
        "average": 20.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7515716552734375,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timings are substantially incorrect and inconsistent with the reference (E1 is given as a start at 330.0s vs reference finish at 355.5s; E2 is 335.0\u2013345.0s vs reference 356.6\u2013365.0s). Only the temporal relation ('after') matches, so major factual elements are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the first speaker is discussing the differences in virtual hearings, when does he mention judges requiring witnesses to demonstrate they are alone?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 394.0,
        "end": 398.0
      },
      "pred_interval": {
        "start": 385.0,
        "end": 395.0
      },
      "iou": 0.07692307692307693,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 3.0,
        "average": 6.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363637,
        "text_similarity": 0.8304784893989563,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relation and roughly locates the discussion, but the timestamp boundaries are inaccurate (E1 start earlier and no end given; E2 starts ~9s early and ends ~3s early compared to the reference), so it is only an approximate match."
      }
    },
    {
      "question_id": "003",
      "question": "Once the second speaker finishes thanking Paul, when does he mention sharing some of his insights?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 435.4,
        "end": 437.5
      },
      "pred_interval": {
        "start": 440.0,
        "end": 445.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.600000000000023,
        "end": 7.5,
        "average": 6.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.41379310344827586,
        "text_similarity": 0.7961695194244385,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct relation and mentions the same event, but the timestamps are substantially off (E1 and E2 shifted later and durations mismatched), so key factual timing details are incorrect."
      }
    }
  ]
}