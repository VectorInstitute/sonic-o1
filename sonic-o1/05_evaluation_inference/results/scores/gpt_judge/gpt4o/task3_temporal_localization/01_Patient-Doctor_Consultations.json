{
  "topic_id": 1,
  "topic_name": "Patient-Doctor Consultations",
  "num_evaluated": 269,
  "aggregated_metrics": {
    "mean_iou": 0.07184759678870664,
    "std_iou": 0.16454347818754603,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.10408921933085502,
      "count": 28,
      "total": 269
    },
    "R@0.5": {
      "recall": 0.048327137546468404,
      "count": 13,
      "total": 269
    },
    "R@0.7": {
      "recall": 0.007434944237918215,
      "count": 2,
      "total": 269
    },
    "mae": {
      "start_mean": 22.111765799256514,
      "end_mean": 3499.9971078066915,
      "average_mean": 1761.0544368029737
    },
    "rationale": {
      "rouge_l_mean": 0.34476631729820784,
      "rouge_l_std": 0.08480609324790336,
      "text_similarity_mean": 0.7217131126325813,
      "text_similarity_std": 0.08769456626048237,
      "llm_judge_score_mean": 3.1524163568773234,
      "llm_judge_score_std": 1.8359796564139566
    },
    "rationale_cider": 0.49653579460494857
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the speaker welcomes viewers and introduces himself as 'Karma Medic', when does he state that he is a 'final year medical student'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 35.0,
        "end": 36.62
      },
      "pred_interval": {
        "start": 24.0,
        "end": 25.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 11.119999999999997,
        "average": 11.059999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.7536231884057971,
        "text_similarity": 0.9433497190475464,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target phrase and the 'after' relationship, but the reported timestamps are substantially incorrect (anchor and especially target times are much earlier than the ground truth), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'Now with that lovely disclaimer out of the way, let's get right into it', when does the text 'before the history' appear on screen?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.06,
        "end": 57.06
      },
      "pred_interval": {
        "start": 52.0,
        "end": 54.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.060000000000002,
        "end": 3.0600000000000023,
        "average": 3.5600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.30000000000000004,
        "text_similarity": 0.8194335699081421,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an immediate/immediately-after relationship, but the timestamps and duration are significantly incorrect (predicts 52.0\u201354.0s vs ground truth 56.03s anchor and 56.06\u201357.06s target), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'So before starting the history, there's generally two things that I try and keep in mind', when does he begin describing 'washing your hands'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 206.36,
        "end": 207.36
      },
      "pred_interval": {
        "start": 164.0,
        "end": 166.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.360000000000014,
        "end": 41.360000000000014,
        "average": 41.860000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.47619047619047616,
        "text_similarity": 0.9021381139755249,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but both the anchor and target timestamps (and target end) are substantially incorrect compared to the ground truth, so it only partially matches. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the acronym 'ICE', when does he explain what it stands for?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 155.7,
        "end": 158.7
      },
      "pred_interval": {
        "start": 150.0,
        "end": 168.0
      },
      "iou": 0.16666666666666666,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.699999999999989,
        "end": 9.300000000000011,
        "average": 7.5
      },
      "rationale_metrics": {
        "rouge_l": 0.31746031746031744,
        "text_similarity": 0.6222728490829468,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly gives the relation ('after') and the expansion of 'ICE', but it misplaces both event timestamps and the duration (E1/E2 times differ substantially from the reference), so key timing facts are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes listing the components of the WIPER acronym, when does he start elaborating on 'washing your hands'?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.0,
        "end": 212.0
      },
      "pred_interval": {
        "start": 222.0,
        "end": 228.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 16.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7717423439025879,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies both events and their ordering, but the timestamps and duration differ substantially from the ground truth (shifted later by ~13\u201316s) and the relation 'after' is less precise than the correct 'once_finished', so it is only a partial match."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what brought the patient in, when does he explain what the 'history of presenting complaint' is about?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 346.0,
        "end": 351.0
      },
      "pred_interval": {
        "start": 335.0,
        "end": 370.0
      },
      "iou": 0.14285714285714285,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 19.0,
        "average": 15.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.747001051902771,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the time offsets are noticeably inaccurate (E2 starts much earlier and ends much later than the reference) and it omits E1's end time, so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "How long after the speaker says he'll put a picture of all possible questions does the \"REVIEW OF SYSTEMS\" checklist first appear on screen?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 539.8,
        "end": 543.7
      },
      "pred_interval": {
        "start": 528.0,
        "end": 531.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.799999999999955,
        "end": 12.700000000000045,
        "average": 12.25
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.6150017976760864,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both answers agree the checklist appears after the anchor, the predicted timestamps diverge substantially from the reference (E1 off by ~9.8s and E2 times do not match the reference's start/fully-visible times), so the prediction is largely inaccurate despite the correct relation."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is giving examples of systems review questions, when does he ask about \"tummy pain\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 565.74,
        "end": 566.422
      },
      "pred_interval": {
        "start": 548.0,
        "end": 549.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.74000000000001,
        "end": 17.422000000000025,
        "average": 17.581000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.6834613680839539,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relation and gives an approximate start for the examples, but it omits the E1 end time and provides significantly incorrect E2 timestamps (off by ~7\u20138s), so the temporal details are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions the \"JAM THREADS\" mnemonic, when does he say the name \"Sketchy Medical\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 699.531
      },
      "pred_interval": {
        "start": 660.0,
        "end": 662.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.0,
        "end": 37.53099999999995,
        "average": 36.765499999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.456140350877193,
        "text_similarity": 0.7372364401817322,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') correct but the timestamps are substantially incorrect for both events (E1 off by ~23s and E2 off by ~36\u201339s and much shorter in duration), so it fails to match the reference materially."
      }
    },
    {
      "question_id": "001",
      "question": "While the speaker describes Sketchy Medical, when does he mention drugs' mechanism of action and side effects?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 701.0,
        "end": 703.982
      },
      "pred_interval": {
        "start": 690.0,
        "end": 710.0
      },
      "iou": 0.14909999999999854,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 6.018000000000029,
        "average": 8.509000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.7879461646080017,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the mention of mechanism and side effects occurs during the Sketchy Medical description and its E2 interval overlaps the ground truth, but the anchor start time (690.0s vs 697.491s) and the E2 end time (710.0s vs 703.982s) are noticeably inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks a general question about family health, when does he suggest being specific about asthma, diabetes, and hypertension?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.914,
        "end": 745.914
      },
      "pred_interval": {
        "start": 735.0,
        "end": 745.0
      },
      "iou": 0.19113065787062627,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.913999999999987,
        "end": 0.9139999999999873,
        "average": 4.413999999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.2926829268292683,
        "text_similarity": 0.6302405595779419,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but the timestamps are inaccurate: E1 is ~0.75s early and E2's start is ~7.9s earlier than the reference (end time close), so the timing details do not align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining the importance of signposting, when does he ask if the patient uses any recreational drugs?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 811.123,
        "end": 812.664
      },
      "pred_interval": {
        "start": 780.0,
        "end": 790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.123000000000047,
        "end": 22.663999999999987,
        "average": 26.893500000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324326,
        "text_similarity": 0.6714434623718262,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: the timestamps for both events are wrong (off by ~30s and durations mismatch), the anchor end time is omitted, and the relationship is weakened to 'after' rather than the correct 'immediately follows.' Only the general ordering (target occurs after anchor) is preserved."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"concerns from ICE\", when does he start saying \"Just generally, if you're feeling stuck\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 880.187,
        "end": 883.471
      },
      "pred_interval": {
        "start": 878.0,
        "end": 879.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.187000000000012,
        "end": 3.9710000000000036,
        "average": 3.0790000000000077
      },
      "rationale_metrics": {
        "rouge_l": 0.5333333333333333,
        "text_similarity": 0.6802389621734619,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the ordering right ('after') but the provided timestamps areincorrect by ~2.2\u20132.5 seconds compared to the reference, so it is factually inaccurate despite preserving the relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"golden rulebook\", when does he open both hands outwards in a gesture?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 895.8,
        "end": 897.5
      },
      "pred_interval": {
        "start": 900.0,
        "end": 901.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.2000000000000455,
        "end": 4.0,
        "average": 4.100000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.47058823529411764,
        "text_similarity": 0.7238096594810486,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') correct but the anchor and target times deviate significantly from the reference (anchor ~6.7s late, target start ~4.2s late) and the predicted answer omits the target's end time interval, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I hope you find this video useful\", when does he say \"Peace\"?",
      "video_id": "STnLNbRHYhw",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 923.0
      },
      "gt_interval": {
        "start": 910.148,
        "end": 910.609
      },
      "pred_interval": {
        "start": 920.0,
        "end": 921.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.851999999999975,
        "end": 10.390999999999963,
        "average": 10.121499999999969
      },
      "rationale_metrics": {
        "rouge_l": 0.456140350877193,
        "text_similarity": 0.7022081613540649,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the 'Peace' utterance occurs after the anchor, but the timestamps are off by ~10 seconds and the relation label is less precise than 'once_finished'; key temporal details do not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying he has an appointment at 10 am, when does the green text 'Sure, what's your name?' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 6.1,
        "end": 8.2
      },
      "pred_interval": {
        "start": 3.0,
        "end": 5.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0999999999999996,
        "end": 3.1999999999999993,
        "average": 3.1499999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7541395425796509,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer matches the qualitative relation (appears immediately after) but gives incorrect and inconsistent timestamps (claims finish at 3.0s and start at 3.0s) that contradict the ground-truth times (finish 5.9s, start 6.1s\u20138.2s), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes stating his name, when does the green text 'Thank you, Lucas. Please take a seat...' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 11.9,
        "end": 19.0
      },
      "pred_interval": {
        "start": 8.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9000000000000004,
        "end": 9.0,
        "average": 6.45
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.6659331321716309,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the ground truth on both key timings (anchor finish 8.0s vs 10.6s; target start 8.0s vs 11.9s) and the temporal relation ('immediate' vs 'once_finished'), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks 'How long is the wait?', when does the green text 'About 10 minutes. Would you like some water while you wait?' appear?",
      "video_id": "6PAwe6jndns",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 37.0
      },
      "gt_interval": {
        "start": 22.1,
        "end": 25.3
      },
      "pred_interval": {
        "start": 15.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.100000000000001,
        "end": 8.3,
        "average": 7.700000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.762645423412323,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the reference times and relation: it misstates both event timestamps (15.0s vs 19.5\u201322.1s) and claims an immediate transition rather than the documented short pause, thus failing to match key facts."
      }
    },
    {
      "question_id": "002",
      "question": "After the video explains the 'we're a team' approach with animated graphics, when does the speaker appear at his desk looking at a computer?",
      "video_id": "WR5dACe-spg",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 59.0
      },
      "gt_interval": {
        "start": 34.6,
        "end": 36.0
      },
      "pred_interval": {
        "start": 21.0,
        "end": 22.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.600000000000001,
        "end": 14.0,
        "average": 13.8
      },
      "rationale_metrics": {
        "rouge_l": 0.2790697674418604,
        "text_similarity": 0.7676544189453125,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but the timestamps are off by ~9\u201313 seconds (placing them at ~20\u201322s vs. the correct ~29.5\u201336s), and it mischaracterizes the timing relationship/overlap between E1 and E2."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker says 'take that extra bit of time to listen', when does the 'OK' hand gesture emoji appear?",
      "video_id": "WR5dACe-spg",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 59.0
      },
      "gt_interval": {
        "start": 44.0,
        "end": 45.5
      },
      "pred_interval": {
        "start": 49.0,
        "end": 50.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 4.5,
        "average": 4.75
      },
      "rationale_metrics": {
        "rouge_l": 0.5217391304347826,
        "text_similarity": 0.8445172905921936,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: both event timings are several seconds later than the reference and it wrongly states the emoji appears after the spoken phrase, whereas the correct annotation shows the 'OK' emoji overlaps the phrase (44.0\u201345.5 within the 42.8\u201344.5 utterance)."
      }
    },
    {
      "question_id": "001",
      "question": "After Nurse Kim mentions graduating as a registered nurse, when does she talk about working for many different pharmaceutical companies?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.0,
        "end": 50.475
      },
      "pred_interval": {
        "start": 32.0,
        "end": 40.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.0,
        "end": 10.475000000000001,
        "average": 10.7375
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.6609633564949036,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relation and roughly identifies the nursing mention, but the timestamps for both events\u2014especially E2\u2014are substantially incorrect (E2 is placed much earlier than the reference), so the answer is factually inaccurate on key timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nurse Kim finishes describing her background as an 'incredible journey', when does she mention training side-by-side with Dr. Jugenberg for five years?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 149.87,
        "end": 153.25
      },
      "pred_interval": {
        "start": 71.0,
        "end": 79.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.87,
        "end": 74.25,
        "average": 76.56
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.6950540542602539,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation correct (once_finished) and identifies both events, but the timestamps are substantially wrong and do not match the reference intervals, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "While Nurse Kim explains options and possible outcomes, when does she begin examining the patient's stomach?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 157.5,
        "end": 160.5
      },
      "pred_interval": {
        "start": 150.0,
        "end": 160.0
      },
      "iou": 0.23809523809523808,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5,
        "end": 0.5,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.40579710144927544,
        "text_similarity": 0.7255789041519165,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction roughly identifies both events but gives inaccurate times (E1 at 150.0s vs 156.8s; E2 at 160.0s vs 157.5\u2013160.5s) and misstates the temporal relation as 'after' whereas the correct answer indicates the stomach exam occurs during her speech (overlap)."
      }
    },
    {
      "question_id": "002",
      "question": "After Nurse Kim finishes discussing the benefits, risks, and possible complications of the procedure, when does she start talking about asymmetry?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 169.7,
        "end": 172.0
      },
      "pred_interval": {
        "start": 170.0,
        "end": 180.0
      },
      "iou": 0.19417475728155317,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.30000000000001137,
        "end": 8.0,
        "average": 4.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.29729729729729726,
        "text_similarity": 0.7850630283355713,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction captures the main relationship (the target occurs immediately after the anchor) and is close in time, but it incorrectly gives identical timestamps (170.0s) rather than reflecting the slight offset where the target starts slightly after the anchor (~169.7s vs 169.5s)."
      }
    },
    {
      "question_id": "003",
      "question": "Once Nurse Kim finishes explaining that the one-hour consultation cannot provide everything you need to know, when does she mention that they are always available?",
      "video_id": "5KcshgQ45XU",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 269.0
      },
      "gt_interval": {
        "start": 201.5,
        "end": 203.71
      },
      "pred_interval": {
        "start": 230.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.5,
        "end": 36.28999999999999,
        "average": 32.394999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.6241937279701233,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies an immediate transition between the two events, but it gives incorrect timestamps (230.0s vs the ground-truth 201.5s), a substantial factual error in timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces himself and the topic, when does the slide change to 'Objectives for today's lesson'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 24.379,
        "end": 24.5
      },
      "pred_interval": {
        "start": 0.0,
        "end": 12.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.379,
        "end": 12.5,
        "average": 18.439500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7637427449226379,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has major timestamp errors for both events (E1 listed at 0.0s vs 4.014\u201314.567s; E2 at 12.0s vs 24.379s), and its stated relation is inconsistent with the provided times; only the verbal relation matches the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes listing the objectives for the lesson, when does the slide change to 'Brain storming time'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 46.529,
        "end": 47.0
      },
      "pred_interval": {
        "start": 12.0,
        "end": 51.0
      },
      "iou": 0.012076923076922989,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.529,
        "end": 4.0,
        "average": 19.2645
      },
      "rationale_metrics": {
        "rouge_l": 0.3137254901960784,
        "text_similarity": 0.531015932559967,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering ('after') but both timestamps are significantly incorrect (E1 12.0s vs 45.800s; E2 51.0s vs 46.529s) and the anchor/target alignment is wrong, so it fails to match key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes defining communication as the successful passage of a message from one person to another, when does he start explaining how good communication manifests in medical practice by informing patients of their diagnosis?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.0,
        "end": 177.0
      },
      "pred_interval": {
        "start": 152.0,
        "end": 170.0
      },
      "iou": 0.68,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 7.0,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.6758275628089905,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction has the correct anchor start (150.0s) but omits the anchor end (should be 150.0\u2013153.0s) and gives incorrect target times (152.0\u2013170.0s vs correct 153.0\u2013177.0s), causing the temporal relation to be wrong ('after' instead of immediately follows). These timing and relation errors materially misalign the answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the 'Importance of communication' slide, when does he begin discussing that good doctor-patient communication has been linked to improved patient satisfaction?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 190.0,
        "end": 198.0
      },
      "pred_interval": {
        "start": 180.0,
        "end": 190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 8.0,
        "average": 9.0
      },
      "rationale_metrics": {
        "rouge_l": 0.253968253968254,
        "text_similarity": 0.6920293569564819,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the target follows the anchor, but the timestamp spans are substantially off (anchor start ~2.5s early and missing end; target given 180\u2013190s vs correct 190\u2013198s), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker starts talking about how a lot of malpractice lawsuits have been documented, when does he explicitly advise being aware of communication's importance to avoid lawsuits?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 226.0,
        "end": 271.0
      },
      "pred_interval": {
        "start": 220.0,
        "end": 230.0
      },
      "iou": 0.0784313725490196,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 41.0,
        "average": 23.5
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.829576849937439,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target follows ('after'), and the anchor timing is within the reference window, but the predicted target timing (220\u2013230s) is substantially earlier and much shorter than the correct 226\u2013271s, omitting a large portion of the advised segment."
      }
    },
    {
      "question_id": "001",
      "question": "After the initial slide 'Communication is not just talking' is displayed, when does the speaker mention that physicians can improve health outcomes?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.28,
        "end": 346.0
      },
      "pred_interval": {
        "start": 335.0,
        "end": 340.0
      },
      "iou": 0.06545454545454793,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.279999999999973,
        "end": 6.0,
        "average": 5.139999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.8077839016914368,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor time and the 'after' relationship, but it misestimates the target event's timing and duration (predicted 335.0\u2013340.0s vs reference 339.28\u2013346.0s), so it's not fully accurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the slide showing two images (bored girl vs. smiling doctor/patient), when does the speaker describe the first image as depicting a 'horribly bored' lady?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 354.8,
        "end": 359.0
      },
      "pred_interval": {
        "start": 370.0,
        "end": 375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.199999999999989,
        "end": 16.0,
        "average": 15.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.379746835443038,
        "text_similarity": 0.7940442562103271,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mentions the slide and the 'horribly bored' description but gives substantially different timestamps and an incorrect temporal relation ('after' rather than during/overlapping the slide); thus it contradicts the key timing details in the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker defines verbal communication as 'using spoken words', when is the next time they define non-verbal communication?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 428.87,
        "end": 433.596
      },
      "pred_interval": {
        "start": 400.0,
        "end": 405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.870000000000005,
        "end": 28.596000000000004,
        "average": 28.733000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7828069925308228,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the non-verbal definition occurs after the verbal one, but the timestamps differ substantially from the reference and it misses that the non-verbal definition immediately follows (adjacency), so it is largely incorrect on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'golden minute', when does he describe the patient's hypothetical response?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 613.818,
        "end": 630.0
      },
      "pred_interval": {
        "start": 520.0,
        "end": 530.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.81799999999998,
        "end": 100.0,
        "average": 96.90899999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.5066666666666667,
        "text_similarity": 0.8795033097267151,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') but the provided timestamps are substantially inaccurate compared to the reference (anchor predicted at 510s vs 568.746\u2013569.929s; target predicted 520\u2013530s vs 613.818\u2013629.974s), so it fails on factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'Checking facts', when does he mention the next essential element of listening?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 641.157,
        "end": 642.461
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.15700000000004,
        "end": 37.46100000000001,
        "average": 39.309000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.8146308064460754,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same elements and the correct 'after' relationship, but the timestamps are substantially inaccurate (predicted ~595\u2013605s vs reference ~613\u2013642s) and the target end time is incorrect, so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Before the speaker says 'So, for example, we have three main types of reflective listening', when does he explain what reflective listening involves?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 667.457,
        "end": 687.051
      },
      "pred_interval": {
        "start": 640.0,
        "end": 650.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.456999999999994,
        "end": 37.051000000000045,
        "average": 32.25400000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.789344310760498,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the 'before' relationship, it gives wrong event labels and significantly incorrect timestamps (hallucinated spans) that contradict the reference; thus the temporal relation is right but the details are largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the three main types of reflective listening, when does he start explaining the 'Repeating' example?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 710.0,
        "end": 737.0
      },
      "pred_interval": {
        "start": 740.0,
        "end": 745.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 8.0,
        "average": 19.0
      },
      "rationale_metrics": {
        "rouge_l": 0.410958904109589,
        "text_similarity": 0.6034777164459229,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the timestamps are substantially off (E1 predicted 735.0s vs 696.1s; E2 predicted 740.0s vs 710.0s) and it adds an extra detail (chest pain) not present in the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining the 'Repeating' example, when does he introduce 'Rephrasing'?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 720.0,
        "end": 720.4
      },
      "pred_interval": {
        "start": 750.0,
        "end": 755.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 34.60000000000002,
        "average": 32.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2580645161290323,
        "text_similarity": 0.532722532749176,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly places 'Rephrasing' after 'Repeating', but the timestamps are substantially different from the ground truth (698s vs 745s for E1; 720s vs 750s for E2), it introduces an extra end time and claims 'immediately after' rather than the stated relation and phrase, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing 'Reflection of feeling by showing empathy', when does the 'Non-verbal' slide appear?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 780.0,
        "end": 821.5
      },
      "pred_interval": {
        "start": 765.0,
        "end": 770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 51.5,
        "average": 33.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3278688524590164,
        "text_similarity": 0.689012885093689,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (Non-verbal follows empathy) but the timestamps are substantially incorrect (765.0s vs correct 778.5s for E1 and 765.0s vs 780.0s for E2) and adds an unsupported end time, so it fails to match the reference. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises to smile, when does he mention checking for signs of pain?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 873.045,
        "end": 882.0
      },
      "pred_interval": {
        "start": 870.0,
        "end": 880.0
      },
      "iou": 0.5795833333333368,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.044999999999959,
        "end": 2.0,
        "average": 2.5224999999999795
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.7115367650985718,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the ordering and gives approximate times (E1 ~870s), but E2's start is about 3s later than the reference (876.0s vs 873.045s) and it omits the explanation end time (~882.0s), so it's slightly inaccurate/incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses the cultural interpretations of folding arms, when does he advise to avoid folding arms?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 932.0,
        "end": 936009.0
      },
      "pred_interval": {
        "start": 895.0,
        "end": 905.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 935104.0,
        "average": 467570.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3934426229508197,
        "text_similarity": 0.7959703207015991,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies both events and their temporal relation ('after'), but the reported timestamps are substantially inaccurate compared to the ground truth (off by tens of seconds), so the answer is factually incorrect in timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to introduce yourself to the patient, when does he advise to explain your role as a student or intern?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 985.0,
        "end": 990.853
      },
      "pred_interval": {
        "start": 1030.0,
        "end": 1040.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.0,
        "end": 49.14700000000005,
        "average": 47.073500000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.6357733011245728,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after')\u2014E2 follows E1\u2014but the absolute timestamps are substantially off from the ground truth, so timing accuracy is lacking."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"if you're in the hospital\", when does he refer to \"inpatient patients\"?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1059.6,
        "end": 1059.8
      },
      "pred_interval": {
        "start": 1056.0,
        "end": 1058.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.599999999999909,
        "end": 1.7999999999999545,
        "average": 2.699999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.5388914942741394,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and that the target occurs after it, but the provided timestamps are significantly inaccurate (anchor start/end mismatch and missing anchor end; target times are several seconds off), so it fails to match the reference precisely."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is explaining how to start a consultation, when does he give the example \"how can I help you today?\"",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1069.0,
        "end": 1070.0
      },
      "pred_interval": {
        "start": 1062.0,
        "end": 1064.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 6.0,
        "average": 6.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6242717504501343,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation (target occurs after the anchor) but the reported timestamps are substantially different from the ground truth and omit the anchor end time, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining the 'golden minute', when does he announce the end of the lecture?",
      "video_id": "junFruKQWI0",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1121.0
      },
      "gt_interval": {
        "start": 1090.0,
        "end": 1094.0
      },
      "pred_interval": {
        "start": 1075.0,
        "end": 1078.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 16.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4262295081967213,
        "text_similarity": 0.6889579892158508,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the reported timestamps deviate substantially from the reference (anchor 1073s vs 1089s; target 1075\u20131078s vs 1090\u20131094s), so key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "While Raquel is talking about the hospital providing opportunities for nurses, when is she shown smiling and opening a package?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 2.0,
        "end": 4.5
      },
      "pred_interval": {
        "start": 5.0,
        "end": 10.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 5.5,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.41791044776119407,
        "text_similarity": 0.6284420490264893,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('while') right but gives incorrect timing: it places E2 at 5.0\u201310.0s (outside the speech) and omits the correct E1 end time, contradicting the reference times of E1: ~0.031\u20135.0s and E2: 2.0\u20134.5s."
      }
    },
    {
      "question_id": "002",
      "question": "Once Maria finishes saying that new nurses will be nudged to become lifelong learners, when does Precious state that the teamwork is strong?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 14.321,
        "end": 16.486
      },
      "pred_interval": {
        "start": 15.0,
        "end": 20.0
      },
      "iou": 0.26166578622997017,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6790000000000003,
        "end": 3.5139999999999993,
        "average": 2.0965
      },
      "rationale_metrics": {
        "rouge_l": 0.3728813559322034,
        "text_similarity": 0.7287377119064331,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the ordinal relation ('after') and roughly locates the speech, but the timestamps are inaccurate (anchor off by ~0.3s, target start ~0.7s late and target end significantly too late by ~3.5s), so it does not match the ground truth. "
      }
    },
    {
      "question_id": "003",
      "question": "After Reny states that the hospital does things up to a magnet level, when does Raquel say her values align with the hospital's values?",
      "video_id": "afTFvR7pRqg",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 53.0
      },
      "gt_interval": {
        "start": 42.854,
        "end": 50.692
      },
      "pred_interval": {
        "start": 45.0,
        "end": 50.0
      },
      "iou": 0.6379178361826996,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.146000000000001,
        "end": 0.6920000000000002,
        "average": 1.4190000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.6513775587081909,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps are notably inaccurate compared to the reference (E1/E2 start and end times differ by ~1.5\u20132s), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that healthcare in Siem Reap is not the best, when is the Royal Angkor International Hospital first shown on screen?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 94.0,
        "end": 99.1
      },
      "pred_interval": {
        "start": 60.0,
        "end": 62.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 37.099999999999994,
        "average": 35.55
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.6778352856636047,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the events and their order but gives an incorrect E2 timestamp (60.0s vs correct 94.0s), omits the precise E1 time (82.215s) and the speaker description start (99.100s), so it is largely inaccurate despite the correct 'after' relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing the Neak Tep Hospital, when does he begin describing the Ly Sreyvyna II Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 180.289,
        "end": 185.074
      },
      "pred_interval": {
        "start": 174.0,
        "end": 176.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.288999999999987,
        "end": 9.074000000000012,
        "average": 7.6815
      },
      "rationale_metrics": {
        "rouge_l": 0.43750000000000006,
        "text_similarity": 0.6853097677230835,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after'), but it omits the E1 timestamp and gives an incorrect E2 start (174.0s vs the correct 180.289s) and omits the E2 end (185.074s), so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he visited a clinic for chest congestion, when does he mention the Paschern Dental Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 209.8,
        "end": 211.4
      },
      "pred_interval": {
        "start": 255.0,
        "end": 260.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.19999999999999,
        "end": 48.599999999999994,
        "average": 46.89999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23188405797101447,
        "text_similarity": 0.683914840221405,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the reported timestamps differ substantially from the reference (and the dental clinic name is slightly misspelled), so it fails to match the ground-truth timing and details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing the Neak Tep Hospital, when does he introduce the Ly Sreyvyna II Clinic?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 184.0,
        "end": 184.8
      },
      "pred_interval": {
        "start": 180.0,
        "end": 185.0
      },
      "iou": 0.16000000000000228,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 0.19999999999998863,
        "average": 2.0999999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.7174547910690308,
        "llm_judge_score": 5,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relation and roughly locates E2, but the temporal boundaries disagree notably with the ground truth (E1 off by ~12s and E2 start off by ~4s), so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker introduces the Cigna International Health Policy, when is the insurance quote form displayed with personal information?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 351.0,
        "end": 360.0
      },
      "pred_interval": {
        "start": 335.0,
        "end": 340.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 20.0,
        "average": 18.0
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7737554311752319,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the prediction correctly mentions the Cigna policy and the insurance quote form, it gives completely different timestamps and a different temporal relation ('after' vs. 'once_finished'), so it does not match the reference timing or relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the voiceover states that the Cigna policy is \"fairly typical of policies of this type\", when does the Cigna website display the form for inputting personal details to get a quote?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 456.0
      },
      "gt_interval": {
        "start": 352.9,
        "end": 358.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.100000000000023,
        "end": 12.0,
        "average": 9.550000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2535211267605634,
        "text_similarity": 0.6161086559295654,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after'), but the timestamps are substantially inaccurate (anchor 351.0s vs predicted ~360s; target 352.9\u2013358.0s vs predicted ~370s) and it omits the target's end time, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the voiceover mentions \"evacuation service, also part of Cigna plan\", when is the Global Rescue website displayed on screen?",
      "video_id": "NiGj1YBR4Y4",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 456.0
      },
      "gt_interval": {
        "start": 384.0,
        "end": 431.0
      },
      "pred_interval": {
        "start": 400.0,
        "end": 410.0
      },
      "iou": 0.2127659574468085,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 21.0,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.7887678146362305,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor, the Global Rescue target, and that the target occurs after the anchor, but the timestamps are significantly off (anchor ~11 s late, target ~16 s late) and it omits key timing detail about the page being scrolled and shown until 431.0s."
      }
    },
    {
      "question_id": "001",
      "question": "After the host concludes his introduction about the fight in modern healthcare, when does he introduce Sarah?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 19.4,
        "end": 22.0
      },
      "pred_interval": {
        "start": 30.0,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.600000000000001,
        "end": 10.0,
        "average": 10.3
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.7585868239402771,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the relation ('after') but the timestamps are significantly offset (predicted ~28\u201332s vs. ground truth 18.0\u201322.0s), so the temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While Sarah is introducing herself and her genetic condition, when does she mention having her very first surgery?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.08,
        "end": 108.8
      },
      "pred_interval": {
        "start": 96.0,
        "end": 99.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.079999999999998,
        "end": 9.799999999999997,
        "average": 8.939999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.7881022691726685,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the utterance and the equivalent relation ('while'), but both event timestamps are significantly off from the ground truth (E1 start and E2 start/end differ by ~13s and ~8\u20139s respectively), so the timing is not accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once Sarah finishes describing her role as a volunteer patient representative for a non-profit organization, when does the static image showing her behind a 'CHILDREN'S TUMOR FOUNDATION' table appear?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 185.0,
        "end": 190.0
      },
      "pred_interval": {
        "start": 191.0,
        "end": 198.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 8.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.7589871287345886,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both state the target follows the anchor ('immediately after'), the predicted timestamps conflict substantially with the ground truth: E1 is given as 190s vs the correct 150s, and E2 is 191\u2013198s vs the correct 185\u2013190s, so the timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Sarah finishes explaining the purpose of the 'Shine a Light Walk' to raise money and awareness, when does the video clip showing children running at an outdoor event play?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 189.0,
        "end": 192.0
      },
      "pred_interval": {
        "start": 232.0,
        "end": 238.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.0,
        "end": 46.0,
        "average": 44.5
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.735669732093811,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the clip order and that the target follows immediately, but the timestamps and durations are substantially incorrect (off by ~50+ seconds and different end times), so it contradicts the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Steve asks if the 'Shine a Light Walk' goes throughout the world, when does Sarah begin to explain that the walks do not?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 253.2,
        "end": 258.88
      },
      "pred_interval": {
        "start": 305.0,
        "end": 312.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.80000000000001,
        "end": 53.120000000000005,
        "average": 52.46000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.725555419921875,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the qualitative relation ('immediately after') but the timestamps are substantially off (predicted ~304\u2013312s vs. reference 252.5\u2013258.88s) and the segment durations do not match, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes asking Sarah what things in miscommunication can lead to delays or misdiagnosis, when does the woman start responding?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 362.48,
        "end": 365.44
      },
      "pred_interval": {
        "start": 345.0,
        "end": 348.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.480000000000018,
        "end": 17.439999999999998,
        "average": 17.460000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.619599461555481,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative relationship ('immediately after') but the absolute timestamps and event boundaries are substantially incorrect (predicted ~344\u2013348s vs reference ~356.48\u2013365.44s), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman gives the example of writing 'hyperthyroid instead of hypothyroid', when does the man respond with 'That that's pretty bad'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 389.2,
        "end": 432.5
      },
      "pred_interval": {
        "start": 371.0,
        "end": 373.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.19999999999999,
        "end": 59.5,
        "average": 38.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.5928667783737183,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the man speaks after the woman, but the timestamps differ substantially from the reference and it wrongly characterizes the timing as 'immediately after' despite an actual short pause, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man says he tried researching miscommunication problems, when does he state his finding about thousands of preventable deaths?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 446.56,
        "end": 535.68
      },
      "pred_interval": {
        "start": 443.0,
        "end": 447.0
      },
      "iou": 0.004747518342684484,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5600000000000023,
        "end": 88.67999999999995,
        "average": 46.119999999999976
      },
      "rationale_metrics": {
        "rouge_l": 0.14084507042253522,
        "text_similarity": 0.43136921525001526,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two statements but the timestamps are several seconds off (E1 end and E2 start/end differ notably from ground truth) and it incorrectly characterizes the relation as 'immediately after' despite a clear ~6s separation in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What's in my budget to fix it?\", when does she start asking, \"How important is it to me to fix this issue?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 518.66,
        "end": 522.26
      },
      "pred_interval": {
        "start": 520.0,
        "end": 522.5
      },
      "iou": 0.5885416666666594,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3400000000000318,
        "end": 0.2400000000000091,
        "average": 0.7900000000000205
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.6538853645324707,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the ordering and approximate duration (target is after the anchor), but the reported times are off: the predicted target start (520.0s) is ~1.34s later than the correct 518.66s, and the prediction gives the anchor end rather than the anchor start, so timings do not exactly match."
      }
    },
    {
      "question_id": "002",
      "question": "After the man finishes saying, \"not continuing medical bills,\" when does he start asking, \"So, what does successful self-advocacy look like?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 643.04,
        "end": 646.32
      },
      "pred_interval": {
        "start": 640.0,
        "end": 642.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0399999999999636,
        "end": 4.32000000000005,
        "average": 3.680000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.7062931656837463,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the provided timestamps substantially deviate from the reference (anchor 634.36s vs 640.0s; target 643.04\u2013646.32s vs 640.0\u2013642.0s), making it factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes explaining what a doctor's follow-up might entail, when does the woman start asking, \"Or will I actually be able to get into your office in two weeks?\"",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 679.0,
        "end": 683.92
      },
      "pred_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 13.580000000000041,
        "average": 14.79000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7554097175598145,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the woman's question occurs after the man's explanation, but the reported timestamps and span are substantially incorrect (off by ~17s and misaligned with the reference spans), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Immediately after the woman asks if she should follow up if she is still experiencing symptoms, when does the man ask what if the symptoms go away?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 699.38,
        "end": 707.15
      },
      "pred_interval": {
        "start": 700.0,
        "end": 705.0
      },
      "iou": 0.643500643500645,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6200000000000045,
        "end": 2.1499999999999773,
        "average": 1.384999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.32911392405063294,
        "text_similarity": 0.7811591625213623,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures that the man's question immediately follows the woman's and provides approximate start times very close to the reference, but it is slightly less precise (minor time offsets and omits the E2 end time)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying to voice symptoms and concerns clearly, when does he give an example about shoulder pain?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 734.59,
        "end": 737.0
      },
      "pred_interval": {
        "start": 740.0,
        "end": 745.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.409999999999968,
        "end": 8.0,
        "average": 6.704999999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7741547226905823,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same anchor/target events but gives timestamps several seconds later and does not reflect that the example immediately follows the anchor; thus it contradicts the precise timing in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes warning not to try putting a hand in an electrical outlet, when does the woman agree and say not to try that?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 810.0,
        "end": 812.0
      },
      "pred_interval": {
        "start": 775.0,
        "end": 780.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.0,
        "end": 32.0,
        "average": 33.5
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.7394187450408936,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order ('once finished') correct but the timestamps for both E1 and E2 are substantially wrong (off by ~35s for E1 and ~35s for E2) and do not match the ground truth, so it fails to align on the key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes saying to assume benevolence of your doctor, when does the man begin to speak?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 878.9,
        "end": 879.1
      },
      "pred_interval": {
        "start": 876.0,
        "end": 878.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8999999999999773,
        "end": 1.1000000000000227,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.41935483870967744,
        "text_similarity": 0.6432250738143921,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction contradicts the reference by giving earlier timestamps (876.0s vs 878.0s and 876.0s vs 878.9s) and asserting the man starts immediately (same time) rather than 0.9s after the woman, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man asks about trying non-surgical options first, when does the woman reply 'Yes'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 899.7,
        "end": 900.1
      },
      "pred_interval": {
        "start": 908.0,
        "end": 910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.299999999999955,
        "end": 9.899999999999977,
        "average": 9.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.6323521137237549,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the woman says 'Yes' after the man asks, but it gives incorrect timestamps (908.0s vs 899.5/899.7s) and an inaccurate temporal relation ('immediate' and same timestamp) versus the correct small delay (once_finished, 899.5s\u2192899.7s). These factual timing errors make the answer unreliable."
      }
    },
    {
      "question_id": "003",
      "question": "After the man concludes his statement about how to ask for another opinion, when does the woman respond that asking for another opinion is definitely valid?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 982.0,
        "end": 988.72
      },
      "pred_interval": {
        "start": 996.0,
        "end": 998.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.0,
        "end": 9.279999999999973,
        "average": 11.639999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6020368933677673,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives incorrect timestamps (996.0s vs ground truth 976.0s and 982.0s) and incorrectly labels the relation as 'immediate' rather than 'after' with a gap; these factual/time errors make it largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the man suggests bringing someone along if you're not feeling safe, when does the woman agree that it's advisable?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1127.0,
        "end": 1130.0
      },
      "pred_interval": {
        "start": 1058.0,
        "end": 1070.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.0,
        "end": 60.0,
        "average": 64.5
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6749755144119263,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the woman's agreement and the correct 'after' relationship, but both event timestamps are off by ~69\u201370 seconds compared to the ground truth, causing a significant temporal misalignment; it also adds extra detail not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman talks about a doctor not trusting a patient's pain because they don't act like they're in pain, when does she give an example of a loved one vouching for the patient?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1167.68,
        "end": 1174.48
      },
      "pred_interval": {
        "start": 1100.0,
        "end": 1120.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.68000000000006,
        "end": 54.48000000000002,
        "average": 61.08000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.31372549019607837,
        "text_similarity": 0.6350361108779907,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') but the timestamps are far off from the ground truth and the predicted quote/detail is unsupported; key factual timing and content are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks if it is legal to be given your own medical records, when does the woman confirm that it is?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1268.6,
        "end": 1270.7
      },
      "pred_interval": {
        "start": 1278.0,
        "end": 1280.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.400000000000091,
        "end": 9.299999999999955,
        "average": 9.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.669135332107544,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relative 'after' relationship and the woman's confirming phrase, but the provided timestamps are shifted by about 12 seconds from the reference and the anchor end time is omitted, so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman mentions that things have changed a lot with electronic medical records, when does the man state that bureaucracy reminds him of common barriers?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1333.0,
        "end": 1339.5
      },
      "pred_interval": {
        "start": 1313.0,
        "end": 1315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 24.5,
        "average": 22.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.8660497665405273,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly captures the relative 'after' relationship, both the anchor and target timestamps are significantly inaccurate compared to the ground truth (both start/end times and durations differ), so the prediction fails to match the reference details."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks about common barriers and how to overcome them, when does the woman share her fear of ants?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1377.36,
        "end": 1383.7
      },
      "pred_interval": {
        "start": 1342.0,
        "end": 1345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.3599999999999,
        "end": 38.700000000000045,
        "average": 37.02999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.43749999999999994,
        "text_similarity": 0.8199201822280884,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the anchor and target timestamps differ substantially from the reference (anchor off by ~15s, target start/end off by ~35\u201338s), so the timing information is inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says to write things down on paper and give it to the doctor, when does he mention a doctor refusing to look at the paper?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1484.96,
        "end": 1490.0
      },
      "pred_interval": {
        "start": 1425.0,
        "end": 1435.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.960000000000036,
        "end": 55.0,
        "average": 57.48000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4285714285714285,
        "text_similarity": 0.8096011877059937,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their relation verbally, but it gives a substantially incorrect time window for the target (\u22481425\u20131435s vs. reference 1485\u20131490s), omits the anchor timing, and thus fails on factual timing/alignment."
      }
    },
    {
      "question_id": "002",
      "question": "While the woman discusses prioritizing cognition, when does she state that she would rather be in pain than have her mental capacity harmed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1534.64,
        "end": 1542.24
      },
      "pred_interval": {
        "start": 1465.0,
        "end": 1475.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.6400000000001,
        "end": 67.24000000000001,
        "average": 68.44000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3370786516853933,
        "text_similarity": 0.7578397393226624,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor concept (prioritizing cognition) but gives entirely wrong timestamps for the target event and mislabels the relationship; therefore it fails on key factual details and temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man asks 'Nord, what is that?', when does the woman state what NORD stands for?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1613.4,
        "end": 1615.4
      },
      "pred_interval": {
        "start": 1594.5,
        "end": 1599.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.90000000000009,
        "end": 16.40000000000009,
        "average": 17.65000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.8265435695648193,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target content and their immediate-after relationship, but the provided timestamps are significantly and consistently incorrect compared to the ground truth, so key factual timing information is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says 'I read that I need to start this at 30', when does she explain why she needs the doctor to order it?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1692.24,
        "end": 1711.28
      },
      "pred_interval": {
        "start": 1675.0,
        "end": 1690.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.24000000000001,
        "end": 21.279999999999973,
        "average": 19.25999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.6832358837127686,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterance and that the explanation follows, but the provided timestamps are substantially earlier and do not overlap the reference E1/E2 intervals, so it mislocates the target event."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman explains how to mirror a planned course of action, when does she suggest asking the doctor what they heard?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1797.0,
        "end": 1799.8
      },
      "pred_interval": {
        "start": 1782.0,
        "end": 1790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 9.799999999999955,
        "average": 12.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.8123363852500916,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relative relation ('after') but the provided timestamps are substantially different from the reference and it omits the detail about the brief explanation of miscommunication, so the answer is largely temporally incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man advises to 'just dig' and not use a medical dictionary, when does he ask if medical language can be 'dumbed down'?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1836.56,
        "end": 1841.52
      },
      "pred_interval": {
        "start": 1808.0,
        "end": 1820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.559999999999945,
        "end": 21.519999999999982,
        "average": 25.039999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6816312074661255,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and their temporal relation ('after'), but both E1 and E2 timestamps are substantially incorrect relative to the ground truth (off by ~20\u201330 seconds), so the answer is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks what to do when doctors look rushed, when does the woman describe slowing down and capturing their attention?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1965.6,
        "end": 1973.5
      },
      "pred_interval": {
        "start": 1975.0,
        "end": 1985.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.400000000000091,
        "end": 11.5,
        "average": 10.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.6132090091705322,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and that the target follows, but it omits the anchor timestamp and gives target start/end times that are significantly later than the ground truth, so key temporal details are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes suggesting a doctor might be having a bad day, when does the man humorously ask if doctors have bad days?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2002.5,
        "end": 2004.0
      },
      "pred_interval": {
        "start": 1995.0,
        "end": 2000.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5,
        "end": 4.0,
        "average": 5.75
      },
      "rationale_metrics": {
        "rouge_l": 0.33766233766233766,
        "text_similarity": 0.7909582257270813,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives times that contradict the ground truth: it places E2 before the anchor (1995\u20132000s) while the correct answer has E2 immediately after E1 (starts at 2002.5), so the temporal relation and timestamps are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man introduces the 'five practical tips to advocate for yourself', when does the woman begin talking about writing down questions?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2195.28,
        "end": 2199.7
      },
      "pred_interval": {
        "start": 2180.0,
        "end": 2188.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.2800000000002,
        "end": 11.699999999999818,
        "average": 13.490000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.16,
        "text_similarity": 0.6473011374473572,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events and their temporal relation ('after'), but both reported timestamps are substantially earlier than the ground truth (anchor off by ~44\u201350s and target off by ~15\u201320s), so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the man's explanation about preparing beforehand, when does he demonstrate by pointing to his neck?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2235.0,
        "end": 2237.0
      },
      "pred_interval": {
        "start": 2254.0,
        "end": 2258.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 21.0,
        "average": 20.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.8122886419296265,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both the anchor (2130s vs correct 2225s) and the target interval (2254\u20132258s vs correct 2235\u20132237s) and thus incorrectly labels the temporal relation ('after' vs during); it only loosely matches the event content but is factually wrong on key timings and relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the man describes getting dizzy when walking up and down stairs, when does the woman mention repeating back what was heard?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2316.0,
        "end": 2317.0
      },
      "pred_interval": {
        "start": 2316.0,
        "end": 2324.0
      },
      "iou": 0.125,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.0,
        "end": 7.0,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7222903966903687,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the woman's utterance start (2316.0s \u2248 2315.5s), but the anchor (E1) time is shifted later than the ground truth (2310.0s vs 2305\u20132307s) and the E2 end time is substantially overextended (2324.0s vs 2317.0s). These timing discrepancies reduce temporal accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman expresses her inability to distract herself from the pain, when does the man advise her to be specific?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2368.7,
        "end": 2369.5
      },
      "pred_interval": {
        "start": 2354.0,
        "end": 2360.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.699999999999818,
        "end": 9.5,
        "average": 12.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.759832501411438,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but misidentifies both event time spans by several seconds (E1 and E2 timestamps differ substantially from the ground truth), so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says 'document everything', when does the woman affirm the advice and tell viewers to take notes?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2504.5,
        "end": 2506.0
      },
      "pred_interval": {
        "start": 2505.0,
        "end": 2515.0
      },
      "iou": 0.09523809523809523,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 9.0,
        "average": 4.75
      },
      "rationale_metrics": {
        "rouge_l": 0.35,
        "text_similarity": 0.6906538605690002,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the temporal relation (after) and has E2 start close to the reference, but E1 timing is off by ~5s, E2 end time is far later, and the quoted E2 content contradicts the reference, indicating significant factual mismatches and added/incorrect details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes asking if one should ask permission before recording their doctor, when does the woman respond?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2531.6,
        "end": 2533.5
      },
      "pred_interval": {
        "start": 2530.0,
        "end": 2540.0
      },
      "iou": 0.1900000000000091,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.599999999999909,
        "end": 6.5,
        "average": 4.0499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.31999999999999995,
        "text_similarity": 0.6776812076568604,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the semantic response ('yes, definitely' and that it occurs after), but the timestamps are significantly misaligned with the ground truth (E1 end ~2.3s earlier, E2 start ~1.6s earlier and E2 end ~6.5s later), so the temporal boundaries are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman begins explaining the hope that doctors will focus more on patients with AI recording, when does she explain why she almost always checks her online appointment notes?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2566.0,
        "end": 2579.0
      },
      "pred_interval": {
        "start": 2560.0,
        "end": 2575.0
      },
      "iou": 0.47368421052631576,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 4.0,
        "average": 5.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3544303797468355,
        "text_similarity": 0.7683993577957153,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relationship ('after') and the predicted intervals largely overlap the ground truth; however, the start/end timestamps are several seconds off (\u22484\u20137s), so it is not a perfect match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man asks if one should be assertive, when does he introduce the topic of emotional intelligence?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2701.0,
        "end": 2710.0
      },
      "pred_interval": {
        "start": 2703.0,
        "end": 2708.0
      },
      "iou": 0.5555555555555556,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 2.0,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.6984229683876038,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the qualitative relationship correct and roughly locates E2, but it incorrectly places E1 (should be 2696\u20132697s, not 2701s) and shifts E2 timing, so it misaligns key absolute timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man says 'You wanna learn some breathing control', when does he start describing box breathing?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2740.0,
        "end": 2747.0
      },
      "pred_interval": {
        "start": 2765.0,
        "end": 2774.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.0,
        "end": 27.0,
        "average": 26.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4109589041095891,
        "text_similarity": 0.7510248422622681,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the box-breathing description follows immediately after the prompt, but the absolute timestamps are significantly off (\u224825\u201333 seconds late) and durations differ, so the timing details are factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the man is saying 'If you want, share your story in the comments', when is the 'COMMENT BELOW' graphic displayed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2850.0,
        "end": 2992.0
      },
      "gt_interval": {
        "start": 2920.0,
        "end": 2923.0
      },
      "pred_interval": {
        "start": 2902.0,
        "end": 2908.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 15.0,
        "average": 16.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2247191011235955,
        "text_similarity": 0.5846685171127319,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and states the graphic appears 'shortly after' the speech, contradicting the ground truth which shows the graphic displayed continuously during the man's speech (around 2920.0\u20132923.0)."
      }
    },
    {
      "question_id": "003",
      "question": "After the thumbs up icon appears on screen, when is the next graphic ('COMMENT BELOW') displayed?",
      "video_id": "9DJXPJsuPsc",
      "video_number": "008",
      "segment": {
        "start": 2850.0,
        "end": 2992.0
      },
      "gt_interval": {
        "start": 2920.0,
        "end": 2923.0
      },
      "pred_interval": {
        "start": 2902.0,
        "end": 2908.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 15.0,
        "average": 16.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3666666666666667,
        "text_similarity": 0.8294455409049988,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the 'COMMENT BELOW' graphic occurs after the thumbs-up, but the reported times (2902.0\u20132908.0s) contradict the ground truth (2920.0\u20132923.0s) and thus fails to provide the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Marissa Fourie introduces herself, when does she mention cross-cultural communication?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 34.2,
        "end": 36.5
      },
      "pred_interval": {
        "start": 17.5,
        "end": 18.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.700000000000003,
        "end": 18.0,
        "average": 17.35
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.788038432598114,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' but both event timestamps are incorrect (E1 given as 0.0s vs 8.1s; E2 given as 17.5\u201318.5s vs 34.2\u201336.5s), so it fails to provide the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After mentioning cross-cultural communication, when does Marissa Fourie next mention personality-specific communication skills?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 37.0,
        "end": 39.0
      },
      "pred_interval": {
        "start": 18.5,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.5,
        "end": 19.0,
        "average": 18.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4666666666666667,
        "text_similarity": 0.710513174533844,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps for both events (17.5/18.5\u201320.0s vs correct 34.2/37.0\u201339.0s) and uses the wrong relation ('after' vs 'next'), so it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After encouraging viewers to join PhysioPlus, when does Marissa Fourie say 'See you there!'?",
      "video_id": "Lu_yCNjFZmU",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 72.0
      },
      "gt_interval": {
        "start": 62.9,
        "end": 63.7
      },
      "pred_interval": {
        "start": 69.0,
        "end": 71.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.100000000000001,
        "end": 7.299999999999997,
        "average": 6.699999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.38095238095238093,
        "text_similarity": 0.702798068523407,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the provided timestamps are substantially inaccurate compared to the ground truth (E1 off by ~16s and E2 off by ~6\u20138s), so key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes mentioning \"the dosage in each area\", when does the woman in blue gloves point to the glabella area of the patient's forehead?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 4.469,
        "end": 4.8
      },
      "pred_interval": {
        "start": 3.0,
        "end": 4.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4690000000000003,
        "end": 0.7999999999999998,
        "average": 1.1345
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.6430622339248657,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and that the pointing occurs after the speech, but the reported timestamps are substantially inaccurate (both start/end times are off by over a second) and the duration/visibility details conflict with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining the dosage for the brow lift, when does the woman in blue gloves point to the patient's upper lip?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 12.121,
        "end": 12.5
      },
      "pred_interval": {
        "start": 8.0,
        "end": 9.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.121,
        "end": 3.5,
        "average": 3.8105
      },
      "rationale_metrics": {
        "rouge_l": 0.3513513513513513,
        "text_similarity": 0.7254641056060791,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (pointing occurs after the speaker finishes), but the timestamps are substantially incorrect and the event duration is misreported (predicted ~8.0\u20139.0s vs ground truth 12.080\u201312.500s), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining the dosage for the lip flip, when does the text \"TIME TO INJECT!\" appear on screen?",
      "video_id": "KjUhUHfcT_w",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 21.0
      },
      "gt_interval": {
        "start": 18.291,
        "end": 21.0
      },
      "pred_interval": {
        "start": 16.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2910000000000004,
        "end": 4.0,
        "average": 3.1455
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7045563459396362,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events (speaker finish and on-screen text) but has significant timing errors: E1 is off by ~0.9s, E2 is predicted to start much earlier (16.0s vs 18.291s) and ends at 17.0s instead of remaining until the end, and the relation/temporal placement is incorrectly specified."
      }
    },
    {
      "question_id": "001",
      "question": "Once the host welcomes Rich, when does Rich begin his response?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.015,
        "end": 34.078
      },
      "pred_interval": {
        "start": 30.0,
        "end": 31.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0150000000000006,
        "end": 3.078000000000003,
        "average": 3.0465000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.2909090909090909,
        "text_similarity": 0.8640073537826538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timings conflict significantly with the reference (predicted E1 at ~28.5s vs correct 31.333s, predicted E2 at 30.0s vs correct 33.015s), so the prediction is largely incorrect; it only matches the vague 'after' relationship but misses the correct timestamps and immediacy."
      }
    },
    {
      "question_id": "002",
      "question": "While Rich is explaining how medicine may have let relationships with patients deteriorate, when does he say that scientific facts will protect us?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 89.0,
        "end": 93.76
      },
      "pred_interval": {
        "start": 93.0,
        "end": 95.0
      },
      "iou": 0.1266666666666675,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 1.2399999999999949,
        "average": 2.6199999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.6569892764091492,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target phrase occurs during Rich's explanation, but it misstates both event times substantially (E1 given ~90s vs 73.611s, E2 given 93.0\u201395.0s vs 89.0\u201393.760s), so the temporal information is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the host asks what trust looks like in the future with intermediaries, when does Rich first discuss the stethoscope in relation to technology in medicine?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 112.0,
        "end": 113.0
      },
      "pred_interval": {
        "start": 112.0,
        "end": 114.0
      },
      "iou": 0.5,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.0,
        "end": 1.0,
        "average": 0.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3859649122807018,
        "text_similarity": 0.7903462052345276,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures that Rich's stethoscope remark occurs after the host's question and provides timestamps very close to the reference (within ~0.7\u20131.7s); minor timing discrepancies prevent a perfect score."
      }
    },
    {
      "question_id": "001",
      "question": "After the man in glasses finishes describing the giant TV screen in a new hospital exam room, when does the video show a patient interacting with a screen in a hospital bed?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 167.6,
        "end": 177.6
      },
      "pred_interval": {
        "start": 165.0,
        "end": 170.0
      },
      "iou": 0.19047619047619102,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5999999999999943,
        "end": 7.599999999999994,
        "average": 5.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.8521167039871216,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the event (patient interacting with a screen) and the temporal relation ('after'), but the reported anchor end time (164.0s) and target start (165.0s) deviate notably from the reference (152.8s and 167.6s) and the prediction omits the target end time, so key factual timing details are incorrect or incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "While the interviewer asks if technology can bring doctors and patients closer together, when is he holding a small white 'Trust tv' card?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 178.0,
        "end": 183.5
      },
      "pred_interval": {
        "start": 175.0,
        "end": 180.0
      },
      "iou": 0.23529411764705882,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 3.5,
        "average": 3.25
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7539966106414795,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the semantic relation ('during') but gives incorrect and inconsistent timestamps (starts at 173.0s/175.0s vs correct 178.0s) and omits end times and the fact the card is held throughout the entire 178.0\u2013183.5s speech, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer thanks Rich and says viewers learned a lot, when does Rich respond 'It's really a pleasure'?",
      "video_id": "Xu67ngXWLXI",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 228.0
      },
      "gt_interval": {
        "start": 210.3,
        "end": 212.1
      },
      "pred_interval": {
        "start": 220.0,
        "end": 222.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.699999999999989,
        "end": 9.900000000000006,
        "average": 9.799999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.5352112676056339,
        "text_similarity": 0.8929144740104675,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that Rich's line follows the interviewer's 'thank you', but it gives substantially different timestamps (219.0/220.0s vs 200.0/210.3s) and fails to reflect the immediate follow-up timing stated in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions learning about 'patient rapport', when does he discuss charting and interacting with other healthcare providers?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 2.075,
        "end": 9.55
      },
      "pred_interval": {
        "start": 2.5,
        "end": 10.0
      },
      "iou": 0.8895899053627762,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.4249999999999998,
        "end": 0.4499999999999993,
        "average": 0.43749999999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.7522943615913391,
        "llm_judge_score": 8,
        "llm_judge_justification": "The predicted timestamps closely match the reference (E1 start within the correct range; E2 start/end near the reference), but it omits E1's end time and uses a less specific relation ('after' vs. 'once_finished' meaning immediately follows)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker talks about developing skills like putting an IV, when does he mention getting a patient discharged?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 15.42,
        "end": 24.583
      },
      "pred_interval": {
        "start": 19.0,
        "end": 28.0
      },
      "iou": 0.44379968203497605,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.58,
        "end": 3.4170000000000016,
        "average": 3.498500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.7191226482391357,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/topic (putting an IV) and the target (patient discharge), but the provided timestamps are notably off and the temporal relation is mislabeled\u2014it's given as 'after' rather than the correct immediate 'once_finished', so key temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Make their problem, your problem', when does he introduce the importance of self-care?",
      "video_id": "6vqzBk6z6U4",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 45.009,
        "end": 48.396
      },
      "pred_interval": {
        "start": 39.0,
        "end": 44.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.009,
        "end": 4.396000000000001,
        "average": 5.202500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.345679012345679,
        "text_similarity": 0.7442755699157715,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('after') and the two events right in concept, but the timestamps are substantially incorrect (E1/E2 times are shifted and even overlap compared to the reference), which contradicts the correct timing."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of herself, when does she mention specializing in wounds?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 22.605,
        "end": 26.329
      },
      "pred_interval": {
        "start": 18.0,
        "end": 21.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.605,
        "end": 5.329000000000001,
        "average": 4.9670000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2465753424657534,
        "text_similarity": 0.3947182595729828,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the speaker says she specializes in wounds and that this occurs during her introduction, but both event timestamps are inaccurate (anchor mis-specified as 0.0s instead of ~18.12s, and the target is placed at ~18.0\u201321.0s instead of ~22.61\u201326.33s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the topic of 'getting the most out of your GP consultation', when does she mention that GP practices are getting a huge injection of funding?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.82,
        "end": 75.533
      },
      "pred_interval": {
        "start": 44.0,
        "end": 49.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.819999999999993,
        "end": 26.533,
        "average": 25.176499999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962025,
        "text_similarity": 0.6885678172111511,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the provided timestamps for both the topic introduction and the funding mention are substantially incorrect compared to the reference, so the answer is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "While the slide titled 'Appointments are precious' is on screen, when does the speaker mention that GP practices are moving back towards face-to-face appointments?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 123.0,
        "end": 129.0
      },
      "pred_interval": {
        "start": 76.0,
        "end": 80.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.0,
        "end": 49.0,
        "average": 48.0
      },
      "rationale_metrics": {
        "rouge_l": 0.36842105263157887,
        "text_similarity": 0.6734668016433716,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('while') correct but both the slide (E1) and speaker mention (E2) timestamps are incorrect and do not match the ground truth, omitting the correct timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that GP practices are very different places now, when does she begin listing the specific roles in a GP practice?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 203.0,
        "end": 204.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 11.0,
        "average": 9.0
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6590880751609802,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') right but the timestamps are significantly incorrect (E1 predicted at 210.0s vs 185.8s; E2 at 215.0s vs 203.0s and wrong span), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide displays the question 'Does it need to be a GP?', when does the speaker mention that paramedics work in primary care?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "pred_interval": {
        "start": 225.0,
        "end": 230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 10.0,
        "average": 10.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4594594594594595,
        "text_similarity": 0.7628368139266968,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation but both event timestamps are substantially off (E1 predicted ~35s late and E2 ~10\u201315s early) so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker talks about paramedics working in primary care, when does she begin to explain the role of Advanced Clinical Practitioners?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 241.0,
        "end": 249.0
      },
      "pred_interval": {
        "start": 240.0,
        "end": 245.0
      },
      "iou": 0.4444444444444444,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 4.0,
        "average": 2.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.7366880774497986,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly indicates the ACP discussion begins around 240s and is after the paramedics segment, but it omits the anchor's end time (\u2248238.5s) and has slight timestamp differences (240.0s vs 241.0s), so it's not an exact match."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the problem of a wound on your foot, when does she strongly advise mentioning if you are diabetic?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.875,
        "end": 343.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 360.0
      },
      "iou": 0.17083333333333334,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.875,
        "end": 17.0,
        "average": 12.4375
      },
      "rationale_metrics": {
        "rouge_l": 0.27848101265822783,
        "text_similarity": 0.6783781051635742,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that the diabetic warning occurs immediately after the wound introduction and gives a close start time (340s vs ~337.9s), but it misstates the anchor start and significantly inflates the target end time, so the timestamps are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses having a new wound on your leg, when does she suggest going to a local pharmacist for simple dressings?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 363.968,
        "end": 366.552
      },
      "pred_interval": {
        "start": 360.0,
        "end": 420.0
      },
      "iou": 0.04306666666666672,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9680000000000177,
        "end": 53.44799999999998,
        "average": 28.708
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.8170989751815796,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that pharmacist advice comes after the prior discussion, but the timestamps are mostly incorrect (E2 times diverge greatly from the reference) and the anchor end time is omitted, so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker explains that a nurse's appointment is needed for long-standing wounds, when does she advise to clearly state how long the wound has been there?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 409.579,
        "end": 439.62
      },
      "pred_interval": {
        "start": 420.0,
        "end": 450.0
      },
      "iou": 0.48539125701986613,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.420999999999992,
        "end": 10.379999999999995,
        "average": 10.400499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2588235294117647,
        "text_similarity": 0.8248499631881714,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the content and that the advice follows the explanation, but the timestamps are notably inaccurate (E2 placed ~10s early, E1 end omitted) and the temporal relationship is misaligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if you feel more short of breath, when does she state that a GP or nurse practitioner might be needed the same day?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 522.783,
        "end": 525.113
      },
      "pred_interval": {
        "start": 514.5,
        "end": 518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.283000000000015,
        "end": 7.113000000000056,
        "average": 7.698000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6577911376953125,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and the content of the target and labels the relation as 'after', but the target timestamps are significantly off (~7.6s earlier than the reference), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises to measure your ankle and calf, when does she give an example of a calf measurement that would 'perk up more interest'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 583.623,
        "end": 586.297
      },
      "pred_interval": {
        "start": 556.0,
        "end": 565.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.623000000000047,
        "end": 21.297000000000025,
        "average": 24.460000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.75677889585495,
        "llm_judge_score": 2,
        "llm_judge_justification": "The anchor time is close to the reference, but the predicted target times are significantly incorrect (556.0\u2013565.0s vs. 583.623\u2013586.297s), misplacing and overextending the example; only the 'after' relation is correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the slide changes to 'Photography', when does the speaker advise to 'expect to be asked for a photo'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 670.384,
        "end": 672.807
      },
      "pred_interval": {
        "start": 622.5,
        "end": 626.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.884000000000015,
        "end": 46.807000000000016,
        "average": 47.345500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.8253259658813477,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'once' relationship, but the provided timestamps (anchor 622.0s; target 622.5\u2013626.0s) conflict substantially with the correct times (anchor 650.676s; target 670.384\u2013672.807s), so the timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions some GP practices use video consultations, when does she state that a good quality photograph is better than a video?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.278,
        "end": 717.251
      },
      "pred_interval": {
        "start": 698.0,
        "end": 705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.27800000000002,
        "end": 12.250999999999976,
        "average": 14.264499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.6824116110801697,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the overall relation ('after') right but misidentifies and conflates the two events, assigns incorrect event labels (anchor vs target), and gives timestamps that differ substantially from the reference, so it fails to match the key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the slide changes to 'Photography tips', when does the speaker begin discussing taking a close-up and further-away picture?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 738.601,
        "end": 740.91
      },
      "pred_interval": {
        "start": 710.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.601,
        "end": 20.909999999999968,
        "average": 24.755499999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.34920634920634924,
        "text_similarity": 0.6945228576660156,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction names the same events but gives substantially incorrect timestamps (710s vs 736.057s and 710\u2013720s vs 738.601s) and the temporal relation ('when') contradicts the reference ('once_finished'), so it fails to match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the slide changes to 'General top tips- face to face appointments', when does the speaker advise to 'Go suitably dressed'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 860.136,
        "end": 860.846
      },
      "pred_interval": {
        "start": 756.0,
        "end": 765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.13599999999997,
        "end": 95.846,
        "average": 99.99099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.6792950630187988,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the relative relation ('after'), both timestamps for the slide change and the advice are incorrect and do not match the ground truth (they are shifted substantially earlier), and the prediction adds an unnecessary end time."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises not to wear tight socks, trousers, or wellies, when does she suggest wearing something with quick access to lower limbs?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 873.0,
        "end": 877.5
      },
      "pred_interval": {
        "start": 870.0,
        "end": 880.0
      },
      "iou": 0.45,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 2.5,
        "average": 2.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7194587588310242,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor content and the 'after' relation and captures the E2 wording, but it misrepresents the timing for E2 (lists start at 870.0s instead of the correct 873.0\u2013877.5s), a factual error."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising not to make chit-chat about the weather, when does she advise not to dodge the real problem?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 893.0,
        "end": 894.5
      },
      "pred_interval": {
        "start": 890.0,
        "end": 900.0
      },
      "iou": 0.15,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 5.5,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.679538369178772,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies E1 end time and the content, but it incorrectly gives E2's start as 890.0s (should be 893.0s) and labels the relation as 'after' rather than the specified 'once_finished', so timing and relation are not accurately matched."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises to take a list of the medications you are actually taking, when does she advise against describing tablets by their appearance?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 948.0,
        "end": 969.0
      },
      "pred_interval": {
        "start": 1000.0,
        "end": 1010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.0,
        "end": 41.0,
        "average": 46.5
      },
      "rationale_metrics": {
        "rouge_l": 0.26506024096385544,
        "text_similarity": 0.7160791158676147,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction paraphrases the content correctly but gives incorrect and inconsistent timestamps (both events at 1000s) that contradict the true temporal relation and the provided event times, so the timing is substantially wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises speaking to the practice in advance about a relative, when does she explain the reason for this advance arrangement due to confidentiality?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1065.0,
        "end": 1095.0
      },
      "pred_interval": {
        "start": 1059.0,
        "end": 1077.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 18.0,
        "average": 12.0
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.667502760887146,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same events but has incorrect temporal boundaries (E2 1059\u20131077s vs correct 1065\u20131095s), omits E1's end time (correct 1055s), and labels the relation as 'after' instead of the more specific 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker suggests writing things down before an appointment to help structure what you say, when does she first ask 'How did it start?' regarding the leg problem?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1130.415,
        "end": 1131.738
      },
      "pred_interval": {
        "start": 1107.0,
        "end": 1115.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.414999999999964,
        "end": 16.738000000000056,
        "average": 20.07650000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.41791044776119407,
        "text_similarity": 0.7266845107078552,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the content of E1 broadly correct but provides incorrect timestamps for both events and a different temporal relation ('after' vs. the correct immediate 'once_finished'), omitting the key detail that E2 starts immediately after E1 ends."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes advising to ask to be referred to a specialist service, when does she start introducing the referrals examples?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1248.105,
        "end": 1249.385
      },
      "pred_interval": {
        "start": 1237.0,
        "end": 1250.0
      },
      "iou": 0.09846153846153637,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.105000000000018,
        "end": 0.6150000000000091,
        "average": 5.860000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.4944012761116028,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the relation right and E1 is close to the reference, but E2's start time is off by ~11 seconds (1237.0s vs 1248.105s) and the predicted answer omits the correct end time, so it is substantially inaccurate/incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that lymphoedema services can be patchy, when does she first advise writing to your MP?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1377.0,
        "end": 1378.0
      },
      "pred_interval": {
        "start": 1344.0,
        "end": 1350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 28.0,
        "average": 30.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2758620689655172,
        "text_similarity": 0.7318326234817505,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the anchor event roughly right and the relation ('after') correct, but the target time is significantly wrong (predicted 1344.0s vs actual 1377.0s), so it fails on the key factual timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions that a GP will assess new leg swelling for onward referral, when does she explain there are many different causes?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1429.846,
        "end": 1432.0
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1430.0
      },
      "iou": 0.01283333333333303,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.846000000000004,
        "end": 2.0,
        "average": 5.923000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.7446069717407227,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the events and the 'after' relation, but the timestamps are substantially inaccurate (E1 off by ~14s and E2 off by ~10s and placed immediately after E1), so temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what information you could take with you, when does she suggest looking up the National Wound Care Strategy Lower Limb Recommendations?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1465.0,
        "end": 1469.5
      },
      "pred_interval": {
        "start": 1480.0,
        "end": 1490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 20.5,
        "average": 17.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.6311715841293335,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates E2 occurs shortly after E1 but omits the timestamps and explicitly labels the relation as 'when', which contradicts the ground truth 'after' and thus is materially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions escalating concerns to the practice manager, when does she mention escalating concerns to your MP?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1523.6,
        "end": 1525.7
      },
      "pred_interval": {
        "start": 1520.0,
        "end": 1530.0
      },
      "iou": 0.21000000000001365,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.599999999999909,
        "end": 4.2999999999999545,
        "average": 3.949999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.6521819233894348,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys that the MP mention occurs after the practice-manager mention, but it omits the precise timestamps/durations given in the reference and uses a vague 'after' rather than the specific 'next' relation and exact timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says 'I'll stop sharing', when does she start reading the first question from a viewer?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1574.5,
        "end": 1578.5
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.5,
        "end": 21.5,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6733270287513733,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that the speaker stops sharing and then immediately reads the first question, matching the temporal ordering, but it omits the precise timestamps given in the reference and uses a less specific relation label ('once' vs 'once_finished')."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker initially suggests the mum needs compression hosiery, when does she mention asking for an appointment with the nurse for stronger compression?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1654.942,
        "end": 1664.2
      },
      "pred_interval": {
        "start": 1596.0,
        "end": 1608.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.94200000000001,
        "end": 56.200000000000045,
        "average": 57.571000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.5142857142857142,
        "text_similarity": 0.8850269317626953,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the event timestamps are substantially incorrect\u2014especially the target (predicted ~1596\u20131608s vs correct 1654.942\u20131664.2s)\u2014so it fails to match the ground truth intervals."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'That is such a good question', when does she state that self-diagnosis via the internet is never a good idea?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1757.815,
        "end": 1762.821
      },
      "pred_interval": {
        "start": 1656.0,
        "end": 1668.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.81500000000005,
        "end": 94.82099999999991,
        "average": 98.31799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.575,
        "text_similarity": 0.8634328842163086,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and the quoted phrases, but both the anchor and target timestamps are substantially incorrect (off by dozens to hundreds of seconds) and the target end time is wrong, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that approaching a GP is about framing the conversation, when does she tell the viewer not to worry about being labeled a difficult patient?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1795.335,
        "end": 1798.383
      },
      "pred_interval": {
        "start": 1734.0,
        "end": 1750.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.335000000000036,
        "end": 48.38300000000004,
        "average": 54.85900000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.30952380952380953,
        "text_similarity": 0.7808782458305359,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('after') and event descriptions right, but both anchor and target timestamps are substantially inaccurate (off by ~55\u201360s) and the predicted target interval does not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker first says, 'Please don't worry about things like that', when does she next advise not to worry about being labelled as a difficult patient?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.66,
        "end": 1831.19
      },
      "pred_interval": {
        "start": 1815.0,
        "end": 1825.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.660000000000082,
        "end": 6.190000000000055,
        "average": 9.425000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.17910447761194032,
        "text_similarity": 0.6666980385780334,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps are notably off (anchor ~8s late, target ~12.7s early) and it omits the target end time; therefore it does not accurately match the precise instances given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, 'What can I do to maintain healthy legs or feet so I don't get any problems?', when does she start listing actions like 'walk' and 'legs up'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1865.412,
        "end": 1883.383
      },
      "pred_interval": {
        "start": 1835.0,
        "end": 1850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.412000000000035,
        "end": 33.38300000000004,
        "average": 31.897500000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6710028648376465,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gives anchor and target times that are ~20\u201330 seconds earlier than the ground truth, so the temporal alignment is incorrect; it correctly identifies the relation as 'after' but fails to match the specified intervals or exact timing of the listed actions."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks how much is in the GP curriculum, when does she say 'I don't know'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1983.7,
        "end": 1984.201
      },
      "pred_interval": {
        "start": 1967.5,
        "end": 1968.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.200000000000045,
        "end": 16.201000000000022,
        "average": 16.200500000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6366223692893982,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target pair and that 'I don't know' immediately follows, but the provided timestamps are significantly off (\u224814\u201317 seconds earlier) compared to the ground truth, so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'I think it is something that Legs Matter can help with', when does she discuss Legs Matter influencing GP curriculums?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2004.063,
        "end": 2009.063
      },
      "pred_interval": {
        "start": 1976.0,
        "end": 1980.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.063000000000102,
        "end": 29.063000000000102,
        "average": 28.563000000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.6355121731758118,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative relation ('after') right, but both the anchor and target timestamps are substantially different from the ground truth ranges (off by ~16\u201329s) and the predicted target interval/end time do not match, so the timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if seeing a nurse practitioner is appropriate, when does she state that nurse practitioners are 'extremely experienced clinicians'?",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2062.584,
        "end": 2066.851
      },
      "pred_interval": {
        "start": 1989.0,
        "end": 1990.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.58399999999983,
        "end": 76.85100000000011,
        "average": 75.21749999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.5306441783905029,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the immediate relationship and the quoted phrase, but the provided timestamps are substantially incorrect (off by ~70 seconds) and do not match the ground-truth anchor/target intervals."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"I understand the issue of smartphones and taking pictures too\", when does she first ask \"is there somebody who can help you?\"",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2320.0
      },
      "gt_interval": {
        "start": 2174.0,
        "end": 2176.0
      },
      "pred_interval": {
        "start": 2150.0,
        "end": 2152.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 24.0,
        "average": 24.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.5860131978988647,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct utterances and the 'after' relation, but the timestamp locations are substantially incorrect (off by ~17 seconds from the ground truth), so the temporal localization is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the speaker discusses the importance of planning phone calls to the GP, when does she ask, \"What am I feeling?\"",
      "video_id": "htFE6LPBdHY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2320.0
      },
      "gt_interval": {
        "start": 2197.721,
        "end": 2198.663
      },
      "pred_interval": {
        "start": 2172.0,
        "end": 2174.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.721000000000004,
        "end": 24.66300000000001,
        "average": 25.192000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.15873015873015872,
        "text_similarity": 0.660722553730011,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relationship, but the reported timestamps are incorrect: the true target is at ~2197.7s while the prediction gives 2172\u20132174s (\u224825s earlier) and misstates the anchor timing, so it fails factual temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Angelos finishes introducing Dr. Tolchin, when does Dr. Tolchin begin speaking?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.128,
        "end": 109.393
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.128,
        "end": 92.893,
        "average": 91.51050000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.7815024852752686,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order right (Dr. Tolchin speaks after the introduction) but the timestamps and duration are drastically incorrect (14.8/15.0s vs 100.128/105.128\u2013109.393s) and thus fails on key factual details; the relation wording is a minor mismatch but less important than the wrong timings."
      }
    },
    {
      "question_id": "002",
      "question": "After Dr. Angelos describes Dr. Tolchin's research on crisis standards of care, when does he describe his research on functional neurological disorders and epilepsy?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.426,
        "end": 116.456
      },
      "pred_interval": {
        "start": 45.0,
        "end": 48.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.426000000000002,
        "end": 68.456,
        "average": 39.941
      },
      "rationale_metrics": {
        "rouge_l": 0.2247191011235955,
        "text_similarity": 0.7410368919372559,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') right but the timestamps are substantially incorrect and contradict the reference (E1/E2 timings and sequence), omitting the correct interval boundaries and thus failing on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes stating the second learning objective, when does he start explaining the third learning objective?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 167.0,
        "end": 181.0
      },
      "pred_interval": {
        "start": 155.0,
        "end": 162.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.0,
        "end": 19.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.6894822120666504,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction has completely incorrect timestamps and span (155.0/162.0s vs ground-truth 16.4/17.0\u201331.0s) and thus contradicts the reference; calling it 'immediately after' is a minor relation match but does not compensate for the major factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "While the slide titled 'Why conduct clinical ethics consultations?' is displayed, when does the speaker discuss moral distress among clinicians and staff?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 285.4,
        "end": 304.0
      },
      "pred_interval": {
        "start": 200.0,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.39999999999998,
        "end": 94.0,
        "average": 89.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.34210526315789475,
        "text_similarity": 0.6980039477348328,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect start/end times for both events and the wrong relation: the reference has E1 at 181.7\u2013307.6 and E2 at 285.4\u2013304.0 (E2 occurs during E1), whereas the prediction places both at 200.0\u2013210.0 and claims they are simultaneous, which is a major timing and relation mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that clinical ethics consultations were helpful, when does he state that they were more likely to achieve consensus in clinical decisions?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 350.2,
        "end": 357.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 360.0
      },
      "iou": 0.4533333333333341,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.199999999999989,
        "end": 3.0,
        "average": 4.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.4482758620689655,
        "text_similarity": 0.7036348581314087,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the time annotations are notably different from the reference (E1 finish omitted and predicted E2 starts earlier and ends later than ground truth), so it omits and alters key temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the topic of resource utilization, when does he specifically state that there was a reduced length of stay?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 438.9,
        "end": 450.3
      },
      "pred_interval": {
        "start": 390.0,
        "end": 405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.89999999999998,
        "end": 45.30000000000001,
        "average": 47.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.4137931034482759,
        "text_similarity": 0.727481484413147,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('after'), its timestamps for both the anchor and target are substantially different from the reference, failing to identify the correct time points."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'to look at disparities', when does he begin to introduce Ellen Fox's team and their survey?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 493.5,
        "end": 499.0
      },
      "pred_interval": {
        "start": 480.0,
        "end": 495.0
      },
      "iou": 0.07894736842105263,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.5,
        "end": 4.0,
        "average": 8.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4262295081967213,
        "text_similarity": 0.7210603952407837,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates Ellen Fox's team is introduced after the anchor, but it provides substantially different timestamps (anchor 465.0s vs 393.0s; target 480.0\u2013495.0s vs 493.5\u2013499.0s) and weakens the relation ('after' vs 'once_finished'), so it is mostly incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says 'hospitals with less than 400 beds', when does he mention 'little or no growth over that two decade period'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 527.809,
        "end": 530.91
      },
      "pred_interval": {
        "start": 530.0,
        "end": 534.0
      },
      "iou": 0.14698756259085183,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.191000000000031,
        "end": 3.090000000000032,
        "average": 2.6405000000000314
      },
      "rationale_metrics": {
        "rouge_l": 0.45,
        "text_similarity": 0.799103856086731,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target and that the target occurs immediately after the anchor, and the predicted starts fall within the true spans; however the anchor time is shifted later and the predicted target end is significantly extended beyond the correct interval, so timing details are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide titled 'Prior Healthcare System Ethics Committees' is fully displayed, when do the images of the six hospitals with their bed counts appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 551.7,
        "end": 552.0
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.299999999999955,
        "end": 53.0,
        "average": 50.64999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.30555555555555564,
        "text_similarity": 0.7417649626731873,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the images appear after the slide (relationship preserved) but the reported timestamps and duration are substantially incorrect compared to the ground-truth times, contradicting key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states the number of ethics consults at Yale New Haven Hospital increased from 50 to 239, when does he describe this as 'approximately a five-fold increase in consult volume'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.7,
        "end": 624.7
      },
      "pred_interval": {
        "start": 635.0,
        "end": 640.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.299999999999955,
        "end": 15.299999999999955,
        "average": 13.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.379746835443038,
        "text_similarity": 0.7506844997406006,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct anchor and target phrases but the timestamps are substantially off and do not match the provided anchor/target intervals; the temporal alignment (anchor completion and immediate follow) is therefore incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker initially mentions the 'Community Bioethics Forum', when does he start describing its community members?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 887.216,
        "end": 905.918
      },
      "pred_interval": {
        "start": 885.0,
        "end": 900.0
      },
      "iou": 0.6111482933358824,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.216000000000008,
        "end": 5.918000000000006,
        "average": 4.067000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.9280878901481628,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct relation ('after') and roughly locates both mentions, but the timestamps are noticeably imprecise (anchor time is earlier than the true interval and the target start/end are shifted and shortened), and the anchor interval end is omitted."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that the primary focus of the Center for Clinical Ethics has been ethics education, when does he start listing 'Systemwide Ethics Forum and Newsletter'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1055.54,
        "end": 1069.28
      },
      "pred_interval": {
        "start": 960.0,
        "end": 975.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 95.53999999999996,
        "end": 94.27999999999997,
        "average": 94.90999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.9161547422409058,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation as 'after' but both event times are substantially misaligned: E1 is placed at 955.0s (should be 938\u2013948s) and E2 is placed at 960\u2013975s instead of 1055.54\u20131069.28s, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker lists 'ICU Walk Rounds', when does he mention 'HEC-C Certification'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1048.0,
        "end": 1052.0
      },
      "pred_interval": {
        "start": 1035.0,
        "end": 1045.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 7.0,
        "average": 10.0
      },
      "rationale_metrics": {
        "rouge_l": 0.40579710144927533,
        "text_similarity": 0.8909868001937866,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (target after anchor) but the timestamps and spans are substantially incorrect (both events placed ~12\u201313s earlier and wrong end times), so it fails to match the reference precisely."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"ethics consultation services,\" when does he start talking about collecting feedback?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1240.8,
        "end": 1249.8
      },
      "pred_interval": {
        "start": 1235.0,
        "end": 1242.0
      },
      "iou": 0.0810810810810844,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.7999999999999545,
        "end": 7.7999999999999545,
        "average": 6.7999999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.339622641509434,
        "text_similarity": 0.6692949533462524,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures that the feedback discussion follows the ethics remark, but the timestamps are significantly incorrect (E1 predicted 1235.0s vs correct 1238.9s; E2 predicted 1235.0s vs correct 1240.8s) and it wrongly asserts they are simultaneous rather than beginning ~1.9s later; it also adds an unnecessary end time."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that participant satisfaction is not the \"be-all and end-all,\" when does he say they have begun the survey process with clinicians?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.3,
        "end": 1282.8
      },
      "pred_interval": {
        "start": 1254.0,
        "end": 1262.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.299999999999955,
        "end": 20.799999999999955,
        "average": 22.549999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.43750000000000006,
        "text_similarity": 0.5921193361282349,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misstates the timestamps (1254.0/1254.0 vs 1275.0/1278.3), incorrectly aligns E2 with E1, and labels the relation differently, so it fails to match the correct temporal details despite preserving a vague 'after' ordering."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes describing the first pie chart about helpful advice/guidance, when does the second pie chart about communication appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1367.5,
        "end": 1367.9
      },
      "pred_interval": {
        "start": 1345.0,
        "end": 1352.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.5,
        "end": 15.900000000000091,
        "average": 19.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.7768818140029907,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts key facts: both E1 and E2 timestamps differ substantially from the ground truth (1345/1345 vs 1356/1376.5) and it asserts the second chart appears immediately after the first, whereas the reference places it 20.5s later; thus it fails on factual accuracy and timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he wants to turn to some of the organizational ethics consultation work, when does the slide showing the 'Organizational ethics consultations' table appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1472.0,
        "end": 1472.5
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.0,
        "end": 47.5,
        "average": 49.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.6123656630516052,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative order (slide appears after the introduction) but the reported timestamps are substantially off from the ground truth (errors of ~24\u201352s) and it omits end times, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining that organizational ethics work is new to them, when do they state that it began during the COVID pandemic?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1469.5,
        "end": 1472.0
      },
      "pred_interval": {
        "start": 1435.0,
        "end": 1440.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.5,
        "end": 32.0,
        "average": 33.25
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.5596233010292053,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a wrong start time for E2 (1435.0s vs correct 1469.5s\u20131472.0s), states an incorrect temporal relation ('after') and omits the precise E1/E2 timestamps, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the display of the 'Organizational ethics consultations' table, when does the speaker mention the 'Blood products scarcity protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1510.0,
        "end": 1513.0
      },
      "pred_interval": {
        "start": 1455.0,
        "end": 1460.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.0,
        "end": 53.0,
        "average": 54.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3666666666666667,
        "text_similarity": 0.6848576068878174,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the items but gives a wrong timestamp (1455s vs the correct 1510\u20131513s) and omits the anchor's display interval (1474\u20131573s); claiming the event is 'during' contradicts the incorrect start time."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the 'sequential organ failure assessment or SOFA score', when does he begin to explain what it is?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1647.6,
        "end": 1697.0
      },
      "pred_interval": {
        "start": 1623.0,
        "end": 1635.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.59999999999991,
        "end": 62.0,
        "average": 43.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.3255813953488372,
        "text_similarity": 0.6615589261054993,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the anchor start roughly correct and captures that the explanation follows the introduction, but the target timestamps are substantially wrong (and earlier than reported) and it hallucinates details about a '25-point scale' and incorrect end time, so it fails on key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that '70% of publicly available crisis standards of care used either the SOFA score or a modified version', when does he mention the SOFA score being used in Alaska?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1726.0,
        "end": 1733.0
      },
      "pred_interval": {
        "start": 1690.0,
        "end": 1695.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.0,
        "end": 38.0,
        "average": 37.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.6570315361022949,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances (the '70%' statement and the Alaska example) but the timestamps are significantly incorrect and the claimed temporal relationship ('immediately after') contradicts the ground truth, so it does not match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the 'SOFA Disparities' slide appears, when does the speaker begin discussing concerns about the score's accuracy and contributions to disparities?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1770.0,
        "end": 1776.606
      },
      "pred_interval": {
        "start": 1750.0,
        "end": 1765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 11.605999999999995,
        "average": 15.802999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.8328059911727905,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the qualitative relationship (speaker addresses the topic immediately) but the anchor and target timestamps (E1/E2 start and E2 end) differ substantially from the ground truth, so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that the center was able to test the triage protocol before it was used, when does he state that they developed a SOFA calculation system?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1799.553,
        "end": 1807.997
      },
      "pred_interval": {
        "start": 1790.0,
        "end": 1805.0
      },
      "iou": 0.3026615547035543,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.553000000000111,
        "end": 2.997000000000071,
        "average": 6.275000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.39473684210526316,
        "text_similarity": 0.8484352827072144,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and the SOFA content, but the timestamps are substantially incorrect (E1 ~10.5s early; E2 start ~9.6s early) and E1 end time is omitted, so it is inaccurate and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the retrospective cohort study, when does he detail the demographic breakdown of the patients?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1846.122,
        "end": 1858.077
      },
      "pred_interval": {
        "start": 1810.0,
        "end": 1825.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.12200000000007,
        "end": 33.077,
        "average": 34.599500000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164387,
        "text_similarity": 0.8155437707901001,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('after') but the event timestamps are substantially incorrect (off by tens of seconds) and it introduces specific demographic numbers not present in the reference, indicating hallucination."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker highlights that non-Hispanic Black patients had greater odds of an elevated SOFA score, when does he state that no significant difference by race in mortality was found when controlling for other factors?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1873.642,
        "end": 1879.694
      },
      "pred_interval": {
        "start": 1830.0,
        "end": 1845.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.64200000000005,
        "end": 34.69399999999996,
        "average": 39.168000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.48101265822784817,
        "text_similarity": 0.8130237460136414,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that E2 occurs after E1, but the provided timestamps differ substantially from the reference (off by ~40\u201350 seconds), so the timing information is incorrect and does not match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the early small cohort out of Wuhan, China, when does he state that subsequent larger cohorts in the United States did not show such high accuracy rates?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1959.0,
        "end": 1966.5
      },
      "pred_interval": {
        "start": 1958.0,
        "end": 1968.0
      },
      "iou": 0.75,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0,
        "end": 1.5,
        "average": 1.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2888888888888889,
        "text_similarity": 0.7713332176208496,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction captures the correct events and the 'after' relationship and even quotes the phrase, but the anchor start time is ~4.1s early and the target boundaries are slightly shifted (start ~1s early, end ~1.5s late), so timings are not exact."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'This graph here is a calibration curve', when does he explain that the diagonal line shows a perfectly calibrated predictor of mortality?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2014.0,
        "end": 2020.0
      },
      "pred_interval": {
        "start": 1995.0,
        "end": 2002.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 18.0,
        "average": 18.5
      },
      "rationale_metrics": {
        "rouge_l": 0.28260869565217384,
        "text_similarity": 0.7712346315383911,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the explanation follows the graph) and locates the target roughly in the same region, but the anchor start time is greatly off and the predicted target times are offset from the reference, so the timing alignment is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that SOFA predicted mortality with less accuracy than age in their own COVID cohort, when does he mention that SOFA predicted mortality with better accuracy than age in the pre-COVID eICU cohort?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2066.0,
        "end": 2069.0
      },
      "pred_interval": {
        "start": 2025.0,
        "end": 2035.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 34.0,
        "average": 37.5
      },
      "rationale_metrics": {
        "rouge_l": 0.32967032967032966,
        "text_similarity": 0.724882960319519,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the content and relative order (that the pre-COVID eICU contrast comes after), but the provided timestamps are substantially different from the reference (both anchor and target times and durations are incorrect), so it is largely temporally inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Omicron surge increasing, when does he talk about working with the healthcare system's legal team?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2153.6,
        "end": 2174.93
      },
      "pred_interval": {
        "start": 2148.0,
        "end": 2156.0
      },
      "iou": 0.08911994058671019,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.599999999999909,
        "end": 18.929999999999836,
        "average": 12.264999999999873
      },
      "rationale_metrics": {
        "rouge_l": 0.36923076923076925,
        "text_similarity": 0.7573589086532593,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the ordering right but the time spans are inaccurate and incomplete: E1 is given only vaguely (before 2148s vs ~2132s) and E2 is shifted earlier and ends much sooner (2148\u20132156s vs 2153.6\u20132174.93s), so key temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating the policy was active until late February of 2022, when does the first 'Scope of protocol' slide appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2194.0,
        "end": 2234.0
      },
      "pred_interval": {
        "start": 2170.0,
        "end": 2175.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 59.0,
        "average": 41.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.7808490991592407,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order ('after') right but the timestamps are significantly incorrect (E1 given as before 2170s vs 2192s true, E2 given 2170\u20132175s vs 2194\u20132234s true) and it misstates E2's duration, so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the second 'Scope of protocol' slide appears, when does the speaker mention 'renal replacement therapy'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2263.679,
        "end": 2254.733
      },
      "pred_interval": {
        "start": 2205.0,
        "end": 2212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.67900000000009,
        "end": 42.733000000000175,
        "average": 50.70600000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.85036301612854,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps for both E1 and E2 are materially incorrect and conflict with the reference times, omitting the precise times given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that goals of care discussions significantly changed, when does the speaker mention that patients were more likely to choose limited life-sustaining interventions?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2320.0,
        "end": 2327.0
      },
      "pred_interval": {
        "start": 2314.0,
        "end": 2319.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 8.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4383561643835616,
        "text_similarity": 0.6293981075286865,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the events, their semantic content, and the 'after' relation, but the timestamps differ slightly from the reference (E1 off by ~3s; E2 given as 2314\u20132319s vs reference 2320s)."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states he wants to highlight some takeaway points, when does the first takeaway point appear on the screen?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2395.0,
        "end": 2400.0
      },
      "pred_interval": {
        "start": 2435.0,
        "end": 2440.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.0,
        "end": 40.0,
        "average": 40.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131147,
        "text_similarity": 0.6946283578872681,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the correct ordering (anchor before target) but the timestamps are substantially off (predicted ~2430\u20132440s vs correct ~2392\u20132400s), so the timing is incorrect despite the right relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"I'll stop and take questions,\" when does an audience member begin speaking?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2541.6,
        "end": 2544.0
      },
      "pred_interval": {
        "start": 2495.0,
        "end": 2496.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.59999999999991,
        "end": 47.5,
        "average": 47.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.368421052631579,
        "text_similarity": 0.6940966844558716,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the 'after' relation but both event timestamps are significantly incorrect (E1 off by ~23.9s, E2 off by ~46.6s) and it wrongly claims an immediate audience response, contradicting the ~23.7s gap in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the audience member finishes complimenting the center, when does he ask a specific question about local hospital ethics committees?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2571.5,
        "end": 2580.5
      },
      "pred_interval": {
        "start": 2505.0,
        "end": 2507.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.5,
        "end": 73.5,
        "average": 70.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3943661971830986,
        "text_similarity": 0.7851407527923584,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the question about hospital ethics committees follows the compliment, but it provides substantially incorrect timestamps (2504/2505s vs. 2565.5/2571.5s), omits the target's end time, and thus fails to match key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After the audience member mentions the low numbers of ethics consultations, when does the speaker begin to answer the question?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2624.0,
        "end": 2634.8
      },
      "pred_interval": {
        "start": 2515.0,
        "end": 2516.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 109.0,
        "end": 118.30000000000018,
        "average": 113.65000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.4197530864197531,
        "text_similarity": 0.6575266122817993,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but both event timestamps are substantially wrong (~107s earlier) and it omits the speaker's end time, so the timing information is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the listener asks about assessing the quality of care across the system, when does the speaker respond by calling it a 'great question'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2744.1,
        "end": 2745.7
      },
      "pred_interval": {
        "start": 2750.0,
        "end": 2752.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.900000000000091,
        "end": 6.300000000000182,
        "average": 6.100000000000136
      },
      "rationale_metrics": {
        "rouge_l": 0.4615384615384615,
        "text_similarity": 0.5179898738861084,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event order and that the speaker says 'that's a great question' after the listener, but it omits the anchor time for E1 and gives an incorrect E2 time (2750.0s vs correct 2744.1s) and wrongly characterizes the timing as 'immediately after.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions starting to survey clinicians for feedback, when does he mention planning to survey patients and families?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.8,
        "end": 2821.6
      },
      "pred_interval": {
        "start": 2795.0,
        "end": 2800.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.800000000000182,
        "end": 21.59999999999991,
        "average": 17.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.6598972678184509,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives incorrect and inconsistent timestamps (places both events earlier and misaligns/switches the event boundaries) and therefore contradicts the ground truth timings; it only roughly captures a relative ordering but fails on key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that hospitals in the healthcare system can join together, when does he state that they will preferentially present cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2854.49,
        "end": 2856.13
      },
      "pred_interval": {
        "start": 2855.0,
        "end": 2865.0
      },
      "iou": 0.10751665080876172,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5100000000002183,
        "end": 8.86999999999989,
        "average": 4.690000000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.41791044776119407,
        "text_similarity": 0.7930204272270203,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the semantic relation ('after') and the E2 start closely matches the reference, but the anchor timestamp is slightly off and the predicted E2 end time is substantially longer than the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces 'a third method of feedback', when does he describe it as 'formal needs assessments'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2877.53,
        "end": 2879.53
      },
      "pred_interval": {
        "start": 2950.0,
        "end": 2960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.4699999999998,
        "end": 80.4699999999998,
        "average": 76.4699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.4225352112676056,
        "text_similarity": 0.7974632978439331,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship between anchor and target, but the provided timestamps and spans are substantially incorrect (off by ~68\u201380s and wrong durations), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'the overwhelming response was number one', when does he specify the first response as 'a lack of ethics education'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2901.56,
        "end": 2903.46
      },
      "pred_interval": {
        "start": 3000.0,
        "end": 3010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 98.44000000000005,
        "end": 106.53999999999996,
        "average": 102.49000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.5066666666666667,
        "text_similarity": 0.8307509422302246,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the timestamps and durations are substantially incorrect (off by ~95\u2013100s and a much longer target span), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"The more medically complex cases tend to transfer,\" when does he start listing examples of such cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3044.3,
        "end": 3048.2
      },
      "pred_interval": {
        "start": 3035.0,
        "end": 3040.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.300000000000182,
        "end": 8.199999999999818,
        "average": 8.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6345909833908081,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives timestamps that substantially conflict with the ground truth (both anchor and target times are several seconds off and the target is placed before the anchor), so while it names similar example content, the timing and ordering are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the questioner asks about the 'escalation of care policy', when does the slide titled 'Escalation of Care Protocol' appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3114.8,
        "end": 3117.8
      },
      "pred_interval": {
        "start": 3110.0,
        "end": 3115.0
      },
      "iou": 0.025641025641001722,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.800000000000182,
        "end": 2.800000000000182,
        "average": 3.800000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.3611111111111111,
        "text_similarity": 0.7193599343299866,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order (slide appears after the question) but the timestamps are substantially incorrect (anchor given ~9s earlier and slide ~4.8s earlier than ground truth) and the duration/end time is omitted; relation phrased vaguely as 'shortly after.'"
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions \"boarding 190 patients in the emergency department\", when does he discuss concerns about the level of care?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3154.983,
        "end": 3143.945
      },
      "pred_interval": {
        "start": 3135.0,
        "end": 3140.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.983000000000175,
        "end": 3.9450000000001637,
        "average": 11.96400000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.7565184235572815,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the qualitative relation ('immediately after') but the provided E1 and E2 timestamps differ substantially from the reference intervals, so the timing information is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker mentions 'in all 26 of those cases', when does he then talk about 'many more cases'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3214.9,
        "end": 3215.4
      },
      "pred_interval": {
        "start": 3210.0,
        "end": 3216.0
      },
      "iou": 0.08333333333333333,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.900000000000091,
        "end": 0.599999999999909,
        "average": 2.75
      },
      "rationale_metrics": {
        "rouge_l": 0.46875000000000006,
        "text_similarity": 0.5698647499084473,
        "llm_judge_score": 5,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relationship, but the timestamps are notably off (predicted E2 starts 2.9s earlier than reference and E1 end time is omitted), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker states that the 'escalation of care protocol' was nice, when does he mention a 'SOFA-based protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3246.0,
        "end": 3249.0
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3228.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 21.0,
        "average": 23.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3880597014925373,
        "text_similarity": 0.7347522974014282,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the provided timestamps and durations are substantially incorrect compared to the reference (anchors off by 13\u201326s and wrong end times), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker says 'SOFA is horrendous', when does he mention 'SOFA's AUC goes up'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3322.32,
        "end": 3324.71
      },
      "pred_interval": {
        "start": 3260.0,
        "end": 3270.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.320000000000164,
        "end": 54.710000000000036,
        "average": 58.5150000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3768115942028986,
        "text_similarity": 0.7239123582839966,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the provided timestamps are substantially incorrect (off by ~60s) and event durations do not match the ground-truth, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the question about equity monitoring is asked, when does the speaker begin explaining the logging process for patient cases?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3401.583,
        "end": 3406.09
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3412.0
      },
      "iou": 0.10463665162716274,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4169999999999163,
        "end": 5.9099999999998545,
        "average": 4.663499999999885
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837845,
        "text_similarity": 0.7406735420227051,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps differ substantially from the reference and reverse the event order implied by the correct times; the E2 span and end time do not match and the quoted phrase appears unsupported, so the timing and relation are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the 'Escalation of Care Protocol', when does the 'Conscientious Practice Policy' slide appear on screen?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3429.8,
        "end": 3430.5
      },
      "pred_interval": {
        "start": 3425.0,
        "end": 3430.0
      },
      "iou": 0.03636363636360329,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.800000000000182,
        "end": 0.5,
        "average": 2.650000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.410958904109589,
        "text_similarity": 0.7531391978263855,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction preserves the correct sequence (the policy slide appears after the escalation explanation) and provides times that are close but not exact (E1 off by ~4s, E2 off by ~4.8s) and uses a less specific relation ('after' vs 'once_finished'); it also adds an unnecessary end time."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Conscientious Practice Policy' slide appears, when does the speaker mention tracking outcomes and looking back retrospectively for this policy?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3548.0
      },
      "gt_interval": {
        "start": 3444.0,
        "end": 3492.0
      },
      "pred_interval": {
        "start": 3435.0,
        "end": 3445.0
      },
      "iou": 0.017543859649122806,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 47.0,
        "average": 28.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5671641791044777,
        "text_similarity": 0.7835602760314941,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the relation ('after') is correct and the anchor time is close, the predicted E2 interval is substantially misaligned (starts ~9s early and ends ~47s early), omitting most of the referenced period, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions an increasing disparity over time, when does he discuss how they can provide support to all hospitals?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 707.399,
        "end": 742.972
      },
      "pred_interval": {
        "start": 710.0,
        "end": 730.0
      },
      "iou": 0.5622241587721027,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.600999999999999,
        "end": 12.97199999999998,
        "average": 7.7864999999999895
      },
      "rationale_metrics": {
        "rouge_l": 0.358974358974359,
        "text_similarity": 0.9113245010375977,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target content and their temporal order, with the predicted times falling within the reference intervals; minor issues are that the anchor is given as a single timestamp rather than an interval and there are slight boundary differences."
      }
    },
    {
      "question_id": "002",
      "question": "While the organizational chart for the Center for Clinical Ethics is displayed, when does the speaker describe the Ethics Education program?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 769.177,
        "end": 786.763
      },
      "pred_interval": {
        "start": 760.0,
        "end": 780.0
      },
      "iou": 0.40440159922280633,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.177000000000021,
        "end": 6.763000000000034,
        "average": 7.970000000000027
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.44878488779067993,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that the description occurs 'while' the slide is displayed and gives approximate timings, but the target start (760s vs 769.177s) and end (780s vs 786.763s) are notably inaccurate and the anchor end time (810s) is omitted."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says he will go into depth on the programs, when does he first mention the Yale Interdisciplinary Center for Bioethics?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 837.605,
        "end": 845.26
      },
      "pred_interval": {
        "start": 820.0,
        "end": 830.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.605000000000018,
        "end": 15.259999999999991,
        "average": 16.432500000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444445,
        "text_similarity": 0.7263983488082886,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and gives a reasonable anchor time, but the target timestamps are substantially inaccurate (820\u2013830s vs reference 837.6\u2013845.3s), so it does not match the ground truth timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states the title 'Systemwide Ethics Forum and Newsletter', when does he describe it as a hybrid meeting?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1070.5,
        "end": 1076.5
      },
      "pred_interval": {
        "start": 1055.0,
        "end": 1065.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.5,
        "end": 11.5,
        "average": 13.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3368421052631579,
        "text_similarity": 0.6902979016304016,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their temporal relation (after), but the provided timestamps differ substantially from the reference and it introduces unsupported visual-cue detail (gestures), so it is partially correct but incomplete/partly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes explaining that they looked through the 26 specific patient cases individually, when does the slide transition to 'Scope of protocol'?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3425.8,
        "end": 3429.0
      },
      "pred_interval": {
        "start": 3402.5,
        "end": 3410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.300000000000182,
        "end": 19.0,
        "average": 21.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.36923076923076925,
        "text_similarity": 0.727497935295105,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps conflict substantially with the reference (off by ~15+ seconds) and it misstates the event boundaries and relationship (claims immediate/overlapping transition), so it does not match the correct timing or relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the 'Scope of protocol' slide finishes being displayed, when does the 'Conscientious Practice Policy' slide appear?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3429.0,
        "end": 3519.5
      },
      "pred_interval": {
        "start": 3425.0,
        "end": 3432.0
      },
      "iou": 0.031746031746031744,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 87.5,
        "average": 45.75
      },
      "rationale_metrics": {
        "rouge_l": 0.4307692307692308,
        "text_similarity": 0.8219008445739746,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly captures that E2 follows E1, but it misstates both timestamps (3425.0s vs correct 3429.0s) and drastically underestimates E2's end time (3432.0s vs 3519.5s), so the factual timing and duration are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes discussing the tracking of equity, socioeconomic status, and other demographic characteristics, when is the presentation window minimized?",
      "video_id": "zq-hPiBgadw",
      "video_number": "014",
      "segment": {
        "start": 3390.0,
        "end": 3547.1899999999996
      },
      "gt_interval": {
        "start": 3530.0,
        "end": 3531.0
      },
      "pred_interval": {
        "start": 3535.0,
        "end": 3540.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 9.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.8060168027877808,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction names the same events but gives substantially incorrect timestamps (E1 wrong by ~26.5s; E2 start/end off by 5\u20139s) and mischaracterizes the temporal relation (implying immediate/simultaneous timing), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states the audience will be on mute, when does he mention that the live event can be paused?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 38.524,
        "end": 43.729
      },
      "pred_interval": {
        "start": 34.0,
        "end": 38.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.524000000000001,
        "end": 5.728999999999999,
        "average": 5.1265
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.7522636651992798,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the provided timestamps and target span are substantially incorrect and do not match the ground-truth start/end times, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses changing the speed of presentations and speakers, when does he advise on what to do if Wi-Fi or connection is lost?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 55.563,
        "end": 59.787
      },
      "pred_interval": {
        "start": 50.0,
        "end": 60.0
      },
      "iou": 0.42239999999999966,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.563000000000002,
        "end": 0.21300000000000097,
        "average": 2.8880000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7374677658081055,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct events and the 'after' relation, and the predicted E2 span includes the true target, but the timestamps are noticeably off (E1 ~2.7s early; E2 start ~5.6s early), reducing temporal accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the presenter mentions Tom Gardner in the background, when does he mention Stephanie Fraser joining in place of Jane Preston?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 168.258,
        "end": 171.201
      },
      "pred_interval": {
        "start": 159.0,
        "end": 162.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.25800000000001,
        "end": 9.200999999999993,
        "average": 9.229500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.8444767594337463,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation and even the quoted phrasing for E2, but the timestamps are grossly inaccurate (12.30s vs 156.0s and ~18.8s vs 159.0s), so it does not match the ground truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the male presenter finishes introducing Stephanie Fraser, when does Stephanie Fraser begin speaking?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 223.86,
        "end": 224.8
      },
      "pred_interval": {
        "start": 175.0,
        "end": 177.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.860000000000014,
        "end": 47.80000000000001,
        "average": 48.33000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444445,
        "text_similarity": 0.8130208849906921,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right (presenter then Stephanie) but the timestamps are vastly incorrect (174/175s vs 222.0/223.86s) and it changes the relation to 'immediately after', so the answer is mostly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "While the speaker is discussing the recent research undertaken by the Neurological Alliance of Scotland, when does she state that 57% of respondents reported not being able to access a face-to-face appointment?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 433.0,
        "end": 434.9
      },
      "pred_interval": {
        "start": 412.0,
        "end": 423.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 11.899999999999977,
        "average": 16.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23404255319148937,
        "text_similarity": 0.5718719959259033,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies the target interval by ~20+ seconds (placing the '57%' utterance at 412\u2013423s instead of ~433.0\u2013434.9s) and gives an incorrect temporal relation ('after' rather than during/within the anchor); anchor timing is also imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that nearly two-thirds of respondents had not had a video appointment, when does she state that telephone appointments were the most common way to access care?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 447.8,
        "end": 452.9
      },
      "pred_interval": {
        "start": 427.0,
        "end": 432.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.80000000000001,
        "end": 20.899999999999977,
        "average": 20.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7873710989952087,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and segment boundaries conflict substantially with the reference (both anchor and target times differ by large margins) and the described relationship/timing is inaccurate, so it does not match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the blue slide with the speaker's title disappears, when does the speaker begin to mention what factors clinicians should consider for appointment formats?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 479.3,
        "end": 480.3
      },
      "pred_interval": {
        "start": 470.0,
        "end": 480.0
      },
      "iou": 0.06796116504854251,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.300000000000011,
        "end": 0.30000000000001137,
        "average": 4.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.850021243095398,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces both events (E1 ~6s earlier and E2 ~9s earlier than the reference) and even lists E2 starting at the same time as E1, making the 'after' relationship inconsistent with the provided timestamps; therefore it does not match the reference timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once Stephanie finishes speaking and hands over to Mark, when does Mark begin to speak?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 606.5,
        "end": 607.0
      },
      "pred_interval": {
        "start": 573.0,
        "end": 574.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.5,
        "end": 32.5,
        "average": 33.0
      },
      "rationale_metrics": {
        "rouge_l": 0.4074074074074074,
        "text_similarity": 0.7030479907989502,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same events (Stephanie ending and Mark starting) but gives entirely different timestamps and an incorrect 'immediate' relation, contradicting the ground-truth times (Mark starts ~12.5s after Stephanie), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mark finishes introducing Calum Duncan, when does Calum Duncan start speaking?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 638.3,
        "end": 639.3
      },
      "pred_interval": {
        "start": 587.0,
        "end": 588.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.299999999999955,
        "end": 50.799999999999955,
        "average": 51.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7123372554779053,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events (Mark finishing and Calum speaking) but gives completely incorrect timestamps (587.0s vs correct ~636\u2013639s) and an inaccurate relation ('immediate' vs once_finished with a ~1.7\u20132.0s gap), so key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Calum Duncan says 'Next slide please', when does the second presentation slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 685.7,
        "end": 686.0
      },
      "pred_interval": {
        "start": 622.0,
        "end": 623.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.700000000000045,
        "end": 62.5,
        "average": 63.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.456140350877193,
        "text_similarity": 0.6571221351623535,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (Calum saying the phrase and the second slide appearing) but both timestamps are off by ~62s and the relation ('immediate') contradicts the correct 'once_finished', so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that 'near me is what we're going to focus on today', when does he describe it as 'internet-based'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 702.7,
        "end": 703.5
      },
      "pred_interval": {
        "start": 694.0,
        "end": 698.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.700000000000045,
        "end": 5.5,
        "average": 7.100000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7341348528862,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation correct (target occurs after the anchor) and identifies the correct phrase, but the provided timestamps are substantially different from the ground truth, so the key factual times are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states there were '330 consultations per week' before the pandemic, when does he mention it went up to '10,000'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 739.0
      },
      "pred_interval": {
        "start": 707.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 29.0,
        "average": 29.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3508771929824562,
        "text_similarity": 0.6357885003089905,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but the anchor and target timestamps are substantially misaligned with the ground truth (both start/end times are incorrect), so it largely fails to match the key factual elements."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Next slide, please' for the first time, when does he point to the map on the screen?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 767.0,
        "end": 767.5
      },
      "pred_interval": {
        "start": 730.0,
        "end": 735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 32.5,
        "average": 34.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3703703703703704,
        "text_similarity": 0.7572703957557678,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events and their temporal relation ('after'), but both anchor and target timestamps are substantially incorrect (off by ~37s), so key factual timing information is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says 'go back to the next slide', when does the slide titled 'Video consulting using near me via attend anywhere platform' appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 874.0,
        "end": 874.1
      },
      "pred_interval": {
        "start": 880.0,
        "end": 920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 45.89999999999998,
        "average": 25.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21818181818181817,
        "text_similarity": 0.7105057239532471,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly links the instruction to the slide and even labels the relationship as 'immediate', but it omits the precise timestamps and wrongly states the slide appears 'a few seconds later' (contradicting the near-instant 0.09s delay), so it is partially correct but inconsistent and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that 'Stephanie Fraser has talked about' the survey, when does he then say 'Back to next slide, Mark, please'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 883.0,
        "end": 884.0
      },
      "pred_interval": {
        "start": 890.0,
        "end": 900.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 16.0,
        "average": 11.5
      },
      "rationale_metrics": {
        "rouge_l": 0.11999999999999998,
        "text_similarity": 0.5598698854446411,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the target follows the anchor and characterizes the transition as immediate, but it uses incorrect event labels (E1/E2 vs E3/E4) and omits the precise timing information provided in the correct answer, making it incomplete and partially inconsistent."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says 'Next slide, please' at the 42-second mark, when does the slide titled 'Clinician and patient experience - Scotland' appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 913.0,
        "end": 913.1
      },
      "pred_interval": {
        "start": 912.0,
        "end": 932.0
      },
      "iou": 0.005000000000001137,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 18.899999999999977,
        "average": 9.949999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2553191489361702,
        "text_similarity": 0.5739882588386536,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor at 912s and that the slide appears immediately/shortly after (relationship 'after'), but it omits the precise target timestamp (913.0s) and is slightly imprecise in timing."
      }
    },
    {
      "question_id": "001",
      "question": "During the discussion of what works well with video calls, when does the speaker express finding it much easier to interact with groups on a video call than on the telephone?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.0,
        "end": 1062.5
      },
      "pred_interval": {
        "start": 1055.0,
        "end": 1065.0
      },
      "iou": 0.625,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 2.5,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903615,
        "text_similarity": 0.7333996295928955,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor time, the content, and the 'during' relation; the only issues are minor timing offsets (E2 shifted ~2s later and extended ~2.5s), which do not change the semantic match."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions technical issues with patient bandwidth, when does he advise to choose patients correctly to avoid those difficulties?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1134.0,
        "end": 1135.5
      },
      "pred_interval": {
        "start": 1105.0,
        "end": 1115.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.0,
        "end": 20.5,
        "average": 24.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.637344479560852,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the events and the 'after' relation but misaligns the timestamps substantially (E1 should be at 1119.0s vs predicted 1100.0s; E2 should be 1134.0\u20131135.5s vs predicted 1105.0\u20131115.0), so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Next slide, please' to introduce the smart phone camera, when does he specifically point out his wife's iPhone on the screen?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1213.0,
        "end": 1215.0
      },
      "pred_interval": {
        "start": 1205.0,
        "end": 1215.0
      },
      "iou": 0.2,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.0,
        "end": 0.0,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.6371290683746338,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction gets the relation ('after') and the E2 end time correct, but both event start times are off (E1 by 3s, E2 by 8s) and E2 is overly extended to include time before the actual mention, so it is only partially accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'Next slide please', when does the 'Sharing content' slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1248.574,
        "end": 1249.574
      },
      "pred_interval": {
        "start": 1235.0,
        "end": 1240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.57400000000007,
        "end": 9.57400000000007,
        "average": 11.57400000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.801413893699646,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (the slide appears after the anchor) but the timestamps are significantly incorrect (predicted anchor end 1234.0s vs ground-truth 1248.173s; predicted target 1235.0s vs 1248.574s), so the timing and duration are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'You can share things', when does he point towards the screen showing the brain scan?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1252.25,
        "end": 1252.85
      },
      "pred_interval": {
        "start": 1242.0,
        "end": 1246.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.25,
        "end": 6.849999999999909,
        "average": 8.549999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7257645130157471,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order ('shortly after') but the reported timestamps are substantially wrong (anchor is ~13.3s earlier and target ~10.3s earlier than ground truth) and it omits the target end time, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the discussion about poor picture quality, when does the speaker suggest clearing browser history?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1313.823,
        "end": 1315.286
      },
      "pred_interval": {
        "start": 1285.0,
        "end": 1290.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.823000000000093,
        "end": 25.286000000000058,
        "average": 27.054500000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444445,
        "text_similarity": 0.7880809307098389,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and their relationship but the provided timestamps are substantially incorrect (anchor ~28s early and target ~29s early) and it omits the target end time, so it fails to match the reference timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying \"Thank you very much for that\", when does he state he is handing over to Jane?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1428.837,
        "end": 1430.682
      },
      "pred_interval": {
        "start": 1422.0,
        "end": 1425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.836999999999989,
        "end": 5.682000000000016,
        "average": 6.259500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.5066666666666667,
        "text_similarity": 0.8144848942756653,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the quoted utterances and that E2 follows E1, but the timestamps differ substantially (off by ~6\u20138s) and the temporal relation is weakened from 'once_finished' to generic 'after', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman (Jane) describes the challenges of managing patients over the telephone, when does she mention that they had a pilot of 'Near Me' even prior to Covid?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1609.855,
        "end": 1624.692
      },
      "pred_interval": {
        "start": 1575.0,
        "end": 1582.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.85500000000002,
        "end": 42.69200000000001,
        "average": 38.77350000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.5057471264367817,
        "text_similarity": 0.7133693695068359,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events, their content and the 'after' relation, but the provided timestamps are substantially earlier than the reference (off by ~37\u201342 seconds), so it fails on temporal accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that using 'Near Me' felt quite adventurous, when does she state that its use became vital to their whole service?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1643.0
      },
      "pred_interval": {
        "start": 1610.0,
        "end": 1618.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 25.0,
        "average": 25.5
      },
      "rationale_metrics": {
        "rouge_l": 0.37209302325581395,
        "text_similarity": 0.752362847328186,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor and the 'after' relation, but it mislocates and misidentifies the target: the predicted E2 span (1610\u20131618s) and content about Covid do not match the reference timing or the 'became vital to our whole service' utterance."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes asking Mark to go back to the previous slide, when does she say 'Thank you'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1676.54,
        "end": 1678.02
      },
      "pred_interval": {
        "start": 1650.0,
        "end": 1652.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.539999999999964,
        "end": 26.019999999999982,
        "average": 26.279999999999973
      },
      "rationale_metrics": {
        "rouge_l": 0.37681159420289856,
        "text_similarity": 0.7914000749588013,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance but the timestamps for both events are drastically wrong and the target span/temporal relation do not match the reference, so it fails to locate when 'Thank you' occurs."
      }
    },
    {
      "question_id": "001",
      "question": "After the 'Training and preparation' slide appears, when does the speaker mention the 'Level 1' training?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1791.0,
        "end": 1791.5
      },
      "pred_interval": {
        "start": 1773.0,
        "end": 1783.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 8.5,
        "average": 13.25
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.6300262212753296,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the event timestamps are substantially incorrect (both E1 and especially E2 differ markedly from the reference), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing tele-swallowing partners as 'our eyes and our hands and our ears', when does she start talking about preparing the clinical room?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1897.0,
        "end": 1901.0
      },
      "pred_interval": {
        "start": 1867.0,
        "end": 1875.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 26.0,
        "average": 28.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.6197912693023682,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct event types but the timestamps are substantially off (~30s earlier) and the E2 interval does not match the reference; the relation 'after' is less precise than 'once_finished', so overall the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker discusses tele-swallowing partners preparing the clinical room, when does she next talk about them providing reassurance to patients?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1906.0,
        "end": 1910.0
      },
      "pred_interval": {
        "start": 1875.0,
        "end": 1883.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.0,
        "end": 27.0,
        "average": 29.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6673882007598877,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same event types but the timestamps differ substantially from the reference and the relation is mislabeled ('after' vs the required 'next'), so it is largely incorrect despite matching the events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning emergency procedures in place onsite, when does the slide change to 'Technology/equipment'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1971.6,
        "end": 1972.0
      },
      "pred_interval": {
        "start": 1955.0,
        "end": 1960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.59999999999991,
        "end": 12.0,
        "average": 14.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.7529000043869019,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal order, but it omits the precise start/end timestamps required by the ground truth and reduces the relation to a vague 'after' rather than the specified 'once_finished', so key factual details are missing."
      }
    },
    {
      "question_id": "002",
      "question": "During the time the 'Technology/equipment' slide is displayed, when does the speaker discuss the need for a device with a webcam and microphone?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2024.079,
        "end": 2026.579
      },
      "pred_interval": {
        "start": 1965.0,
        "end": 1970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.07899999999995,
        "end": 56.57899999999995,
        "average": 57.82899999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3478260869565218,
        "text_similarity": 0.7508342862129211,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the mention occurs 'during' the slide, but it omits the required timestamp details (anchor and target start/end times) and does not state that the target falls within the specified anchor interval."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker introduces the general category of 'certain resources' for teleswallow sessions, when does she mention 'appropriate diet and fluid consistencies'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2058.952,
        "end": 2061.952
      },
      "pred_interval": {
        "start": 1990.0,
        "end": 1995.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.95200000000023,
        "end": 66.95200000000023,
        "average": 67.95200000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.7473111152648926,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target and that the target comes after the anchor ('after' \u2248 'next'), but it omits the precise timestamps and the detail that the target follows other listed resources, which are key factual elements of the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that remote swallowing assessments are not intended to fully replace face-to-face assessments, when does she mention that they are a very useful addition?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2159.677,
        "end": 2162.619
      },
      "pred_interval": {
        "start": 2139.0,
        "end": 2145.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.677000000000135,
        "end": 17.619000000000142,
        "average": 19.14800000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.6157474517822266,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the target immediately follows the anchor, but the timestamps are factually wrong (off by about 20 seconds: correct E1 ends at 2159.0s / E2 begins at 2159.677s vs predicted 2139.0s\u20132145.0s), so it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes mentioning gathering feedback from those who completed the training, when does she start talking about evaluating quantitative data?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2164.643,
        "end": 2186.427
      },
      "pred_interval": {
        "start": 2169.0,
        "end": 2175.0
      },
      "iou": 0.27543150936467,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.356999999999971,
        "end": 11.427000000000135,
        "average": 7.892000000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837834,
        "text_similarity": 0.7456486225128174,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an immediate continuation relationship but the timestamps are substantially incorrect (predicted E1/E2 around 2169s vs correct E1=2185.427s and E2=2186.427s), so it fails to match the key factual timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the first speaker finishes her presentation by saying 'thank you very much for listening', when does the video visually transition to the male presenter?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2257.0,
        "end": 2258.0
      },
      "pred_interval": {
        "start": 2214.0,
        "end": 2216.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.0,
        "end": 42.0,
        "average": 42.5
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.7320724725723267,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the relationship (immediate continuation) but the key timestamps are substantially incorrect (2214s vs correct 2256\u20132257s) and it invents an end time for E2, so it fails on critical factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that picking up cues is difficult, when does she start talking about 'points to consider' for virtual technology?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2491.8,
        "end": 2498.2
      },
      "pred_interval": {
        "start": 2490.0,
        "end": 2495.0
      },
      "iou": 0.39024390243901086,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.800000000000182,
        "end": 3.199999999999818,
        "average": 2.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6244595050811768,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives incorrect timestamps (2490.0s vs 2491.8s and 2495.0s vs 2491.8s) and the wrong temporal relation ('after' instead of immediate/once_finished), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions conducting a 'sprint audit' with patients, when does she state that 'most were very satisfied' with the virtual appointments?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2515.0,
        "end": 2516.0
      },
      "pred_interval": {
        "start": 2520.0,
        "end": 2525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 9.0,
        "average": 7.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3278688524590164,
        "text_similarity": 0.7535792589187622,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches, both event timestamps are incorrect: E1 is given as 2515.0s (should be 2509.5s) and E2 as 2520.0s (should be 2515.0\u20132516.0s), so it fails to match the key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying that patients found virtual technology 'more acceptable', when does she say 'So moving on to the next slide'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2638.0,
        "end": 2639.3
      },
      "pred_interval": {
        "start": 2580.0,
        "end": 2585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.0,
        "end": 54.30000000000018,
        "average": 56.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285714,
        "text_similarity": 0.6479712724685669,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the order (E2 occurs after E1) but the reported timestamps are substantially wrong (~62s earlier) and the E2 end time is omitted; the relation label also differs, so it fails to match key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes discussing confidentiality, when does she begin to mention the subtlety of the therapeutic relationship?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2693.583,
        "end": 2697.126
      },
      "pred_interval": {
        "start": 2683.0,
        "end": 2687.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.583000000000084,
        "end": 10.126000000000204,
        "average": 10.354500000000144
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.7544453144073486,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction's timestamps are substantially off (E1 ~5.6s early and E2 ~10.6s early) and it misplaces the target event, though it correctly indicates an 'after' relation; overall timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'It all comes down to Wi-Fi', when does she state that 'delivery of remote therapy is very, very difficult'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2727.0,
        "end": 2729.0
      },
      "pred_interval": {
        "start": 2704.0,
        "end": 2708.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.0,
        "end": 21.0,
        "average": 22.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5263157894736842,
        "text_similarity": 0.8474897146224976,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gives incorrect timestamps for both events (anchors at 2704.0s vs. correct 2722.041s; target at 2704.0s vs. correct 2727.0\u20132729.0s) and even contradicts its own timing by stating the target is 'after' while using the same start time; only the qualitative 'after' relation matches."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'So next slide', when does the slide visually change to 'Practical considerations'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2884.0,
        "end": 2884.2
      },
      "pred_interval": {
        "start": 2862.0,
        "end": 2863.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.0,
        "end": 20.699999999999818,
        "average": 21.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.4776119402985075,
        "text_similarity": 0.6580231189727783,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the relative relation ('immediately after') and the 1s offset, but both absolute timestamps are incorrect by ~22 seconds (2861/2862 vs. 2883/2884), which are key factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing 'Practical considerations', when does she first mention 'increasing reflective feedback'?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2913.483,
        "end": 2916.268
      },
      "pred_interval": {
        "start": 2875.0,
        "end": 2877.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.483000000000175,
        "end": 39.26800000000003,
        "average": 38.8755000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.762455403804779,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relative relation ('after') but both timestamp values are substantially inaccurate (E1 off by ~12s and E2 off by ~38s compared to the reference), so it fails on factual timing correctness."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"for the patients\", when does the slide change to \"WHERE WE ARE NOW\"?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3067.769,
        "end": 3068.2
      },
      "pred_interval": {
        "start": 3035.5,
        "end": 3036.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.26899999999978,
        "end": 31.699999999999818,
        "average": 31.984499999999798
      },
      "rationale_metrics": {
        "rouge_l": 0.4137931034482759,
        "text_similarity": 0.7752894163131714,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps are about 31.7 seconds earlier than the ground truth and give incorrect end/visibility times, so the temporal alignment is largely wrong; the relation 'immediately after' is close to 'after' but does not compensate for the major timing errors."
      }
    },
    {
      "question_id": "002",
      "question": "After the man says \"open up for some discussion\", when does the discussion slide appear?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3163.435,
        "end": 3163.7
      },
      "pred_interval": {
        "start": 3070.0,
        "end": 3071.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.43499999999995,
        "end": 92.69999999999982,
        "average": 93.06749999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.49056603773584906,
        "text_similarity": 0.7636572122573853,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the slide appears after the utterance, but the timestamps are substantially wrong (E1 off by ~51s, E2 off by ~93s) and it falsely claims an 'immediately after' relation and different visibility timing, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the first male speaker asks about attendees' experience with Near Me, when does the second male speaker begin talking about starting to use NearMe?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3268.9,
        "end": 3312.0
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3235.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.90000000000009,
        "end": 77.0,
        "average": 62.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.41791044776119407,
        "text_similarity": 0.6929583549499512,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps substantially conflict with the ground truth (predicted E2 starts before the ground-truth E1 end and much earlier overall), and thus the temporal relation and event boundaries are incorrect despite labeling the relation 'after'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second male speaker finishes stating the advantages and utility of NearMe, when does he mention supplementing normal activities?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3288.4,
        "end": 3293.32
      },
      "pred_interval": {
        "start": 3235.0,
        "end": 3245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.40000000000009,
        "end": 48.320000000000164,
        "average": 50.86000000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.585253894329071,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events (utility discussion and mention of supplementing activities) but the timestamps are substantially incorrect and the relation is vague ('after' vs the specified 'once_finished'), so it fails to match the ground truth timing and relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the first man finishes reading Jenny's chat message, when does he ask the audience if they would find guidance helpful?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3411.0,
        "end": 3415.0
      },
      "pred_interval": {
        "start": 3407.0,
        "end": 3411.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 4.0,
        "average": 4.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.690858006477356,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('after') and roughly locates E1, but it gives incorrect timing for E2/target (predicts 3407.0\u20133411.0 versus the correct 3411.0\u20133415.0) and thus fails to match the key temporal span."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first man finishes reading John Hogan's comment about clinical interviewing, when does he state he was quite skeptical?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3434.9,
        "end": 3437.7
      },
      "pred_interval": {
        "start": 3420.0,
        "end": 3424.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.900000000000091,
        "end": 13.699999999999818,
        "average": 14.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.37333333333333335,
        "text_similarity": 0.7754738926887512,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures a generic 'after' relation but the anchor and target timestamps/spans are substantially incorrect (several to ~15s off) and do not match the reference target span, so key factual timing is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the second woman mentions neuropsychology bringing out guidance, when is the next time a woman speaks about professional guidance?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3511.043,
        "end": 3528.447
      },
      "pred_interval": {
        "start": 3458.0,
        "end": 3464.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.04300000000012,
        "end": 64.44700000000012,
        "average": 58.74500000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.8227121829986572,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events (second woman then another woman on professional guidance) but gives substantially incorrect timestamps for both E1 and E2, a wrong target span, and mislabels the relation ('after' vs. 'next'), so it is factually largely incorrect and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that 36 people joined the session, when does he talk about taking the next steps with Richard and the team?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3574.7,
        "end": 3576.5
      },
      "pred_interval": {
        "start": 3575.0,
        "end": 3584.0
      },
      "iou": 0.16129032258064202,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.3000000000001819,
        "end": 7.5,
        "average": 3.900000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.812648355960846,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor start time and the target occurs after it, with the predicted target start (3575.0s) very close to the reference (3574.7s). It is penalized because the predicted target end (3584.0s) significantly overextends beyond the reference finish (~3576.5s) and the anchor end time is not specified as in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker makes a plea to fill in the survey, when does he ask if listeners would like to engage with the advisory committee?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3592.9,
        "end": 3594.1
      },
      "pred_interval": {
        "start": 3584.0,
        "end": 3592.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.900000000000091,
        "end": 2.099999999999909,
        "average": 5.5
      },
      "rationale_metrics": {
        "rouge_l": 0.41463414634146345,
        "text_similarity": 0.8065608739852905,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the correct answer, the anchor and target timestamps are substantially incorrect (target start/end are far off and even equal to the anchor start), and the prediction contradicts its own timing vs. relation, so it fails on factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes thanking everyone for joining the session today, when does he mention that the session will be recorded and resources will be provided?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3618.0
      },
      "gt_interval": {
        "start": 3599.8,
        "end": 3603.2
      },
      "pred_interval": {
        "start": 3592.0,
        "end": 3600.0
      },
      "iou": 0.017857142857126907,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.800000000000182,
        "end": 3.199999999999818,
        "average": 5.5
      },
      "rationale_metrics": {
        "rouge_l": 0.38961038961038963,
        "text_similarity": 0.8636466264724731,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct content (session recording and resources) but the timestamps are substantially incorrect and inconsistent with the ground truth (anchor and target start/end times and ordering are misaligned), so it fails as a precise temporal match."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks 'where did we start?', when does she mention considering moving to Near Me for patient contacts?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2332.719,
        "end": 2336.344
      },
      "pred_interval": {
        "start": 2321.0,
        "end": 2329.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.719000000000051,
        "end": 7.344000000000051,
        "average": 9.531500000000051
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.7287634611129761,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the quoted phrases but the timestamps are substantially off (anchor ~10s early; target ~11s early and wrong duration) and the relation is vaguely 'after' rather than the required direct follow-up, so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says the pandemic came along, when does she mention adopting Near Me as their default for routine people?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.217,
        "end": 2412.045
      },
      "pred_interval": {
        "start": 2345.0,
        "end": 2356.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.2170000000001,
        "end": 56.04500000000007,
        "average": 39.131000000000085
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7725637555122375,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the causal 'after' relation, but the anchor and target timestamps are substantially wrong (both start/end times differ markedly from the reference) and the predicted target window misses the actual longer target segment, making it largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker describes the results of the focus groups for the qualitative study, when does she introduce the quotes from the participants?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2511.0,
        "end": 2512.0
      },
      "pred_interval": {
        "start": 2460.0,
        "end": 2470.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.0,
        "end": 42.0,
        "average": 46.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.806662380695343,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') but the timestamps are substantially incorrect\u2014correct E2 begins at 2511s while the prediction places it at 2460\u20132470s, and the anchor timing is inconsistent, representing a major temporal mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks to fill in the survey, when does he ask if listeners want to engage with the advisory committee?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3591.7,
        "end": 3595.8
      },
      "pred_interval": {
        "start": 3589.0,
        "end": 3594.0
      },
      "iou": 0.3382352941176648,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.699999999999818,
        "end": 1.800000000000182,
        "average": 2.25
      },
      "rationale_metrics": {
        "rouge_l": 0.47500000000000003,
        "text_similarity": 0.8368569612503052,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and their 'after' relationship, with an acceptable approximation for the anchor time; it slightly misestimates the target start (\u22482.7s earlier) and omits end times. "
      }
    },
    {
      "question_id": "002",
      "question": "Before the speaker thanks the speakers for their expertise, when does he mention the session will be recorded and resources will be provided?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3599.9,
        "end": 3603.7
      },
      "pred_interval": {
        "start": 3599.0,
        "end": 3605.0
      },
      "iou": 0.6333333333332879,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.900000000000091,
        "end": 1.300000000000182,
        "average": 1.1000000000001364
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7550038695335388,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that the recording/resources mention occurs before the thanks, and roughly locates the start (~3599s), but it mislabels anchor/target, gives an incorrect earlier 'thanking the audience' event (~3594s) instead of thanks to the speakers (~3604s\u20133605s), and slightly inflates the end time to 3605.0s, so key details are wrong or inconsistent."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker initially thanks the audience for joining, when does he deliver his final 'thank you very much' for the session?",
      "video_id": "cOpdhlVsyM8",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3617.017
      },
      "gt_interval": {
        "start": 3614.6,
        "end": 3615.4
      },
      "pred_interval": {
        "start": 3610.0,
        "end": 3613.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.599999999999909,
        "end": 2.400000000000091,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.32608695652173914,
        "text_similarity": 0.7956842184066772,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction identifies the same anchor and target utterances and correctly states the temporal relation ('after'), but the provided timestamps are noticeably earlier than the reference (off by several seconds), so it's largely correct but imprecise."
      }
    },
    {
      "question_id": "001",
      "question": "After Mark introduces Dr. John Mckeown and Dr. Naomi Dow, when does he ask Dr. Dow to describe how they've been using Near Me?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 31.48,
        "end": 34.4
      },
      "pred_interval": {
        "start": 23.5,
        "end": 25.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.98,
        "end": 9.399999999999999,
        "average": 8.69
      },
      "rationale_metrics": {
        "rouge_l": 0.3823529411764706,
        "text_similarity": 0.760013222694397,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation 'after' correct but both event timestamps are substantially incorrect (E1 given as 0.0s vs correct finish 15.72s; E2 given as 23.5s vs correct ~31.48\u201334.4s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Dr. Naomi Dow finishes explaining how students take part in consultations, when does Mark ask Dr. Mckeown about the impact on the teaching team?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 118.96,
        "end": 124.4
      },
      "pred_interval": {
        "start": 64.0,
        "end": 65.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.959999999999994,
        "end": 58.900000000000006,
        "average": 56.93
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.716454267501831,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted relation 'after' is a loose match to 'once_finished', but both event timestamps are far off (predicted E1 40.0s vs 117.60s and predicted E2 64.0s vs 118.96\u2013124.4s), so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker introduces the concept of emotions in the session, when does the female speaker first mention 'real patients'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 201.9,
        "end": 202.6
      },
      "pred_interval": {
        "start": 192.0,
        "end": 198.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.900000000000006,
        "end": 4.599999999999994,
        "average": 7.25
      },
      "rationale_metrics": {
        "rouge_l": 0.41935483870967744,
        "text_similarity": 0.7800000905990601,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies E1 and the temporal relation ('after'), but the E2 timestamps are significantly wrong (192.0\u2013198.0s vs. the correct 201.9\u2013202.6s), omitting the key factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer finishes asking the question about comparing models, when does the female speaker finish explaining the advantages of 'Near Me' regarding real patients and capacity?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 198.7,
        "end": 306.9
      },
      "pred_interval": {
        "start": 198.0,
        "end": 240.0
      },
      "iou": 0.37924701561065216,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6999999999999886,
        "end": 66.89999999999998,
        "average": 33.79999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.5851619243621826,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the female speaker begins shortly after the question, but it misstates the interviewer's finish time and severely underestimates the female speaker's end time (240.0s vs 306.9s), making it factually incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "During the time the man is speaking on screen, when does he mention 'Near Me'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 342.0,
        "end": 344.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 335.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.0,
        "end": 9.0,
        "average": 10.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.6961339712142944,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('during') right but misreports key timestamps (E1 and E2 starts differ by ~7\u20138s) and omits end times, while adding unrelated detail about 'consultations'; thus it fails to match the correct temporal facts."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying 'Thank you and goodbye', when do the 'NHS Scotland' and 'Near Me' logos appear with text links?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 351.0,
        "end": 360.0
      },
      "pred_interval": {
        "start": 357.0,
        "end": 361.0
      },
      "iou": 0.3,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 1.0,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.6749539971351624,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the order (logos appear after the man finishes) but the timestamps are substantially off (E1/E2 times differ by ~7 seconds from the reference) and the prediction omits the E2 end time and the precise 'once_finished' immediacy, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the initial voiceover concludes with 'patient that day', when does the man on screen begin to say 'Thanks very much John and Amy'?",
      "video_id": "YJFNgX2pmAw",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 361.0
      },
      "gt_interval": {
        "start": 336.4,
        "end": 341.6
      },
      "pred_interval": {
        "start": 335.0,
        "end": 339.0
      },
      "iou": 0.39393939393939603,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.3999999999999773,
        "end": 2.6000000000000227,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.7383139133453369,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the relation ('after') and gives approximate start times, but it misnames 'Amy' as 'Naomi', omits the E2 end time, and has small but nontrivial timestamp discrepancies from the reference."
      }
    }
  ]
}