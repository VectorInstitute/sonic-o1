{
  "model": "videollama",
  "experiment_name": "frames_64",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.13812905306313647,
            "rouge_l_std": 0.025434342767942663,
            "text_similarity_mean": 0.496979346498847,
            "text_similarity_std": 0.07929801080245218,
            "llm_judge_score_mean": 2.4375,
            "llm_judge_score_std": 0.7880950133074057
          },
          "short": {
            "rouge_l_mean": 0.13162551062397573,
            "rouge_l_std": 0.03280831162356705,
            "text_similarity_mean": 0.4878589455038309,
            "text_similarity_std": 0.12023262849393894,
            "llm_judge_score_mean": 1.6875,
            "llm_judge_score_std": 0.982264602843857
          },
          "cider": {
            "cider_detailed": 2.8926659149008865e-06,
            "cider_short": 0.006831652880558254
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.12696705219373247,
            "rouge_l_std": 0.021316462623061215,
            "text_similarity_mean": 0.47838500709760756,
            "text_similarity_std": 0.13560773087043962,
            "llm_judge_score_mean": 2.3333333333333335,
            "llm_judge_score_std": 1.3213749452868198
          },
          "short": {
            "rouge_l_mean": 0.13338057534106737,
            "rouge_l_std": 0.06697798983715426,
            "text_similarity_mean": 0.45055384117932545,
            "text_similarity_std": 0.17282171787232434,
            "llm_judge_score_mean": 2.0476190476190474,
            "llm_judge_score_std": 1.4630753805464016
          },
          "cider": {
            "cider_detailed": 0.0004347977340220841,
            "cider_short": 2.0802756182620564e-05
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.12741091760176024,
            "rouge_l_std": 0.03658168184694336,
            "text_similarity_mean": 0.330379205254408,
            "text_similarity_std": 0.17498502603364302,
            "llm_judge_score_mean": 0.9230769230769231,
            "llm_judge_score_std": 0.8284868934053085
          },
          "short": {
            "rouge_l_mean": 0.09998651893098662,
            "rouge_l_std": 0.04095117183029242,
            "text_similarity_mean": 0.2888646920999655,
            "text_similarity_std": 0.16898893086146574,
            "llm_judge_score_mean": 0.8461538461538461,
            "llm_judge_score_std": 0.8634593969478326
          },
          "cider": {
            "cider_detailed": 0.0024133440780724933,
            "cider_short": 0.0005130917867555334
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.1308356742862097,
          "text_similarity_mean": 0.4352478529502875,
          "llm_judge_score_mean": 1.8979700854700854
        },
        "short": {
          "rouge_l_mean": 0.1216642016320099,
          "text_similarity_mean": 0.40909249292770733,
          "llm_judge_score_mean": 1.5270909645909645
        },
        "cider": {
          "cider_detailed_mean": 0.0009503448260031595,
          "cider_short_mean": 0.0024551824744988024
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.5784313725490197,
          "correct": 59,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.21570779489901443,
            "rouge_l_std": 0.07914632199842049,
            "text_similarity_mean": 0.625086834252465,
            "text_similarity_std": 0.16684187627583294,
            "llm_judge_score_mean": 5.519607843137255,
            "llm_judge_score_std": 3.969813528877161
          },
          "rationale_cider": 0.12176868138606906
        },
        "02_Job_Interviews": {
          "accuracy": 0.67,
          "correct": 67,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.21339071418008346,
            "rouge_l_std": 0.07846988086928516,
            "text_similarity_mean": 0.6085129880532623,
            "text_similarity_std": 0.17396879274153787,
            "llm_judge_score_mean": 6.6,
            "llm_judge_score_std": 3.5071355833500366
          },
          "rationale_cider": 0.09985522478291575
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.5304347826086957,
          "correct": 61,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.19368268166890829,
            "rouge_l_std": 0.07992442700249024,
            "text_similarity_mean": 0.560196517618454,
            "text_similarity_std": 0.20349455550910786,
            "llm_judge_score_mean": 4.478260869565218,
            "llm_judge_score_std": 3.877129879931818
          },
          "rationale_cider": 0.04142705358149631
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.5929553850525718,
        "rationale": {
          "rouge_l_mean": 0.2075937302493354,
          "text_similarity_mean": 0.5979321133080604,
          "llm_judge_score_mean": 5.532622904234157
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.042397799211613414,
          "std_iou": 0.10547218463585056,
          "median_iou": 0.0030551182557280133,
          "R@0.3": {
            "recall": 0.03007518796992481,
            "count": 8,
            "total": 266
          },
          "R@0.5": {
            "recall": 0.015037593984962405,
            "count": 4,
            "total": 266
          },
          "R@0.7": {
            "recall": 0.007518796992481203,
            "count": 2,
            "total": 266
          },
          "mae": {
            "start_mean": 242.80596616541354,
            "end_mean": 3777.037436090225,
            "average_mean": 2009.9217011278195
          },
          "rationale": {
            "rouge_l_mean": 0.24961546826049044,
            "rouge_l_std": 0.09986932358005221,
            "text_similarity_mean": 0.5193570497606818,
            "text_similarity_std": 0.1844581116146802,
            "llm_judge_score_mean": 2.4586466165413534,
            "llm_judge_score_std": 2.134610169180371
          },
          "rationale_cider": 0.28569479779340173
        },
        "02_Job_Interviews": {
          "mean_iou": 0.042835859644300936,
          "std_iou": 0.09135311391755714,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.023809523809523808,
            "count": 6,
            "total": 252
          },
          "R@0.5": {
            "recall": 0.007936507936507936,
            "count": 2,
            "total": 252
          },
          "R@0.7": {
            "recall": 0.003968253968253968,
            "count": 1,
            "total": 252
          },
          "mae": {
            "start_mean": 223.79827777777777,
            "end_mean": 221.43364285714287,
            "average_mean": 222.61596031746032
          },
          "rationale": {
            "rouge_l_mean": 0.22922950143670873,
            "rouge_l_std": 0.09792052315844924,
            "text_similarity_mean": 0.47869767722422407,
            "text_similarity_std": 0.19176840739789458,
            "llm_judge_score_mean": 2.2976190476190474,
            "llm_judge_score_std": 2.1571941021979972
          },
          "rationale_cider": 0.27194724815726834
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.03394961109120652,
          "std_iou": 0.08929712218371409,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.01749271137026239,
            "count": 6,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.011661807580174927,
            "count": 4,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0029154518950437317,
            "count": 1,
            "total": 343
          },
          "mae": {
            "start_mean": 417.8414081632653,
            "end_mean": 432.52393877551015,
            "average_mean": 425.1826734693878
          },
          "rationale": {
            "rouge_l_mean": 0.24088447437506683,
            "rouge_l_std": 0.09808309172741696,
            "text_similarity_mean": 0.5352618502121712,
            "text_similarity_std": 0.20881492742600682,
            "llm_judge_score_mean": 2.1661807580174925,
            "llm_judge_score_std": 2.0314780102432928
          },
          "rationale_cider": 0.17779702128003974
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.03972775664904029,
        "mae_average": 885.9067783048891,
        "R@0.3": 0.023792474383237006,
        "R@0.5": 0.011545303167215089,
        "R@0.7": 0.004800834285259634,
        "rationale": {
          "rouge_l_mean": 0.23990981469075534,
          "text_similarity_mean": 0.511105525732359,
          "llm_judge_score_mean": 2.3074821407259645
        }
      }
    }
  }
}