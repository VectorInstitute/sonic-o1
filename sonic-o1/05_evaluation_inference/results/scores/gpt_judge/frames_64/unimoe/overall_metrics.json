{
  "model": "unimoe",
  "experiment_name": "frames_64",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.22602170362879642,
            "rouge_l_std": 0.050623109018836605,
            "text_similarity_mean": 0.7545124031603336,
            "text_similarity_std": 0.0818667235396239,
            "llm_judge_score_mean": 6.0,
            "llm_judge_score_std": 1.8027756377319946
          },
          "short": {
            "rouge_l_mean": 0.2140051281491634,
            "rouge_l_std": 0.07071541132614173,
            "text_similarity_mean": 0.6863287501037121,
            "text_similarity_std": 0.10057971357517498,
            "llm_judge_score_mean": 5.8125,
            "llm_judge_score_std": 1.7399263633843818
          },
          "cider": {
            "cider_detailed": 0.031788351608154686,
            "cider_short": 0.03125422615459586
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.22215570749728045,
            "rouge_l_std": 0.043694053048616406,
            "text_similarity_mean": 0.7678974355970111,
            "text_similarity_std": 0.07368789228405813,
            "llm_judge_score_mean": 6.380952380952381,
            "llm_judge_score_std": 1.7313960916855091
          },
          "short": {
            "rouge_l_mean": 0.15578809440157831,
            "rouge_l_std": 0.05590534383886701,
            "text_similarity_mean": 0.5963542787801652,
            "text_similarity_std": 0.132456769638493,
            "llm_judge_score_mean": 4.809523809523809,
            "llm_judge_score_std": 2.0614153163727007
          },
          "cider": {
            "cider_detailed": 0.08685939680684164,
            "cider_short": 0.0071855331504719665
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.19789308192580862,
            "rouge_l_std": 0.0595801728822893,
            "text_similarity_mean": 0.6286860704421997,
            "text_similarity_std": 0.15726669350687486,
            "llm_judge_score_mean": 3.923076923076923,
            "llm_judge_score_std": 1.6853001769389728
          },
          "short": {
            "rouge_l_mean": 0.1404998624966072,
            "rouge_l_std": 0.061490361900231834,
            "text_similarity_mean": 0.5521292164921761,
            "text_similarity_std": 0.19216809004059174,
            "llm_judge_score_mean": 3.3846153846153846,
            "llm_judge_score_std": 2.1675388928623645
          },
          "cider": {
            "cider_detailed": 0.056502352103832684,
            "cider_short": 0.0005368118751859506
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.21535683101729516,
          "text_similarity_mean": 0.7170319697331814,
          "llm_judge_score_mean": 5.434676434676434
        },
        "short": {
          "rouge_l_mean": 0.17009769501578298,
          "text_similarity_mean": 0.6116040817920178,
          "llm_judge_score_mean": 4.6688797313797314
        },
        "cider": {
          "cider_detailed_mean": 0.05838336683960967,
          "cider_short_mean": 0.012992190393417926
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9313725490196079,
          "correct": 95,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.3108946475314931,
            "rouge_l_std": 0.09559630261654403,
            "text_similarity_mean": 0.7559251633344912,
            "text_similarity_std": 0.0973808127355684,
            "llm_judge_score_mean": 8.862745098039216,
            "llm_judge_score_std": 2.0724108140240474
          },
          "rationale_cider": 0.2743844545255492
        },
        "02_Job_Interviews": {
          "accuracy": 0.96,
          "correct": 96,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.29930723695950495,
            "rouge_l_std": 0.08471586039098285,
            "text_similarity_mean": 0.748736577630043,
            "text_similarity_std": 0.09167631997962379,
            "llm_judge_score_mean": 9.13,
            "llm_judge_score_std": 1.6891121928397772
          },
          "rationale_cider": 0.2298176671700588
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9043478260869565,
          "correct": 104,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.29554208095243756,
            "rouge_l_std": 0.09553417955996493,
            "text_similarity_mean": 0.7561892638387887,
            "text_similarity_std": 0.11378576651897929,
            "llm_judge_score_mean": 8.68695652173913,
            "llm_judge_score_std": 2.1563464164356474
          },
          "rationale_cider": 0.12264135701691055
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.931906791702188,
        "rationale": {
          "rouge_l_mean": 0.30191465514781185,
          "text_similarity_mean": 0.7536170016011076,
          "llm_judge_score_mean": 8.89323387325945
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.017166201849366316,
          "std_iou": 0.07280390222527762,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015151515151515152,
            "count": 4,
            "total": 264
          },
          "R@0.5": {
            "recall": 0.007575757575757576,
            "count": 2,
            "total": 264
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 264
          },
          "mae": {
            "start_mean": 1010.4640671522096,
            "end_mean": 4550.1804084952155,
            "average_mean": 2780.3222378237124
          },
          "rationale": {
            "rouge_l_mean": 0.23978489203616654,
            "rouge_l_std": 0.09084530162934089,
            "text_similarity_mean": 0.5369474994914719,
            "text_similarity_std": 0.1805901098234052,
            "llm_judge_score_mean": 2.0795454545454546,
            "llm_judge_score_std": 1.8962977740676392
          },
          "rationale_cider": 0.2781717001223131
        },
        "02_Job_Interviews": {
          "mean_iou": 0.021689681264144934,
          "std_iou": 0.10495207467961339,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02834008097165992,
            "count": 7,
            "total": 247
          },
          "R@0.5": {
            "recall": 0.020242914979757085,
            "count": 5,
            "total": 247
          },
          "R@0.7": {
            "recall": 0.012145748987854251,
            "count": 3,
            "total": 247
          },
          "mae": {
            "start_mean": 592.162807393889,
            "end_mean": 589.8333111195029,
            "average_mean": 590.9980592566961
          },
          "rationale": {
            "rouge_l_mean": 0.22453943565030898,
            "rouge_l_std": 0.08993475034464789,
            "text_similarity_mean": 0.4876192328350445,
            "text_similarity_std": 0.2001730710429343,
            "llm_judge_score_mean": 1.951417004048583,
            "llm_judge_score_std": 1.8587978833232233
          },
          "rationale_cider": 0.21873626397566637
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.00738585896987864,
          "std_iou": 0.04917356884036554,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.008797653958944282,
            "count": 3,
            "total": 341
          },
          "R@0.5": {
            "recall": 0.002932551319648094,
            "count": 1,
            "total": 341
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 341
          },
          "mae": {
            "start_mean": 1527.9804540070982,
            "end_mean": 1529.297320639754,
            "average_mean": 1528.6388873234262
          },
          "rationale": {
            "rouge_l_mean": 0.24274851051653326,
            "rouge_l_std": 0.09475336274404911,
            "text_similarity_mean": 0.5484990700751631,
            "text_similarity_std": 0.20890125043728616,
            "llm_judge_score_mean": 2.005865102639296,
            "llm_judge_score_std": 1.8074367957577402
          },
          "rationale_cider": 0.1795407327370385
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.015413914027796629,
        "mae_average": 1633.3197281346118,
        "R@0.3": 0.01742975002737312,
        "R@0.5": 0.010250407958387586,
        "R@0.7": 0.004048582995951417,
        "rationale": {
          "rouge_l_mean": 0.2356909460676696,
          "text_similarity_mean": 0.5243552674672265,
          "llm_judge_score_mean": 2.0122758537444447
        }
      }
    }
  }
}