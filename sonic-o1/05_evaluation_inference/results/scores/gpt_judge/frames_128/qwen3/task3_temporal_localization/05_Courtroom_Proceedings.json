{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 342,
  "aggregated_metrics": {
    "mean_iou": 0.03725644484584807,
    "std_iou": 0.12138796429638138,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.05263157894736842,
      "count": 18,
      "total": 342
    },
    "R@0.5": {
      "recall": 0.02046783625730994,
      "count": 7,
      "total": 342
    },
    "R@0.7": {
      "recall": 0.005847953216374269,
      "count": 2,
      "total": 342
    },
    "mae": {
      "start_mean": 47.2269678362573,
      "end_mean": 48.565856725146205,
      "average_mean": 47.89641228070175
    },
    "rationale": {
      "rouge_l_mean": 0.2819292817320851,
      "rouge_l_std": 0.08394578202794545,
      "text_similarity_mean": 0.7222495460719393,
      "text_similarity_std": 0.09752101116005353,
      "llm_judge_score_mean": 2.783625730994152,
      "llm_judge_score_std": 1.4408293421693092
    },
    "rationale_cider": 0.04695614959957251
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the attorney states they will only proceed with the sentencing hearing for disorderly conduct and not the breach of bail, when does Frank ask if the breach of bail charge is being dropped?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.292,
        "end": 41.433
      },
      "pred_interval": {
        "start": 52.4,
        "end": 54.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.107999999999997,
        "end": 12.767000000000003,
        "average": 12.4375
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.597608745098114,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates the attorney's utterance and gives incorrect timestamps for both events (contradicting the reference), only matching the temporal relation; therefore it fails to preserve key facts and timing."
      }
    },
    {
      "question_id": "002",
      "question": "After Frank states he is being persecuted for exercising his First Amendment rights, when does he say he was found guilty for being loud due to a disability?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 133.165,
        "end": 141.734
      },
      "pred_interval": {
        "start": 109.3,
        "end": 116.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.864999999999995,
        "end": 25.634000000000015,
        "average": 24.749500000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2363636363636364,
        "text_similarity": 0.758198618888855,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and the 'after' relation, but the timestamps are significantly incorrect (E2 is placed much earlier than the ground-truth 133.165\u2013141.734s and E1 timing is off), causing a major temporal mismatch."
      }
    },
    {
      "question_id": "003",
      "question": "After the attorney asks Frank why he is getting angry, when does Frank respond that he is angry because he is there for nothing?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 119.983,
        "end": 123.427
      },
      "pred_interval": {
        "start": 119.0,
        "end": 121.4
      },
      "iou": 0.3200813191777726,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9830000000000041,
        "end": 2.027000000000001,
        "average": 1.5050000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680854,
        "text_similarity": 0.7566565275192261,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the utterances and the 'after' relation and adds harmless visual detail; however, the timestamp spans deviate moderately from the reference (E1 ~0.1s late, E2 starts ~1s early and ends ~2s early), so it's not a perfect temporal match."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'Frank, you're in a courthouse,' when does the man respond by telling her to arrest him?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 173.35,
        "end": 176.25
      },
      "pred_interval": {
        "start": 174.4,
        "end": 176.4
      },
      "iou": 0.6065573770491762,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0500000000000114,
        "end": 0.15000000000000568,
        "average": 0.6000000000000085
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777778,
        "text_similarity": 0.542647123336792,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the man's reply content and roughly the E2 interval, but it misplaces E1 by about 6 seconds (174.4s vs 168.0\u2013169.3s) and incorrectly labels the relation as 'immediately after,' so the temporal alignment and relation are not accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the defendant was convicted of three counts of homicide by intoxicated use of a motor vehicle, when does he mention a count of injury by intoxicated use of a motor vehicle?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 16.341,
        "end": 19.565
      },
      "pred_interval": {
        "start": 129.9,
        "end": 131.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 113.559,
        "end": 111.935,
        "average": 112.747
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6013690233230591,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the injury count follows immediately after the homicide counts, but it is factually incorrect about the event timestamps (majorly different from 10.7\u201319.565s) and thus fails to match the reference timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the defendant was serving a significant prison sentence for those convictions, when does he specify the total sentence length?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.5,
        "end": 46.7
      },
      "pred_interval": {
        "start": 151.6,
        "end": 154.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 110.1,
        "end": 108.10000000000001,
        "average": 109.1
      },
      "rationale_metrics": {
        "rouge_l": 0.15217391304347827,
        "text_similarity": 0.6090161800384521,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relation and describes the content of E2, the event timestamps are completely mismatched from the reference (wrong offsets and durations) and thus fail to locate the specified events accurately."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker laments that Tim had a long life ahead of him, when does he state that rehabilitation is one of the reasons people are sent to prison?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.786,
        "end": 207.069
      },
      "pred_interval": {
        "start": 232.5,
        "end": 233.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.714,
        "end": 26.731000000000023,
        "average": 27.72250000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.22641509433962262,
        "text_similarity": 0.6332667469978333,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the anchor phrase and the relation 'after' but gives substantially incorrect timestamps for both events and misstates the target content (it describes rehabilitation as a possibility for Tim rather than that rehabilitation is a reason people are sent to prison), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male attorney finishes his address to the court, when does the judge ask if there are representatives of the victim?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 304.214,
        "end": 307.942
      },
      "pred_interval": {
        "start": 153.0,
        "end": 155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 151.214,
        "end": 152.942,
        "average": 152.078
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.5597543120384216,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the immediate post-conclusion relationship (judge speaks after the attorney), but it gives entirely different and incorrect timestamps for both events (152.3s/153.0s vs 300.0s/304.214\u2013307.942s), a major factual mismatch."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge states that the person has to use the microphone, when does the man in the white t-shirt (victim) begin moving to the microphone?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 356.0
      },
      "pred_interval": {
        "start": 179.0,
        "end": 181.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 173.0,
        "end": 175.0,
        "average": 174.0
      },
      "rationale_metrics": {
        "rouge_l": 0.41904761904761906,
        "text_similarity": 0.652405858039856,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal relation ('after'), but the provided timestamps are significantly incorrect compared to the ground truth and it omits the completion time of the initial movement, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "During the man's speech to the court, when does he say, 'You killed somebody that meant a lot'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 401.276,
        "end": 403.024
      },
      "pred_interval": {
        "start": 275.0,
        "end": 279.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 126.27600000000001,
        "end": 124.024,
        "average": 125.15
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168677,
        "text_similarity": 0.6625894904136658,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps for both the speech start (266.1s vs 368.0s) and the target phrase (275.0\u2013279.0s vs 401.276\u2013403.024s), contradicting the ground truth; it also adds an unverified detail about looking at the judge. These major temporal mismatches make the prediction essentially incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge warns the man in the white shirt about interrupting, when does the judge get up and leave the bench?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.13,
        "end": 331.15
      },
      "pred_interval": {
        "start": 448.1,
        "end": 450.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 116.97000000000003,
        "end": 119.75,
        "average": 118.36000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.815057635307312,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction grossly misstates the event timestamps and durations (placing events at ~448\u2013451s instead of ~330.5\u2013331.15s), and changes timing details; while it preserves the general 'after' relationship, the factual temporal information is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes asking the man if he wishes to address him, when does the man respond?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.38,
        "end": 331.39
      },
      "pred_interval": {
        "start": 454.8,
        "end": 455.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.42000000000002,
        "end": 124.50999999999999,
        "average": 123.965
      },
      "rationale_metrics": {
        "rouge_l": 0.345679012345679,
        "text_similarity": 0.8425196409225464,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and their immediate temporal relationship, but the timestamps are substantially incorrect and the response text omits 'Yes,' (saying only 'Sir'), so key factual timing and content details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the man in the white shirt states his child's birth date, when does he describe being happy and proud about becoming a parent?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.55,
        "end": 331.58
      },
      "pred_interval": {
        "start": 460.8,
        "end": 466.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.25,
        "end": 134.92000000000002,
        "average": 132.085
      },
      "rationale_metrics": {
        "rouge_l": 0.3191489361702127,
        "text_similarity": 0.8452727794647217,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the same event order but gives substantially different timestamps and durations (events at ~460.8s vs ~331.5s in the ground truth) and adds an 'immediately after' relation not supported by the reference; thus it contradicts key factual timing information."
      }
    },
    {
      "question_id": "001",
      "question": "After the man finishes his emotional statement, when does the woman with long dark hair start walking towards the speaker's table?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.096,
        "end": 512.12
      },
      "pred_interval": {
        "start": 510.0,
        "end": 513.0
      },
      "iou": 0.008000000000000304,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0960000000000036,
        "end": 0.8799999999999955,
        "average": 1.4879999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315794,
        "text_similarity": 0.5827116966247559,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the woman walks after the man's speech and describes the target event, but the provided timestamps are substantially wrong (off by ~1.7\u20132.1s) and it omits the woman's actual arrival time, so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman with long dark hair finishes sitting down at the table, when does she say 'Good afternoon'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.245,
        "end": 512.259
      },
      "pred_interval": {
        "start": 531.8,
        "end": 532.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.55499999999995,
        "end": 20.04099999999994,
        "average": 19.797999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.34210526315789475,
        "text_similarity": 0.5329867005348206,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation right (speech begins immediately after she sits) but the timestamps are significantly incorrect (off by ~19\u201320 seconds for start/end), so key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman says 'my son did not deserve this', when does she list his family relationships?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 513.109,
        "end": 513.197
      },
      "pred_interval": {
        "start": 535.5,
        "end": 540.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.390999999999963,
        "end": 27.102999999999952,
        "average": 24.746999999999957
      },
      "rationale_metrics": {
        "rouge_l": 0.09876543209876543,
        "text_similarity": 0.4180871248245239,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that she lists family relationships but gives substantially different and incorrect timing (535.5\u2013540.3s) and claims it immediately follows, whereas the reference places the listing at 513.109\u2013513.197s with a short pause after 512.593s; thus the timing and continuity are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states she misses the things that irritated her about her son, when does she request life without the possibility of parole?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 779.1,
        "end": 786.0
      },
      "pred_interval": {
        "start": 795.0,
        "end": 815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.899999999999977,
        "end": 29.0,
        "average": 22.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17307692307692307,
        "text_similarity": 0.5975762605667114,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the anchor and target time boundaries are grossly mislocalized (off by tens of seconds and with an overstated E2 end), so it fails to accurately match the reference timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first woman finishes her statement saying 'That's it', when does attorney Koenig begin describing the defendant's difficult upbringing?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 829.7,
        "end": 831.0
      },
      "pred_interval": {
        "start": 862.0,
        "end": 864.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.299999999999955,
        "end": 33.0,
        "average": 32.64999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.26168224299065423,
        "text_similarity": 0.7084486484527588,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially incorrect timestamps and quotes a different portion of speech, failing to identify Koenig's line about a 'difficult upbringing'; only the vague temporal relation ('once_finished') aligns with the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After attorney Koenig explains that the defendant did not pursue an NGI defense, when does she state that the NGI defense actually came from her and attorney Zawada?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 900.0
      },
      "pred_interval": {
        "start": 918.0,
        "end": 928.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 28.0,
        "average": 27.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2680412371134021,
        "text_similarity": 0.5564965605735779,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although the predicted correctly identifies the relative order and quotes the NGI-origin statement, the timestamps are substantially incorrect (shifted much later and extending beyond the video end), so it fails to match the key temporal details in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"mental illness\", when does she state that Joshua Skolman needs treatment in the institution?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.573,
        "end": 922.798
      },
      "pred_interval": {
        "start": 927.5,
        "end": 931.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.927000000000021,
        "end": 8.402000000000044,
        "average": 7.664500000000032
      },
      "rationale_metrics": {
        "rouge_l": 0.13725490196078433,
        "text_similarity": 0.5817479491233826,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the target statement follows the anchor, but the provided timestamps are significantly incorrect and the anchor/quote locations do not match the ground-truth intervals, so it is largely misaligned."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks Joshua Skolman if he has anything to tell him, when does Skolman explicitly deny having mental illness?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1001.283,
        "end": 1002.784
      },
      "pred_interval": {
        "start": 952.5,
        "end": 955.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.783000000000015,
        "end": 47.083999999999946,
        "average": 47.93349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777776,
        "text_similarity": 0.6936746835708618,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the denial phrase and that it follows the judge's question, but the provided timestamps are substantially different from the ground truth, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After Joshua Skolman states \"I don't have mental illness\", when does he next explicitly claim that \"you guys\" have the mental illness or defect?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1006.129,
        "end": 1009.331
      },
      "pred_interval": {
        "start": 959.5,
        "end": 962.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.62900000000002,
        "end": 46.63099999999997,
        "average": 46.629999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.849057137966156,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and segments do not match the ground truth (off by ~50 seconds) and thus fail to identify the correct immediate next claim; only the relative order is preserved, so it receives minimal credit."
      }
    },
    {
      "question_id": "001",
      "question": "After the Judge says, 'Thank you, Mr. Scolman', when does he state that only God creates life?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1149.0,
        "end": 1151.0
      },
      "pred_interval": {
        "start": 1170.0,
        "end": 1190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 39.0,
        "average": 30.0
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058815,
        "text_similarity": 0.6881159543991089,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after'), but both the anchor and target timestamps (and durations) are substantially different from the ground-truth intervals, making the prediction largely inaccurate. "
      }
    },
    {
      "question_id": "002",
      "question": "Once the clerk finishes commanding silence, when does the deputy say 'Be seated'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1109.8,
        "end": 1110.5
      },
      "pred_interval": {
        "start": 1158.0,
        "end": 1161.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.200000000000045,
        "end": 50.5,
        "average": 49.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.7019125819206238,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation (immediate follow-up) correct but gives incorrect and internally inconsistent timestamps (1158.0\u20131161.0 vs. 1109.6\u20131109.8) and misplaces the target event, so it fails to match the reference timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the Judge states 'Every life has value', when does he begin speaking about humanity's 'terrible, terrible dark side'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1164.5,
        "end": 1169.5
      },
      "pred_interval": {
        "start": 1254.0,
        "end": 1256.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.5,
        "end": 86.5,
        "average": 88.0
      },
      "rationale_metrics": {
        "rouge_l": 0.32653061224489793,
        "text_similarity": 0.6662170886993408,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and the quoted content, but both event timestamps and durations are substantially incorrect compared to the ground truth, representing major factual errors."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states he doesn't think it's a mental illness, when does he define a thoughtful conviction?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.453,
        "end": 1238.14
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1235.0
      },
      "iou": 0.19004914004914084,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4529999999999745,
        "end": 3.1400000000001,
        "average": 3.2965000000000373
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.6974684596061707,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct utterances and even labels the relation as 'after', but the provided timestamps are inaccurate (end times omitted and E2 start is misaligned and contradicts the temporal relation), so it does not match the reference precisely."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if he needs to recite historical crimes, when does he state that it causes one to pause and lose their breath?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1259.682,
        "end": 1264.588
      },
      "pred_interval": {
        "start": 1255.0,
        "end": 1258.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.682000000000016,
        "end": 6.587999999999965,
        "average": 5.634999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7289620637893677,
        "llm_judge_score": 4,
        "llm_judge_justification": "The anchor timing is roughly correct, and the relation 'after' matches the true relation, but the predicted target timestamp is wrong (it incorrectly starts at 1255.0s, overlapping the anchor instead of at ~1259.68s), so the prediction is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge declares that civilized society values life, when does he state that taking a life is the highest crime?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1362.484,
        "end": 1366.753
      },
      "pred_interval": {
        "start": 1290.0,
        "end": 1295.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.48399999999992,
        "end": 71.75299999999993,
        "average": 72.11849999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.5692014694213867,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the quoted lines and the 'after' relationship, but the timestamps are incorrect (both set to 1290.0 and far from the reference times 1349\u20131366s) and the identical start times contradict the claimed ordering."
      }
    },
    {
      "question_id": "001",
      "question": "After the inmate first looks down at the paper handed to him, when is he handed paper again by an officer?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1603.0,
        "end": 1603.4
      },
      "pred_interval": {
        "start": 1628.6,
        "end": 1630.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.59999999999991,
        "end": 27.299999999999955,
        "average": 26.449999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.8381725549697876,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events (initial look and being handed paper) but gives substantially different timestamps and durations, states an incorrect temporal relation (simultaneous/immediate rather than after), and includes inaccurate details, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the inmate turns his head to the side, when does he start walking away, escorted by the officers?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1626.0,
        "end": 1627.0
      },
      "pred_interval": {
        "start": 1634.9,
        "end": 1636.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.900000000000091,
        "end": 9.5,
        "average": 9.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3047619047619048,
        "text_similarity": 0.8696108460426331,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the same events (head turn then walking) but gets all timestamps wrong, claims the target begins simultaneously with the anchor and labels the relation 'immediately after', and adds unwarranted motion detail\u2014contradicting the correct answer's timing (1626s vs 1600s/1601s) and temporal order."
      }
    },
    {
      "question_id": "003",
      "question": "After the distinct sound of a door or gate opening, when does the inmate walk through the door?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1637.0
      },
      "pred_interval": {
        "start": 1636.8,
        "end": 1640.4
      },
      "iou": 0.04545454545455485,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7999999999999545,
        "end": 3.400000000000091,
        "average": 2.1000000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.30612244897959184,
        "text_similarity": 0.8906644582748413,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the ground truth on key facts: the anchor (door sound) timing is wrong by ~33s, the target timing and end are incorrect, the temporal relation ('same time as') conflicts with the correct 'significantly after', and it adds unsupported details."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states, \"You will never be released from the state prison system,\" when does he mention a \"compass evaluation\" as part of the extended supervision conditions?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1433.0,
        "end": 1436.0
      },
      "pred_interval": {
        "start": 1508.0,
        "end": 1515.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.0,
        "end": 79.0,
        "average": 77.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2931034482758621,
        "text_similarity": 0.7367384433746338,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their ordering ('after'), but the timestamps and durations are substantially offset from the ground truth (off by ~70+ seconds), so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge mentions that the 25-year sentence for Count 2 will be consecutive to any sentence the defendant is now serving, when does the camera cut to a shot of the back of the defendant's head?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.8,
        "end": 1440.5
      },
      "pred_interval": {
        "start": 1517.0,
        "end": 1521.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.20000000000005,
        "end": 80.5,
        "average": 78.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.22429906542056074,
        "text_similarity": 0.7512052059173584,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering right (camera cuts after the judge's statement) and quotes the judge, but the timestamps are substantially incorrect compared to the reference (off by ~70s) and thus fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge orders the $5,000 paid by the Compensation Panel to be part of the restitution order, when does the defendant stand up in the courtroom?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1539.0,
        "end": 1542.0
      },
      "pred_interval": {
        "start": 1536.0,
        "end": 1541.0
      },
      "iou": 0.3333333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 1.0,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953488,
        "text_similarity": 0.7501387596130371,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the defendant standing (timings close to ground truth) and the quoted judge line, but it substantially mislocates the judge's utterance (predicted E1 ends ~1535.8s vs ground-truth 1465.0s), so the temporal relation and E1 timing are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man looks up and turns his head to his left, when do the man and deputies open the door and exit the room?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1642.992
      },
      "gt_interval": {
        "start": 1631.0,
        "end": 1634.0
      },
      "pred_interval": {
        "start": 1641.4,
        "end": 1643.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.400000000000091,
        "end": 9.0,
        "average": 9.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.7136449217796326,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction describes the same actions (man looks up/turns and they exit) but the timestamps are substantially incorrect and the temporal relation is wrong (predicted 'simultaneous' vs correct sequential timing); it also adds unsupported details (specific deputies count) and mismatches durations."
      }
    },
    {
      "question_id": "001",
      "question": "While the male anchor announces a jury has reached a verdict in the Chandler Halderson case, when does the on-screen text 'JURY REACHES VERDICT' first appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 4.6,
        "end": 22.8
      },
      "pred_interval": {
        "start": 1.0,
        "end": 1.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5999999999999996,
        "end": 21.2,
        "average": 12.399999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4810126582278481,
        "text_similarity": 0.7959811687469482,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes the anchor starts immediately, but it misstates the on-screen text timing (1.0s vs correct 4.6s), omits an end time, and incorrectly labels the relation as 'after' instead of 'during', so key temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female anchor states Halderson faces eight charges, when does the on-screen graphic detailing the charges appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.7,
        "end": 35.8
      },
      "pred_interval": {
        "start": 27.0,
        "end": 34.0
      },
      "iou": 0.578512396694215,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.3000000000000007,
        "end": 1.7999999999999972,
        "average": 2.549999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.8523085117340088,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys that the graphic follows the anchor's statement, but the reported timestamps are several seconds off from the ground truth and internally inconsistent (graphic start at 27.0s precedes the predicted anchor end at 27.5s), so the timing is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male anchor says 'Let's actually go to the courtroom now live', when does the judge begin speaking?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.7,
        "end": 204.9
      },
      "pred_interval": {
        "start": 200.0,
        "end": 200.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.6999999999999886,
        "end": 4.099999999999994,
        "average": 3.8999999999999915
      },
      "rationale_metrics": {
        "rouge_l": 0.47222222222222227,
        "text_similarity": 0.9173471927642822,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the judge as the next speaker but the timestamps are substantially incorrect (anchor 200.9s vs 199.4s; judge start 203.7s vs 200.0s) and it wrongly asserts the judge begins immediately after the anchor despite an actual ~2.8s gap."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking if there is anything from the state, when does the state reply?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.02,
        "end": 151.03
      },
      "pred_interval": {
        "start": 300.2,
        "end": 300.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 149.17999999999998,
        "end": 149.77,
        "average": 149.475
      },
      "rationale_metrics": {
        "rouge_l": 0.2985074626865672,
        "text_similarity": 0.7551829814910889,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the state replies immediately after the judge and matches the utterances, but it gives entirely incorrect event timestamps (300s vs ~151s), so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the female reporter states the jury deliberated for just over two hours, when does the male reporter comment on the quick verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.45,
        "end": 152.5
      },
      "pred_interval": {
        "start": 330.7,
        "end": 331.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 178.25,
        "end": 178.60000000000002,
        "average": 178.425
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6283812522888184,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the correct utterances but gives completely incorrect timestamps (330s vs ~152s) and wrongly labels the relation as 'immediately after' despite the reference noting intervening discussion; major factual errors in timing and temporal relation reduce the match to minimal."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes asking if the jury reached a verdict on all counts, when does the jury foreman respond?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.2,
        "end": 153.2
      },
      "pred_interval": {
        "start": 354.4,
        "end": 354.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 201.2,
        "end": 201.60000000000002,
        "average": 201.4
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7473602294921875,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the judge's question, the jury foreman's reply, and that the reply follows the question, but the timestamps are substantially incorrect (354s vs 153s in the ground truth), so it fails to match the key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "After the foreperson states that the jury reached a verdict, when does the court staff receive the verdict folder?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.2,
        "end": 357.9
      },
      "pred_interval": {
        "start": 430.0,
        "end": 434.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.80000000000001,
        "end": 76.10000000000002,
        "average": 74.45000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7138909101486206,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general ordering (verdict then folder handoff) but gives completely different timestamps, adds unsupported details (naming 'Randy' and a bailiff action), and thus contradicts and omits key factual timing information from the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes reading the verdict for Count 1, when does he begin reading the verdict for Count 2?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.7,
        "end": 445.2
      },
      "pred_interval": {
        "start": 381.0,
        "end": 384.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.69999999999999,
        "end": 61.19999999999999,
        "average": 60.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7598929405212402,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relational notion that Count 2 begins immediately after Count 1, but the timestamps for both E1 and E2 (381.0\u2013384.0s) contradict the ground truth (441.7s start and 445.2s end) and thus the answer is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge finishes reading the verdict for Count 8, when does he state that 'not guilty' verdict forms were also returned?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 630.9,
        "end": 641.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 517.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 120.89999999999998,
        "end": 124.0,
        "average": 122.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.29702970297029707,
        "text_similarity": 0.7533130645751953,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and event span (510.0\u2013517.0s) do not match the reference times (~628.8s for E1 and 630.9\u2013641.0s for E2), and thus contradict the correct temporal relation; the answer is therefore incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge finishes reading the final guilty verdict for count 8, when does he begin to inquire if the jury members agreed with the verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 528.9,
        "end": 619.0
      },
      "pred_interval": {
        "start": 637.5,
        "end": 641.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.60000000000002,
        "end": 22.0,
        "average": 65.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.30158730158730157,
        "text_similarity": 0.7678602933883667,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction notes the judge's jury inquiry and correct temporal relation ('after'), but it misidentifies the anchor event and gives completely incorrect timestamps for both E1 (637.5s vs 513.0s) and E2 (637.5\u2013641.0s vs 528.9\u2013619.0s), omitting key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the last juror confirms their agreement to the verdicts, when does the judge begin his speech thanking the jury for their service?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 621.0,
        "end": 665.0
      },
      "pred_interval": {
        "start": 649.0,
        "end": 654.5
      },
      "iou": 0.125,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.0,
        "end": 10.5,
        "average": 19.25
      },
      "rationale_metrics": {
        "rouge_l": 0.25742574257425743,
        "text_similarity": 0.7023390531539917,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the key timestamps are substantially wrong (E1 at 649.0s vs 617.0s; E2 start at 649.0s vs 621.0s; E2 end 654.5s vs 665.0s) and it adds an unconstrained quoted line, so it is largely inaccurate despite the correct ordering."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes instructing the defendant's table to be seated, when does Attorney Brown make his motion?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 741.0
      },
      "pred_interval": {
        "start": 688.5,
        "end": 690.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.5,
        "end": 50.5,
        "average": 49.5
      },
      "rationale_metrics": {
        "rouge_l": 0.26262626262626265,
        "text_similarity": 0.7526299357414246,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same types of events (judge telling to be seated and Attorney Brown making a motion) but the timestamps and durations are materially incorrect (688.5\u2013690.5s vs. correct 732.0s and 737.0\u2013741.0s), and the timing/relationship is inconsistent, so it fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking about any pre-sentence request, when does Attorney Brown state that a pre-sentence investigation should be performed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "pred_interval": {
        "start": 699.7,
        "end": 701.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.7000000000000455,
        "end": 3.7000000000000455,
        "average": 4.2000000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.14893617021276595,
        "text_similarity": 0.7155138254165649,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the topic (Attorney Brown addressing the pre-sentence investigation) but gives incorrect time spans and an incorrect relation: E2 is placed much later (699.7s) and labeled 'after' rather than immediately following the judge (correct E2 starts ~695.0s and relation 'once_finished'), so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge orders a pre-sentence investigation, when does he specify that there should not be any recommendations made within the report?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.6,
        "end": 754.5
      },
      "pred_interval": {
        "start": 708.1,
        "end": 711.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.5,
        "end": 43.10000000000002,
        "average": 42.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21505376344086022,
        "text_similarity": 0.7509992122650146,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same events (judge ordering the PSI and specifying no recommendations) but the timestamps are significantly incorrect and the relation is labeled 'after' instead of the correct immediate 'once_finished', so key factual alignment is missing."
      }
    },
    {
      "question_id": "003",
      "question": "After the news anchor finishes stating the sentencing hearing dates, when does the District Attorney begin his statement to the media?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 935.0,
        "end": 938.5
      },
      "pred_interval": {
        "start": 731.5,
        "end": 733.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 203.5,
        "end": 205.10000000000002,
        "average": 204.3
      },
      "rationale_metrics": {
        "rouge_l": 0.3092783505154639,
        "text_similarity": 0.846795916557312,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the qualitative relation ('after') right, it is largely incorrect: the predicted time ranges and start/end times differ drastically from the reference (\u2248721\u2013733s vs \u2248903.8\u2013938.5s) and it includes an unsupported quoted remark, so it fails on factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the system does not work without jurors, when does he mention that the jury of 18 sacrificed a month of their life?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 901.4,
        "end": 908.9
      },
      "pred_interval": {
        "start": 880.7,
        "end": 901.8
      },
      "iou": 0.014184397163119795,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.699999999999932,
        "end": 7.100000000000023,
        "average": 13.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.20370370370370372,
        "text_similarity": 0.552087664604187,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the semantic relation and phrasing right (the jury sacrifice follows the DA's remark), but the anchor and target timestamps are substantially off from the ground truth, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the District Attorney states that the process had to take a whole week off because of the virus, when does he commend people for sacrificing their time and potentially their safety?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 971.4,
        "end": 982.1
      },
      "pred_interval": {
        "start": 905.7,
        "end": 911.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.69999999999993,
        "end": 70.39999999999998,
        "average": 68.04999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.2476190476190476,
        "text_similarity": 0.6967980265617371,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the semantic link (virus comment followed by commendation) but the timestamps are substantially incorrect and contradict the ground truth (off by ~66s), so it cannot be considered correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer asks for a message to Barton Krista's family, when does the District Attorney state he will speak to them after?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1027.2,
        "end": 1028.7
      },
      "pred_interval": {
        "start": 914.4,
        "end": 916.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 112.80000000000007,
        "end": 112.0,
        "average": 112.40000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.29885057471264365,
        "text_similarity": 0.642806887626648,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the speaker's content and the 'after' relationship, but the timestamps are substantially incorrect (off by ~114 seconds) and do not match the reference timing, so the alignment is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the Sheriff's Department could get the case to trial in short order, when does he talk about the professionalism and integrity of law enforcement?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1088.6,
        "end": 1095.4
      },
      "pred_interval": {
        "start": 1105.0,
        "end": 1111.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.40000000000009,
        "end": 15.599999999999909,
        "average": 16.0
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.7286657094955444,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same events and correctly labels the relation as 'after', but the timestamps for both events\u2014especially E2\u2014are substantially later than the reference, and it adds an unsupported causal conclusion about the DA's intent. These factual/time inaccuracies and the extra inference reduce correctness and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer asks about the historic significance and challenges of the trial, when does the District Attorney confirm the trial was unprecedented?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.2,
        "end": 1202.0
      },
      "pred_interval": {
        "start": 1142.0,
        "end": 1144.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.200000000000045,
        "end": 58.0,
        "average": 58.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.6727313995361328,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the anchor and target timestamps are ~60s earlier than the reference, the relation is labeled 'after' instead of the immediate 'once_finished', and the described target utterance/meaning does not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the District Attorney says they may never know 'the why' behind the crime, when does the news anchor cut in to summarize this point?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1358.6,
        "end": 1367.8
      },
      "pred_interval": {
        "start": 1183.0,
        "end": 1185.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 175.5999999999999,
        "end": 182.79999999999995,
        "average": 179.19999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.26,
        "text_similarity": 0.8173218965530396,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction partially captures that the anchor summarizes the DA's point, but it gives entirely incorrect timestamps and an incorrect relation label relative to the reference (events occur much later and are 'next' in the ground truth). The mismatch in timing and sequence makes the answer largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the news anchor finishes stating that the system worked and the verdict graphic is fully shown, when does the narrator begin listing the specific guilty verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1265.0,
        "end": 1275.0
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1231.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.0,
        "end": 44.0,
        "average": 39.5
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.7661057710647583,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the ground truth on both timestamps and sequence: E1 is given as 1230.0s vs the correct 1257.0s, and E2 is placed at 1231.0s (with a quoted verdict) instead of the correct 1265.0\u20131275.0s interval, so the timing and relation are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After reporter Jaymes Langrehr finishes saying that the sheer volume of evidence proved to be overwhelming for the jurors, when does he explain that DNA analysts proved bloodstains belonged to the parents?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1350.0,
        "end": 1364.0
      },
      "pred_interval": {
        "start": 1279.0,
        "end": 1281.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.0,
        "end": 83.0,
        "average": 77.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.7206919193267822,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the ordering and gist of the DNA claim right, but it gives substantially incorrect timestamps (both E1 and E2) and omits the E2 end time; thus key temporal details from the correct answer are missing/mismatched."
      }
    },
    {
      "question_id": "003",
      "question": "Once reporter Jaymes Langrehr finishes saying there was no one specific piece of evidence that 'really stuck out', when is the next time he mentions bringing in the DNA analysts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1348.0,
        "end": 1352.0
      },
      "pred_interval": {
        "start": 1279.0,
        "end": 1280.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.0,
        "end": 72.0,
        "average": 70.5
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424238,
        "text_similarity": 0.7466337084770203,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence and mention of DNA analysts and labels the relation as 'immediately after', but the timestamps are substantially incorrect (E1 and E2 times differ from the reference) and the E2 end time is omitted, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After MS. NULAND finishes asking \"What's your response to that?\", when does the Sheriff begin responding about his faith in the team?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.47,
        "end": 1430.395
      },
      "pred_interval": {
        "start": 1461.8,
        "end": 1463.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.32999999999993,
        "end": 32.70499999999993,
        "average": 34.01749999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.42424242424242425,
        "text_similarity": 0.7632693648338318,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the Sheriff's response follows Ms. Nuland's question, but it gives substantially incorrect timestamps and changes the relation to 'immediately after,' contradicting the ground truth timing and relation details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Sheriff finishes explaining what he will remember most about the case, when does a reporter ask about the case informing future work?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1491.452,
        "end": 1494.597
      },
      "pred_interval": {
        "start": 1463.1,
        "end": 1464.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.35200000000009,
        "end": 30.297000000000025,
        "average": 29.324500000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.3939393939393939,
        "text_similarity": 0.7231174111366272,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps for both events and an incorrect relation label; it does not match the correct event timings (1490.792s/1491.452\u20131494.597s) or the specified 'once_finished' relation, so it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the reporter finishes their commentary thanking Sheriff Calvin Baer and his deputies for their hard work, when does the reporter next mention that \"A lot of resources involved for a very long time\"?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1528.402,
        "end": 1530.927
      },
      "pred_interval": {
        "start": 1477.7,
        "end": 1478.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.702,
        "end": 52.526999999999816,
        "average": 51.61449999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.28205128205128205,
        "text_similarity": 0.6441214084625244,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the same events but gives entirely incorrect timestamps (1477.7\u20131478.4s vs. correct 1528.303\u20131530.927s) and a different temporal relation phrasing; due to the major timing mismatch it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the main anchor introduces Tahlil Maudeen, when does Tahlil begin reporting on the emotion in the gallery?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1701.902,
        "end": 1708.327
      },
      "pred_interval": {
        "start": 1620.7,
        "end": 1623.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.202,
        "end": 84.92699999999991,
        "average": 83.06449999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.10638297872340427,
        "text_similarity": 0.44621729850769043,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct semantic content (Tahlil describes emotion in the gallery) but the reported timestamps are substantially incorrect and contradict the ground-truth timings, so the factual timing information is unreliable."
      }
    },
    {
      "question_id": "002",
      "question": "After the main anchor asks Tahlil if she talked to Chandler Halderson's defense attorneys, when does the anchor next ask about whom Tahlil interviewed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1765.828,
        "end": 1767.07
      },
      "pred_interval": {
        "start": 1654.7,
        "end": 1657.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.12799999999993,
        "end": 109.76999999999998,
        "average": 110.44899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.5749019384384155,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the next question's content ('And what about the prosecutors?') but gives completely incorrect timestamps and wrongly claims the follow-up was immediate; it therefore fails to match the correct temporal details and relation."
      }
    },
    {
      "question_id": "003",
      "question": "After Tahlil Maudeen reports that the defense attorneys were unavailable for interviews, when does she report that the DA seemed pleased with the verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1769.555,
        "end": 1783.597
      },
      "pred_interval": {
        "start": 1682.8,
        "end": 1686.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.75500000000011,
        "end": 97.29700000000003,
        "average": 92.02600000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.37808772921562195,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the sequence (DA comment comes after the defense-unavailable remark) but the timestamps are substantially incorrect (off by ~100s) and do not match the ground-truth segment boundaries, so it fails to correctly locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "After the reporter asks if people were surprised by the quick jury return, when does she explain why they were expecting it?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1789.692,
        "end": 1798.408
      },
      "pred_interval": {
        "start": 1821.5,
        "end": 1832.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.807999999999993,
        "end": 34.0920000000001,
        "average": 32.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.25882352941176473,
        "text_similarity": 0.788190484046936,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the same question, the reporter's explanatory utterance, and that the target occurs after the anchor, but the provided timestamps are substantially off from the ground truth (by ~30\u201340 seconds), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host says, 'Thank you all,' when does she begin to introduce the website for more information on the case?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1809.891,
        "end": 1815.742
      },
      "pred_interval": {
        "start": 1833.5,
        "end": 1847.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.608999999999924,
        "end": 31.758000000000038,
        "average": 27.68349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837845,
        "text_similarity": 0.8092096447944641,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the website intro follows, but the timestamps are substantially off (starts ~24s later and ends much later than the reference) and it omits the fact the target immediately follows the anchor, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the host says, 'Thanks for joining us,' when does she announce the return to regular programming?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1830.005,
        "end": 1831.628
      },
      "pred_interval": {
        "start": 1853.0,
        "end": 1858.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.99499999999989,
        "end": 26.37200000000007,
        "average": 24.68349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3488372093023256,
        "text_similarity": 0.790762722492218,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives substantially incorrect timestamps (about 23s later) and an incorrect temporal relation (says simultaneous vs ground truth 'immediately follows'), so it is largely inaccurate despite matching the phrases."
      }
    },
    {
      "question_id": "001",
      "question": "After the narrator explains that the judges stopped the video, when does the judge in the embedded video actually stop the video and question the man?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 217.92,
        "end": 221.605
      },
      "pred_interval": {
        "start": 21.1,
        "end": 22.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 196.82,
        "end": 199.605,
        "average": 198.21249999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.38961038961038963,
        "text_similarity": 0.7521921992301941,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the temporal relation right but the event time spans are largely incorrect (both E1 and E2 timestamps differ drastically from the ground truth), and it misplaces the judge's interruption by over three minutes, so it fails to match the correct annotations."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if the video is counsel for the case, when does the man reply, 'I generated that'?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 224.77,
        "end": 225.951
      },
      "pred_interval": {
        "start": 23.7,
        "end": 24.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 201.07000000000002,
        "end": 201.15099999999998,
        "average": 201.1105
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.746319055557251,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same utterances and the 'after' relation, but the event timestamps are massively incorrect (predicted ~22\u201324s vs. ground-truth ~218\u2013226s), so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes telling the man he is not going to use the courtroom as a business launch, when does she instruct him to stand up for oral argument time?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 323.425,
        "end": 328.018
      },
      "pred_interval": {
        "start": 56.3,
        "end": 57.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 267.125,
        "end": 270.818,
        "average": 268.9715
      },
      "rationale_metrics": {
        "rouge_l": 0.20454545454545456,
        "text_similarity": 0.7615748047828674,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly quotes the judge's statement but gives completely incorrect timestamps and misidentifies E2 (it quotes a different line rather than the instruction to stand for oral argument), so the temporal relation and key event content do not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says \"That is not a real person\", when does the judge say it would have been nice to know that information?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 154.321,
        "end": 158.401
      },
      "pred_interval": {
        "start": 169.3,
        "end": 170.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.979000000000013,
        "end": 12.198999999999984,
        "average": 13.588999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.6507053375244141,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the quoted lines and the 'after' relationship, but the provided timestamps are substantially incorrect compared to the ground truth intervals, so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes describing the man's past verbal conversations with her staff, when does she say she doesn't appreciate being misled?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 177.031,
        "end": 181.351
      },
      "pred_interval": {
        "start": 198.3,
        "end": 199.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.269000000000005,
        "end": 18.449000000000012,
        "average": 19.85900000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.6410460472106934,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event relation and quoted phrase right, but the temporal boundaries are substantially incorrect (predicted ~198.3s vs. ground-truth ~168\u2013181s), so it mislocates both anchor and target."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge states that the man is not going to use the courtroom as a launch for his business, when does she command him to \"shut that off\"?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 200.411,
        "end": 201.981
      },
      "pred_interval": {
        "start": 210.0,
        "end": 210.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.588999999999999,
        "end": 8.919000000000011,
        "average": 9.254000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.7131834030151367,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly shifts both timestamps to ~210s and asserts the command occurs immediately at the same time the anchor ends, which contradicts the ground truth timings (anchor ~191\u2013197s, target ~200\u2013202s); it only correctly notes that the target follows the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interrogator asks what kind of objects were used, when does the witness name a toothbrush and shaving utensil?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 150.14,
        "end": 150.22
      },
      "pred_interval": {
        "start": 158.2,
        "end": 158.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.060000000000002,
        "end": 8.280000000000001,
        "average": 8.170000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7291088104248047,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the objects named and the 'immediately after' relationship, but the timestamps are substantially off (~8 seconds later) and do not match the precise event boundaries given in the ground truth, so it only partially matches."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interrogator asks if the abuser decided to use something besides a toothbrush, when does the witness answer 'Yes'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.11,
        "end": 151.12
      },
      "pred_interval": {
        "start": 167.4,
        "end": 167.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.289999999999992,
        "end": 16.680000000000007,
        "average": 16.485
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7538740634918213,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the interrogator's question, the witness 'Yes', and the 'immediately after' relation, but it mislocates both events by about 16 seconds (167.4/167.8s vs. 151.01/151.11s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interrogator asks what his mom said, when does the witness recount his mother's full response?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.06,
        "end": 153.23
      },
      "pred_interval": {
        "start": 185.2,
        "end": 185.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.139999999999986,
        "end": 32.27000000000001,
        "average": 32.205
      },
      "rationale_metrics": {
        "rouge_l": 0.297029702970297,
        "text_similarity": 0.7720438241958618,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor question, the witness reply content, and the 'immediately after' relation, but the timestamps are significantly incorrect (off by ~32 seconds) and thus fail to match the precise temporal grounding in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What did your mom say?\", when does the man respond about his mom telling him to stop and that he was exaggerating?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 340.0
      },
      "pred_interval": {
        "start": 339.333,
        "end": 356.111
      },
      "iou": 0.030165980733570325,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.333000000000027,
        "end": 16.11099999999999,
        "average": 10.722000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.8136798143386841,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the speaker and that his reply follows the question, but it significantly misstates the timestamps (both anchor and target), extends the target far beyond the correct end time, and changes the relation to 'immediately after' while introducing unfounded timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he was afraid to tell anyone because his dad didn't want him to, when does he reveal his dad said it was 'our secret'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 387.0,
        "end": 389.0
      },
      "pred_interval": {
        "start": 474.0,
        "end": 485.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.0,
        "end": 96.0,
        "average": 91.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4235294117647058,
        "text_similarity": 0.8534443974494934,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the semantic relation (the revelation follows the fear statement) and reproduces the quoted phrase, but it gives incorrect segment timings for both anchor and target and thus fails to align the events with the correct video timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes asking, \"What did you do to your brother?\", when does the man describe taking him to the woods and playing with a toothbrush in the same way?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 427.0,
        "end": 438.0
      },
      "pred_interval": {
        "start": 522.0,
        "end": 535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 95.0,
        "end": 97.0,
        "average": 96.0
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.614695131778717,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the semantic sequence (question then the man's description) but gives substantially different timestamps (off by ~95\u2013100s) and labels the relation as 'immediately after' rather than the specified 'once_finished', so it is not temporally accurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once Lyle Menendez removes his hand from his mouth/face, when does he put his fingers back into his mouth while crying?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 515.8,
        "end": 517.7
      },
      "pred_interval": {
        "start": 513.8,
        "end": 515.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 2.1000000000000227,
        "average": 2.0500000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.6310234069824219,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an immediate-after relationship and the gesture, but it gives wrong timestamps (513s vs 515.7\u2013515.8s), refers to the wrong person (Erik vs Lyle), and misstates event endpoints, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the female voice asks, 'It stopped for you with your dad when you were eight,' when is Erik Menendez first shown on screen?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 536.0,
        "end": 579.0
      },
      "pred_interval": {
        "start": 532.2,
        "end": 532.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.7999999999999545,
        "end": 46.200000000000045,
        "average": 25.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.79826420545578,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same events, relation ('during'), and the visual cue (wide to close-up), but the timestamps are significantly off (E1 predicted at 532.0s vs 533.5s; E2 predicted at 532.2s vs 536.0s), so it contradicts the correct timing of when Erik Menendez first appears."
      }
    },
    {
      "question_id": "003",
      "question": "After Erik Menendez says 'Yes' in response to doing something about it, when does the female voice ask 'What did you do?'",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 560.0,
        "end": 560.8
      },
      "pred_interval": {
        "start": 536.6,
        "end": 537.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.399999999999977,
        "end": 23.0,
        "average": 23.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.6042826175689697,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering (female asks after Erik) but gives incorrect timestamps (off by ~21s) and wrongly characterizes the gap as 'immediately after' despite the reference showing a several-second pause, so key temporal details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the camera shows Lyle Menendez crying and sniffling, when does the female voice ask about the abuse stopping when he was eight?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 533.5,
        "end": 536.5
      },
      "pred_interval": {
        "start": 510.0,
        "end": 517.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.5,
        "end": 18.700000000000045,
        "average": 21.100000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.345679012345679,
        "text_similarity": 0.6864288449287415,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relation, it provides substantially incorrect timestamps for both E1 and E2 that conflict with the ground truth, so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "During the female voice asking if he thought it might be happening to someone else, when is Erik Menendez shown with a distressed facial expression?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 539.0,
        "end": 545.8
      },
      "pred_interval": {
        "start": 523.2,
        "end": 527.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.799999999999955,
        "end": 18.0,
        "average": 16.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.5316455696202531,
        "text_similarity": 0.6840181350708008,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event timings and their overlap: the correct events occur from 539.0\u2013545.8s (with Erik shown throughout), whereas the prediction gives 523.2\u2013527.8s and 527.8\u2013528.8s respectively, which do not match and do not actually overlap as claimed."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female voice finishes asking 'And who did you think it was happening to?', when does Erik Menendez answer 'Eric'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 551.0,
        "end": 551.5
      },
      "pred_interval": {
        "start": 530.0,
        "end": 535.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 15.700000000000045,
        "average": 18.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3733333333333333,
        "text_similarity": 0.6470047235488892,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation right (E2 after E1) but the timestamps and event intervals are substantially incorrect compared with the ground truth (off by ~18\u201321 seconds), so the answer is largely wrong despite the correct relative label."
      }
    },
    {
      "question_id": "001",
      "question": "After the Presiding Justice asks for the appearance of counsel, when does the appellant's counsel introduce himself?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.401,
        "end": 18.83
      },
      "pred_interval": {
        "start": 21.7,
        "end": 25.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.299,
        "end": 6.970000000000002,
        "average": 8.634500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.7151630520820618,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and that the introduction occurs after the request, but it gives substantially incorrect start/end timestamps (off by ~14s), so key factual timing details do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "During the Presiding Justice's question about public importance, when does Mr. Lifrak remain silent and attentive?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.5,
        "end": 103.0
      },
      "pred_interval": {
        "start": 177.3,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.8,
        "end": 77.0,
        "average": 107.4
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7743070721626282,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'during' relation and that Mr. Lifrak is silent, but it gives completely different and incorrect timestamps for both the Presiding Justice's question and Mr. Lifrak's silence, contradicting the ground truth temporal span."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Lifrak finishes asking to reserve time for rebuttal, when does the Presiding Justice grant permission?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 109.412,
        "end": 110.2
      },
      "pred_interval": {
        "start": 188.9,
        "end": 190.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.488,
        "end": 80.10000000000001,
        "average": 79.79400000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.41237113402061853,
        "text_similarity": 0.8348335027694702,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the events and their 'once_finished' relation, but the timestamps differ substantially from the ground truth (an ~82s offset), so the answer is factually inaccurate on key details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (bottom left) mentions Mr. Hothi injected himself into controversies, when does he state that Mr. Hothi said very publicly on Twitter that Mr. Musk and Tesla were lying?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 196.5,
        "end": 201.5
      },
      "pred_interval": {
        "start": 174.8,
        "end": 184.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.69999999999999,
        "end": 17.400000000000006,
        "average": 19.549999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.1728395061728395,
        "text_similarity": 0.7031400203704834,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the verbal content of E2 but the time alignments are substantially incorrect (both anchor and target times are ~15\u201320s earlier than the reference) and thus do not match the referenced events/ordering; major timestamp misalignment warrants a low score."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker (bottom left) is detailing Mr. Hothi's actions, when does he mention Mr. Hothi hitting an employee with a car?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 283.6,
        "end": 285.5
      },
      "pred_interval": {
        "start": 192.3,
        "end": 194.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 91.30000000000001,
        "end": 91.19999999999999,
        "average": 91.25
      },
      "rationale_metrics": {
        "rouge_l": 0.14457831325301201,
        "text_similarity": 0.6232246160507202,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates the timestamps\u2014placing the target at 192.3\u2013194.3s rather than the correct 283.6\u2013285.5s\u2014and thus contradicts the reference timing despite correctly identifying the phrase as the target."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge (top left) finishes expressing her doubt about the relevance of Mr. Hothi's actions, when does the speaker (bottom left) begin his response?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 339.9,
        "end": 350.0
      },
      "pred_interval": {
        "start": 210.3,
        "end": 212.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.59999999999997,
        "end": 137.3,
        "average": 133.45
      },
      "rationale_metrics": {
        "rouge_l": 0.24691358024691354,
        "text_similarity": 0.7677789926528931,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly gives substantially different timestamps and even misidentifies the speaker/content (saying 'Mr. Musk' and ending at 210.3s vs the ground-truth 338.0s/339.9s), so despite claiming an 'immediately after' relation it is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge (top right) asks what if Mr. Musk made an ad hominem attack on Mr. Hothi, when does the lawyer (bottom left) respond by giving examples of unrelated attacks?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 380.5
      },
      "pred_interval": {
        "start": 330.0,
        "end": 345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.19999999999999,
        "end": 35.5,
        "average": 39.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2295081967213115,
        "text_similarity": 0.790030837059021,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the speakers and nature of the exchange and notes the target occurs after the anchor, but the provided timestamps are substantially wrong (both anchor and target times differ markedly from the ground truth), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge (top center) states that the lawsuit is about Mr. Hothi almost killing Tesla employees, when does the lawyer (bottom left) clarify Mr. Musk's statements regarding Mr. Hothi harassing employees and almost killing one?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 553.0,
        "end": 561.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 208.0,
        "end": 206.0,
        "average": 207.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2173913043478261,
        "text_similarity": 0.7650644779205322,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the content of the statements, but it grossly mislocalizes both the anchor and target timestamps and thus fails to match the ground-truth temporal annotations or their true temporal relationship."
      }
    },
    {
      "question_id": "003",
      "question": "Once the lawyer (bottom left) finishes explaining why 'almost killed' is not a false statement of fact using a sidewalk analogy, when does the judge (top center) ask if that just means the lawyer might win the lawsuit?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 586.8
      },
      "pred_interval": {
        "start": 370.0,
        "end": 375.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 214.0,
        "end": 211.79999999999995,
        "average": 212.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2616822429906542,
        "text_similarity": 0.8538427948951721,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the sequence and wording (judge question following the lawyer's remark) but the provided timestamps are completely incorrect and do not match the ground-truth intervals, a major factual error."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (bottom left) states the problem is that two people would be debating under different rules because Mr. Hothi is protected by anti-SLAPP, when does he state that the target of Mr. Hothi's speech would not have the same protection?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.405,
        "end": 511.559
      },
      "pred_interval": {
        "start": 644.8,
        "end": 650.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 133.39499999999998,
        "end": 138.94099999999997,
        "average": 136.16799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.16352201257861634,
        "text_similarity": 0.568345308303833,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relationship and content (that the target would lack the same protection) but the timestamps and durations are incorrect and do not match the ground truth events, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom right) begins his turn by saying 'I mean, I think that the difficulty, Mr. Lefebvre, that I have is', when does he explain that the argument is based on Mr. Hothi entering the public sphere?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.597,
        "end": 512.074
      },
      "pred_interval": {
        "start": 695.4,
        "end": 700.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 183.803,
        "end": 188.1260000000001,
        "average": 185.96450000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.1639344262295082,
        "text_similarity": 0.689746618270874,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct verbal content (the follow-up about Mr. Hothi entering the public sphere) but the timestamps are vastly incorrect and the temporal alignment is wrong, so it fails on factual timing and precise relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker (bottom right) finishes giving the example of Mr. Musk commenting on Mr. Houthi's methodology and data, when does he question the relevance of 'almost killing someone in the parking lot'?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.302,
        "end": 512.387
      },
      "pred_interval": {
        "start": 741.1,
        "end": 744.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 228.798,
        "end": 232.51300000000003,
        "average": 230.65550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.7556487321853638,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the same utterances/themes but the timestamps and event ordering are substantially incorrect (events placed ~229s later and not shown as the immediate next point), so it fails at precise temporal localization and sequencing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks why this is less attenuated than in the Wilson case, when does he begin explaining the issue?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 703.5
      },
      "pred_interval": {
        "start": 712.9,
        "end": 715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.899999999999977,
        "end": 11.5,
        "average": 14.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.6867294907569885,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the explanation follows the question and even quotes the start of the explanation, but the timestamps are substantially different from the reference (off by ~20s) and the relation label differs, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that their analysis is relevant to 'what he did', when does he give the example of trespassing?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 764.0,
        "end": 768.7
      },
      "pred_interval": {
        "start": 752.8,
        "end": 754.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.200000000000045,
        "end": 14.700000000000045,
        "average": 12.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.6235433220863342,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering and example content right but the timestamps are substantially misaligned (off by ~11\u201312s) and the relation label differs slightly; because temporal offsets are key, this is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the presiding justice turns the floor over to the opponent for replies, when does the opponent begin speaking?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 802.5
      },
      "pred_interval": {
        "start": 783.8,
        "end": 785.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.200000000000045,
        "end": 16.600000000000023,
        "average": 16.400000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.18421052631578946,
        "text_similarity": 0.7344104051589966,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same speakers and that the opponent speaks after the presiding justice, but the timestamps are substantially wrong (off by several seconds) and the relation 'immediately after' contradicts the reference timing, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mr. Greenspan finishes saying that the purpose was not to monitor or harass employees, when does he begin explaining that the person was counting cars off the production line?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.67,
        "end": 1058.61
      },
      "pred_interval": {
        "start": 1056.3,
        "end": 1060.0
      },
      "iou": 0.3649289099526022,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6299999999998818,
        "end": 1.3900000000001,
        "average": 2.009999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.18,
        "text_similarity": 0.6981104612350464,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the general sequence and E1 start time and relation ('immediately after') right, but it significantly misstates the event boundaries (E1/E2 end and start times) and introduces a different speaker name and shifted timestamps, so it is not temporally accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Greenspan finishes asking if there are any specific questions, when does the Presiding Justice respond by asking the panel if they have questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.969,
        "end": 1135.213
      },
      "pred_interval": {
        "start": 1120.0,
        "end": 1121.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.969000000000051,
        "end": 13.812999999999874,
        "average": 12.890999999999963
      },
      "rationale_metrics": {
        "rouge_l": 0.14876033057851237,
        "text_similarity": 0.5329439640045166,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the sequence (Presiding Justice responds immediately after Greenspan), but the timestamps are materially incorrect (off by ~11\u201312 seconds and with wrong durations), so it fails to match the key temporal facts in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Liffrec finishes explaining that the Nidal case was about a university building something in a public park and that was the public debate, when does he state that the university then made statements about protesters inciting violence?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1159.099,
        "end": 1164.925
      },
      "pred_interval": {
        "start": 1171.2,
        "end": 1175.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.101000000000113,
        "end": 10.174999999999955,
        "average": 11.138000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.1272727272727273,
        "text_similarity": 0.5539232492446899,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the semantic sequence that the university blamed protesters after the debate, but the timecodes conflict significantly with the reference (predicted events are ~9\u201312s later) and the relation label differs slightly; therefore content is partially correct but temporally inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's discussion about harassment, when does he mention \"extensive evidence of harassment\"?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1240.5,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 1258.7,
        "end": 1262.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.200000000000045,
        "end": 20.700000000000045,
        "average": 19.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2903225806451613,
        "text_similarity": 0.7551968693733215,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives incorrect temporal spans (E2 at 1258.7\u20131262.7 vs. correct 1240.5\u20131242.0) and does not match the correct E1 interval (1236.2\u20131246.6); claiming the relation is 'during' is inconsistent with the provided predicted timestamps, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom left) concludes his argument, when does the Presiding Justice (center top) begin asking if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1361.2,
        "end": 1362.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.41599999999994,
        "end": 63.27099999999996,
        "average": 64.34349999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.7595382332801819,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and the Presiding Justice asking questions, but the timestamps are substantially off (about 66s later) and the predicted answer omits the E2 end time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the Justice (top right) states that the Nadel case came out in 1994, when does he ask if the court would have considered things the same way with the Filmon Supreme Court's decision?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1310.105,
        "end": 1318.758
      },
      "pred_interval": {
        "start": 1372.7,
        "end": 1375.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.59500000000003,
        "end": 56.74199999999996,
        "average": 59.668499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1956521739130435,
        "text_similarity": 0.6140303611755371,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the utterance content and that the Filmon question occurs after the Nadel statement, but the provided timestamps differ substantially from the ground truth, so the timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'And with that, I'd submit', when does the Presiding Justice ask if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1330.1,
        "end": 1333.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.3159999999998,
        "end": 33.87099999999987,
        "average": 34.093499999999835
      },
      "rationale_metrics": {
        "rouge_l": 0.35789473684210527,
        "text_similarity": 0.614845871925354,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the Presiding Justice speaks after the speaker, but the timestamps are substantially off (\u224832s later than the ground truth) and the temporal labeling is imprecise, so it fails to match the correct absolute timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the Presiding Justice asks if there are any other questions, when does Justice Sanchez start speaking about the Nadel case?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1300.609,
        "end": 1302.492
      },
      "pred_interval": {
        "start": 1344.2,
        "end": 1345.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.59100000000012,
        "end": 42.80799999999999,
        "average": 43.19950000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.31111111111111106,
        "text_similarity": 0.6580389142036438,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relative relation ('after') and identifies the same speakers/utterance, but the absolute timestamps are significantly off and it omits the intermediate Marquardt 'No' event; thus it is factually inconsistent and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After Senator Cruz asks if the same principle applies to other protected characteristics, when does he ask if he could decide he was an Asian man?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 14.506,
        "end": 16.388
      },
      "pred_interval": {
        "start": 17.0,
        "end": 21.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4939999999999998,
        "end": 4.611999999999998,
        "average": 3.552999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.7451918721199036,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer preserves the event descriptions and correctly labels the relation as 'after', but both event time intervals are substantially incorrect compared to the ground truth, so it fails to provide accurate temporal boundaries."
      }
    },
    {
      "question_id": "002",
      "question": "Once Judge Jackson finishes explaining that she is not able to answer hypotheticals, when does Senator Cruz interrupt to re-ask his question about assessing standing?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 30.6,
        "end": 32.439
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.399999999999999,
        "end": 3.561,
        "average": 3.9804999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.25742574257425743,
        "text_similarity": 0.8044394254684448,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer gets the relation ('once_finished') right, it misstates the key temporal facts: both the judge's end time and Senator Cruz's interruption times are several seconds later than the ground truth, so the predicted events are not correctly aligned with the reference timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "While Judge Jackson is explaining how she would assess standing, when does she state that she would 'consider the relevant precedents'?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 45.0,
        "end": 47.8
      },
      "pred_interval": {
        "start": 65.0,
        "end": 67.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 19.200000000000003,
        "average": 19.6
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903615,
        "text_similarity": 0.7762573957443237,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event boundaries: E1 timing differs substantially and E2 is placed at 65.0\u201367.0s (after E1), contradicting the reference which has E2 occurring during E1 at 45.0\u201347.8s; thus the relation is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain why that detective asked her to do it?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 37.335,
        "end": 44.121
      },
      "pred_interval": {
        "start": 37.4,
        "end": 42.8
      },
      "iou": 0.7957559681697609,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.06499999999999773,
        "end": 1.321000000000005,
        "average": 0.6930000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.2912621359223301,
        "text_similarity": 0.6268441677093506,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that Pettis first says she was asked to go and then gives a reason, but it mislabels the event time windows (swapping/shifted timestamps for E1 and E2) and misstates the temporal relation ('immediately after' vs the separate intervals given), so the alignment is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mickey Haller finishes stating that the witness pointed to Detective Lee Langford, when does Langford begin his angry outburst?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 66.887,
        "end": 72.795
      },
      "pred_interval": {
        "start": 54.8,
        "end": 56.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.087000000000003,
        "end": 16.295,
        "average": 14.191000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.525924563407898,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and that the outburst follows immediately, but the timestamps and duration are substantially wrong (off by ~12+ seconds and incorrect end time), so it fails on factual accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After Detective Lee Langford's angry outburst about the witness, when does the judge declare the court in recess?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 82.826,
        "end": 85.59
      },
      "pred_interval": {
        "start": 105.6,
        "end": 107.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.774,
        "end": 21.61,
        "average": 22.192
      },
      "rationale_metrics": {
        "rouge_l": 0.380952380952381,
        "text_similarity": 0.7738620042800903,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events (Langford's outburst and the judge declaring recess) but is factually incorrect about the timings and their relation\u2014it gives wrong timestamps (105.6s vs. 72.795s and 82.826\u201385.59s) and incorrectly claims the recess is 'immediately after,' so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mickey Haller finishes asking Detective Pettis if she was in the Bonaventure Hotel on October 20th, when does Detective Pettis answer?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 16.239,
        "end": 16.76
      },
      "pred_interval": {
        "start": 22.7,
        "end": 23.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.4609999999999985,
        "end": 6.739999999999998,
        "average": 6.6004999999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.30952380952380953,
        "text_similarity": 0.8746002912521362,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially incorrect timestamps (anchor end 22.7s vs correct 16.219s; target start 22.7s vs correct 16.239s) and misstates the relationship rather than the target immediately following the anchor; only the general notion that the answer follows the question is preserved."
      }
    },
    {
      "question_id": "002",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain her personal motivation for complying?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 46.707,
        "end": 55.417
      },
      "pred_interval": {
        "start": 37.3,
        "end": 42.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.407000000000004,
        "end": 13.317,
        "average": 11.362000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.38095238095238093,
        "text_similarity": 0.8160482048988342,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the correct 'after' relation, but the timestamps are substantially off: predicted anchor (34.6\u201336.9s) does not match the correct 32.008s and the predicted target (37.3\u201342.1s) is much earlier than the correct 46.707\u201355.417s, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Mickey Haller asks if the man Detective Pettis made a deal with is in the courtroom, when does she point him out?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 61.92,
        "end": 62.701
      },
      "pred_interval": {
        "start": 60.7,
        "end": 65.3
      },
      "iou": 0.16978260869565212,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.2199999999999989,
        "end": 2.5989999999999966,
        "average": 1.9094999999999978
      },
      "rationale_metrics": {
        "rouge_l": 0.2641509433962264,
        "text_similarity": 0.824890673160553,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives incorrect and inconsistent timestamps and dialogue details (anchor and target times are off by several seconds and the utterances/ordering are misattributed/extended), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chaturved introduces Mr. Uday Hula, when does he state that Mr. Uday Hula has become a popular name pan India and beyond?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.33,
        "end": 43.1
      },
      "pred_interval": {
        "start": 52.1,
        "end": 55.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.770000000000003,
        "end": 12.0,
        "average": 11.385000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3783783783783784,
        "text_similarity": 0.7466189861297607,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation ('after') and the quoted phrase right, but both event timestamps are incorrect (predicted E1 at 51.8s vs reference 19.992s; predicted E2 at 52.1\u201355.1s vs reference 41.33\u201343.1s), so it fails to match the key temporal facts."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Vikas Chaturved says 'Over to you Mr. Trikram', when does Mr. Trikram start speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 153.433,
        "end": 154.776
      },
      "pred_interval": {
        "start": 160.5,
        "end": 162.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.067000000000007,
        "end": 7.923999999999978,
        "average": 7.495499999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.35616438356164387,
        "text_similarity": 0.7576980590820312,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and dialogue but the timestamps are substantially incorrect (both E1 and E2 are ~8\u20139s later than ground truth) and the temporal relation is mislabeled as 'immediately after' rather than the reported 'once_finished', so it fails on factual timing and relation."
      }
    },
    {
      "question_id": "003",
      "question": "After Mr. Trikram finishes his welcome, when does Mr. Uday Holla begin his speech?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "pred_interval": {
        "start": 198.0,
        "end": 200.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.0,
        "end": 28.0,
        "average": 28.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.7096953988075256,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', it gives substantially incorrect timestamps for both events (197.7s/198\u2013200s vs. 147.207s/169\u2013172s), so the key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker emphasizes that facts are important, when does he state that a lawyer must have the patience of a crane?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 354.8
      },
      "pred_interval": {
        "start": 373.3,
        "end": 377.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.30000000000001,
        "end": 22.5,
        "average": 21.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.6779521703720093,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives incorrect timestamps (373.3\u2013377.3s vs correct 345.6\u2013354.8s), misstates their relative timing (starts are conflated and 'immediately after' is incorrect), so it is largely factually wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that appearing for government becomes more difficult, when does he recount his personal experience as Advocate General?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.1,
        "end": 412.9
      },
      "pred_interval": {
        "start": 417.2,
        "end": 425.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.099999999999966,
        "end": 12.300000000000011,
        "average": 12.699999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.11965811965811966,
        "text_similarity": 0.6257513761520386,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the anecdote's wording but grossly misaligns both timestamps (shifted ~37\u201340s later) and incorrectly positions E1 relative to E2; major factual/temporal errors despite partial content match."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a lawyer must know the law to draft better, when does he offer an illustration?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 504.9,
        "end": 506.3
      },
      "pred_interval": {
        "start": 477.1,
        "end": 480.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.799999999999955,
        "end": 26.30000000000001,
        "average": 27.049999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.6546298861503601,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterances and the 'after' relation, but both event timestamps are substantially wrong (and E2 is placed overlapping E1), so it fails to locate when the illustration actually occurs."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that there has been an amendment, when does he state that everment is taken out?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.3,
        "end": 532.0
      },
      "pred_interval": {
        "start": 528.4,
        "end": 530.2
      },
      "iou": 0.2500000000000237,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8999999999999773,
        "end": 1.7999999999999545,
        "average": 1.349999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.2790697674418605,
        "text_similarity": 0.782550573348999,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance and the immediate-after relationship, but it gives incorrect timing for the target event (starts at 528.4s and ends at 530.2s) versus the ground truth (529.3s\u2013532.0s), a significant factual error."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that readiness and willingness was mandatory prior to 2018, when does he explicitly state that the suit was liable to be dismissed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.568,
        "end": 583.193
      },
      "pred_interval": {
        "start": 564.3,
        "end": 567.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.268000000000029,
        "end": 15.893000000000029,
        "average": 15.580500000000029
      },
      "rationale_metrics": {
        "rouge_l": 0.3578947368421052,
        "text_similarity": 0.7440677285194397,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it misidentifies and swaps the anchor and target events and gives timestamps (564.3\u2013567.3s) that do not match or overlap the reference times (anchor 533.4\u2013553.9s; target 579.568\u2013583.193s)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the balance of the purchase amount, when does he explain the necessity for the plaintiff to prove they had the necessary funds?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 634.565,
        "end": 644.056
      },
      "pred_interval": {
        "start": 582.6,
        "end": 591.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.96500000000003,
        "end": 52.65600000000006,
        "average": 52.31050000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.25688073394495414,
        "text_similarity": 0.6522541642189026,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and their logical 'after' relation and matches the E2 content, but the timestamps for both anchor and target are substantially different from the ground truth, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a well-drafted plaint helps portray personality to the judge, when does he begin talking about the second benefit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.9,
        "end": 708.7
      },
      "pred_interval": {
        "start": 730.0,
        "end": 740.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.100000000000023,
        "end": 31.299999999999955,
        "average": 30.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21782178217821782,
        "text_similarity": 0.6811333894729614,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the ground-truth timestamps (predicts ~729\u2013730s vs correct ~699.7\u2013700.9s) and thus is factually incorrect; although it correctly identifies a 'Secondly' transition, the timing and quoted wording appear fabricated and do not match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a civil case lingers for a number of years, when does he explain that one becomes a senior by the time the case comes up for evidence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 719.5,
        "end": 724.9
      },
      "pred_interval": {
        "start": 770.0,
        "end": 776.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.5,
        "end": 51.10000000000002,
        "average": 50.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.22916666666666663,
        "text_similarity": 0.6714950799942017,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances semantically, but the timestamps are substantially incorrect (off by ~55 seconds) compared to the reference, so the temporal answer is not accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing client satisfaction with plaint length and associated fees, when does he introduce the strategy of putting facts at the beginning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 795.264,
        "end": 805.511
      },
      "pred_interval": {
        "start": 870.0,
        "end": 872.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.73599999999999,
        "end": 66.48900000000003,
        "average": 70.61250000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.18947368421052632,
        "text_similarity": 0.6830399036407471,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the correct strategy phrase but gives substantially incorrect timestamps (off by ~75s) and adds minor unsupported audio-detail, so it fails on factual timing alignment despite matching content."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Supreme Court, when does he state the specific paragraph number of the judgment?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 925.404,
        "end": 950.0
      },
      "pred_interval": {
        "start": 935.7,
        "end": 942.5
      },
      "iou": 0.27646771832818157,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.29600000000005,
        "end": 7.5,
        "average": 8.898000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.2708333333333333,
        "text_similarity": 0.7197245359420776,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target (the 'paragraph 240' utterance) and the 'after' relationship, but it misidentifies the anchor: the predicted anchor phrase and timestamp do not match the correct 'the Supreme Court' span, so the anchor timing/content is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing Justice Sanjay Kishan Kaul's judgment, when does he mention the 'Ren and Martin principle of pressie writing'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.581,
        "end": 990.021
      },
      "pred_interval": {
        "start": 966.6,
        "end": 976.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.980999999999995,
        "end": 13.320999999999913,
        "average": 16.650999999999954
      },
      "rationale_metrics": {
        "rouge_l": 0.4814814814814815,
        "text_similarity": 0.8905403017997742,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relevant phrases but mislocates the target temporally and thus gets the temporal relationship wrong (the correct target occurs later at ~986.6\u2013990.0s, not within the anchor at ~972.9\u2013975.0s), so content matches but timing/relationship are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using 'short paragraphs, short sentences, direct sentences' in a plaint, when does he warn that a judge will lose patience with lengthy paragraphs?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1005.669,
        "end": 1012.012
      },
      "pred_interval": {
        "start": 993.7,
        "end": 1002.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.968999999999937,
        "end": 9.111999999999966,
        "average": 10.540499999999952
      },
      "rationale_metrics": {
        "rouge_l": 0.2982456140350877,
        "text_similarity": 0.7123348712921143,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the content and the 'after' relationship (advice then warning), but both anchor and target timestamps are significantly shifted from the reference, so it is not temporally aligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a civil dispute is always before a civil court, when does he emphasize that the lawyer must be thorough with the civil procedure code?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1077.5,
        "end": 1083.7
      },
      "pred_interval": {
        "start": 1101.7,
        "end": 1111.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.200000000000045,
        "end": 27.799999999999955,
        "average": 26.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.6740177869796753,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the reported timestamps for both anchor and target are significantly different from the ground truth, so it fails to match the key temporal annotations; the added commentary about emphasis is extraneous but not contradictory."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that settlement will not benefit in the short term, when does he explain how it will benefit in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1250.671,
        "end": 1254.734
      },
      "pred_interval": {
        "start": 1176.1,
        "end": 1190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.57100000000014,
        "end": 64.73399999999992,
        "average": 69.65250000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2654867256637168,
        "text_similarity": 0.7348103523254395,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the contrast and 'after' relationship and paraphrases the long-term explanation, but the timestamp anchors/targets are substantially different from the ground truth, so it fails the required temporal localization."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that the entire procedure for civil disputes is in the civil procedure code, when does he mention that states also have civil rules of practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1098.4,
        "end": 1101.7
      },
      "pred_interval": {
        "start": 1142.3,
        "end": 1151.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.899999999999864,
        "end": 50.200000000000045,
        "average": 47.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.28,
        "text_similarity": 0.7580972909927368,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the semantic content and the 'after' relationship, but the anchor and target timestamps are significantly shifted (\u224855s later) and the target duration differs, so it fails to match the ground-truth temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how settlement will benefit you, when does he state that it will definitely help in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1238.9,
        "end": 1241.9
      },
      "pred_interval": {
        "start": 1254.0,
        "end": 1261.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.099999999999909,
        "end": 19.09999999999991,
        "average": 17.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.7533981800079346,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer captures the correct verbal content and the 'after' relation, but the annotated timestamps are substantially incorrect (\u224821s later) and it omits the detail that the target immediately follows the anchor; the anchor/target intervals are also mismatched."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker first mentions there's a saying in Kannada, when does he explain the saying about winners and losers in court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1273.6,
        "end": 1278.4
      },
      "pred_interval": {
        "start": 1290.0,
        "end": 1297.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.40000000000009,
        "end": 18.59999999999991,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7107370495796204,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the explanation comes after the mention and paraphrases the saying, but both the anchor and target timestamps are significantly off from the ground truth (much later), so the localization is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker begins discussing the drafting of the plaint, when does he advise to draft it in a professional manner?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1319.385,
        "end": 1344.757
      },
      "pred_interval": {
        "start": 1333.0,
        "end": 1336.0
      },
      "iou": 0.11824058016711302,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.615000000000009,
        "end": 8.757000000000062,
        "average": 11.186000000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.20779220779220778,
        "text_similarity": 0.8473089933395386,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the phrasing about drafting professionally but is largely inaccurate: it misidentifies both event timestamps (correct E1 is 1315.8\u20131319.5 and E2 1319.385\u20131344.757) and gives an incorrect temporal relation, so it fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states 'You have order two, rule two, which specifies that', when does he elaborate on what it specifies?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.147,
        "end": 1451.484
      },
      "pred_interval": {
        "start": 1436.3,
        "end": 1443.8
      },
      "iou": 0.30644099051633494,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.84699999999998,
        "end": 7.683999999999969,
        "average": 5.2654999999999745
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680854,
        "text_similarity": 0.6966822743415833,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the elaboration occurs after the anchor and gives the elaboration content, but its timestamps conflict substantially with the reference (anchor and target times differ by many seconds), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'life of the lawyer becomes easier', when does he list Order Six, Seven, and Eight?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1460.096,
        "end": 1410.578
      },
      "pred_interval": {
        "start": 1460.3,
        "end": 1465.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.2039999999999509,
        "end": 54.62200000000007,
        "average": 27.41300000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.379746835443038,
        "text_similarity": 0.735937237739563,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the listing and roughly the start time and phrasing, but the E1 timestamp is about 1.1s early and the E2 end is ~2.6s early compared with the ground truth, so the timing and duration are noticeably inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Then comes the stage of evidence', when does he explain that a lawyer must sit with clients to understand what kind of evidence to lead?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1553.662,
        "end": 1566.557
      },
      "pred_interval": {
        "start": 1514.2,
        "end": 1525.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.46199999999999,
        "end": 41.45700000000011,
        "average": 40.45950000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.20754716981132074,
        "text_similarity": 0.7280997037887573,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer quotes the correct explanation but the E1 and E2 timestamps are substantially different from the reference (predicted times are much earlier), so the temporal alignment is incorrect; the content match is partial but the key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the written statement, when does he begin talking about Order 8?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.811,
        "end": 1624.02
      },
      "pred_interval": {
        "start": 1592.93,
        "end": 1596.64
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.880999999999858,
        "end": 27.37999999999988,
        "average": 25.13049999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.7401559352874756,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances' content but gives substantially incorrect timestamps and states the relation as 'during/immediately after' whereas the ground truth has Order 8 starting well after the written-statement segment; thus it contradicts key temporal facts."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a general denial is not sufficient, when does he mention the mistake lawyers normally commit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1651.02,
        "end": 1671.11
      },
      "pred_interval": {
        "start": 1649.34,
        "end": 1654.22
      },
      "iou": 0.14699127239320386,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6800000000000637,
        "end": 16.889999999999873,
        "average": 9.284999999999968
      },
      "rationale_metrics": {
        "rouge_l": 0.26804123711340205,
        "text_similarity": 0.6729135513305664,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the same utterances and the 'after' relation, and includes matching quoted content; minor timestamp discrepancies (E1 ~1.2s earlier and E2 ending much earlier than the reference span) warrant a small penalty."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that a lawyer must know the law, when does he explain the specific areas a lawyer should be thorough with?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.607,
        "end": 1763.816
      },
      "pred_interval": {
        "start": 1713.84,
        "end": 1720.89
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.76700000000005,
        "end": 42.92599999999993,
        "average": 43.84649999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.37383177570093457,
        "text_similarity": 0.6483138203620911,
        "llm_judge_score": 9,
        "llm_judge_justification": "The predicted answer correctly captures the anchor and target utterances and their causal/temporal relation (the speaker emphasizes knowing the law then immediately details civil procedure and rules). It only slightly deviates in exact timestamps and specifies 'immediately after' rather than the more general 'after.'"
      }
    },
    {
      "question_id": "001",
      "question": "When is the next time the speaker mentions an 'Order six, Rule' after mentioning 'Order six, Rule four'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.2,
        "end": 1830.9
      },
      "pred_interval": {
        "start": 1807.0,
        "end": 1815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.200000000000045,
        "end": 15.900000000000091,
        "average": 18.050000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.7280012965202332,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer misidentifies both the anchor and target timestamps and the target mention \u2014 it places the 'Order six, Rule four' at 1799.0s and the 'Rule eight' correction at 1807\u20131815s, whereas the correct instances are at 1773.5\u20131775.0s and 1827.2\u20131830.9s, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker explains that fraud and undue influence require specific pleas, when does he state that a general plea is not sufficient?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1802.1,
        "end": 1806.5
      },
      "pred_interval": {
        "start": 1840.0,
        "end": 1848.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.90000000000009,
        "end": 41.5,
        "average": 39.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2291666666666667,
        "text_similarity": 0.7033277750015259,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures related content (specific vs general pleas) but it reverses the anchor/target roles and gives substantially different timestamps; thus it fails to align the correct events and timings."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using short sentences and small paragraphs in a written statement, when does he transition to discussing 'evidence'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1908.4,
        "end": 1914.4
      },
      "pred_interval": {
        "start": 1882.0,
        "end": 1887.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.40000000000009,
        "end": 27.40000000000009,
        "average": 26.90000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6278082132339478,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the shift to discussing evidence and the quoted phrase, but both event time intervals are substantially earlier and do not match the reference timestamps, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that leading questions are to be eschewed, when does he advise on how to prepare for examination?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1964.967,
        "end": 1965.937
      },
      "pred_interval": {
        "start": 2027.3,
        "end": 2031.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.332999999999856,
        "end": 65.16300000000001,
        "average": 63.747999999999934
      },
      "rationale_metrics": {
        "rouge_l": 0.33707865168539325,
        "text_similarity": 0.695292592048645,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly captures the content and the 'after' relation, but both event timestamps are substantially different from the ground truth (off by ~65\u201366 seconds), so key factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes how some lawyers approach cross-examination without preparation, when does he explain what a good lawyer always does?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2010.4,
        "end": 2018.651
      },
      "pred_interval": {
        "start": 2112.4,
        "end": 2114.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 102.0,
        "end": 95.94899999999984,
        "average": 98.97449999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.2376237623762376,
        "text_similarity": 0.627951979637146,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic contrast between unprepared and good lawyers but mislabels and mis-times the segments (predicted E1/E2 times are ~104s off and the quoted line is the good-lawyer statement, not the unpreparedness segment), so the relation and segment boundaries are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's explanation of pitfalls when unprepared for cross-examination, when does he mention forgetting to ask relevant questions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2044.393,
        "end": 2049.878
      },
      "pred_interval": {
        "start": 2132.2,
        "end": 2134.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.80699999999979,
        "end": 84.12199999999984,
        "average": 85.96449999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.14141414141414144,
        "text_similarity": 0.682572066783905,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the content (forgetting to ask relevant questions as one of two pitfalls) but misstates both event timestamps and the relation (uses a different temporal/causal phrasing), so it is only partially aligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that cross-examination is an art by itself, when does he state that watching a good cross-examiner will help enormously?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2187.557,
        "end": 2201.817
      },
      "pred_interval": {
        "start": 2163.9,
        "end": 2171.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.656999999999698,
        "end": 30.516999999999825,
        "average": 27.08699999999976
      },
      "rationale_metrics": {
        "rouge_l": 0.2545454545454545,
        "text_similarity": 0.7986336946487427,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the correct phrasing and that the target follows the anchor, but the timestamps are substantially wrong (off by ~20s) and the target duration/location do not match the ground truth, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker describes law as a noble profession, when does he mention lawyers dedicating themselves to clients to ensure justice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2222.5,
        "end": 2233.2
      },
      "pred_interval": {
        "start": 2224.1,
        "end": 2230.6
      },
      "iou": 0.607476635514029,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.599999999999909,
        "end": 2.599999999999909,
        "average": 2.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.19819819819819823,
        "text_similarity": 0.6433346271514893,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the same phrases, relation, and uses timestamps that fall within the reference ranges, preserving the meaning and timing; however it introduces an unsupported visual detail (eye contact) and narrows the event boundaries slightly compared to the broader reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that delays are endemic, when does he provide the reason as a call for settlement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2338.838,
        "end": 2346.208
      },
      "pred_interval": {
        "start": 2320.9,
        "end": 2324.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.938000000000102,
        "end": 21.9079999999999,
        "average": 19.923000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2352941176470588,
        "text_similarity": 0.7550621032714844,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the causal linkage verbally but is largely incorrect: the timestamps and durations for both anchor and target differ significantly from the reference and the target interval is unrealistically short, so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that delays are endemic, when does he ask to ensure settlements occur?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2346.0
      },
      "pred_interval": {
        "start": 2336.0,
        "end": 2340.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 6.0,
        "average": 5.0
      },
      "rationale_metrics": {
        "rouge_l": 0.43373493975903615,
        "text_similarity": 0.8618593215942383,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the anchor phrase timing but gives incorrect end time and substantially misstates the target's start/end times and temporal relation (claims overlap/immediate succession), contradicting the correct timestamps and ordering."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker notes it's already 40 minutes, when does he state that he will give some time for questioning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.8,
        "end": 2371.0
      },
      "pred_interval": {
        "start": 2364.0,
        "end": 2369.0
      },
      "iou": 0.17142857142854545,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.800000000000182,
        "end": 2.0,
        "average": 2.900000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.48275862068965514,
        "text_similarity": 0.8754613399505615,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct phrase about giving time for questions but has incorrect timings (E1 off by ~1\u20132s, E2 start is much earlier than the reference and overlaps the anchor, and E2 end is truncated) and misstates the relation ('after' vs immediately follows), so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions postponing because he was busy, when does he thank someone for 'pestering' him to educate the lawyer community?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2391.454,
        "end": 2399.123
      },
      "pred_interval": {
        "start": 2385.0,
        "end": 2392.0
      },
      "iou": 0.03866034128724916,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.454000000000178,
        "end": 7.123000000000047,
        "average": 6.788500000000113
      },
      "rationale_metrics": {
        "rouge_l": 0.3571428571428571,
        "text_similarity": 0.7972942590713501,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the thank-you event follows the postponement, but both event time spans are substantially incorrect (E1 off by ~4.5s and E2 starts much earlier and ends far earlier than reference), and it fails to reflect the immediate-following relationship and correct E2 boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "How long does the speaker talk about the importance of reading entire judgments, not just highlighted portions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2578.041,
        "end": 2584.494
      },
      "pred_interval": {
        "start": 2539.0,
        "end": 2562.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.04100000000017,
        "end": 22.494000000000142,
        "average": 30.767500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.2553191489361702,
        "text_similarity": 0.42507097125053406,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct 'during' relation and the relevant phrases, but the start/end timestamps are substantially incorrect (2539.0/2562.0 vs 2568.041/2578.041), so the duration is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes saying he will take questions, when does Mr. Vikas begin speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2614.902,
        "end": 2617.184
      },
      "pred_interval": {
        "start": 2636.0,
        "end": 2646.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.097999999999956,
        "end": 28.815999999999804,
        "average": 24.95699999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.8601932525634766,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates both anchor and target timestamps (E1 and E2 are off by ~39s and ~21s respectively) and gives a much later/longer end time, though it correctly asserts an immediate transition; the large temporal errors make it mostly incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that online law lexicons are not sufficient, when does he directly say 'You must read. A lawyer must read.'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2522.2,
        "end": 2525.3
      },
      "pred_interval": {
        "start": 2613.0,
        "end": 2617.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.80000000000018,
        "end": 91.69999999999982,
        "average": 91.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4367816091954023,
        "text_similarity": 0.7746299505233765,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their immediate-following relationship and paraphrases E1, but the provided timestamps are drastically incorrect/mismatched with the reference and the E2 timing is inaccurate, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he would fall asleep reading the Civil Procedure Code as it is, when does he explain that he would be enthusiastic if he had a case on hand?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2688.6,
        "end": 2699.0
      },
      "pred_interval": {
        "start": 2706.0,
        "end": 2726.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.40000000000009,
        "end": 27.0,
        "average": 22.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2363636363636364,
        "text_similarity": 0.6992496848106384,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the semantic relation right and captures the contrast between boredom and enthusiasm, but the temporal anchors and segment boundaries are significantly offset from the ground truth (wrong start/end times and inaccurate segmentation), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'Specific relief act, what are the sections?', when does he advise to go to the AR manual?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2720.5,
        "end": 2722.3
      },
      "pred_interval": {
        "start": 2780.0,
        "end": 2783.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.5,
        "end": 60.69999999999982,
        "average": 60.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.34951456310679613,
        "text_similarity": 0.7109026908874512,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor question and that the target is advice to consult the AR manual and that it follows the question, but it contains significant errors in timestamps (\u224860s off), misquotes the target utterance, and overstates the timing as 'immediately after' contrary to the reference, so it is only partially aligned."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions he suffered from COVID, when does he start describing a doctor's experience during the peak of COVID?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.119,
        "end": 2850.7
      },
      "pred_interval": {
        "start": 2804.0,
        "end": 2808.0
      },
      "iou": 0.018865096359740076,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.119000000000142,
        "end": 42.69999999999982,
        "average": 22.90949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.21568627450980393,
        "text_similarity": 0.6309304237365723,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same utterances and the 'after' relationship, but the timestamps differ substantially from the reference (both start times are earlier) and the prediction omits the target end time and the anchor end time, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once Vikas Chatrath finishes asking how to structure arguments, when does Udaya Holla begin to explain what one must always remember?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.46,
        "end": 2963.1
      },
      "pred_interval": {
        "start": 2877.9,
        "end": 2880.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.559999999999945,
        "end": 82.29999999999973,
        "average": 60.429999999999836
      },
      "rationale_metrics": {
        "rouge_l": 0.271604938271605,
        "text_similarity": 0.7031335830688477,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('once_finished') and sequence, but the anchor and target timestamps are substantially different from the ground truth (off by ~36\u201338 seconds), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After Udaya Holla suggests preparing a list of dates and synopsis, when does he mention that the High Court has adopted this practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2941.0,
        "end": 2942.8
      },
      "pred_interval": {
        "start": 2913.3,
        "end": 2916.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.699999999999818,
        "end": 26.600000000000364,
        "average": 27.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7393876910209656,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the High Court comment follows the initial suggestion, but the timestamps are materially incorrect compared to the reference (predicted ~2913\u20132916s vs. ground truth ~2929.5\u20132941\u20132942.8s), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes asking how to specifically plead in a summary suit, when does Udaya Holla ask for clarification 'In a summary suit, is it?'",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2999.596,
        "end": 3000.717
      },
      "pred_interval": {
        "start": 3012.0,
        "end": 3015.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.403999999999996,
        "end": 15.083000000000084,
        "average": 13.74350000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.29268292682926833,
        "text_similarity": 0.6690006256103516,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the interaction direction right (Udaya responds to Vikas) but the timestamps are significantly incorrect compared to the reference, so it misrepresents the temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes lawyers arguing endlessly for hours on end, when does he state that the judge then sleeps?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3046.2,
        "end": 3047.7
      },
      "pred_interval": {
        "start": 3068.2,
        "end": 3070.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.0,
        "end": 22.300000000000182,
        "average": 22.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.8279107213020325,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relationship, but the provided timestamps are significantly off from the ground truth (\u224827s difference) and the anchor interval is mis-specified, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker confirms the order number as '37, correct. Sorry.', when does the other speaker (Vikas Chaprath) begin to introduce a common question?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3157.242,
        "end": 3163.028
      },
      "pred_interval": {
        "start": 3096.8,
        "end": 3098.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.44200000000001,
        "end": 64.82799999999997,
        "average": 62.63499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.7624219655990601,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event order and even the utterance introducing the question, but the timestamps are wildly incorrect (off by ~54 seconds) and the event durations/intervals do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that preliminary objections concerning maintainability must appear in the beginning of a written statement, when does he begin describing how lawyers often present their defense?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3301.7,
        "end": 3309.9
      },
      "pred_interval": {
        "start": 3169.6,
        "end": 3171.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 132.0999999999999,
        "end": 138.9000000000001,
        "average": 135.5
      },
      "rationale_metrics": {
        "rouge_l": 0.22916666666666669,
        "text_similarity": 0.7989011406898499,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation (target follows anchor) and the content of E2 roughly right, but the timestamps are significantly incorrect (off by ~100\u2013140 seconds), so the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker recommends setting out your own facts in the written statement, when does he mention preliminary objections?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3223.3,
        "end": 3224.548
      },
      "pred_interval": {
        "start": 3250.0,
        "end": 3255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.699999999999818,
        "end": 30.452000000000226,
        "average": 28.576000000000022
      },
      "rationale_metrics": {
        "rouge_l": 0.2696629213483146,
        "text_similarity": 0.6873271465301514,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels the relation as 'after' but the timestamps are significantly inaccurate (off by ~27\u201330s) and it invents a quoted phrase not present in the reference, so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions misjoinder or non-joinder of parties, when does he list territorial lack of jurisdiction as an objection?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3254.547,
        "end": 3258.914
      },
      "pred_interval": {
        "start": 3274.0,
        "end": 3282.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.452999999999975,
        "end": 23.085999999999785,
        "average": 21.26949999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.25287356321839083,
        "text_similarity": 0.6881855130195618,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and that the territorial jurisdiction objection follows the misjoinder remark, but the provided timestamps are off by ~20 seconds from the ground truth and the relation is labeled 'after' rather than the more specific 'next', so it is only partly correct."
      }
    },
    {
      "question_id": "003",
      "question": "After Vikas finishes asking his question about the Advocate General's experience, when does Udaya Holla begin to respond about the State Council's disabilities?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3417.766,
        "end": 3429.231
      },
      "pred_interval": {
        "start": 3345.0,
        "end": 3350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.76600000000008,
        "end": 79.23100000000022,
        "average": 75.99850000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677422,
        "text_similarity": 0.542339563369751,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speaker's response and the 'after' relation, but the crucial timestamps (E1 and E2 start times) are substantially incorrect (off by ~72\u201375s) and thus factually misleading."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying the colloquial Kannada phrase, when does he translate it to English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.2,
        "end": 3417.7
      },
      "pred_interval": {
        "start": 3545.3,
        "end": 3550.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 132.10000000000036,
        "end": 133.10000000000036,
        "average": 132.60000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.1758241758241758,
        "text_similarity": 0.6939007043838501,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes that the English translation follows the Kannada phrase and even provides quoted utterances, but the timestamps are substantially different from the ground truth and the relation label ('after' vs 'once_finished') and timings do not match, making it largely factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes talking about the difficulties faced by state councils in tough situations, when does the second speaker say 'Vikram'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.82,
        "end": 3472.161
      },
      "pred_interval": {
        "start": 3606.2,
        "end": 3607.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 134.37999999999965,
        "end": 135.33899999999994,
        "average": 134.8594999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.6245033740997314,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event ordering (second speaker speaks after the first) but the timestamps are substantially incorrect (off by ~130s) and the relation label ('after') differs from the specified 'once_finished'; key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises Kannada medium students to acquire more knowledge of English, when does he suggest joining websites for better English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3527.318,
        "end": 3535.0
      },
      "pred_interval": {
        "start": 3653.3,
        "end": 3656.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 125.98199999999997,
        "end": 121.0,
        "average": 123.49099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.5814441442489624,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly captures the events and the 'after' relation, but the provided timestamps are significantly different from the ground truth, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker talks about drafting in Kannada, when does he emphasize having mastery over the Kannada language?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3590.3,
        "end": 3592.0
      },
      "pred_interval": {
        "start": 3604.4,
        "end": 3612.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.099999999999909,
        "end": 20.5,
        "average": 17.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.4081632653061224,
        "text_similarity": 0.9259589910507202,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the mastery remark follows the mention of Kannada, but the timestamps and durations are substantially incorrect and it fails to reflect that the target occurs immediately after the anchor, so key factual temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks about scheduling a lawyer's day, when does the first speaker refer to the question as a 'multi-million dollar question'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3696.0,
        "end": 3697.2
      },
      "pred_interval": {
        "start": 3635.2,
        "end": 3641.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.80000000000018,
        "end": 56.09999999999991,
        "average": 58.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.28888888888888886,
        "text_similarity": 0.7505872249603271,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the phrase but has major factual errors: the timestamps differ substantially from the reference (\u22483634\u20133641s vs correct 3682.5\u20133697.2s) and it mischaracterizes the temporal relation as 'immediately after' rather than the target occurring clearly after the anchor finished."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises taking books on management, when does he mention keeping one's wife happy at home?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3701.248,
        "end": 3706.6
      },
      "pred_interval": {
        "start": 3661.5,
        "end": 3667.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.74800000000005,
        "end": 39.5,
        "average": 39.624000000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.3564356435643565,
        "text_similarity": 0.8203233480453491,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same semantic relation ('after') and the wife-happiness content, but the anchor/target timestamps and exact time boundaries are substantially incorrect and the target quote is incomplete, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises lawyers on how to handle client pleas, when does he suggest setting out facts in the client's presence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.2,
        "end": 3750.22
      },
      "pred_interval": {
        "start": 3774.12,
        "end": 3780.14
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.920000000000073,
        "end": 29.920000000000073,
        "average": 26.920000000000073
      },
      "rationale_metrics": {
        "rouge_l": 0.30379746835443044,
        "text_similarity": 0.6389338970184326,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction references the same phrase but gives substantially different timestamps and wrongly labels the temporal relation as 'same time as' instead of the target clearly following the anchor, so it fails on key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that a judge would have three drafts of his judgment, when does he elaborate on what the first draft is for?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.51,
        "end": 3750.56
      },
      "pred_interval": {
        "start": 3835.44,
        "end": 3843.86
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 84.92999999999984,
        "end": 93.30000000000018,
        "average": 89.11500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.7589360475540161,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer largely mismatches the reference timestamps and misstates the target interval (different start/end times and an overlapping/identical start), though it correctly identifies the temporal relation as 'after'; therefore it is mostly incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Palkiwala's initial lack of work, when does he refer to Justice Chawla's memoir?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3903.539,
        "end": 3917.722
      },
      "pred_interval": {
        "start": 3911.72,
        "end": 3917.96
      },
      "iou": 0.4161985992649942,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.180999999999585,
        "end": 0.23799999999982901,
        "average": 4.209499999999707
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7852857112884521,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted E1 time is close but the E2 start is off by ~8s (3911.72 vs 3903.539), and labeling the relation as 'after' is incorrect given the reference spans earlier; only the E2 end is roughly similar, so key timing and relation are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Make it a habit', when does he mention biting the nail?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3936.731,
        "end": 3942.304
      },
      "pred_interval": {
        "start": 3951.0,
        "end": 3954.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.268999999999778,
        "end": 11.695999999999913,
        "average": 12.982499999999845
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.8985776901245117,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event order and phrasing roughly right, but the anchor and target timestamps are significantly off (by ~13\u201315s) and the predicted relationship is less precise than the ground truth's 'directly after'\u2014key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker finishes requesting the first speaker to talk about staying fit, when does the first speaker repeat 'How to stay fit, physically fit, sir'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3986.204,
        "end": 3987.967
      },
      "pred_interval": {
        "start": 4019.0,
        "end": 4023.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.79599999999982,
        "end": 35.0329999999999,
        "average": 33.91449999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.8501744270324707,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures that the target utterance occurs immediately after the anchor, but the provided timestamps deviate substantially from the ground truth, so the temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that 'fitness is absolutely essential in any profession', when does he explain that 'our work transcends the entire waking hours of our time'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4056.898,
        "end": 4064.789
      },
      "pred_interval": {
        "start": 4084.0,
        "end": 4087.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.10199999999986,
        "end": 22.210999999999785,
        "average": 24.656499999999824
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.8016911149024963,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the quoted lines and the 'immediately after' relationship, but both event timestamps are substantially incorrect compared to the ground truth, so it is factually wrong on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the judge smiling for the first time, when does he explain why the judge smiled?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4157.778,
        "end": 4164.121
      },
      "pred_interval": {
        "start": 4154.5,
        "end": 4159.3
      },
      "iou": 0.1581956137615549,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.2780000000002474,
        "end": 4.820999999999913,
        "average": 4.04950000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705885,
        "text_similarity": 0.6762163639068604,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and the explanatory target and their 'after' relationship and even quotes the explanation; however the provided timestamps differ from the reference by a few seconds (E1 end and E2 start/end are slightly shifted), so it's not a perfect match."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker uses a cricket analogy involving Kumble, when does he advise the audience to 'Go and observe'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4289.867,
        "end": 4291.509
      },
      "pred_interval": {
        "start": 4276.6,
        "end": 4278.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.266999999999825,
        "end": 12.609000000000378,
        "average": 12.938000000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953488,
        "text_similarity": 0.8121988773345947,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the anchor (Kumble analogy), the target phrase ('Go and observe'), and the temporal relation ('after'), but the reported timestamps are substantially inaccurate (anchor end and target start differ by several seconds from the ground truth), so it's a partial but flawed match."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker recommends reading Dale Carnegie's book, when does he mention other books on management techniques?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4206.04,
        "end": 4211.851
      },
      "pred_interval": {
        "start": 4255.3,
        "end": 4259.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.26000000000022,
        "end": 47.54899999999998,
        "average": 48.4045000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3177570093457943,
        "text_similarity": 0.7549033164978027,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the mention of other books follows the Dale Carnegie recommendation, but the timestamps are substantially different from the reference (off by ~42s) and it includes specific phrasing not present in the ground truth, so key factual details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises lawyers not to sit in the canteen, when does he instruct them to sit in a court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4301.616,
        "end": 4305.419
      },
      "pred_interval": {
        "start": 4314.6,
        "end": 4316.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.984000000000378,
        "end": 11.381000000000313,
        "average": 12.182500000000346
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.6456249952316284,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the instruction follows the advice, but it gives substantially incorrect timestamps and does not match the precise start/end times or duration in the ground truth, so it is largely factually mismatched."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nitika finishes asking her question on YouTube, when does the speaker ask for the question to be repeated?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4378.889,
        "end": 4380.233
      },
      "pred_interval": {
        "start": 4410.3,
        "end": 4412.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.411000000000058,
        "end": 32.66699999999946,
        "average": 32.03899999999976
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.7378498911857605,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the speaker asks for a repeat immediately after Nitika's question, but the timestamps and duration are significantly incorrect (off by ~33s) and it introduces a quoted phrase/duration not supported by the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'Then which are the sections?' related to the Contract Act, when does he begin describing the illustration about a property purchase agreement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4437.734,
        "end": 4450.995
      },
      "pred_interval": {
        "start": 4462.9,
        "end": 4465.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.165999999999258,
        "end": 14.00500000000011,
        "average": 19.585499999999683
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753088,
        "text_similarity": 0.553091287612915,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the rhetorical question as the anchor and that the illustration follows (including the quoted phrase), but the provided timestamps are substantially inaccurate compared to the ground truth, the end time of the illustration is omitted, and the temporal relationship is mischaracterized."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he closes his case, when does he explain why?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4475.036,
        "end": 4480.501
      },
      "pred_interval": {
        "start": 4491.0,
        "end": 4506.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.963999999999942,
        "end": 25.498999999999796,
        "average": 20.73149999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.35443037974683544,
        "text_similarity": 0.7700484991073608,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target utterances and the 'after' relation, but the timestamps are significantly shifted (~+29s) from the reference and the explanation wording differs (\"balance sheet\" vs cited \"bank statement\") with mismatched durations, so it is only partially aligned."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises lawyers to practice in trial court, when does he explain the main skill acquired there?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4559.082,
        "end": 4593.185
      },
      "pred_interval": {
        "start": 4539.0,
        "end": 4561.0
      },
      "iou": 0.035397250161477364,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.082000000000335,
        "end": 32.1850000000004,
        "average": 26.133500000000367
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7300498485565186,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies both events (advice to go to trial court and explanation about cross\u2011examination) and the 'after' relation, but the time intervals are notably misaligned (E1 and E2 start times differ substantially from the reference)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that 'habit would be derived of five words', when does he proceed to list and explain those words?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4629.048,
        "end": 4640.287
      },
      "pred_interval": {
        "start": 4601.0,
        "end": 4618.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.047999999999774,
        "end": 22.287000000000262,
        "average": 25.167500000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.3555555555555555,
        "text_similarity": 0.776716411113739,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the anchor utterance roughly correct but the target interval is substantially earlier than the ground truth and the relation/temporal alignment is incorrect, so key timing information is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking about working parallelly in High Court and Trial Court, when does another speaker give an affirmative answer?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4668.869,
        "end": 4673.473
      },
      "pred_interval": {
        "start": 4663.1,
        "end": 4665.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.768999999999323,
        "end": 8.472999999999956,
        "average": 7.12099999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.748119592666626,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an affirmative reply occurring immediately after the question, but the speaker timestamps and durations are substantially incorrect and inconsistent with the reference (misplaced start/end times and an unfounded \"Yes\" onset), so it fails on factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains how a youngster can gain practical experience in a smaller trial court office, when does he state this would not be possible in a larger office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4722.411,
        "end": 4727.419
      },
      "pred_interval": {
        "start": 4692.5,
        "end": 4695.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.911000000000058,
        "end": 32.01900000000023,
        "average": 30.965000000000146
      },
      "rationale_metrics": {
        "rouge_l": 0.21505376344086022,
        "text_similarity": 0.7620892524719238,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target content and the 'after' relation, but the timestamps are substantially incorrect (off by ~20\u201330s) and the target quote is slightly garbled, so it is not an accurate match."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes sharing the Japanese quote about a frog in a well, when does another speaker interject with a related Sanskrit saying?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4760.624,
        "end": 4763.847
      },
      "pred_interval": {
        "start": 4717.7,
        "end": 4720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.92399999999998,
        "end": 43.84699999999975,
        "average": 43.385499999999865
      },
      "rationale_metrics": {
        "rouge_l": 0.35000000000000003,
        "text_similarity": 0.8313289880752563,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the order (E2 interjects after E1) and the Sanskrit phrase, but the timestamps are substantially incorrect (off by ~39 seconds) and it misrepresents the exact timing/continuity (says 'immediately after' vs. the speaker continuing slightly), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks how a new associate creates space in a big office with many existing associates, when does the host add that the guest also has a very big office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4866.42,
        "end": 4872.823
      },
      "pred_interval": {
        "start": 4864.5,
        "end": 4870.0
      },
      "iou": 0.4301333653730373,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.9200000000000728,
        "end": 2.82300000000032,
        "average": 2.3715000000001965
      },
      "rationale_metrics": {
        "rouge_l": 0.37209302325581395,
        "text_similarity": 0.7766945362091064,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation right (E2 after E1) and E2 falls within the correct window, but the anchor and event start times and quoted phrasings are inaccurate and do not match the ground-truth segments, indicating a poor semantic/timestamp alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (guest) finishes explaining that a junior lawyer catches the senior's eye by preparing research and synopses, when does the speaker pose a rhetorical question about how a junior lawyer could get attention by arriving late?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4940.902,
        "end": 4951.577
      },
      "pred_interval": {
        "start": 4890.2,
        "end": 4896.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.702000000000226,
        "end": 55.577000000000226,
        "average": 53.139500000000226
      },
      "rationale_metrics": {
        "rouge_l": 0.40506329113924044,
        "text_similarity": 0.8160054087638855,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the rhetorical question follows the explanation, but the event timecodes are substantially off (~44\u201351s earlier than ground truth) and the quoted wording differs from the reference, indicating a misalignment in precise timing and content."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (guest) emphasizes that hard work is the most important thing in the legal profession, when does the speaker begin to describe a scenario where a client observes their lawyer arguing a case diligently?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4985.39,
        "end": 4996.139
      },
      "pred_interval": {
        "start": 4905.0,
        "end": 4910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.39000000000033,
        "end": 86.13900000000012,
        "average": 83.26450000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.3655913978494624,
        "text_similarity": 0.8594935536384583,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and the 'after' relation, but the provided timestamps are substantially different from the ground truth, so the temporal information is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker emphasizes that youngsters must do hard work, when does the second speaker agree with the point about hard work?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5024.62,
        "end": 5033.21
      },
      "pred_interval": {
        "start": 5014.42,
        "end": 5018.54
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.199999999999818,
        "end": 14.670000000000073,
        "average": 12.434999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.1386138613861386,
        "text_similarity": 0.6165748238563538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the second speaker agrees after the first, but the timestamp intervals are substantially incorrect (off by ~14s) and the quoted phrasing appears hallucinated; thus it fails to match the ground-truth temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks how to master lower court procedures without much practice, when does the first speaker suggest sitting in courts to learn?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5044.49,
        "end": 5051.81
      },
      "pred_interval": {
        "start": 5026.44,
        "end": 5028.98
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.050000000000182,
        "end": 22.830000000000837,
        "average": 20.44000000000051
      },
      "rationale_metrics": {
        "rouge_l": 0.12765957446808512,
        "text_similarity": 0.43125373125076294,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic content (question and the advice to 'sit there') but the timestamps are incorrect and the anchor/target assignment is inconsistent with the reference, so it fails to match the correct temporal annotations."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker asks if one should have High Court experience before practicing in the Supreme Court, when does the first speaker suggest that one is better off going to the trial court first?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5126.422,
        "end": 5141.99
      },
      "pred_interval": {
        "start": 5052.3,
        "end": 5054.74
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.12199999999939,
        "end": 87.25,
        "average": 80.6859999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.12,
        "text_similarity": 0.4528113901615143,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content of the advice but the speaker labels and all timestamps are incorrect (predicted times ~5046\u20135054s vs. reference 5090.9\u20135141.99s); due to these factual/time mismatches it does not align with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the difference between 'learn' and 'earn', when does he say, 'So thank you everyone'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5198.8,
        "end": 5199.7
      },
      "pred_interval": {
        "start": 5194.0,
        "end": 5195.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.800000000000182,
        "end": 4.199999999999818,
        "average": 4.5
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.6898583173751831,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as immediate/once finished, but the event timestamps are inaccurate and inconsistent with the ground truth (off by ~3.9s and misaligned start/end times), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker thanks 'Mr. Hola' and mentions it's always a pleasure connecting, when does the second speaker say 'Thank you very much'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5219.7,
        "end": 5221.2
      },
      "pred_interval": {
        "start": 5223.0,
        "end": 5225.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.300000000000182,
        "end": 3.800000000000182,
        "average": 3.550000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.5217391304347827,
        "text_similarity": 0.7265777587890625,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the 'after' relation, but the provided timestamps for both E1 and E2 are several seconds later than the ground truth, so the timing information is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker says 'You are the educator of lawyers', when is the next time the first speaker says 'Thank you'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5224.9,
        "end": 5226.9
      },
      "pred_interval": {
        "start": 5227.5,
        "end": 5228.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.600000000000364,
        "end": 1.6000000000003638,
        "average": 2.100000000000364
      },
      "rationale_metrics": {
        "rouge_l": 0.4081632653061225,
        "text_similarity": 0.7824024558067322,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: both event timestamps are misaligned with the ground truth (predicted E1 at 5227.0 vs actual 5221.3\u20135223.3, predicted E2 at 5227.5\u20135228.5 vs actual 5224.9\u20135226.9) and it fails to match the correct skipping of the second speaker's 'Thank you'."
      }
    },
    {
      "question_id": "001",
      "question": "After Trivikram thanks Mr. Vikas Chaturvedi, when does he extend a warm welcome to Mr. Udaya Holla?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.288,
        "end": 166.858
      },
      "pred_interval": {
        "start": 192.4,
        "end": 196.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.111999999999995,
        "end": 29.341999999999985,
        "average": 29.22699999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23809523809523808,
        "text_similarity": 0.7261083126068115,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') and contains 'thanks' and 'welcome' utterances, but it misreports the event timestamps by ~36s and misattributes the speaker for the welcome, so the key factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During his discussion of strategies for a civil dispute, when does Mr. Udaya Holla state that preparation is what counts the most?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.8,
        "end": 254.67
      },
      "pred_interval": {
        "start": 253.4,
        "end": 257.1
      },
      "iou": 0.23962264150943002,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5999999999999943,
        "end": 2.4300000000000352,
        "average": 2.015000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.35555555555555557,
        "text_similarity": 0.6627519726753235,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction substantially misidentifies E1 (correct E1 is at 219.424s but predicted at ~251\u2013253s), and although E2 overlaps somewhat with the correct window it is shifted later; the stated relation ('after') contradicts the correct relation (E2 falls within the broader discussion)."
      }
    },
    {
      "question_id": "001",
      "question": "After Vikas Chatrath finishes explaining the difference between 'learn' and 'earn', when does he thank everyone and wish them to stay safe and blessed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5198.086,
        "end": 5203.11
      },
      "pred_interval": {
        "start": 5198.3,
        "end": 5202.4
      },
      "iou": 0.8160828025477542,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.2139999999999418,
        "end": 0.7100000000000364,
        "average": 0.4619999999999891
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.7935510873794556,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the thank-you segment (E2) and the temporal relation 'after', with E2 timings close to the ground truth, but it mislabels/mistimes the anchor (E1) by about 2 seconds\u2014conflating the end of the explanation with the start of the thank-you\u2014so it's only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "During the announcement of the next session, when does Vikas Chatrath mention Mr. Shingar Murali?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5207.213,
        "end": 5209.213
      },
      "pred_interval": {
        "start": 5216.0,
        "end": 5218.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.787000000000262,
        "end": 8.98700000000008,
        "average": 8.887000000000171
      },
      "rationale_metrics": {
        "rouge_l": 0.38888888888888884,
        "text_similarity": 0.7281514406204224,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relationship but gives substantially incorrect timestamps (shifted by ~6\u20139s) compared to the ground truth, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes telling everyone to stay safe and blessed, when does he say it's always a pleasure connecting with Mr. Hola?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5201.609,
        "end": 5204.971
      },
      "pred_interval": {
        "start": 5221.5,
        "end": 5224.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.89099999999962,
        "end": 19.329000000000633,
        "average": 19.610000000000127
      },
      "rationale_metrics": {
        "rouge_l": 0.2469135802469136,
        "text_similarity": 0.5725380182266235,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the phrase and the 'once_finished' relation, but the key factual timestamps are significantly off (about 20s later) and thus contradict the ground truth, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the prosecutor finishes introducing his opening statement, when does he explain why he gets to go first?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.329,
        "end": 50.118
      },
      "pred_interval": {
        "start": 57.92,
        "end": 65.16
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.591000000000001,
        "end": 15.041999999999994,
        "average": 14.816499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3440860215053763,
        "text_similarity": 0.8695728778839111,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (finishing opening and explaining burden of proof) but gives timestamps that differ substantially from the reference (57.92\u201365.16s vs 41.646\u201350.118s) and labels the relation as 'after' instead of 'immediately follows', so it is largely incorrect despite capturing the event types."
      }
    },
    {
      "question_id": "002",
      "question": "After the prosecutor mentions the bartender, John Thomas, and the server, Roberta Jones, when does John observe Mr. Miller pass a small glass bottle?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 150.298,
        "end": 158.469
      },
      "pred_interval": {
        "start": 155.84,
        "end": 167.54
      },
      "iou": 0.1524765108456091,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.542000000000002,
        "end": 9.070999999999998,
        "average": 7.3065
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7817862629890442,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') and partially overlaps the true E2, but it misplaces the anchor (E1) earlier than the reference and shifts/extends the target interval later than the ground truth, omitting the correct E1 timing and not matching the precise E2 bounds."
      }
    },
    {
      "question_id": "003",
      "question": "Once John observes the unknown man pull a gun on the defendant, when does the gunman pull out his shield and shout 'Police! You are under arrest!'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 176.477,
        "end": 185.645
      },
      "pred_interval": {
        "start": 200.18,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.703000000000003,
        "end": 24.35499999999999,
        "average": 24.028999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.3366336633663366,
        "text_similarity": 0.8263083696365356,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events and their order, but the timestamps are substantially incorrect (shifted ~20\u201325s) and it fails to match the correct start/end times or the fact that the shout immediately follows the anchor event."
      }
    },
    {
      "question_id": "001",
      "question": "After John observes Mr. Miller pass a small glass bottle with white powder, when does John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.6,
        "end": 166.8
      },
      "pred_interval": {
        "start": 169.0,
        "end": 173.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.400000000000006,
        "end": 6.199999999999989,
        "average": 5.799999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.4556962025316456,
        "text_similarity": 0.8451278805732727,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the anchor event, but it omits the anchor end time and gives substantially different start/end times for the target (shifting it to a later narrated segment), so the timing information is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the officer gives chase and the defendant pushes him, when does the officer trip and hit his head, opening a big gash on his forehead?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 216.0,
        "end": 226.824
      },
      "pred_interval": {
        "start": 199.0,
        "end": 206.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 20.824000000000012,
        "average": 18.912000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3434343434343434,
        "text_similarity": 0.8223700523376465,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but misidentifies both event intervals and key timestamps (push at 196\u2013199s vs ground truth ~214.6\u2013218.0s and target 216.0\u2013226.824s), so it fails on factual alignment and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes sees the defendant exiting the saloon like a bat out of hell, when does she decide she should get back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 343.5
      },
      "pred_interval": {
        "start": 301.0,
        "end": 306.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 37.5,
        "average": 35.25
      },
      "rationale_metrics": {
        "rouge_l": 0.34545454545454546,
        "text_similarity": 0.8245370388031006,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and that the target occurs after the anchor, but both event time spans are substantially misaligned with the ground truth and the predicted target wording/times do not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Reyes finishes thinking 'something seems wrong', when does she think 'I should get back to that bar'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.3,
        "end": 340.5
      },
      "pred_interval": {
        "start": 339.667,
        "end": 343.217
      },
      "iou": 0.14078080108163438,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.366999999999962,
        "end": 2.7169999999999845,
        "average": 2.541999999999973
      },
      "rationale_metrics": {
        "rouge_l": 0.48351648351648346,
        "text_similarity": 0.892691433429718,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same events and a subsequent/after relation, but the timestamp spans are substantially misaligned (E1 and E2 times are off by several seconds and do not match the reference intervals), so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the car was seized, when does he mention a forensic technician finding cocaine residue?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.1,
        "end": 428.2
      },
      "pred_interval": {
        "start": 397.483,
        "end": 408.483
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.61700000000002,
        "end": 19.716999999999985,
        "average": 21.167
      },
      "rationale_metrics": {
        "rouge_l": 0.5357142857142857,
        "text_similarity": 0.9262130260467529,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the quoted phrases and the 'after' relation, but the reported timestamps are substantially incorrect and do not match the ground-truth time intervals, making the temporal localization largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker specifies the time and date of the incident, when does he mention the car being seized?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 415.9,
        "end": 419.1
      },
      "pred_interval": {
        "start": 397.483,
        "end": 398.983
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.416999999999973,
        "end": 20.11700000000002,
        "average": 19.266999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.3838383838383838,
        "text_similarity": 0.9056795835494995,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it misassigns the anchor/target (anchor should be the time/date mention), gives wrong timecodes, and states a 'simultaneous' relation instead of 'after'; only the seizure phrase matches the ground truth content."
      }
    },
    {
      "question_id": "001",
      "question": "After the prosecution states that the defendant Carl Miller was meeting with his business clients, when does the evidence show that the defendant was in the saloon at 3:55 p.m.?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 510.41,
        "end": 510.45
      },
      "pred_interval": {
        "start": 689.0,
        "end": 692.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 178.58999999999997,
        "end": 181.55,
        "average": 180.07
      },
      "rationale_metrics": {
        "rouge_l": 0.4201680672268907,
        "text_similarity": 0.7624526023864746,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the ordering ('after') but the timestamps are grossly incorrect: the anchor interval is vastly overextended and the target is placed hundreds of seconds later than the true 510.41\u2013510.45s, so the key temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once John observed Mr. Miller pass a small glass bottle with a white powder in it to his buddy, when did John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 631.0,
        "end": 638.07
      },
      "pred_interval": {
        "start": 703.0,
        "end": 706.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.0,
        "end": 67.92999999999995,
        "average": 69.96499999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.5242718446601942,
        "text_similarity": 0.8419312238693237,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') and the event descriptions, but both event timestamps are significantly different from the ground-truth intervals (off by ~80s), so the alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the gunman shouted, 'Police! You are under arrest!', when did the defendant immediately take off running towards the exit?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 673.544,
        "end": 683.655
      },
      "pred_interval": {
        "start": 719.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.45600000000002,
        "end": 36.34500000000003,
        "average": 40.90050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.37254901960784315,
        "text_similarity": 0.7923306226730347,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') correct, but both the anchor and target timestamps are substantially incorrect/mismatched with the ground truth intervals, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Dr. Reyes sees the defendant exiting the saloon, when does she wonder if something is wrong?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.4,
        "end": 751.6
      },
      "pred_interval": {
        "start": 723.0,
        "end": 743.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.399999999999977,
        "end": 8.600000000000023,
        "average": 16.0
      },
      "rationale_metrics": {
        "rouge_l": 0.39252336448598124,
        "text_similarity": 0.9063906073570251,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct and the general idea that her thought follows seeing the defendant, but the event timestamps are substantially incorrect compared to the ground truth and the predicted answer introduces a specific quoted thought not present in the reference, so it is largely mismatched."
      }
    },
    {
      "question_id": "002",
      "question": "After the defendant jumped into a convertible, when does he look again at Dr. Reyes while starting the car?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.3,
        "end": 773.5
      },
      "pred_interval": {
        "start": 813.0,
        "end": 823.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.700000000000045,
        "end": 49.5,
        "average": 47.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.9166247844696045,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the event identities and 'after' relation right but the timestamps are substantially incorrect for both anchor (770.0s vs correct ~761.2s) and target (813.0\u2013823.0s vs correct 768.3\u2013773.5s), so it is largely factually misaligned."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes writes down the Mustang's plate number, when does she decide to go back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 797.9,
        "end": 808.8
      },
      "pred_interval": {
        "start": 880.0,
        "end": 890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.10000000000002,
        "end": 81.20000000000005,
        "average": 81.65000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.41025641025641024,
        "text_similarity": 0.8842738270759583,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation and the thought prompting Dr. Reyes to return, but its timestamps are substantially incorrect compared to the ground truth, so the answer is factually inaccurate on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"You will not hear one single person,\" when does the speaker mention the date \"September 8th, 2020\"?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 882.8,
        "end": 885.6
      },
      "pred_interval": {
        "start": 877.7,
        "end": 882.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.099999999999909,
        "end": 3.6000000000000227,
        "average": 4.349999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.8480631113052368,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance and the temporal relation ('after'), and the anchor time falls within the reference interval, but the target event timestamps are noticeably earlier and do not match the correct start/end interval, so the timing details are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that the car was seized and taken to the crime lab, when does the speaker begin describing the finding of cocaine residue by a forensic technician?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 891.0,
        "end": 904.4
      },
      "pred_interval": {
        "start": 898.0,
        "end": 899.8
      },
      "iou": 0.13432835820895206,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 4.600000000000023,
        "average": 5.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.8111056089401245,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('immediately after') and the described span ordering, but the provided timestamps conflict substantially with the ground truth (E1: 897.8s vs 890.9s; E2 start: 898.0s vs 891.0s) and it omits the E2 end time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's statement about finding the defendant Carl Miller guilty of felonies, when is \"fleeing and eluding\" specifically mentioned?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 930.8,
        "end": 947.9
      },
      "pred_interval": {
        "start": 923.6,
        "end": 927.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.199999999999932,
        "end": 20.899999999999977,
        "average": 14.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.27956989247311825,
        "text_similarity": 0.840146005153656,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and the temporal relation as 'within,' but the timestamps are inaccurate: E1 start is off by ~1s and E2 is placed much earlier (predicted 923.6\u2013927.0s vs correct 930.8\u2013947.9s), a substantive factual error."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks the witness to state her name and spell her last name, when does the witness spell out her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.561,
        "end": 36.805
      },
      "pred_interval": {
        "start": 30.7,
        "end": 36.5
      },
      "iou": 0.48140868140868137,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8610000000000007,
        "end": 0.3049999999999997,
        "average": 1.5830000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2736842105263158,
        "text_similarity": 0.7442092895507812,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the relation ('after') and the witness spelling 'MENDOZA', but the event timings differ substantially from the ground truth (E1 and E2 start/end times are off by ~1.6\u20132.9 seconds, with E2 start notably earlier), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks the witness if she was working as a plumber at a construction site on the evening of October 27th, 2020, when does the witness describe finding the broken window of her vehicle?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 69.075,
        "end": 74.643
      },
      "pred_interval": {
        "start": 72.4,
        "end": 87.0
      },
      "iou": 0.1251324965132494,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.325000000000003,
        "end": 12.357,
        "average": 7.841000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6435234546661377,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation (witness describes finding the broken window after the question) but the event time spans are substantially incorrect and the speaker labeling is inconsistent with the ground truth, omitting the precise correct timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker asks the witness to describe the area where she parked her car and if she was worried about its safety, when does the witness explain that she was not worried because she always parked near a bar?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.14,
        "end": 128.77
      },
      "pred_interval": {
        "start": 134.5,
        "end": 145.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.36,
        "end": 16.92999999999998,
        "average": 20.64499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1732283464566929,
        "text_similarity": 0.6875134706497192,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the semantic content and relation ('after') of the witness's explanation, but the timestamps are substantially incorrect and do not match the ground-truth time intervals, so it fails on the primary temporal accuracy requirement."
      }
    },
    {
      "question_id": "001",
      "question": "After Ms. Mendoza reports that her tablet and ice skates were stolen, when does the lawyer ask her if she made any attempt to look for the perpetrators or immediately contacted the police?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 164.191,
        "end": 172.955
      },
      "pred_interval": {
        "start": 190.1,
        "end": 212.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.908999999999992,
        "end": 39.744999999999976,
        "average": 32.826999999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.16470588235294117,
        "text_similarity": 0.7119728326797485,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation, but it gives substantially incorrect event timestamps (E1/E2 are shifted by ~27\u201340 seconds from the reference), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza explains that she noticed a suspicious man around her car while on the phone with the operator, when does the lawyer ask her what the officer did?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 261.353,
        "end": 262.466
      },
      "pred_interval": {
        "start": 226.8,
        "end": 231.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.553,
        "end": 31.165999999999997,
        "average": 32.8595
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7738284468650818,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both answers label the relation as 'after', the predicted timestamps for E1 and E2 are substantially incorrect and the events are mislocalized relative to the ground truth; it also fails to reflect that the lawyer's question follows the officer's arrival and the suspect being pointed out."
      }
    },
    {
      "question_id": "003",
      "question": "After Ms. Mendoza states that the thief got out of the car and started running down the street, when does she describe the thief punching the arresting officer?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 321.13,
        "end": 331.68
      },
      "pred_interval": {
        "start": 276.3,
        "end": 283.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.829999999999984,
        "end": 48.28000000000003,
        "average": 46.55500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.6829981803894043,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') and that the punch was a reaction to the arrest, but it gives substantially incorrect event timestamps (both E1 and E2 times differ greatly from the ground truth) and thus fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza to point out the man who assaulted the officer, when does Ms. Mendoza describe him?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 349.986,
        "end": 355.253
      },
      "pred_interval": {
        "start": 345.0,
        "end": 352.4
      },
      "iou": 0.23544328489222574,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.98599999999999,
        "end": 2.8530000000000086,
        "average": 3.9194999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3119266055045871,
        "text_similarity": 0.76478111743927,
        "llm_judge_score": 8,
        "llm_judge_justification": "The predicted answer correctly captures the relation, the content of the description ('skinny' with 'gray hair'), and that the response follows the lawyer's question; however, the timestamps differ by a few seconds and the E1 phrasing omits explicit mention of 'describe' (saying only 'identify'), so it's not an exact match."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking if the man settled down and cooperated after being put in the patrol car, when does Ms. Mendoza reply that he did not?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 457.548,
        "end": 460.233
      },
      "pred_interval": {
        "start": 414.0,
        "end": 417.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.548,
        "end": 43.033000000000015,
        "average": 43.29050000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24000000000000002,
        "text_similarity": 0.7030920386314392,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the lawyer's question and Ms. Mendoza's denial (paraphrased), but it gives substantially incorrect timestamps, adds an unsupported audio-cue claim, and slightly mismatches the temporal relation wording, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the new lawyer says 'Good morning', when does he ask Ms. Mendoza to state and spell her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 502.241,
        "end": 504.904
      },
      "pred_interval": {
        "start": 447.0,
        "end": 450.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.240999999999985,
        "end": 54.103999999999985,
        "average": 54.672499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.4421052631578948,
        "text_similarity": 0.7260192632675171,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation, but the timestamps are substantially different and inconsistent (E2 overlaps/starts at the same time as E1), and it adds an unsupported audio-cue claim; thus it is largely incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza if she prefers to testify with an interpreter, when does Ms. Mendoza confirm her preference?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.733,
        "end": 528.876
      },
      "pred_interval": {
        "start": 65.0,
        "end": 75.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 460.73299999999995,
        "end": 453.876,
        "average": 457.30449999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.34408602150537637,
        "text_similarity": 0.6231489181518555,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the events and the 'after' relation, but it provides incorrect timestamps (65\u201375s vs. ~525\u2013529s) and omits the precise end time of the lawyer's question, so key factual temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza states that she saw someone inside her car through the broken window, when does the lawyer acknowledge her statement?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 559.358,
        "end": 561.878
      },
      "pred_interval": {
        "start": 85.0,
        "end": 95.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 474.35799999999995,
        "end": 466.87800000000004,
        "average": 470.618
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068966,
        "text_similarity": 0.6985423564910889,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies both events and the 'after' relation, but it gives completely incorrect timestamps (85\u201395s vs ~559s) and thus fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the lawyer asks Ms. Mendoza if anything was removed from her vehicle, when does Ms. Mendoza list the stolen items?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.801,
        "end": 634.921
      },
      "pred_interval": {
        "start": 155.0,
        "end": 165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 467.80100000000004,
        "end": 469.92100000000005,
        "average": 468.86100000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2413793103448276,
        "text_similarity": 0.5933809280395508,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and the 'after' relation and even matches the initial 'Por supuesto' ('Of course'), but it gives substantially incorrect timestamps and durations compared to the ground truth, which is a significant factual mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After the witness finishes describing the man looking through the car window, when does the lawyer state that a deputy was arriving?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 711.914,
        "end": 714.633
      },
      "pred_interval": {
        "start": 703.3,
        "end": 708.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.614000000000033,
        "end": 6.5330000000000155,
        "average": 7.573500000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.7856703400611877,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the speaker roles and the 'after' relation and mentions a deputy arriving, but the event time spans are substantially different from the ground truth (predicted timestamps are many seconds off and do not match the correct event boundaries), so the answer is not correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking what the officer did, when does the witness describe the officer speaking into his radio and telling her to stay put?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.233,
        "end": 766.142
      },
      "pred_interval": {
        "start": 729.7,
        "end": 734.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.532999999999902,
        "end": 31.242000000000075,
        "average": 21.88749999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.7320543527603149,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation ('once_finished') and the witness's content (radio call and 'stay put'), but the reported timestamps for both E1 and E2 are substantially incorrect compared to the ground truth, so the temporal alignment is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the witness finishes stating she is absolutely sure about identifying the defendant, when does the lawyer ask about the suspect's behavior after being cuffed?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 851.101,
        "end": 861.493
      },
      "pred_interval": {
        "start": 764.8,
        "end": 768.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.30100000000004,
        "end": 93.1930000000001,
        "average": 89.74700000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.37037037037037035,
        "text_similarity": 0.7507264614105225,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the two utterances and the 'after' relation, it gives substantially incorrect start/end timestamps (off by ~88\u201393 seconds) and misaligns the event boundaries, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the male lawyer asks what happened when they got to the patrol car, when does Ms. Mendoza describe the officers searching the suspect?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 885.63,
        "end": 894.463
      },
      "pred_interval": {
        "start": 890.0,
        "end": 894.0
      },
      "iou": 0.452847277255747,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.3700000000000045,
        "end": 0.46299999999996544,
        "average": 2.416499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.2654867256637168,
        "text_similarity": 0.6090606451034546,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that Ms. Mendoza describes the officers searching the suspect after the lawyer's question and roughly when the description ends, but the E1/E2 timestamps are substantially off and even overlap (contradicting the 'after' timing), and the prediction introduces specific found items not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ms. Mendoza finishes explaining she got closer to see what was happening, when does she state that's why she remembers it well?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 919.062,
        "end": 923.688
      },
      "pred_interval": {
        "start": 904.0,
        "end": 908.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.062000000000012,
        "end": 15.687999999999988,
        "average": 15.375
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.7365736961364746,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the content (she got closer and says she remembers it well) but the temporal annotations are substantially wrong (predicted spans overlap and place both events ~904\u2013908s versus reference 913\u2013923s) and the relation/timing is misaligned."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male lawyer finishes asking if the suspect settled down and cooperated in the patrol car, when does Ms. Mendoza state he did not cooperate?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 937.892,
        "end": 940.207
      },
      "pred_interval": {
        "start": 914.0,
        "end": 918.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.892000000000053,
        "end": 22.206999999999994,
        "average": 23.049500000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.11320754716981134,
        "text_similarity": 0.6318115592002869,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the core idea that she said he 'didn't' cooperate, but it has incorrect timestamps, adds hallucinated details about being taken to the slammer, misaligns turn timing, and labels the relation differently, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Good evening, friends,' when does he mention that their journey started in 2020 during the COVID times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 5.453,
        "end": 8.514
      },
      "pred_interval": {
        "start": 13.2,
        "end": 15.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.746999999999999,
        "end": 7.286000000000001,
        "average": 7.516500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.6938532590866089,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target phrases, but it gives substantially incorrect timestamps (13.2s vs. 3.592s anchor; 13.2\u201315.8s vs. 5.453\u20138.514s target) and mischaracterizes the temporal relation ('during' rather than the target occurring after the anchor)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker (Vikas Chatrath) says 'I think there's some issue with the net,' when does the screen go black with his name displayed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.726,
        "end": 75.546
      },
      "pred_interval": {
        "start": 131.4,
        "end": 131.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.67400000000001,
        "end": 56.354,
        "average": 60.014
      },
      "rationale_metrics": {
        "rouge_l": 0.1971830985915493,
        "text_similarity": 0.5887085199356079,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the black screen follows the speaker, but the timestamps are wildly incorrect (predicted ~131s vs correct anchor ending at 63.456s and target 67.726\u201375.546s), so it contradicts key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath says 'Over to you, sir,' when does Mr. R.S. Cheema begin speaking?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.165,
        "end": 175.752
      },
      "pred_interval": {
        "start": 168.2,
        "end": 170.4
      },
      "iou": 0.1635328389830522,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9650000000000034,
        "end": 5.352000000000004,
        "average": 3.1585000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.635886549949646,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct relation ('immediately after') and gives start times close to the reference, but the timestamps differ by ~1\u20131.2s and it omits the target's end time, so it is only an approximate match."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chatrath finishes his speech and invites Mr. Cheema, when does Mr. Cheema describe the topic as very generic and vast?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.048,
        "end": 204.229
      },
      "pred_interval": {
        "start": 155.7,
        "end": 158.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.34800000000001,
        "end": 45.82900000000001,
        "average": 45.08850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2650602409638554,
        "text_similarity": 0.6430681347846985,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event labels and the 'after' relation right and captures the quoted utterance, but its timestamps are grossly incorrect (\u2248150s/155.7\u2013158.4s vs ground truth \u224815.45\u201317.52s and 50.05\u201354.23s), so it fails the required temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After Mr. Cheema says he agreed with the choice of the broad topic, when does he explain the reason for his agreement?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 229.669,
        "end": 242.242
      },
      "pred_interval": {
        "start": 175.7,
        "end": 179.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.96900000000002,
        "end": 62.44199999999998,
        "average": 58.2055
      },
      "rationale_metrics": {
        "rouge_l": 0.3291139240506329,
        "text_similarity": 0.7362426519393921,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events (agreement and its explanation) but assigns completely incorrect timestamps (175.7s vs 224.6\u2013227.3s for E1 and 175.7\u2013179.8s vs 229.669\u2013242.242s for E2) and even overlaps the segments; thus it is largely temporally incorrect despite capturing the relation direction."
      }
    },
    {
      "question_id": "003",
      "question": "During Mr. Cheema's description of going into division bench courts to hear idols and icons, when does he mention that the judges are not in a hurry?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.317,
        "end": 313.619
      },
      "pred_interval": {
        "start": 246.1,
        "end": 249.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.21700000000001,
        "end": 64.21900000000002,
        "average": 61.71800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.42222222222222217,
        "text_similarity": 0.7911138534545898,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'within/during' relation, but the timestamps are substantially incorrect and do not overlap with the ground-truth intervals, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions he has been appearing in three High Courts, when does he state that criminal appeals have not been argued for a long time in the Punjab and Haryana High Court?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 379.0,
        "end": 385.0
      },
      "pred_interval": {
        "start": 355.7,
        "end": 360.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.30000000000001,
        "end": 24.19999999999999,
        "average": 23.75
      },
      "rationale_metrics": {
        "rouge_l": 0.18691588785046728,
        "text_similarity": 0.7520620226860046,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the semantic relation right (E2 occurs after E1) and identifies the correct content, but both event time ranges are substantially incorrect compared to the ground truth, so the answer is factually inaccurate in temporal localization."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions there is very little hearing in appeal, when does he state that the most 'scaring part' is the flood of outstanding litigation causing the hearing to get a beating?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 408.915,
        "end": 418.887
      },
      "pred_interval": {
        "start": 398.4,
        "end": 407.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.515000000000043,
        "end": 11.786999999999978,
        "average": 11.15100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21818181818181817,
        "text_similarity": 0.8127506971359253,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but both event time boundaries are substantially incorrect (E1 and E2 timestamps do not match the reference and even overlap incorrectly), so key factual elements are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says, 'Let's first also define what the appeals are', when does he begin to define criminal appeals as generally understood?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.605,
        "end": 487.757
      },
      "pred_interval": {
        "start": 464.2,
        "end": 465.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.40500000000003,
        "end": 21.956999999999994,
        "average": 15.681000000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.15730337078651685,
        "text_similarity": 0.7703139781951904,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the definition follows the prompt, but it gives incorrect and significantly shifted timestamps for both events (E1 and E2), underestimates E2's duration, and uses a different relation label ('immediately after' vs 'once_finished'), so it is largely misaligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes stating that the real purpose of this exercise is to scuttle the path of the litigant, when does he discuss today's purpose?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 562.2,
        "end": 567.6
      },
      "pred_interval": {
        "start": 686.0,
        "end": 693.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.79999999999995,
        "end": 125.39999999999998,
        "average": 124.59999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2823529411764706,
        "text_similarity": 0.7538561224937439,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and their sequence, but the timestamps are substantially incorrect and the relation is only generically described as 'after' rather than the immediate follow noted in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is listing the second category of appeals, when does he mention the 'Essential Commodities Act'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 595.2,
        "end": 601.662
      },
      "pred_interval": {
        "start": 707.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.79999999999995,
        "end": 108.33799999999997,
        "average": 110.06899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111116,
        "text_similarity": 0.7893227934837341,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the qualitative relation ('during') but misstates both anchor and target timestamps by over 100 seconds and gives incorrect target span endpoints; key temporal details are thus factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating he will confine today's discussion to the Indian Penal Code, when does he explain that he doesn't want to just touch and go?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.624,
        "end": 635.538
      },
      "pred_interval": {
        "start": 714.0,
        "end": 719.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.37599999999998,
        "end": 83.46199999999999,
        "average": 84.41899999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953488,
        "text_similarity": 0.6076545715332031,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the 'don't touch and go' remark follows the IPC confinement remark, but the timestamps are substantially incorrect (predicted ~714s vs ground truth ~626\u2013636s), so the alignment is factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that their judgments are not very clear, when does he begin to explain that, unlike IPC, other acts *do* have provisions for vicarious liability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.063,
        "end": 753.951
      },
      "pred_interval": {
        "start": 750.0,
        "end": 760.0
      },
      "iou": 0.28348999067231245,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.937000000000012,
        "end": 6.048999999999978,
        "average": 4.992999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.18867924528301888,
        "text_similarity": 0.8021166324615479,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the semantic content and the 'after' relation right, but the timestamp annotations are substantially inaccurate (E1 is placed ~7s late and E2 timing is shifted/extended), causing incorrect temporal alignment and overlap."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the first clause of the provisions, when does he explain that an individual is a 'deemed accused' by virtue of their office?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 772.927,
        "end": 777.037
      },
      "pred_interval": {
        "start": 810.0,
        "end": 820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.07299999999998,
        "end": 42.962999999999965,
        "average": 40.01799999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3404255319148936,
        "text_similarity": 0.8248556852340698,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct relation and the 'deemed accused' content, but it misstates the event times by a large margin (E1 ~805s vs 771.695s; E2 810\u2013820s vs 772.927\u2013777.037s), making it factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that the 'second part is most important', when does he begin describing this second part of the provisions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 786.039,
        "end": 799.875
      },
      "pred_interval": {
        "start": 830.0,
        "end": 840.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.96100000000001,
        "end": 40.125,
        "average": 42.043000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.33999999999999997,
        "text_similarity": 0.7116265892982483,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the semantic relation (speaker immediately transitions to the second part) but the timestamps for both E1 and E2 are substantially different from the reference, so the answer fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking if people are vicariously liable, when does he state that the answer is 'yes and no both'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.653,
        "end": 886.655
      },
      "pred_interval": {
        "start": 900.6,
        "end": 904.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.947000000000003,
        "end": 17.44500000000005,
        "average": 16.696000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.3902439024390244,
        "text_similarity": 0.7403810620307922,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their order, but the timestamps are significantly off (~13\u201316s later) and the relation is labeled simply 'after' instead of the precise 'once_finished', so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the Food Safety and Security Act allows a company to appoint a competent person as a nominee, when does he mention the Environment Act?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 954.035,
        "end": 958.702
      },
      "pred_interval": {
        "start": 961.7,
        "end": 962.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.665000000000077,
        "end": 3.798000000000002,
        "average": 5.73150000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25531914893617014,
        "text_similarity": 0.7720550894737244,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the temporal relation as 'after', it misstates both event timestamps (placing E1 and E2 several seconds later and inconsistent with the reference intervals), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes that the rest of the appeals would always be technical, when does he introduce the topic of drafting an appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1052.092,
        "end": 1055.115
      },
      "pred_interval": {
        "start": 978.3,
        "end": 980.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.79200000000014,
        "end": 74.81500000000005,
        "average": 74.3035000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3368421052631579,
        "text_similarity": 0.5987218618392944,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but misidentifies both timestamps and the content of E2 (saying 'arms act' instead of 'drafting an appeal'), so it omits key factual elements and is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions looking into the nuances of drafting, when does he state what he found regarding drafting and appeals?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1054.8,
        "end": 1058.3
      },
      "pred_interval": {
        "start": 1110.0,
        "end": 1116.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.200000000000045,
        "end": 57.700000000000045,
        "average": 56.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.8877403736114502,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative ordering and paraphrases the content, but the timestamps for both E1 and E2 are substantially incorrect (off by ~59\u201360s) and the durations do not match the reference, which is a major factual error."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that criminal proceedings are not adversarial, when does he discuss how formal errors and oversights can be overcome?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1120.624,
        "end": 1125.731
      },
      "pred_interval": {
        "start": 1144.0,
        "end": 1146.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.375999999999976,
        "end": 20.269000000000005,
        "average": 21.82249999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21176470588235294,
        "text_similarity": 0.818473756313324,
        "llm_judge_score": 5,
        "llm_judge_justification": "The predicted answer correctly captures the quoted statements and the 'after' relationship, but the temporal localization is significantly off from the ground truth (timestamps ~29s later), so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that the 313 was not correctly recorded in a Ropar case, when does he recall the accused saying he was made to sign a blank paper?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1194.217,
        "end": 1254.6
      },
      "pred_interval": {
        "start": 1177.0,
        "end": 1180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.2170000000001,
        "end": 74.59999999999991,
        "average": 45.908500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.26373626373626374,
        "text_similarity": 0.8694446682929993,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the relative relation ('after') and quotes the content, but the reported time intervals are substantially different from the reference (off by ~15\u201318 seconds) and the annotated span boundaries are incorrect, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what they should do, when does he ask if an application needs to be filed for additional evidence in the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1251.721,
        "end": 1266.539
      },
      "pred_interval": {
        "start": 1328.8,
        "end": 1336.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.07899999999995,
        "end": 70.16100000000006,
        "average": 73.62
      },
      "rationale_metrics": {
        "rouge_l": 0.3469387755102041,
        "text_similarity": 0.767474889755249,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor utterance and the 'after' relation, but it mislocalizes the target event by a large margin (predicted 1328.8\u20131336.7s vs. reference 1251.7\u20131266.5s), so the timing is factually incorrect and the answer is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the court made a mistake by not formally proving a cassette recording, when does he explain that they had to make an application for evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.0,
        "end": 1294.14
      },
      "pred_interval": {
        "start": 1364.6,
        "end": 1376.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.59999999999991,
        "end": 82.15999999999985,
        "average": 77.87999999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.22000000000000003,
        "text_similarity": 0.6962664127349854,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the two events and their causal/temporal order (speaker explains the application after the mistake), but the timestamps markedly differ from the ground truth and the relation label ('after' vs 'once_finished') is not an exact match, so the localization is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that first reactions may never come again if not noted down, when does he state that he made a practice of noting his reactions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1394.269,
        "end": 1401.682
      },
      "pred_interval": {
        "start": 1412.2,
        "end": 1421.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.93100000000004,
        "end": 19.917999999999893,
        "average": 18.924499999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.18556701030927836,
        "text_similarity": 0.6400708556175232,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the same utterances and the 'after' relation, but the provided timestamps for E1 and E2 are substantially different from the ground-truth spans, so the alignment is incorrect/incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says one should not put the detail in an appeal, when does he suggest that subtle points may be hinted at when drafting grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1444.582,
        "end": 1452.109
      },
      "pred_interval": {
        "start": 1420.0,
        "end": 1424.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.582000000000107,
        "end": 28.108999999999924,
        "average": 26.345500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.22499999999999995,
        "text_similarity": 0.7495327591896057,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target follows it, but the provided time spans\u2014especially the target (1420\u20131424s vs. gold 1444.582\u20131452.109s)\u2014are significantly incorrect, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'What is the key question?', when does he compare a trial or appeal to writing a novel?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1536.909,
        "end": 1545.661
      },
      "pred_interval": {
        "start": 1450.0,
        "end": 1451.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.9090000000001,
        "end": 93.8610000000001,
        "average": 90.3850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25882352941176473,
        "text_similarity": 0.5018153190612793,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the speaker immediately gives the novel analogy as a direct response, but the timestamps and durations are substantially incorrect (off by ~89s and wrong lengths), so key factual details do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions keeping the core issue in mind, when does he advise that the first reading of an appeal should be relaxed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1594.74,
        "end": 1607.579
      },
      "pred_interval": {
        "start": 1533.0,
        "end": 1535.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.74000000000001,
        "end": 71.779,
        "average": 66.7595
      },
      "rationale_metrics": {
        "rouge_l": 0.15217391304347827,
        "text_similarity": 0.7792836427688599,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the advice content and that it follows the anchor, but the timestamps are markedly incorrect compared to the reference (predicted ~1531\u20131535.8s vs. correct ~1593.7\u20131607.579s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says that the first reading of an appeal should be relaxed, when does he compare it to watching a soap opera, reading a novel, or listening to music?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1608.151,
        "end": 1615.0
      },
      "pred_interval": {
        "start": 1610.6,
        "end": 1617.2
      },
      "iou": 0.48624157365455867,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4489999999998417,
        "end": 2.2000000000000455,
        "average": 2.3244999999999436
      },
      "rationale_metrics": {
        "rouge_l": 0.22988505747126434,
        "text_similarity": 0.612320601940155,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction accurately captures both quoted events and their temporal relation (after/once finished); timestamp differences are minor and do not change the semantics."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises being as neutral as possible, when does he explain that this enables objectivity and unbiased case knowledge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1638.375,
        "end": 1647.567
      },
      "pred_interval": {
        "start": 1623.2,
        "end": 1632.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.174999999999955,
        "end": 15.166999999999916,
        "average": 15.170999999999935
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.6091165542602539,
        "llm_judge_score": 6,
        "llm_judge_justification": "The predicted answer correctly captures the quoted phrases and the 'after' relation, but the event timestamps substantially differ from the reference, so the temporal alignment is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if he has a good case, when does he follow up by asking if he has a bad case or one in between?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1674.0,
        "end": 1681.0
      },
      "pred_interval": {
        "start": 1648.6,
        "end": 1652.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.40000000000009,
        "end": 28.200000000000045,
        "average": 26.800000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.38554216867469876,
        "text_similarity": 0.804580569267273,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic content (initial question and follow-up about a bad/in-between case) but the temporal annotations are substantially incorrect (off by ~20s) and the relation label differs, so it fails to align with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that three to four things would matter when preparing an appeal, what is the first thing he suggests?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1824.432,
        "end": 1828.85
      },
      "pred_interval": {
        "start": 1770.0,
        "end": 1780.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.432000000000016,
        "end": 48.84999999999991,
        "average": 51.64099999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.7503350973129272,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the semantic content of the anchor and target phrases, but the timestamps are substantially incorrect (~50s early) and it only states 'after' rather than the correct immediate follow relation, so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker emphasizes that stages in an appeal should be clear in one's mind, when does he start talking about the 'finest lawyers'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1890.082,
        "end": 1904.09
      },
      "pred_interval": {
        "start": 1840.0,
        "end": 1850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.08200000000011,
        "end": 54.08999999999992,
        "average": 52.08600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.741707980632782,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering and quoted phrases roughly right, but the timestamps are significantly incorrect (off by ~46 seconds) and do not match the provided anchor/target boundaries, so it largely fails to align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes his point about preparation, when does he announce moving to the next phase of the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1921.825,
        "end": 1924.428
      },
      "pred_interval": {
        "start": 1880.0,
        "end": 1890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.825000000000045,
        "end": 34.42800000000011,
        "average": 38.12650000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.8111683130264282,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction quotes plausible utterances but gives substantially incorrect timestamps (1880/1885s vs correct ~1918\u20131921s) and mislabels the relationship as 'after' instead of 'immediately follows,' so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a judge will eventually respond to an appeal, when does he mention being placed before a newly transferred judge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1984.778,
        "end": 1991.706
      },
      "pred_interval": {
        "start": 1974.0,
        "end": 1977.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.77800000000002,
        "end": 14.305999999999813,
        "average": 12.541999999999916
      },
      "rationale_metrics": {
        "rouge_l": 0.1415929203539823,
        "text_similarity": 0.6666241884231567,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the mention of a transferred judge but mislocates both event spans and gives the opposite temporal relation ('during' vs correct 'after'), contradicting the reference timings and relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes how he handles a client's ticklish case by listening to their story, when does he next explain that judges would analyze in a similar way and evolve their style?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2000.528,
        "end": 2006.954
      },
      "pred_interval": {
        "start": 2005.4,
        "end": 2009.2
      },
      "iou": 0.17919741697415306,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.872000000000071,
        "end": 2.2460000000000946,
        "average": 3.5590000000000828
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.7635659575462341,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures similar wording for the judge's line but missegments the timestamps (anchor incorrectly extends into the true target and target start/end are shifted later) and labels the relation as 'after' instead of the correct 'next', so key temporal and relation details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a diligent lawyer would form and correct their opinion about the bench every day, when does he next say that this practice saves the court's time?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2071.617,
        "end": 2074.641
      },
      "pred_interval": {
        "start": 2015.8,
        "end": 2018.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.817000000000235,
        "end": 56.44100000000003,
        "average": 56.12900000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.4415584415584415,
        "text_similarity": 0.696534276008606,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer captures the same utterances and relative ordering, but the timestamps are substantially incorrect (off by ~56 seconds) and the relation label differs slightly; these factual timing errors make it a poor match to the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what makes a good lawyer, when does he begin to answer by describing a man who has prepared his brief fully?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.444,
        "end": 2200.017
      },
      "pred_interval": {
        "start": 2162.5,
        "end": 2165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.94399999999996,
        "end": 35.016999999999825,
        "average": 32.48049999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.23809523809523808,
        "text_similarity": 0.7623031735420227,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and roughly the target wording, but the anchor and target timestamps are substantially wrong and the predicted end time omits the speaker's actual completion; overall the timing and completeness are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a fresh judge saying 'no reading of evidence', when does he express his opinion that it's illegal and unconstitutional?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2237.752,
        "end": 2243.727
      },
      "pred_interval": {
        "start": 2186.0,
        "end": 2189.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.75199999999995,
        "end": 54.72699999999986,
        "average": 53.23949999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.22784810126582278,
        "text_similarity": 0.7547767758369446,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the quoted content and that the target follows the anchor, but the provided timestamps are far off (by ~50+ seconds) and the target end time is incorrect, so the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to make the judge read Section 54 of the Evidence Act, when does he explain its content regarding the previous character of an accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2302.58,
        "end": 2310.354
      },
      "pred_interval": {
        "start": 2228.5,
        "end": 2234.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.07999999999993,
        "end": 75.85399999999981,
        "average": 74.96699999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.7222131490707397,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation (target follows the anchor) and the quoted content, but the reported timestamps are substantially incorrect (off by ~60\u201375s) and do not match the ground truth, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of the second question as a heinous crime, when does he describe specific examples like 'chopped off the hand'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2354.498,
        "end": 2357.601
      },
      "pred_interval": {
        "start": 2385.0,
        "end": 2391.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.501999999999953,
        "end": 33.39899999999989,
        "average": 31.95049999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.43243243243243246,
        "text_similarity": 0.7377621531486511,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted relation 'during' is correct, the predicted anchor and target timestamps are substantially off from the reference and the prediction adds hallucinated/incorrect phrasing; key timing and content details do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states 'this is the second kind of roadblock', when does he introduce the 'Third kind of roadblock'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2414.579,
        "end": 2418.664
      },
      "pred_interval": {
        "start": 2414.0,
        "end": 2415.0
      },
      "iou": 0.09026586620922006,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5790000000001783,
        "end": 3.6640000000002146,
        "average": 2.1215000000001965
      },
      "rationale_metrics": {
        "rouge_l": 0.36,
        "text_similarity": 0.7914970517158508,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the third roadblock immediately follows the second and gives a roughly similar start time, but the anchor/target timestamps are notably inaccurate and the target quote and exact durations are incomplete compared to the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes explaining 'these are the questions which are put to you', when does he transition to discussing myths about 'a dying declaration'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2462.739,
        "end": 2468.126
      },
      "pred_interval": {
        "start": 2480.0,
        "end": 2481.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.260999999999967,
        "end": 12.873999999999796,
        "average": 15.067499999999882
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.751805305480957,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order and topic transition correct but the anchor/target timestamps are significantly off (by ~12\u201317s) and the relation is labeled differently; these factual timing errors make it largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker extensively discusses the 1925 Lahore Bakshish Singh judgment, when does he bring up the Lakshmi versus Om Prakash case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2549.448,
        "end": 2555.294
      },
      "pred_interval": {
        "start": 2589.3,
        "end": 2610.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.85200000000032,
        "end": 55.20600000000013,
        "average": 47.529000000000224
      },
      "rationale_metrics": {
        "rouge_l": 0.19148936170212766,
        "text_similarity": 0.672135055065155,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the cases and the 'after' relation, but the timestamps are substantially incorrect (off by ~40\u201360 seconds) and durations do not match the reference, so it fails to align with the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying, 'These are the cases when you have opened the door of a hearing', when does he state, 'These are situations which we must know'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2605.712,
        "end": 2610.678
      },
      "pred_interval": {
        "start": 2705.4,
        "end": 2712.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.6880000000001,
        "end": 102.02199999999993,
        "average": 100.85500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.6067067980766296,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct phrasing and the immediate-after relationship, but the reported timestamps are substantially incorrect (E1 predicted 2705.4s vs correct 2605.172s; E2 predicted 2706.5s vs correct 2605.712s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises, 'But when you go to this, don't offend the judge', when does he describe making notes in 'three phases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2647.594,
        "end": 2653.382
      },
      "pred_interval": {
        "start": 2747.6,
        "end": 2758.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.00599999999986,
        "end": 104.81799999999976,
        "average": 102.41199999999981
      },
      "rationale_metrics": {
        "rouge_l": 0.3125,
        "text_similarity": 0.7347677946090698,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' and matches the quoted content, but the timestamp values and event boundaries are substantially different from the reference (off by ~101 seconds and different span endpoints), so the timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises a lawyer to stop if their best argument isn't selling, when does he suggest the lawyer might say, 'Your lordship may hear me on other issues'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2687.877,
        "end": 2690.499
      },
      "pred_interval": {
        "start": 2720.79,
        "end": 2730.85
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.91300000000001,
        "end": 40.35100000000011,
        "average": 36.63200000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.23157894736842105,
        "text_similarity": 0.8139867782592773,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly labels the relation as 'after', it gives substantially incorrect timestamps for both E1 and E2 and falsely asserts the example was given immediately, so the temporal details are largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how language helps with flexibility and playing cool in arguments, when does he introduce 'sense of humor' as an important quality?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2718.862,
        "end": 2725.929
      },
      "pred_interval": {
        "start": 2765.59,
        "end": 2775.72
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.728000000000065,
        "end": 49.79099999999971,
        "average": 48.25949999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.3092783505154639,
        "text_similarity": 0.8403202295303345,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the speaker immediately transitions to introducing 'sense of humor' as the important quality, but the provided timestamps are substantially different from the ground truth (start ~47s later and an incorrect end time), so it is factually inaccurate on key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker identifies 'trap cases' and 'DA cases' as the majority of Corruption Act cases, when does he introduce 'scam cases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2786.395,
        "end": 2789.04
      },
      "pred_interval": {
        "start": 2853.79,
        "end": 2859.89
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.39499999999998,
        "end": 70.84999999999991,
        "average": 69.12249999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3043478260869565,
        "text_similarity": 0.8927857279777527,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the sequence (scam cases introduced after trap/DA cases) but the timestamps are significantly incorrect and do not match the ground truth, so key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if a busy or young lawyer needs a register of important judgments, when does he directly ask if it is a necessary tool?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2898.935,
        "end": 2905.299
      },
      "pred_interval": {
        "start": 2957.6,
        "end": 2963.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.664999999999964,
        "end": 58.60100000000011,
        "average": 58.63300000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5922186374664307,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the semantic relation (the question follows the statement) but gives incorrect and inconsistent timestamps (anchor/target times differ greatly from the reference and the prediction even lists E1 and E2 as starting simultaneously while claiming an 'after' relation), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes emphasizing the need to know, read, and re-read foundational judgments, when does he state he will recount a few of them?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.268,
        "end": 2935.553
      },
      "pred_interval": {
        "start": 3021.2,
        "end": 3025.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.93199999999979,
        "end": 89.94700000000012,
        "average": 90.43949999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1818181818181818,
        "text_similarity": 0.5951056480407715,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the immediate follow-up relationship (the speaker begins recounting right after finishing), but the timestamps are substantially different and inconsistent with the reference, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing the question about a false defense or wrong answer given by the accused, when does he explicitly state his suggestion and request for foundational judgments?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3029.268,
        "end": 3033.995
      },
      "pred_interval": {
        "start": 3060.0,
        "end": 3060.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.73199999999997,
        "end": 26.00500000000011,
        "average": 28.36850000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.2476190476190476,
        "text_similarity": 0.5384051203727722,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction matches the 'after' relation, it gives completely incorrect timestamps and a zero-duration target, and wrongly asserts the suggestion immediately follows the anchor despite the reference noting intervening speech, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'Virsar Singh's judgment, 1958', when does he mention 'Vivian Bose's judgment, a classic'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3045.106,
        "end": 3052.317
      },
      "pred_interval": {
        "start": 3107.73,
        "end": 3116.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.623999999999796,
        "end": 64.08300000000008,
        "average": 63.35349999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.5724356770515442,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the anchor and target timestamps are substantially different from the ground truth (off by ~60\u201370s), and it adds extraneous detail, so it does not accurately match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker refers to the case 'Prabhu versus the Emperor, 1941', when does he ask if the initial onus on the prosecution transfers itself?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.51,
        "end": 3129.351
      },
      "pred_interval": {
        "start": 3154.51,
        "end": 3161.69
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 32.33899999999994,
        "average": 33.16949999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.656865656375885,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the relation ('after') and the quoted phrases, but the anchor and target timestamps are significantly off (about 35\u201340s later) compared to the ground truth, so the timing information is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying '3.', when does he explain that the court has a 'broader picture' with the accused's evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3158.541,
        "end": 3164.576
      },
      "pred_interval": {
        "start": 3204.09,
        "end": 3211.57
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.54899999999998,
        "end": 46.99400000000014,
        "average": 46.27150000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.7870980501174927,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly labels the relation as 'once_finished' and matches the described content, but the anchor and target timestamps are substantially off (~46 seconds later) from the ground truth, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 1954 Supreme Court case, when does he detail the specific allegation and offense?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3228.663,
        "end": 3239.981
      },
      "pred_interval": {
        "start": 3220.0,
        "end": 3225.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.663000000000011,
        "end": 14.981000000000222,
        "average": 11.822000000000116
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.8195441365242004,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relation ('after') is correct and the anchor is approximately located, the predicted timestamps are substantially inaccurate and the predicted target segment is much earlier and far shorter than the ground-truth E2, failing to cover the full description of the offense."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying that Tanu Bedi is present, when does he begin describing the case involving an Afghan Airlines pilot?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3267.18,
        "end": 3280.178
      },
      "pred_interval": {
        "start": 3234.0,
        "end": 3236.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.179999999999836,
        "end": 44.177999999999884,
        "average": 38.67899999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.6955828666687012,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mentions Tanu Bedi but gives substantially different timestamps and a different subsequent utterance/topic than the reference (wrong start/end times and content), so it is largely incorrect despite correctly identifying a follow-on relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if they can continue for 10 minutes, when does someone respond with 'Yes, of course, of course'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3407.405,
        "end": 3409.588
      },
      "pred_interval": {
        "start": 3368.0,
        "end": 3370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.4050000000002,
        "end": 39.58800000000019,
        "average": 39.4965000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.35000000000000003,
        "text_similarity": 0.754433810710907,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances but assigns incorrect timestamps for both (both at 3368.0s instead of ~3389\u20133409s) and wrongly labels the relation as 'simultaneously with' rather than the correct 'once_finished', so it fails on crucial temporal/relation details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how long they can continue, when does he ask if they can go on for 10 minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3405.21,
        "end": 3408.57
      },
      "pred_interval": {
        "start": 3465.0,
        "end": 3471.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.789999999999964,
        "end": 62.429999999999836,
        "average": 61.1099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6566458344459534,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the utterances and that the target occurs after the anchor, but it gives incorrect timestamps (and single timepoints instead of the reference time ranges), so the temporal details are factually wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions the judge saying 'no, you are convicted', when does he recall playing basketball?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3490.94,
        "end": 3501.65
      },
      "pred_interval": {
        "start": 3557.0,
        "end": 3563.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.05999999999995,
        "end": 61.34999999999991,
        "average": 63.70499999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168677,
        "text_similarity": 0.755784273147583,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the same anchor/target utterances and the 'after' relationship, but the reported timestamps are substantially off (by ~70\u201380 seconds) compared to the ground truth, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises that 'Your case must bother you', when does he begin telling another interesting case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3552.29,
        "end": 3555.63
      },
      "pred_interval": {
        "start": 3600.0,
        "end": 3600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.710000000000036,
        "end": 44.36999999999989,
        "average": 46.039999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.7980259656906128,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the target occurs after the anchor, but it gives entirely different timestamps and incorrect quoted utterances for both anchor and target, so it is largely factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the detailed injury on the forehead, when does he state that the benefit of doubt went to the accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3636.678,
        "end": 3640.261
      },
      "pred_interval": {
        "start": 3664.5,
        "end": 3669.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.822000000000116,
        "end": 28.739000000000033,
        "average": 28.280500000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7028180360794067,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the provided timestamps are substantially incorrect (predicted ~3662\u20133669s vs correct 3594.77\u20133640.26s), so the temporal details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's description of the trickster character from Maupassant's story and his famous acts, when does he specifically mention the trickster showing different tricks to people?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3732.928,
        "end": 3736.294
      },
      "pred_interval": {
        "start": 3708.5,
        "end": 3711.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.427999999999884,
        "end": 25.29399999999987,
        "average": 24.860999999999876
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.6450453400611877,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: event time spans and anchor content differ substantially, and the relation is labeled 'after' rather than the correct 'during'; only the general topic (trickster showing tricks) matches weakly."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing that there are 'two more cases', when does he name the location of the first of these new cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3665.138,
        "end": 3668.806
      },
      "pred_interval": {
        "start": 3712.5,
        "end": 3715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.36200000000008,
        "end": 46.19399999999996,
        "average": 46.77800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.16842105263157897,
        "text_similarity": 0.6535537838935852,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events (intro and naming 'Kurukshetra') and their order, but the timestamps are significantly off (~47s later) and the relation 'after' is less precise than the ground-truth 'once_finished', so it is factually inaccurate on timing and relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once the main speaker finishes stating that 'So that persuaded the court', when does he begin talking about other interesting cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.541,
        "end": 3838.467
      },
      "pred_interval": {
        "start": 3799.8,
        "end": 3801.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.740999999999985,
        "end": 37.06700000000001,
        "average": 35.403999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.20930232558139533,
        "text_similarity": 0.710667610168457,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives substantially incorrect timestamps (off by ~8\u201332s) and misstates the temporal relation/ordering compared to the ground truth, so it is largely temporally inaccurate despite matching some wording."
      }
    },
    {
      "question_id": "002",
      "question": "After the host says 'Thank you, sir', when does he describe how the session shifted to English?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3905.664,
        "end": 3912.762
      },
      "pred_interval": {
        "start": 3879.5,
        "end": 3885.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.164000000000215,
        "end": 27.662000000000262,
        "average": 26.91300000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.23376623376623376,
        "text_similarity": 0.7408401370048523,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relational label ('after') and roughly identifies the anchor utterance, but both event timestamps are substantially incorrect (anchor off by ~6s and target off by ~25s) and the target timing/content do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the host announces that there are questions received on the chat, when does he ask the first question about supplementary grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3949.503,
        "end": 3955.411
      },
      "pred_interval": {
        "start": 3896.2,
        "end": 3901.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.30300000000034,
        "end": 53.610999999999876,
        "average": 53.45700000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.16528925619834708,
        "text_similarity": 0.6372220516204834,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction recognizes a question follows the announcement but is largely incorrect: the event timestamps differ substantially (~100s apart), the relation is labeled less precisely ('after' vs the correct 'next'), and the predicted anchor/target intervals do not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they can combine the two questions, when does he state that all questions of fact can be raised?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3972.976,
        "end": 3975.039
      },
      "pred_interval": {
        "start": 3972.0,
        "end": 3976.0
      },
      "iou": 0.5157500000000255,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9760000000001128,
        "end": 0.9609999999997854,
        "average": 0.9684999999999491
      },
      "rationale_metrics": {
        "rouge_l": 0.41558441558441556,
        "text_similarity": 0.7715129852294922,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their 'after' relationship; timings are approximate (E1 given as a single timestamp and E2 start/end off by about 1s) but preserve the original meaning without contradiction."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that supplementary grounds are not needed in a normal case, when does he explain that all questions of fact and law are open in a normal case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4030.938,
        "end": 4036.242
      },
      "pred_interval": {
        "start": 3999.0,
        "end": 4006.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.938000000000102,
        "end": 30.24200000000019,
        "average": 31.090000000000146
      },
      "rationale_metrics": {
        "rouge_l": 0.21978021978021978,
        "text_similarity": 0.6473220586776733,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship (the target follows the anchor) and the target phrasing, but the timestamp intervals are substantially incorrect (predicted ~3998\u20134006s vs correct ~4030\u20134036s), so the answer fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining how a long note was required in the Sajan Kumar case, when does he describe the judge quoting his written note 80 times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4130.358,
        "end": 4139.64
      },
      "pred_interval": {
        "start": 4128.0,
        "end": 4137.0
      },
      "iou": 0.5706185567009999,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3580000000001746,
        "end": 2.6400000000003274,
        "average": 2.499000000000251
      },
      "rationale_metrics": {
        "rouge_l": 0.23404255319148937,
        "text_similarity": 0.7355983257293701,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the judge quote and that it occurs after the case explanation, but the provided timestamps are inconsistent with the reference (anchor/target times are shifted and the predicted target starts before the reference anchor ends), so the timing alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking about making an opening statement to impress the court, when does the main speaker begin explaining that a prepared lawyer can present their case in two minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4186.419,
        "end": 4209.102
      },
      "pred_interval": {
        "start": 4110.0,
        "end": 4120.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 76.41899999999987,
        "end": 89.10199999999986,
        "average": 82.76049999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.6545751094818115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but the timestamps are substantially incorrect (about 100s earlier) and the target segment duration/ boundaries do not match the reference, so it fails to capture the correct span and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "During the main speaker's narration of the Churchill story, when does he say that Churchill responded he needed '15 days before' to prepare for a two-minute speech?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4275.956,
        "end": 4281.618
      },
      "pred_interval": {
        "start": 4196.0,
        "end": 4198.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.95600000000013,
        "end": 83.6180000000004,
        "average": 81.78700000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.1818181818181818,
        "text_similarity": 0.7338070869445801,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the ground truth (off by ~85s) and its E2 (4196\u20134198s) is not actually within the predicted E1 (4180\u20134196s), so the claimed 'during' relation is incorrect despite mentioning the same line."
      }
    },
    {
      "question_id": "003",
      "question": "After the main speaker finishes explaining that the less one has to speak, the more deeply one has to prepare, when does he begin defining a summary as a test of a lawyer's ability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4306.932,
        "end": 4319.831
      },
      "pred_interval": {
        "start": 4244.0,
        "end": 4250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.93199999999979,
        "end": 69.83100000000013,
        "average": 66.38149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826084,
        "text_similarity": 0.6421545743942261,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') and the sequence right, but the reported start/end times for both E1 and E2 are substantially incorrect compared to the ground truth, so it fails to match the key temporal annotations."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks about the most challenging situations, when does the guest start explaining the amount of preparation needed for short vs long speeches?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4289.354,
        "end": 4303.141
      },
      "pred_interval": {
        "start": 4290.0,
        "end": 4320.0
      },
      "iou": 0.4287998433726991,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6459999999997308,
        "end": 16.85900000000038,
        "average": 8.752500000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.3255813953488372,
        "text_similarity": 0.8759629726409912,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction identifies the correct events and the 'after' relationship, but the timestamps are shifted later than the ground truth (E1 and E2 start times differ by several seconds) and the predicted E2 end time is substantially inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes rephrasing the question about the impact of oral advocacy versus drafting, when does the guest immediately state that oral advocacy is more impactful?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4347.622,
        "end": 4350.188
      },
      "pred_interval": {
        "start": 4331.0,
        "end": 4334.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.6220000000003,
        "end": 16.188000000000102,
        "average": 16.4050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2790697674418605,
        "text_similarity": 0.8525832295417786,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event types and their immediate relationship, but the timestamps are substantially incorrect (off by ~15\u201316 seconds) and the anchor/target boundaries differ from the ground truth, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the guest confirms that the framing of questions of law is 'all important', when does he state that 'manifest injustice' is the category for most cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4407.975,
        "end": 4412.261
      },
      "pred_interval": {
        "start": 4366.0,
        "end": 4378.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.975000000000364,
        "end": 34.26100000000042,
        "average": 38.11800000000039
      },
      "rationale_metrics": {
        "rouge_l": 0.3366336633663366,
        "text_similarity": 0.823946475982666,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but the event timestamps and quoted utterances conflict significantly with the ground truth, indicating incorrect timing and some transcript inaccuracies."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interviewer finishes asking how the speaker's different fields and readings affect their argument style, when does the speaker respond by saying 'Sir, you're absolutely right'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4537.667,
        "end": 4541.776
      },
      "pred_interval": {
        "start": 4526.82,
        "end": 4528.82
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.847000000000662,
        "end": 12.956000000000131,
        "average": 11.901500000000397
      },
      "rationale_metrics": {
        "rouge_l": 0.17948717948717952,
        "text_similarity": 0.8175133466720581,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the response phrase but gives timestamps that differ substantially (\u224810\u201313s earlier) from the reference and mischaracterizes the temporal relation as 'after' rather than the immediate/contiguous timing indicated by the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that 'Advocacy is a communicative skill', when does he elaborate that 'One part of a communicative skill is language'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4562.343,
        "end": 4567.786
      },
      "pred_interval": {
        "start": 4573.64,
        "end": 4576.24
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.29700000000048,
        "end": 8.453999999999724,
        "average": 9.875500000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.8314618468284607,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two utterances but gives timestamps shifted by ~7s and treats E2 as occurring after E1; this contradicts the ground truth which shows E2 overlapping and directly elaborating on E1, so the timing and relation are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the interviewer mentions watching Justice Muralidhar's YouTube, when does the interviewer give an example from him about being asked 'did you murder?'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4617.635,
        "end": 4624.683
      },
      "pred_interval": {
        "start": 4651.98,
        "end": 4658.38
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.344999999999345,
        "end": 33.697000000000116,
        "average": 34.02099999999973
      },
      "rationale_metrics": {
        "rouge_l": 0.44999999999999996,
        "text_similarity": 0.7504154443740845,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps are substantially different from the ground truth (off by ~11\u201334 seconds) and do not match the correct intervals; while it preserves the 'after' order, the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines the appeal as an 'extension of the trial', when does he explain that notes can 'awaken you to the subtleties of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4673.51,
        "end": 4680.899
      },
      "pred_interval": {
        "start": 4733.4,
        "end": 4742.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.88999999999942,
        "end": 61.20100000000002,
        "average": 60.54549999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.6873753070831299,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target occurs after the anchor, but the provided timestamps are substantially incorrect for both E1 and E2 compared with the reference (anchor off by ~2\u201310s and target off by ~53\u201360s), so it fails to match the correct temporal intervals."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising how to objectively approach an appeal, when does he state that 'You have to absorb all the witnesses of the surrounding environment'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4746.348,
        "end": 4752.661
      },
      "pred_interval": {
        "start": 4783.2,
        "end": 4793.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.85199999999986,
        "end": 40.439000000000306,
        "average": 38.645500000000084
      },
      "rationale_metrics": {
        "rouge_l": 0.24000000000000005,
        "text_similarity": 0.7564257979393005,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the target phrase but the reported timestamps are substantially different from the reference and the relation is mislabeled (should be immediately following/continuing, not simply 'after'), so key factual timing and relation details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'then I become a lawyer', when does he describe the 'story of the lawyer of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4805.697,
        "end": 4824.178
      },
      "pred_interval": {
        "start": 4826.1,
        "end": 4860.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.403000000000247,
        "end": 35.822000000000116,
        "average": 28.112500000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.2028985507246377,
        "text_similarity": 0.7094268798828125,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') correct but the timestamps for both anchor and target are substantially incorrect and the target span/duration does not match the reference, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the 'art of communication' as the 'foundation of law', when does he state that its effectiveness depends on the 'quality of your preparation'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4848.488,
        "end": 4862.369
      },
      "pred_interval": {
        "start": 4919.3,
        "end": 4925.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.8119999999999,
        "end": 63.03099999999995,
        "average": 66.92149999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.3125,
        "text_similarity": 0.8659273386001587,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') and the quoted phrase, but it misaligns both temporal spans substantially (anchor ~8s early and target ~71s late versus the reference), so it fails to match the correct segments and timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the initial speaker says 'Thank you, sir', when does the second speaker begin apologizing for not taking all questions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4960.598,
        "end": 4970.611
      },
      "pred_interval": {
        "start": 4981.3,
        "end": 4984.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.702000000000226,
        "end": 13.78899999999976,
        "average": 17.245499999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.1951219512195122,
        "text_similarity": 0.7949994802474976,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (thank-you and apology) but gives markedly incorrect timestamps and duration, and incorrectly asserts they are immediate/concurrent rather than occurring ~1.1s later as in the reference, so the factual timing and relation are largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the host refers to 'Q and Q, quality and quantity insurance', when does he mention the three important judgments: Prabhu, Rishikesh, and Vijay?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4996.141,
        "end": 5009.076
      },
      "pred_interval": {
        "start": 4998.6,
        "end": 5002.4
      },
      "iou": 0.293776575183545,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.459000000000742,
        "end": 6.676000000000386,
        "average": 4.567500000000564
      },
      "rationale_metrics": {
        "rouge_l": 0.38,
        "text_similarity": 0.8055694699287415,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the timestamps deviate substantially from the ground truth (anchor ~4s late; target start ~3.9s late and target end ~6.7s early) and it adds an interpretive remark; timing errors make it only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes quoting 'abhi na jao chhod ke, dil abhi bhara nahi', when does he state 'That's what the common sense is'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5019.2,
        "end": 5022.6
      },
      "pred_interval": {
        "start": 5028.4,
        "end": 5029.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.199999999999818,
        "end": 7.199999999999818,
        "average": 8.199999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363637,
        "text_similarity": 0.7062841653823853,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same phrases and a sequential relation, but the timestamps are substantially off (~11s later) and the temporal relation differs from the reference (ground truth indicates immediate/overlapping timing 'once_finished' while prediction labels it 'after'), so it is largely temporally incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker expresses surprise about people increasing viewership on YouTube, when does he mention everybody was viewed in the Zoom chat?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5030.3,
        "end": 5032.8
      },
      "pred_interval": {
        "start": 5035.0,
        "end": 5036.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.699999999999818,
        "end": 3.399999999999636,
        "average": 4.049999999999727
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777778,
        "text_similarity": 0.6996885538101196,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the Zoom-chat phrase but mislabels and mis-times the events (timestamps differ and the anchor/target are conflated), and therefore gives the wrong temporal relation ('during' vs correct 'after')."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Thank you, sir' for the first time, when does he specifically thank Tanu Bedi?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5046.2,
        "end": 5049.1
      },
      "pred_interval": {
        "start": 5043.6,
        "end": 5045.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5999999999994543,
        "end": 4.100000000000364,
        "average": 3.349999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.28235294117647064,
        "text_similarity": 0.7152899503707886,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the provided timestamps deviate significantly from the reference (both E1 and E2 times are earlier and misaligned), so the answer is factually inaccurate on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Alex finishes asking about the key challenges facing individuals who are called to give evidence, when does Paul begin to talk about nervousness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.237,
        "end": 36.762
      },
      "pred_interval": {
        "start": 31.8,
        "end": 35.2
      },
      "iou": 0.3956066102378076,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4370000000000012,
        "end": 1.5619999999999976,
        "average": 1.4994999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.5298484563827515,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the immediate transition and that Paul begins by mentioning 'nervousness,' but the provided timestamps are off by about 1.2\u20131.4 seconds compared to the reference, so it is factually inaccurate on timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once Alex finishes asking what makes the actual process of giving evidence so difficult, when does Paul begin to describe the cross-examination process?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.92,
        "end": 92.191
      },
      "pred_interval": {
        "start": 120.4,
        "end": 123.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.480000000000004,
        "end": 31.009,
        "average": 33.2445
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131145,
        "text_similarity": 0.4410976469516754,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are significantly incorrect (predicted ~120\u2013121s vs. ground truth 83.718s and 84.92s), so it fails on factual timing; while it correctly notes Paul discusses cross-examination, the key temporal details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes describing the anecdote about the expert witness losing credibility, when does he start describing the other example of a fact witness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 172.683,
        "end": 180.236
      },
      "pred_interval": {
        "start": 198.9,
        "end": 201.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.217000000000013,
        "end": 21.464,
        "average": 23.840500000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424243,
        "text_similarity": 0.643045961856842,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content/phrase of the transition but the timestamps are significantly incorrect (198.9\u2013199.0s vs. actual ~171.9\u2013172.7s), so it fails on the key factual timing element."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker describes the sarcastic comments made about expert witness answers, when does he recount the witness's emotional reaction?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.3,
        "end": 164.2
      },
      "pred_interval": {
        "start": 247.0,
        "end": 256.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.69999999999999,
        "end": 91.80000000000001,
        "average": 90.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.8213678598403931,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relation as 'after', it gives completely different timestamps (offset by ~97s) and adds a quoted line not present in the reference, so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker asks what witness familiarisation actually means, when does the first speaker begin to define the term?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 197.7,
        "end": 204.0
      },
      "pred_interval": {
        "start": 291.0,
        "end": 292.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.30000000000001,
        "end": 88.0,
        "average": 90.65
      },
      "rationale_metrics": {
        "rouge_l": 0.3058823529411765,
        "text_similarity": 0.4999786615371704,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'once_finished' relation and the ordering (question then definition), but the timestamps are substantially off (~94s difference) and speaker attributions/phrasing do not match the reference, so key factual details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker finishes describing the benefits of witness familiarisation for individuals, when does he next outline the benefits for instructing solicitors?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 300.4,
        "end": 308.0
      },
      "pred_interval": {
        "start": 319.0,
        "end": 328.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.600000000000023,
        "end": 20.0,
        "average": 19.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.8417491912841797,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target phrase and that the solicitor-focused remarks follow the individual-focused remarks, but it gives substantially incorrect timestamps (off by ~19s) and a different relation label ('after' vs 'next'), so it fails on key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker explains why they don't jump straight into cross-examination, when does he start talking about the need for theory?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 356.6,
        "end": 365.0
      },
      "pred_interval": {
        "start": 367.2,
        "end": 370.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.599999999999966,
        "end": 5.600000000000023,
        "average": 8.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.1875,
        "text_similarity": 0.7309783697128296,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but the anchor and target timestamps are substantially off (~10\u201311s later than the ground truth), and it introduces a quoted phrase/timing that contradicts the correct timeline."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the first speaker is discussing the differences in virtual hearings, when does he mention judges requiring witnesses to demonstrate they are alone?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 394.0,
        "end": 398.0
      },
      "pred_interval": {
        "start": 408.5,
        "end": 415.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.5,
        "end": 17.80000000000001,
        "average": 16.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.22448979591836737,
        "text_similarity": 0.767975926399231,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the quoted statement and the 'during' relation, but the provided timestamps conflict with the ground truth (especially E2), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the second speaker finishes thanking Paul, when does he mention sharing some of his insights?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 435.4,
        "end": 437.5
      },
      "pred_interval": {
        "start": 452.8,
        "end": 456.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.400000000000034,
        "end": 18.69999999999999,
        "average": 18.05000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.19277108433734938,
        "text_similarity": 0.6713771820068359,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the mention of sharing insights follows the thank-you, but the timestamps are substantially incorrect and the relation label/details conflict with the reference (different times and phrasing), so it fails to match the ground truth."
      }
    }
  ]
}