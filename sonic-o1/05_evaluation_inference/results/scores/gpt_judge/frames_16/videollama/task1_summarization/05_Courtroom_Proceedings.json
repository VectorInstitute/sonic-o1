{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 13,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.12747511382347188,
      "rouge_l_std": 0.026955224143461312,
      "text_similarity_mean": 0.325778535925425,
      "text_similarity_std": 0.11898305165641122,
      "llm_judge_score_mean": 1.1538461538461537,
      "llm_judge_score_std": 0.7692307692307693
    },
    "short": {
      "rouge_l_mean": 0.09445248272660185,
      "rouge_l_std": 0.036661950294252545,
      "text_similarity_mean": 0.23928267451433036,
      "text_similarity_std": 0.15031716119059507,
      "llm_judge_score_mean": 0.46153846153846156,
      "llm_judge_score_std": 0.6343239424027171
    },
    "cider": {
      "cider_detailed": 4.065497176835587e-05,
      "cider_short": 0.0007073588670965569
    }
  },
  "per_entry_results": [
    {
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.14835164835164835,
        "text_similarity": 0.41302457451820374,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only generic courtroom visuals and the final Odyssey link but omits nearly all key factual elements from the reference (no attorney/defendant names, sentencing context, dropped breach charge, Frank's protest/First Amendment claims, disability claim, or the detailed promotional censorship narrative)."
      },
      "short": {
        "rouge_l": 0.13559322033898308,
        "text_similarity": 0.32209765911102295,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only notes basic courtroom visuals and the Odyssey link, which partially matches the video's promotion, but it omits almost all key narrative points (charge dismissal/sentencing, Frank's protests and First Amendment claims, explicit quotes, and the censorship discussion), so it is largely incomplete."
      }
    },
    {
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.13194444444444445,
        "text_similarity": 0.19749072194099426,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer only gives superficial visual details of a courtroom scene and omits all substantive content (criminal history, victim impact statements, defendant's admission/no remorse, and the judge's comments), so it fails to capture the correct summary."
      },
      "short": {
        "rouge_l": 0.09150326797385622,
        "text_similarity": 0.05371856689453125,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives unrelated visual details and omits all key factual elements of the correct summary (prosecutor's account, victim impact statements, defendant's admissions/lack of remorse, and the judge's response), so it is completely incorrect."
      }
    },
    {
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.12813370473537605,
        "text_similarity": 0.40408122539520264,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is largely incorrect and misleading: it misidentifies key actors (portrays Ismael Ozanne as the defendant), invents unrelated cases, omits the Chandler Halderson verdict and supporting evidence, and includes hallucinatory visual details."
      },
      "short": {
        "rouge_l": 0.08376963350785341,
        "text_similarity": 0.47036638855934143,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely incorrect and misleading: it misidentifies the parties (wrongly stating Ismael Ozanne as the defendant and guilty), introduces unrelated case names, and omits all key facts from the correct summary about Chandler Halderson and the trial details."
      }
    },
    {
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.17619047619047623,
        "text_similarity": 0.47844207286834717,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer only describes superficial visual shots and a single matching on-screen line, but it omits the central facts that a 74-year-old played an AI-generated avatar in court, the judge halted and rebuked him, his admission/promotional intent, and the video's broader discussion of AI's legal impact; it also includes likely hallucinated text."
      },
      "short": {
        "rouge_l": 0.17708333333333334,
        "text_similarity": 0.41973209381103516,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction describes unrelated visual snippets and dialogue and omits the core facts (an elderly man used an AI-generated lawyer avatar, the judge stopped it, he admitted generating the AI and was promoting a startup, and ensuing ethical/legal discussion). It is largely irrelevant and misses key elements of the correct summary."
      }
    },
    {
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.09836065573770493,
        "text_similarity": 0.24119862914085388,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction accurately describes the visual scene of a man testifying and his emotional demeanor, but it fails to mention the key factual content from the correct answer\u2014identifying Lyle Menendez and his claims of sexual abuse\u2014making it substantially incomplete."
      },
      "short": {
        "rouge_l": 0.08163265306122448,
        "text_similarity": 0.15096502006053925,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only describes the speaker's appearance and gestures without mentioning Lyle Menendez, his testimony about alleged sexual abuse, or the Menendez Brothers case context, thus omitting key factual elements."
      }
    },
    {
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.07669616519174041,
        "text_similarity": 0.3324517607688904,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary: it omits all legal names, arguments, case law, and courtroom context, and instead fabricates a video-call about recycling and energy, making it completely incorrect."
      },
      "short": {
        "rouge_l": 0.055248618784530384,
        "text_similarity": 0.2599963843822479,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated and factually incorrect\u2014it describes a discussion about recycling/energy and a video call setting, while the correct answer details a legal oral argument in Hothi v. Musk with specific legal issues, attorneys, and case citations."
      }
    },
    {
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.12574850299401197,
        "text_similarity": 0.44531774520874023,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only notes superficial visual details and the general topic (judge confirmation) but omits all key substantive elements from the correct answer\u2014no mention of Senator Cruz, Judge Ketanji Brown Jackson, the standing/self-identification hypotheticals, or her judicial-response\u2014so it is largely incomplete."
      },
      "short": {
        "rouge_l": 0.06015037593984962,
        "text_similarity": 0.1543646603822708,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer only describes visual shots of people and omits all substantive content from the correct summary (Cruz's questioning, hypotheticals about self-identification and race, reference to Harvard, and Judge Jackson's response and judicial reasoning)."
      }
    },
    {
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.17297297297297298,
        "text_similarity": 0.3498886227607727,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer only offers vague, generic visual descriptions and omits all key factual elements (Pettis' admission of coercion, naming Lankford, the accusation, Lankford's outburst and contempt, and the adjournment), while even introducing irrelevant details (a church scene), so it fails to capture the correct summary."
      },
      "short": {
        "rouge_l": 0.08588957055214724,
        "text_similarity": 0.2523748278617859,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only gives generic visual descriptions (people in suits, courtroom) and omits all key facts from the correct answer (cross-examination, admission of threats, identification of Lee Lankford, contempt and recess), even adding an irrelevant church scene."
      }
    },
    {
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.09933774834437087,
        "text_similarity": 0.12188345193862915,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer only describes superficial visual details (men in suits on phones) and omits all substantive content of the correct summary about Mr. Uday Holla's detailed legal advice, recommendations on litigation, research, professional management, career guidance, and fitness; it therefore fails to match the reference."
      },
      "short": {
        "rouge_l": 0.10526315789473684,
        "text_similarity": 0.11092455685138702,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction only describes superficial visuals (men in suits on phones) and fails to capture any of the substantive legal advice, procedural points, or recommendations presented in the correct summary, omitting all key content."
      }
    },
    {
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.13846153846153847,
        "text_similarity": 0.5319613218307495,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only very high-level, related elements (car, gun, cocaine, location/people) but omits nearly all key factual specifics from the reference (date, O'Hara's saloon, exact witness identities, undercover officer, the altercation and injury, license plate/address, and forensic match) and adds hallucinated details (document list, lawyer meeting, wrong witness name), so it is largely incorrect and incomplete."
      },
      "short": {
        "rouge_l": 0.15053763440860216,
        "text_similarity": 0.5434014201164246,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely fails to capture the video's key facts (undercover officer, bottle exchange, flight and battery, Dr. Reyes, license plate, 606 Glendale, forensic cocaine match) and includes hallucinated or incorrect details (a gun, phone list, incorrect witness name), so it is almost entirely mismatched."
      }
    },
    {
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.11578947368421051,
        "text_similarity": 0.23664820194244385,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely fails to match the reference: it characterizes the video as static court documents with no actions, omitting all key events (the break-in, the suspect rummaging, the deputy being punched, the arrest, items found, and the courtroom ID), and thus is largely incorrect."
      },
      "short": {
        "rouge_l": 0.05454545454545455,
        "text_similarity": 0.11981545388698578,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction fails to capture any of the key facts (vandalism, theft, Merchant's identification, arrest, weapons found, assault on an officer) and instead describes unrelated/incorrect visual content, so it is completely incorrect."
      }
    },
    {
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.11940298507462686,
        "text_similarity": 0.20831601321697235,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only describes the video's superficial visual setting and speaker demeanor, completely omitting the substantive legal discussion, case examples, and advice on appellate advocacy present in the correct answer."
      },
      "short": {
        "rouge_l": 0.08144796380090498,
        "text_similarity": 0.15945738554000854,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction only describes on-screen people and setting and omits all substantive content of the correct summary (Cheema's discussion on criminal appeals, categories, advocacy advice, and case examples), so it is completely incorrect."
      }
    },
    {
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.12578616352201258,
        "text_similarity": 0.27441662549972534,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely unrelated and incorrect: it omits the core topic of witness preparation, cross-examination challenges, and Paul Gilbert/Bond Solon training methodology, instead hallucinating content about mentorship, networking, and entrepreneurship with only vague visual descriptions."
      },
      "short": {
        "rouge_l": 0.06521739130434782,
        "text_similarity": 0.09346035122871399,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated to the correct answer: it discusses mentorship, networking, and visual details, while the correct answer concerns witness preparation for trials, courtroom challenges, and mock cross-examinations; there is no substantive overlap."
      }
    }
  ]
}