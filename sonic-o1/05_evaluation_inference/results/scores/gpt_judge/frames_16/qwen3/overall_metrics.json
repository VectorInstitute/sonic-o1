{
  "model": "qwen3",
  "experiment_name": "frames_16",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.26012092142256793,
            "rouge_l_std": 0.05972616415370176,
            "text_similarity_mean": 0.8052126578986645,
            "text_similarity_std": 0.03287401067827589,
            "llm_judge_score_mean": 7.25,
            "llm_judge_score_std": 1.75
          },
          "short": {
            "rouge_l_mean": 0.23333491135631584,
            "rouge_l_std": 0.07207656683642555,
            "text_similarity_mean": 0.7432150319218636,
            "text_similarity_std": 0.058104980236364734,
            "llm_judge_score_mean": 6.4375,
            "llm_judge_score_std": 1.539835624344365
          },
          "cider": {
            "cider_detailed": 0.10197373534940947,
            "cider_short": 0.024057924026209217
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.26033512001822423,
            "rouge_l_std": 0.04152443816063703,
            "text_similarity_mean": 0.7621389882905143,
            "text_similarity_std": 0.07639983284837806,
            "llm_judge_score_mean": 7.380952380952381,
            "llm_judge_score_std": 1.1328930717495542
          },
          "short": {
            "rouge_l_mean": 0.2400083215263738,
            "rouge_l_std": 0.07942652752615668,
            "text_similarity_mean": 0.670218594017483,
            "text_similarity_std": 0.15917190661883082,
            "llm_judge_score_mean": 7.0476190476190474,
            "llm_judge_score_std": 1.8381199110113127
          },
          "cider": {
            "cider_detailed": 0.043065006403804604,
            "cider_short": 0.045820241921565995
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.23680995103675662,
            "rouge_l_std": 0.07050075741526705,
            "text_similarity_mean": 0.712341118317384,
            "text_similarity_std": 0.13177019152849412,
            "llm_judge_score_mean": 6.0,
            "llm_judge_score_std": 2.511511956509924
          },
          "short": {
            "rouge_l_mean": 0.2144855222030989,
            "rouge_l_std": 0.09055728193569666,
            "text_similarity_mean": 0.6546258788842422,
            "text_similarity_std": 0.17818078788490033,
            "llm_judge_score_mean": 5.384615384615385,
            "llm_judge_score_std": 2.2373907034229212
          },
          "cider": {
            "cider_detailed": 0.06263322826436533,
            "cider_short": 0.08030984219196156
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.25242199749251626,
          "text_similarity_mean": 0.7598975881688542,
          "llm_judge_score_mean": 6.8769841269841265
        },
        "short": {
          "rouge_l_mean": 0.22927625169526286,
          "text_similarity_mean": 0.6893531682745295,
          "llm_judge_score_mean": 6.289911477411477
        },
        "cider": {
          "cider_detailed_mean": 0.06922399000585981,
          "cider_short_mean": 0.05006266937991225
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9313725490196079,
          "correct": 95,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.3988301999101026,
            "rouge_l_std": 0.12138330307852169,
            "text_similarity_mean": 0.7861178751085319,
            "text_similarity_std": 0.10065860416163255,
            "llm_judge_score_mean": 9.333333333333334,
            "llm_judge_score_std": 1.9011520415178058
          },
          "rationale_cider": 0.4793742809560234
        },
        "02_Job_Interviews": {
          "accuracy": 0.99,
          "correct": 99,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.3918770517609439,
            "rouge_l_std": 0.11276973033050784,
            "text_similarity_mean": 0.7814429545402527,
            "text_similarity_std": 0.09361134044900765,
            "llm_judge_score_mean": 9.67,
            "llm_judge_score_std": 0.9905049217444605
          },
          "rationale_cider": 0.2741088056009482
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9826086956521739,
          "correct": 113,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.376962931115286,
            "rouge_l_std": 0.11451479426620685,
            "text_similarity_mean": 0.7954207907876243,
            "text_similarity_std": 0.11157019725800014,
            "llm_judge_score_mean": 9.217391304347826,
            "llm_judge_score_std": 1.65125832754061
          },
          "rationale_cider": 0.30514792499784815
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9679937482239273,
        "rationale": {
          "rouge_l_mean": 0.38922339426211083,
          "text_similarity_mean": 0.7876605401454696,
          "llm_judge_score_mean": 9.406908212560387
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.02649090902036288,
          "std_iou": 0.08133461525131154,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.022304832713754646,
            "count": 6,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.0037174721189591076,
            "count": 1,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 269
          },
          "mae": {
            "start_mean": 135.79507063197025,
            "end_mean": 3614.196118959108,
            "average_mean": 1874.9955947955389
          },
          "rationale": {
            "rouge_l_mean": 0.2908631718451561,
            "rouge_l_std": 0.08069861703437636,
            "text_similarity_mean": 0.6940862268874193,
            "text_similarity_std": 0.1007222815031427,
            "llm_judge_score_mean": 2.802973977695167,
            "llm_judge_score_std": 1.6968784163627149
          },
          "rationale_cider": 0.0926815360787819
        },
        "02_Job_Interviews": {
          "mean_iou": 0.047463734079392285,
          "std_iou": 0.1467099842088084,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.06692913385826772,
            "count": 17,
            "total": 254
          },
          "R@0.5": {
            "recall": 0.03543307086614173,
            "count": 9,
            "total": 254
          },
          "R@0.7": {
            "recall": 0.01968503937007874,
            "count": 5,
            "total": 254
          },
          "mae": {
            "start_mean": 102.52025196850396,
            "end_mean": 104.21924803149606,
            "average_mean": 103.36975
          },
          "rationale": {
            "rouge_l_mean": 0.29090969504371167,
            "rouge_l_std": 0.08451385062322907,
            "text_similarity_mean": 0.694349271223301,
            "text_similarity_std": 0.10556350155051282,
            "llm_judge_score_mean": 2.748031496062992,
            "llm_judge_score_std": 1.5896738829477284
          },
          "rationale_cider": 0.08175561435673381
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.0443701244039928,
          "std_iou": 0.13913732120910993,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.07331378299120235,
            "count": 25,
            "total": 341
          },
          "R@0.5": {
            "recall": 0.03225806451612903,
            "count": 11,
            "total": 341
          },
          "R@0.7": {
            "recall": 0.011730205278592375,
            "count": 4,
            "total": 341
          },
          "mae": {
            "start_mean": 130.27404985337242,
            "end_mean": 132.51512316715545,
            "average_mean": 131.39458651026393
          },
          "rationale": {
            "rouge_l_mean": 0.28497935553613185,
            "rouge_l_std": 0.07637001273703324,
            "text_similarity_mean": 0.7213619516567051,
            "text_similarity_std": 0.10019032783975754,
            "llm_judge_score_mean": 2.780058651026393,
            "llm_judge_score_std": 1.6572106717723922
          },
          "rationale_cider": 0.050084857691007824
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.03944158916791599,
        "mae_average": 703.2533104352675,
        "R@0.3": 0.05418258318774157,
        "R@0.5": 0.023802869167076624,
        "R@0.7": 0.010471748216223703,
        "rationale": {
          "rouge_l_mean": 0.2889174074749999,
          "text_similarity_mean": 0.7032658165891418,
          "llm_judge_score_mean": 2.777021374928184
        }
      }
    }
  }
}