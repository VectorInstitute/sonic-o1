{
  "topic_id": 3,
  "topic_name": "Parent-Teacher Conferences",
  "num_evaluated": 526,
  "aggregated_metrics": {
    "mean_iou": 0.009391988803267996,
    "std_iou": 0.05966607037735632,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.009505703422053232,
      "count": 5,
      "total": 526
    },
    "R@0.5": {
      "recall": 0.009505703422053232,
      "count": 5,
      "total": 526
    },
    "R@0.7": {
      "recall": 0.0,
      "count": 0,
      "total": 526
    },
    "mae": {
      "start_mean": 1422.3177863908475,
      "end_mean": 1423.7050874388665,
      "average_mean": 1423.011436914857
    },
    "rationale": {
      "rouge_l_mean": 0.23102684343083774,
      "rouge_l_std": 0.08522560306235304,
      "text_similarity_mean": 0.5421381343807671,
      "text_similarity_std": 0.18205652539658343,
      "llm_judge_score_mean": 2.214828897338403,
      "llm_judge_score_std": 1.9277463420577496
    },
    "rationale_cider": 0.2077791409046163
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "Once the speaker states he used to be a teacher, when does he explain why he would be called upon to interpret?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 25.699,
        "end": 29.902
      },
      "pred_interval": {
        "start": 11.128838396229272,
        "end": 13.297430470939787
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.57016160377073,
        "end": 16.604569529060214,
        "average": 15.587365566415471
      },
      "rationale_metrics": {
        "rouge_l": 0.169811320754717,
        "text_similarity": 0.4135790467262268,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states he explains after saying he used to be a teacher (relation once_finished), but it gives an incorrect timestamp (11.13s vs ~23.5\u201329.9s) and adds unsupported/detail-hallucinated specifics about school/Spanish interpreters not present in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker announces the opening poll, when does he start explaining how to format the name for the certificate?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 71.748,
        "end": 78.603
      },
      "pred_interval": {
        "start": 24.361592190788603,
        "end": 26.530184265499123
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.386407809211406,
        "end": 52.07281573450087,
        "average": 49.72961177185614
      },
      "rationale_metrics": {
        "rouge_l": 0.15189873417721517,
        "text_similarity": 0.39087414741516113,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (24.36s) contradicts the correct start time (~71.75s) and thus the chronology; it also adds hallucinatory details about using initials that are not in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Next, after the speaker states that those who don't need a certificate can ignore the poll, when does he start explaining what to do if the pop-up doesn't appear?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 83.778,
        "end": 88.445
      },
      "pred_interval": {
        "start": 26.95945945945946,
        "end": 29.63125340599455
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.818540540540546,
        "end": 58.813746594005444,
        "average": 57.816143567272995
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.49692028760910034,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives a wrong start time (26.96s vs ~83.78s), misorders the sequence relative to the cited instruction, and adds details not supported by the reference, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker mentions not leaving the webinar by 'X-ing out', when does he instruct to use the 'red button' to close out?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 172.8,
        "end": 174.7
      },
      "pred_interval": {
        "start": 156.17063431512742,
        "end": 159.65432098765433
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.629365684872596,
        "end": 15.045679012345659,
        "average": 15.837522348609127
      },
      "rationale_metrics": {
        "rouge_l": 0.14141414141414144,
        "text_similarity": 0.5906311273574829,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation in text, but provides incorrect and inconsistent timestamps (target given at 156\u2013159s vs. correct 172.8\u2013174.7s) and omits anchor timing, so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker says he will leave up the poll, when does he mention sharing links in the chat?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 193.9,
        "end": 195.4
      },
      "pred_interval": {
        "start": 182.73809523809524,
        "end": 185.47619047619048
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.161904761904765,
        "end": 9.923809523809524,
        "average": 10.542857142857144
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.637602686882019,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but it gives incorrect/missing timestamps: it places the target at ~182.7\u2013185.5s (overlapping the anchor) instead of the correct 193.9\u2013195.4s, and it does not provide the numeric anchor timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker discusses the second link, a webinar called 'Translation for Teachers', when does he introduce the third link for the Refugee Services of Texas charity?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 231.5,
        "end": 236.0
      },
      "pred_interval": {
        "start": 251.3095238095238,
        "end": 255.23809523809524
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.809523809523796,
        "end": 19.23809523809524,
        "average": 19.52380952380952
      },
      "rationale_metrics": {
        "rouge_l": 0.20408163265306123,
        "text_similarity": 0.684977650642395,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and their 'after' relationship, but it omits the anchor timestamps and gives incorrect target timestamps (predicted 251.31\u2013255.24s vs correct 231.5\u2013236.0s), so key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once Graciela finishes asking the participants to unmute and then mute themselves, when does she say \"Perfect\"?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 380.807,
        "end": 381.3
      },
      "pred_interval": {
        "start": 50.83333333333333,
        "end": 52.166666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 329.9736666666667,
        "end": 329.1333333333333,
        "average": 329.5535
      },
      "rationale_metrics": {
        "rouge_l": 0.34920634920634924,
        "text_similarity": 0.7813209295272827,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the ordering ('after') right but the timestamps are far off from the ground truth (predicted ~51\u201352s vs ground truth ~374\u2013381s), so it is essentially incorrect. "
      }
    },
    {
      "question_id": "002",
      "question": "Once Graciela finishes explaining how questions will be handled, when does she mention the glossary of terms?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 425.281,
        "end": 429.266
      },
      "pred_interval": {
        "start": 44.833333333333336,
        "end": 46.166666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 380.4476666666667,
        "end": 383.09933333333333,
        "average": 381.7735
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.5795120000839233,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes that the glossary is mentioned after Graciela's explanation, but the timestamps are completely incorrect/misaligned, the temporal relation ('after' vs immediate transition) is misstated, and event boundaries are incomplete, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the modes of interpretation used in educational settings, when does the slide transition to 'Educational Settings'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 560.8,
        "end": 560.9
      },
      "pred_interval": {
        "start": 53.1,
        "end": 57.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 507.69999999999993,
        "end": 503.0,
        "average": 505.34999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.4358974358974359,
        "text_similarity": 0.8095563054084778,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the slide transition occurs after the speaker segment, but the timestamps are substantially incorrect and key timing details (exact spans and target time) do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker defines an educational setting, when does she list the types of institutions that can be included?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 575.0,
        "end": 585.8
      },
      "pred_interval": {
        "start": 66.9,
        "end": 71.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 508.1,
        "end": 514.5,
        "average": 511.3
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7153903245925903,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the listing occurs after the definition, but the timestamps are substantially different from the reference and it fails to note that the listing 'immediately follows' the definition; thus only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining where interpreters are mostly called, when does the slide change to an image with a diploma and US flag?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.0,
        "end": 628.1
      },
      "pred_interval": {
        "start": 72.2,
        "end": 77.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 555.8,
        "end": 551.1,
        "average": 553.45
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.7094221115112305,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction refers to completely different events and timestamps (\u224872\u201377s about US education) than the ground truth (612\u2013628s about interpreter call settings and the diploma/flag slide), so it fails to match the correct anchor, target, or timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating the mandatory schooling age in the United States, when do they state the mandatory schooling age in Mexico?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.416,
        "end": 751.6
      },
      "pred_interval": {
        "start": 19.6,
        "end": 25.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 729.816,
        "end": 726.5,
        "average": 728.158
      },
      "rationale_metrics": {
        "rouge_l": 0.21951219512195122,
        "text_similarity": 0.785202145576477,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the reference on all key facts: it swaps US/Mexico events, gives completely different timestamps, and asserts an incorrect temporal relation, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the grades for middle school or junior high in the United States, when do they describe the equivalent 'secundaria' grades in Mexico?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 804.2,
        "end": 809.5
      },
      "pred_interval": {
        "start": 60.9,
        "end": 67.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 743.3000000000001,
        "end": 742.3,
        "average": 742.8
      },
      "rationale_metrics": {
        "rouge_l": 0.20930232558139533,
        "text_similarity": 0.8558975458145142,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely incorrect: its timestamps (60.9s\u201365.7s) do not match the reference (799.0\u2013803.6s and 804.2\u2013809.5s), and it incorrectly asserts both events start simultaneously rather than the target occurring after the anchor finishes."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the number of public schools in the U.S. in 2021, when does she state the average number of students per public school?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 873.167,
        "end": 877.377
      },
      "pred_interval": {
        "start": 44.1,
        "end": 49.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 829.067,
        "end": 828.377,
        "average": 828.722
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.6229065656661987,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps (44.1s and 49.0s) do not match the correct times (~873.167s\u2013877.377s); while it preserves the sequence, the provided timing is entirely incorrect, so the answer is essentially wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the total allocation for K-12 schools, when does she mention the average amount per student?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.147,
        "end": 925.432
      },
      "pred_interval": {
        "start": 52.4,
        "end": 55.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 867.7470000000001,
        "end": 869.6320000000001,
        "average": 868.6895000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.4833826422691345,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (52.4s and 55.8s) do not match the ground-truth intervals (~911.8\u2013920.1s for the total and 920.1\u2013925.4s for the per-student amount), so the answer is incorrect despite preserving the temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes mentioning 'disciplinary meetings', when does she mention 'classroom instruction'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1191.5,
        "end": 1192.5
      },
      "pred_interval": {
        "start": 1055.0,
        "end": 1100.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 136.5,
        "end": 92.5,
        "average": 114.5
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869565,
        "text_similarity": 0.35599660873413086,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is unrelated and incorrect: it does not provide the requested timing or mention 'classroom instruction' and instead hallucinates a different participant ('District Representative')."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes listing the participants of an ARD meeting, when does she begin discussing the purpose of the meeting?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1249.5,
        "end": 1252.8
      },
      "pred_interval": {
        "start": 1.8,
        "end": 6.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1247.7,
        "end": 1246.8,
        "average": 1247.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.4602539837360382,
        "llm_judge_score": 0,
        "llm_judge_justification": "Completely incorrect: the predicted timestamps (1.8s and 10.6s) do not match the reference (listing ends at 1248.0s; purpose begins at 1249.5s), so it fails to identify the correct start of the new topic."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks about the reasons for an ARD meeting, when does she state the first reason?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1294.4,
        "end": 1296.1
      },
      "pred_interval": {
        "start": 10.8,
        "end": 15.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1283.6000000000001,
        "end": 1280.6,
        "average": 1282.1
      },
      "rationale_metrics": {
        "rouge_l": 0.19607843137254904,
        "text_similarity": 0.38357219099998474,
        "llm_judge_score": 6,
        "llm_judge_justification": "The predicted timing (10.8s) aligns with the correct answer if absolute\u2192relative conversion is applied, but it introduces an unsupported detail about 'listing the participants' and omits the precise end time and full context, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks about the different purposes of the ARD meeting, when does she start listing the initial purposes?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1445.358,
        "end": 1450.41
      },
      "pred_interval": {
        "start": 154.5,
        "end": 161.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1290.858,
        "end": 1289.41,
        "average": 1290.134
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142858,
        "text_similarity": 0.48330458998680115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the listing happens after the question, but the provided timestamps are far from the ground truth (predicted 154.5\u2013161.0s vs actual ~1445.36\u20131450.41s), so the timing information is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide fully displays the 'Review Assessment' section, when does the speaker specifically mention the 'full individual evaluation'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1428.092,
        "end": 1432.125
      },
      "pred_interval": {
        "start": 145.3,
        "end": 150.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1282.7920000000001,
        "end": 1281.925,
        "average": 1282.3585
      },
      "rationale_metrics": {
        "rouge_l": 0.2142857142857143,
        "text_similarity": 0.609578013420105,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the provided timestamps are drastically wrong (145.3\u2013150.2s vs correct 1428.092\u20131432.125s), so it is largely factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing parental input, when does the text for 'Review PLAAFP' appear on the slide?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1531.58,
        "end": 1532.59
      },
      "pred_interval": {
        "start": 157.6,
        "end": 161.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1373.98,
        "end": 1370.8899999999999,
        "average": 1372.435
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.6116737127304077,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the ordering (appears after parental input) and the start time aligns with a relative conversion, but the end time (161.7s) deviates notably from the correct fully-visible time (1532.59s / ~158.6s relative) and thus lacks the required temporal precision."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions being 'completely clueless' about the ARD meetings, when does she state that the terminology was 'so scary'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1602.75,
        "end": 1607.38
      },
      "pred_interval": {
        "start": 112.1768034139511,
        "end": 113.45115134864267
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1490.5731965860489,
        "end": 1493.9288486513574,
        "average": 1492.2510226187032
      },
      "rationale_metrics": {
        "rouge_l": 0.0923076923076923,
        "text_similarity": 0.1624700278043747,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single, incorrect timestamp (~112.18s) that does not match the correct target interval (0:12.750\u20130:17.380) and fails to indicate the relative ordering with the anchor; thus it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states 'I am an interpreter', when does she recount the other interpreter responding 'I have to interpret'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1656.82,
        "end": 1658.744
      },
      "pred_interval": {
        "start": 126.97935050668752,
        "end": 132.26161702361804
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1529.8406494933124,
        "end": 1526.4823829763818,
        "average": 1528.161516234847
      },
      "rationale_metrics": {
        "rouge_l": 0.08163265306122448,
        "text_similarity": 0.266728013753891,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single, wildly incorrect timestamp (~127s vs ~1655\u20131658s) and omits the precise start/end times and immediate-follow relation; it only correctly states the response occurs after the anchor."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker reviews the IEP goals and objectives, when does she begin to review accommodations?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.266,
        "end": 1760.028
      },
      "pred_interval": {
        "start": 156.35318667808676,
        "end": 161.8934459697834
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1601.9128133219133,
        "end": 1598.1345540302166,
        "average": 1600.023683676065
      },
      "rationale_metrics": {
        "rouge_l": 0.2641509433962264,
        "text_similarity": 0.38456207513809204,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (\u2248156.35s) drastically contradicts the correct start time (~1758.27s) and introduces an unsupported detail ('state testing') not in the reference, so it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes discussing the previous assessment, when does she mention the proposal of the STAAR assessment?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1783.7,
        "end": 1791.4
      },
      "pred_interval": {
        "start": 38.3,
        "end": 59.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1745.4,
        "end": 1732.3000000000002,
        "average": 1738.8500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.16216216216216217,
        "text_similarity": 0.41486382484436035,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer mismatches the reference timestamps and durations (38\u201360s vs 1780\u20131791s) and gives the wrong relation ('after' rather than immediate/once_finished), so it fails to align with the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker concludes the topic of district-wide assessments and accommodations, when does she introduce the least restrictive environment?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1810.5,
        "end": 1815.2
      },
      "pred_interval": {
        "start": 60.8,
        "end": 66.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1749.7,
        "end": 1748.3,
        "average": 1749.0
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428564,
        "text_similarity": 0.612356424331665,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer has entirely different timestamps and a different utterance for the target (saying 'Where is the student?' vs 'And then we talk about the least restrictive environment'), and its relation ('after') is imprecise compared to the correct immediate/once_finished relation, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing the potential harmful effects of an instruction setting, when does she question if the benefits outweigh the harm?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1854.7,
        "end": 1857.3
      },
      "pred_interval": {
        "start": 72.7,
        "end": 78.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1782.0,
        "end": 1779.2,
        "average": 1780.6
      },
      "rationale_metrics": {
        "rouge_l": 0.24719101123595508,
        "text_similarity": 0.4832903742790222,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic sequence (question follows discussion) but gives incorrect and inconsistent timestamps (wrong absolute times, omits E1 end) and only loosely matches the 'once_finished' relation, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says she will send glossaries to Marco, when does she introduce disciplinary action meetings?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1969.3,
        "end": 1975.0
      },
      "pred_interval": {
        "start": 34.7,
        "end": 39.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1934.6,
        "end": 1935.5,
        "average": 1935.05
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.6488184928894043,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect timestamps (34.7s vs ~1958\u20131975s), fails to preserve the correct temporal order and omits end times; it therefore does not match the reference facts."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining when disciplinary actions happen, when does the slide transition to 'Potential disciplinary outcomes'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2006.4,
        "end": 2007.1
      },
      "pred_interval": {
        "start": 24.0,
        "end": 27.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1982.4,
        "end": 1980.1,
        "average": 1981.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307692,
        "text_similarity": 0.5774480104446411,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction recognizes a transition to 'Potential disciplinary outcomes' but gives an incorrect timestamp and implies the events are simultaneous; it also omits the slide settling time (2007.1s) and does not match the correct relation/timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what skills are needed to interpret in an educational setting, when does she define the interpreter's role?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2161.0,
        "end": 2168.0
      },
      "pred_interval": {
        "start": 12.583333333333334,
        "end": 24.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2148.4166666666665,
        "end": 2143.75,
        "average": 2146.083333333333
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714288,
        "text_similarity": 0.5819600224494934,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (after) and provides a plausible relative start, but the end time is incorrect and it omits the question (E1) timing, so it is temporally inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker stops sharing the slides, when does the video switch to the gallery view of the participants?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2465.33,
        "end": 2475.337
      },
      "pred_interval": {
        "start": 2450.0,
        "end": 2464.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.329999999999927,
        "end": 11.336999999999989,
        "average": 13.333499999999958
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753088,
        "text_similarity": 0.6307458877563477,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely contradicts the reference: it misstates the stop-share time (2450.0s vs 2463.525s), attributes a different utterance, and gives inconsistent transition timing, though its gallery onset (2464.0s) is roughly near the true 2465.33s."
      }
    },
    {
      "question_id": "002",
      "question": "After the 'Simultaneous Interpreting' slide is displayed, when does the speaker mention ARD meetings as a use case?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2409.449,
        "end": 2418.605
      },
      "pred_interval": {
        "start": 2398.0,
        "end": 2410.0
      },
      "iou": 0.026741082261583615,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.44900000000007,
        "end": 8.605000000000018,
        "average": 10.027000000000044
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.630104660987854,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction is partially correct in placing the ARD mention around 2410s, but it also gives a conflicting incorrect timestamp (2398.0s), misstates the ordering relative to the slide, and omits the precise start/end times from the reference, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if everyone is still awake, when does she begin to explain the practice method?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2490.175,
        "end": 2497.855
      },
      "pred_interval": {
        "start": 2498.0,
        "end": 2508.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.824999999999818,
        "end": 10.144999999999982,
        "average": 8.9849999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.5093069672584534,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right (explanation after the question) but the timestamps are substantially off (question ~33s later and explanation ~17s after the true start/completion) and it introduces an unsupported detail ('reading the script'), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that everyone at home will be interpreting, when does she ask if everyone is good with the plan?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2513.9,
        "end": 2515.1
      },
      "pred_interval": {
        "start": 11.8,
        "end": 12.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2502.1,
        "end": 2502.7,
        "average": 2502.3999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2926829268292683,
        "text_similarity": 0.5122763514518738,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction lists two similar utterances but provides incorrect timestamps and mislabels the first event (saying the speaker 'asks' about interpreting rather than explaining the plan) and omits that the question directly follows the explanation, so it does not match the reference timing or semantic sequence."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks to discuss a classroom concern about Peter, when does she describe Peter as a sweet boy who enjoys stacking blocks?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2558.467,
        "end": 2564.21
      },
      "pred_interval": {
        "start": 30.5,
        "end": 37.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2527.967,
        "end": 2526.31,
        "average": 2527.1385
      },
      "rationale_metrics": {
        "rouge_l": 0.125,
        "text_similarity": 0.4034480154514313,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (30.5s) does not match the correct interval (~2558.467\u20132564.210s) and omits the context that this description occurs after the initial inquiry, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Peter may benefit from special education services, when does she discuss the social worker's assessment and concerns at home?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2595.389,
        "end": 2610.228
      },
      "pred_interval": {
        "start": 38.6,
        "end": 42.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2556.789,
        "end": 2567.328,
        "average": 2562.0585
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272727,
        "text_similarity": 0.7159261703491211,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies both events and their order (social worker discussion follows the special-education mention), but the provided timestamps do not match the ground-truth intervals (and the prediction omits the precise start/end spans), so the timing is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "During the main speaker's instructions for showing thumbs up, sideways, or down, when do multiple participants start showing their reactions?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2731.0,
        "end": 2736.0
      },
      "pred_interval": {
        "start": 2872.0,
        "end": 2876.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 141.0,
        "end": 140.0,
        "average": 140.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7927689552307129,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps differ substantially from the ground truth and the temporal relation is incorrect: the ground truth indicates reactions occur during the speaker's instructions (2731\u20132736 within 2728.833\u20132741.307), whereas the prediction places both events later and states the reactions occur after the instructions."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker asks Marco if he has anything to say, when does Marco start talking about the link he shared?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2760.32,
        "end": 2763.065
      },
      "pred_interval": {
        "start": 2904.0,
        "end": 2944.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 143.67999999999984,
        "end": 180.93499999999995,
        "average": 162.3074999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.367816091954023,
        "text_similarity": 0.7686101198196411,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship, but the absolute timestamps for both events are substantially incorrect (off by ~150s) and the target event duration does not match the ground truth, so it fails on factual temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Okay' to transition, when does she start talking about questions related to a child's behavior for a rating scale?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2838.512,
        "end": 2846.226
      },
      "pred_interval": {
        "start": 2984.0,
        "end": 3036.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 145.48799999999983,
        "end": 189.7739999999999,
        "average": 167.63099999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.30927835051546393,
        "text_similarity": 0.7607349753379822,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the relational ordering ('after'), the anchor and target timestamps are significantly incorrect (off by ~150s) and the predicted durations/end times do not match the reference, making it factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes instructing to write 'DK' if the answer is unknown, when does she start reading the first child-related question?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2867.07,
        "end": 2872.84
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.02747619047619039,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.070000000000164,
        "end": 187.15999999999985,
        "average": 102.11500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.10416666666666667,
        "text_similarity": 0.13778747618198395,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely matches that the instruction precedes the first question but contradicts the precise timing: the provided timestamps (2850.0s start and 3060.0s end) do not match the reference (anchor ends 2859.87s; target 2867.07\u20132872.84s) and it adds an unsupported visual cue, so key factual details are incorrect or hallucinated."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'My child often argues with adults', when does she start reading the next child-related question?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2888.82,
        "end": 2892.66
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.018285714285712813,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.820000000000164,
        "end": 167.34000000000015,
        "average": 103.08000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.2197802197802198,
        "text_similarity": 0.18754610419273376,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer contradicts the correct timestamps (anchors at 2882.78s and next question at 2888.82s) by giving substantially later times (\u22482890s and 2920s) and adds unsupported visual details; it therefore fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the video screen changes to a black view displaying names, when does the speaker read the question about the child blurring out answers?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.0,
        "end": 2934.78
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.022761904761905715,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.0,
        "end": 125.2199999999998,
        "average": 102.6099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.15730337078651685,
        "text_similarity": 0.3469145596027374,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the question occurs after the transition to the black screen, but the timestamp is far off (~2980s vs 2930\u20132934.78s) and it introduces an unsupported visual cue (orange 'V'), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes a child having difficulty waiting for their turn, when does she describe a child being constantly on the go?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3036.8,
        "end": 3044.7
      },
      "pred_interval": {
        "start": 45.59259033203125,
        "end": 47.45925845191592
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2991.207409667969,
        "end": 2997.240741548084,
        "average": 2994.2240756080264
      },
      "rationale_metrics": {
        "rouge_l": 0.11904761904761904,
        "text_similarity": 0.2602924704551697,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps (\u224845\u201347s vs reference 3030\u20133044s), mislocating the events and failing to reflect that the target immediately follows the anchor; thus it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing a child being often forgetful in daily activities, when does she ask the audience how they did?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3099.8,
        "end": 3101.4
      },
      "pred_interval": {
        "start": 54.64864761439994,
        "end": 56.51531573428462
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3045.1513523856,
        "end": 3044.8846842657154,
        "average": 3045.018018325658
      },
      "rationale_metrics": {
        "rouge_l": 0.10714285714285714,
        "text_similarity": 0.2673237919807434,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives two incorrect timestamps that do not correspond to the reference event (wrong values and scale) and fails to reflect the correct event ordering/relationship; it is therefore largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks about the remaining time, when does Marco start responding?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3134.1,
        "end": 3139.2
      },
      "pred_interval": {
        "start": 61.64864761439994,
        "end": 63.31531573428462
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3072.4513523856,
        "end": 3075.8846842657154,
        "average": 3074.1680183256576
      },
      "rationale_metrics": {
        "rouge_l": 0.07017543859649122,
        "text_similarity": 0.39646273851394653,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction provides two unrelated relative times that do not match the reference start time (3134.1s) or its relative equivalent and omits the single direct-following response\u2014thus it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once Frank finishes asking if the 504 plan is inside the subject of special education, when does the woman in green confirm that it is?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3235.829,
        "end": 3239.914
      },
      "pred_interval": {
        "start": 44.5,
        "end": 46.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3191.329,
        "end": 3193.614,
        "average": 3192.4715
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.6069451570510864,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures the general ordering (confirmation occurs after the question) but mislabels the events, gives incorrect timestamps and a zero-length target, and uses the vague relation 'after' instead of the precise 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "Once Frank finishes stating that he thinks 504 is federal language, when does another woman ask for confirmation?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3268.61,
        "end": 3269.733
      },
      "pred_interval": {
        "start": 64.1,
        "end": 65.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3204.51,
        "end": 3204.1330000000003,
        "average": 3204.3215
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.617933988571167,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the order and gives relative start times, but it misreports E1 as a start time rather than the required end time, sets E2 end equal to its start (zero duration) instead of the correct end, and uses a looser 'after' relation rather than the precise 'once_finished'."
      }
    },
    {
      "question_id": "003",
      "question": "Once Jesse Thompson finishes asking how interpreters can stand up for themselves, when does the woman in green explain what interpreters have control over?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3361.672,
        "end": 3367.782
      },
      "pred_interval": {
        "start": 220.4,
        "end": 230.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3141.272,
        "end": 3137.482,
        "average": 3139.377
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.6511644124984741,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation as 'after', but the timestamps are wildly incorrect (uses starts instead of the anchor's finish and gives an impossible zero-duration target), failing to match the reference times and durations."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying she doesn't like being 'used' as an interpreter, when does she begin explaining what she needs for a successful encounter?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3404.5,
        "end": 3411.6
      },
      "pred_interval": {
        "start": 38.8,
        "end": 42.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3365.7,
        "end": 3368.7,
        "average": 3367.2
      },
      "rationale_metrics": {
        "rouge_l": 0.07920792079207921,
        "text_similarity": 0.0837721973657608,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that she shifts to practical advice (short sentences, pre-session rules) immediately after the complaint, but it gives an incorrect/irrelevant timestamp and misquotes the anchor phrasing, so it is only partially aligned with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that rules can be set ahead of time for pre-sessions, when does she explain what raising a hand means?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3433.1,
        "end": 3436.5
      },
      "pred_interval": {
        "start": 50.2,
        "end": 52.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3382.9,
        "end": 3383.7,
        "average": 3383.3
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352942,
        "text_similarity": 0.16175809502601624,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the target event happens after the pre-session rules, but it gives an incorrect timestamp (50.2s vs the correct 3433.1\u20133436.5s) and introduces an unverified/incorrect detail about the meaning of raising a hand, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female speaker replies 'Yes' to the question about doing work on Zoom, when does she explain how consecutive interpreting works on Zoom?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.0,
        "end": 3493.0
      },
      "pred_interval": {
        "start": 55.8,
        "end": 56.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3415.2,
        "end": 3436.3,
        "average": 3425.75
      },
      "rationale_metrics": {
        "rouge_l": 0.1149425287356322,
        "text_similarity": 0.3239001929759979,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction names the topic but gives a completely incorrect timestamp (55.8s vs ~3471s) and adds unfounded details about ease for schools/parents, failing to match the correct event timing and content."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"And then you as the interpreter will go into that room as well,\" when does she explain what the attendees can hear?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3584.889,
        "end": 3595.545
      },
      "pred_interval": {
        "start": 13.75,
        "end": 16.125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3571.139,
        "end": 3579.42,
        "average": 3575.2795
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.6152682304382324,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the explanation comes after the instruction, but it misaligns timestamps (claims both start at 13.75s and overlaps E1/E2) and alters the described content (says attendees hear the main speaker in English), so it fails to match the reference timing and details."
      }
    },
    {
      "question_id": "002",
      "question": "After Martha Rosenbaum mentions that schools receive a lot of funding from the government, when does she ask how to change the use of teachers as translators?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3629.646,
        "end": 3632.125
      },
      "pred_interval": {
        "start": 60.375,
        "end": 62.125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3569.271,
        "end": 3570.0,
        "average": 3569.6355000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962028,
        "text_similarity": 0.5344045162200928,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect: timestamps are off by orders of magnitude, the anchor event is misidentified (intro vs funding mention), and it even gives the same start time for both events despite claiming an 'after' relation; only the relation label matches."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker first mentions \"Executive Order 13166,\" when does she expand on its details, including its signing by President Clinton?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3669.853,
        "end": 3689.291
      },
      "pred_interval": {
        "start": 75.25,
        "end": 79.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3594.603,
        "end": 3610.041,
        "average": 3602.322
      },
      "rationale_metrics": {
        "rouge_l": 0.25287356321839083,
        "text_similarity": 0.7219157218933105,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the elaboration occurs after the mention, but it mislabels which segment is the initial mention versus the elaboration and gives completely different timestamps; thus it fails to match the reference's key temporal mappings and event alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that organizations receiving federal funds must provide meaningful language access, when does she suggest starting those conversations?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3762.0,
        "end": 3764.7
      },
      "pred_interval": {
        "start": 4.866665988498263,
        "end": 5.466666748210644
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3757.1333340115016,
        "end": 3759.233333251789,
        "average": 3758.1833336316454
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.22133299708366394,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (\u22484.87s and \u22485.47s) are completely inconsistent with the reference (3762.0s start/end) and it fails to state that the target immediately follows the anchor, so it is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the male speaker jokes about Google Translate replacing human interpreters, when does Maria E. Mendoza respond with 'Exactly!'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3789.6,
        "end": 3790.5
      },
      "pred_interval": {
        "start": 25.66666674821064,
        "end": 26.266666748210643
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3763.9333332517895,
        "end": 3764.233333251789,
        "average": 3764.0833332517896
      },
      "rationale_metrics": {
        "rouge_l": 0.07692307692307691,
        "text_similarity": 0.2056363821029663,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives different single timestamps that do not match the reference absolute time spans (3786.4\u20133789.4 and 3789.6\u20133790.5) and the conversion/offsets are inconsistent; it only correctly conveys that the response follows the joke but is largely incorrect on precise timing and span details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Susanna finishes asking if the Zoom environment for interpreting school meetings is common in other states, when does Maria E. Mendoza begin to respond?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3933.2,
        "end": 3934.2
      },
      "pred_interval": {
        "start": 46.66666674821064,
        "end": 47.266666748210646
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3886.5333332517894,
        "end": 3886.933333251789,
        "average": 3886.733333251789
      },
      "rationale_metrics": {
        "rouge_l": 0.1111111111111111,
        "text_similarity": 0.22097249329090118,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates Maria responds immediately after Susanna, but it gives incorrect absolute timestamps, mischaracterizes the question topic, and omits the quoted start phrase, so it fails to match key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Maria E. Mendosa finishes saying people are getting more comfortable with in-person meetings, when does the next speaker begin to add her point?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3956.6,
        "end": 3957.8
      },
      "pred_interval": {
        "start": 3930.0,
        "end": 4050.0
      },
      "iou": 0.010000000000002274,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.59999999999991,
        "end": 92.19999999999982,
        "average": 59.399999999999864
      },
      "rationale_metrics": {
        "rouge_l": 0.18000000000000002,
        "text_similarity": 0.7067394256591797,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: it gives wrong timestamps (3930.0s vs ~3956s) and claims the events are simultaneous, whereas the reference shows the target starts ~0.9s after the anchor finishes; it therefore contradicts the ground truth and includes hallucinated details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Maria E. Mendosa interrupts to say 'This is gonna help you feel better', when does she start talking about her conversation with a school district client?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4002.0,
        "end": 4007.6
      },
      "pred_interval": {
        "start": 4050.0,
        "end": 4130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.0,
        "end": 122.40000000000009,
        "average": 85.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.24299065420560748,
        "text_similarity": 0.6883295178413391,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives wrong absolute timestamps and durations, misidentifies the anchor/target timing, and contradicts the correct start (4002.0s) and end (4007.6s) times, though it broadly notes the target follows the anchor."
      }
    },
    {
      "question_id": "003",
      "question": "After the host asks to hear from Alejandra Mendez, when does Alejandra Mendez start speaking?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4096.7,
        "end": 4098.5
      },
      "pred_interval": {
        "start": 4130.0,
        "end": 4140.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.30000000000018,
        "end": 41.5,
        "average": 37.40000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.26190476190476186,
        "text_similarity": 0.8145984411239624,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misstates the event timings and boundaries (reference: anchor finishes 4092.0s, target 4096.7\u20134098.5s), giving incorrect starts/ends (4130.0s/4140.0s) and even saying both events start simultaneously; while it notes an 'after' relation, the numeric and boundary details contradict the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that the meetings are emotionally exhausting, when does Maria ask if the volume of meetings for Spanish-speaking families has increased?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4154.13,
        "end": 4166.78
      },
      "pred_interval": {
        "start": 38.18888888888889,
        "end": 40.47222222222222
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4115.941111111111,
        "end": 4126.307777777777,
        "average": 4121.124444444444
      },
      "rationale_metrics": {
        "rouge_l": 0.15873015873015875,
        "text_similarity": 0.3700614869594574,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies the events (anchors the wrong moment and swaps the emotional-exhaustion comment with the target question), provides no timestamps, and fails to match the correct temporal relation\u2014thus it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that there is more parent participation because it is easier, when does she mention a teacher requesting an in-person interpreter for initial ARD meetings?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4232.2,
        "end": 4238.0
      },
      "pred_interval": {
        "start": 25.055555555555554,
        "end": 27.833333333333332
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4207.144444444444,
        "end": 4210.166666666667,
        "average": 4208.655555555555
      },
      "rationale_metrics": {
        "rouge_l": 0.14925373134328357,
        "text_similarity": 0.27962857484817505,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target content and their sequence, but it omits the precise timestamps and the judge's notes about relative timing and minor start-time adjustment, which are key details in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the host says, 'let's do one more question from Jesse,' when does Jesse begin asking about teachers interpreting?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4340.45,
        "end": 4347.8
      },
      "pred_interval": {
        "start": 4309.0,
        "end": 4338.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.449999999999818,
        "end": 9.800000000000182,
        "average": 20.625
      },
      "rationale_metrics": {
        "rouge_l": 0.2682926829268293,
        "text_similarity": 0.5311115980148315,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies that Jesse's question occurs after the host and gives an approximately correct E2 timing and end, but it misreports E1's start substantially (off by ~21s) and slightly shifts the E2 start (~2.5s), so it is mostly correct but imprecise on timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "Once Jesse finishes asking about legal liability for teachers interpreting, when does Maria state there isn't a government push for certification?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4367.0,
        "end": 4370.11
      },
      "pred_interval": {
        "start": 4348.0,
        "end": 4421.0
      },
      "iou": 0.04260273972602291,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 50.89000000000033,
        "average": 34.945000000000164
      },
      "rationale_metrics": {
        "rouge_l": 0.24444444444444444,
        "text_similarity": 0.3909614086151123,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly matches E1's start time but misplaces E2 by ~54 seconds and mischaracterizes the relation (it claims the target begins when the question finishes rather than Maria directly responding), so timestamps and semantic alignment are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Maria instructs to Google 'Executive Order 13166', when does she mention the CLASs standards for healthcare organizations?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4461.82,
        "end": 4474.23
      },
      "pred_interval": {
        "start": 4436.0,
        "end": 4500.0
      },
      "iou": 0.19390624999999773,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.81999999999971,
        "end": 25.770000000000437,
        "average": 25.795000000000073
      },
      "rationale_metrics": {
        "rouge_l": 0.1702127659574468,
        "text_similarity": 0.35058102011680603,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially incorrect for both events (E1 and E2) and E2 is given a zero-length span; it also fails to reflect that Maria immediately elaborates on the CLASs resource, so the prediction does not align with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After Maria E. Mendoza explains Executive Order 13166, when does Jon Thompson ask about the regulation's enforcement mechanisms?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4470.0,
        "end": 4612.51
      },
      "gt_interval": {
        "start": 4479.38,
        "end": 4496.58
      },
      "pred_interval": {
        "start": 51.6,
        "end": 53.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4427.78,
        "end": 4443.38,
        "average": 4435.58
      },
      "rationale_metrics": {
        "rouge_l": 0.21875,
        "text_similarity": 0.33447790145874023,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that Thompson asks after Mendoza and references the enforcement question, but it gives an incorrect/irrelevant time (51.6s) and omits the correct start/end timestamps and the specified 'after' relation details, so it does not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once Maria E. Mendoza finishes stating her uncertainty about the executive order's trickle-down effect to individual schools, when does she mention the federal government's right to withdraw funds?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4470.0,
        "end": 4612.51
      },
      "gt_interval": {
        "start": 4508.48,
        "end": 4516.58
      },
      "pred_interval": {
        "start": 56.4,
        "end": 58.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4452.08,
        "end": 4458.48,
        "average": 4455.28
      },
      "rationale_metrics": {
        "rouge_l": 0.17500000000000002,
        "text_similarity": 0.336163192987442,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction preserves the sequence (target follows anchor) but the timestamps are wildly incorrect (seconds differ by orders of magnitude and the target interval/end do not match the reference), so it is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Maria E. Mendoza finishes giving the healthcare example of funding withdrawal for lack of interpreters, when does she state that 'there are very strict laws'?",
      "video_id": "6CdKh4ayAzM",
      "video_number": "001",
      "segment": {
        "start": 4470.0,
        "end": 4612.51
      },
      "gt_interval": {
        "start": 4552.98,
        "end": 4555.18
      },
      "pred_interval": {
        "start": 58.4,
        "end": 59.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4494.58,
        "end": 4495.280000000001,
        "average": 4494.93
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.22577112913131714,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (58.4\u201359.9s) do not match the correct timestamps (approx. 4552.98\u20134555.18s); the predicted answer is therefore incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that what's most important is helping her kid understand their experience, when does she explain that we often center the other person instead of our kid?",
      "video_id": "Z6o8S8JDg00",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 44.150000000000006
      },
      "gt_interval": {
        "start": 10.281,
        "end": 17.954
      },
      "pred_interval": {
        "start": 12.9,
        "end": 19.3
      },
      "iou": 0.5603725468455483,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6189999999999998,
        "end": 1.346,
        "average": 1.9825
      },
      "rationale_metrics": {
        "rouge_l": 0.06896551724137931,
        "text_similarity": 0.23578579723834991,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction roughly locates the correct segment (substantial overlap with 10.281\u201317.954s) but the start time is ~2.6s late and the end time extends ~1.3s beyond the ground truth; it also omits the anchor timestamp and the explicit after relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'Could we do this course together?', when does she say 'That would just be great. You don't have to agree with anything. I think that would be great.'?",
      "video_id": "Z6o8S8JDg00",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 44.150000000000006
      },
      "gt_interval": {
        "start": 25.207,
        "end": 27.812
      },
      "pred_interval": {
        "start": 25.3,
        "end": 29.7
      },
      "iou": 0.5590919207656357,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.09299999999999997,
        "end": 1.8879999999999981,
        "average": 0.990499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.043478260869565216,
        "text_similarity": 0.21121802926063538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is incorrect: it labels the timing of the question rather than the subsequent statement and gives inaccurate timestamps (23.585\u201324.946 vs predicted 25.3\u201329.7); while there is partial overlap with the correct target segment, the event and spans are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces herself as April Rehrig, when does the text 'IEPs' appear on screen?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 44.229,
        "end": 45.479
      },
      "pred_interval": {
        "start": 5.2,
        "end": 6.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.028999999999996,
        "end": 39.278999999999996,
        "average": 39.153999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142855,
        "text_similarity": 0.818115234375,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps are completely different from the reference (off by ~35+ seconds) and the target timing is incorrect; the relation 'after' does not match the specified 'next'. Overall the prediction contradicts the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker explains that Section 504 is a civil rights statute, when does the text 'Civil Rights Statute' appear?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.493,
        "end": 71.953
      },
      "pred_interval": {
        "start": 47.1,
        "end": 50.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.392999999999994,
        "end": 21.553000000000004,
        "average": 20.973
      },
      "rationale_metrics": {
        "rouge_l": 0.30985915492957744,
        "text_similarity": 0.7216455936431885,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps for both the speaker and the text are substantially incorrect and the predicted relation 'after' directly contradicts the ground-truth relation 'during', so it fails to match the correct temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions '504 meetings with five tips', when does she explain what to do before the meeting?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 26.541,
        "end": 33.582
      },
      "pred_interval": {
        "start": 160.9,
        "end": 165.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 134.359,
        "end": 132.318,
        "average": 133.3385
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526316,
        "text_similarity": 0.772787868976593,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the explanation occurs after the mention, but the timestamps and event boundaries are substantially wrong (events misplaced by ~135s and durations differ), so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"This is a problem because pro tip\", when does the \"Pro-Tip\" visual with a thumbs-up icon appear?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 177.7,
        "end": 179.0
      },
      "pred_interval": {
        "start": 28.333333333333332,
        "end": 31.666666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 149.36666666666665,
        "end": 147.33333333333334,
        "average": 148.35
      },
      "rationale_metrics": {
        "rouge_l": 0.16,
        "text_similarity": 0.4668623208999634,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the visual appears after the spoken line, but the timestamps disagree substantially with the reference (predicted times are far off and it omits the disappearance time); thus it is largely incorrect. "
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"Now it's time to talk about tip two\", when does the text \"Parent Letter of Attachment\" appear on screen?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 234.0,
        "end": 236.0
      },
      "pred_interval": {
        "start": 55.77777777777778,
        "end": 58.111111111111114
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 178.22222222222223,
        "end": 177.88888888888889,
        "average": 178.05555555555554
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947364,
        "text_similarity": 0.539107084274292,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the target text appears after the anchor, but the provided timestamps do not match the reference (large numeric discrepancy) and it omits the anchor end time and the target disappearance time, so key temporal details are incorrect or missing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says it's time to jump into the second part of what they will talk about, when do the animated files appear?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.05,
        "end": 358.05
      },
      "pred_interval": {
        "start": 33.0,
        "end": 35.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 324.05,
        "end": 322.35,
        "average": 323.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.18666666666666665,
        "text_similarity": 0.4425198435783386,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but gives incorrect/shifted timestamps (33.0s vs 350.05s and 35.7s vs 357.05\u2013358.05s) and omits the precise animated interval and anchor end alignment, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's question 'What is inside a 504 plan?', when do the question mark graphics appear and disappear?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 447.5,
        "end": 457.0
      },
      "pred_interval": {
        "start": 29.6,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 417.9,
        "end": 425.0,
        "average": 421.45
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.48061221837997437,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and an incorrect relationship: it misstates the question start/end times, omits the disappearance time, and wrongly labels the relation as 'start' rather than occurring throughout the question and subsequent speech."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'now it's time to dive into tip four', when does the large golden number '4' graphic appear?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 442.05,
        "end": 444.05
      },
      "pred_interval": {
        "start": 49.7,
        "end": 53.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 392.35,
        "end": 390.65000000000003,
        "average": 391.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.394403874874115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (speech then graphic) but gives entirely different timestamps and duration (49.7\u219253.4s vs. 440.05\u2192444.05s) and incorrectly indicates a delayed start; key temporal details and the immediate appearance are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the Parent Report, when does she start explaining what a Parent Report is?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 521.0,
        "end": 526.5
      },
      "pred_interval": {
        "start": 27.5,
        "end": 31.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 493.5,
        "end": 495.4,
        "average": 494.45
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.37856602668762207,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the reported timestamps do not match the reference intervals (and the prediction omits the full target/end interval details), so it is factually inaccurate despite the correct relation."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's explanation about getting her free guide, when does the visual graphic of the guide appear on the screen?",
      "video_id": "CS23nY2tX-4",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.8,
        "end": 584.0
      },
      "pred_interval": {
        "start": 30.9,
        "end": 33.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 548.9,
        "end": 550.7,
        "average": 549.8
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.49498414993286133,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies that the graphic appears concurrently with the speaker's explanation, but the provided timestamps are incorrect (and the anchor end time is omitted), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once Bobbi finishes reading the admission details from the tablet, when does her sustained ecstatic reaction begin?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 102.7,
        "end": 131.0
      },
      "pred_interval": {
        "start": 198.33333333333334,
        "end": 225.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 95.63333333333334,
        "end": 94.0,
        "average": 94.81666666666666
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.6374933123588562,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and event descriptions that contradict the ground truth (E1/E2 times and content), so it fails to match the correct timing and sequencing."
      }
    },
    {
      "question_id": "001",
      "question": "After the daughter mentions Berkeley, when does she explain why she probably won't get in?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 175.1,
        "end": 180.0
      },
      "pred_interval": {
        "start": 17.0,
        "end": 19.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 158.1,
        "end": 160.3,
        "average": 159.2
      },
      "rationale_metrics": {
        "rouge_l": 0.32727272727272727,
        "text_similarity": 0.539976954460144,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer preserves the order (mention then explanation) but the timestamps are wildly incorrect (off by ~156 seconds) and do not match the precise intervals given, so it fails to correctly locate the events."
      }
    },
    {
      "question_id": "002",
      "question": "Once the mother asks how scouting works, when does the daughter start explaining her basketball options?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 191.5,
        "end": 195.9
      },
      "pred_interval": {
        "start": 23.3,
        "end": 26.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 168.2,
        "end": 169.3,
        "average": 168.75
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.47127217054367065,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is timing-wise far off: the daughter actually starts explaining at 191.5s (about 1.6s after the mother's question), not 23.3s later, so the answer is incorrect and omits key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "While the daughter says she is weighing her options and is not sure yet, when does the mother appear in the frame, leaning on the bed?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 252.3,
        "end": 256.5
      },
      "pred_interval": {
        "start": 28.0,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 224.3,
        "end": 224.5,
        "average": 224.4
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.5412520170211792,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the mother's appearance at 28.0s (which falls within the daughter's utterance) but omits the full event intervals and the mother's end/settling time given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman on the right says she didn't go to Howard, when does she mention where she did go?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 365.0,
        "end": 368.0
      },
      "pred_interval": {
        "start": 49.28571428571429,
        "end": 51.547619047619044
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 315.7142857142857,
        "end": 316.45238095238096,
        "average": 316.08333333333337
      },
      "rationale_metrics": {
        "rouge_l": 0.10526315789473685,
        "text_similarity": 0.5098217129707336,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct anchor/target content (denying Howard then naming UT Austin) but gives drastically incorrect timestamps (49.3s/51.5s vs the correct 356.9\u2013357.6s and 365.0\u2013368.0s), failing the key temporal accuracy required."
      }
    },
    {
      "question_id": "002",
      "question": "After the daughter laughs, when does she state that she has many options for college?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 423.0,
        "end": 428.5
      },
      "pred_interval": {
        "start": 58.04761904761905,
        "end": 60.54761904761905
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 364.95238095238096,
        "end": 367.95238095238096,
        "average": 366.45238095238096
      },
      "rationale_metrics": {
        "rouge_l": 0.08219178082191782,
        "text_similarity": 0.41592133045196533,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the utterance about having many options but gives a completely wrong timestamp (57.5s vs. 423.0s) and adds an unrelated/coinciding event not present in the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the mother asks if every college has sororities and fraternities, when does the daughter state that not every school has Greek life?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 538.294,
        "end": 539.697
      },
      "pred_interval": {
        "start": 129.6,
        "end": 147.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 408.69399999999996,
        "end": 392.397,
        "average": 400.54549999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.8373222947120667,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it swaps the mother/daughter utterances, gives entirely different timestamps, and misidentifies the events; only the temporal relation ('after') coincides. Overall it fails to match the correct events and times."
      }
    },
    {
      "question_id": "002",
      "question": "After the mother says her one regret in life is not joining a sorority, when does she mention her AP at school is a 'die hard AKA'?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 587.969,
        "end": 592.336
      },
      "pred_interval": {
        "start": 515.3,
        "end": 532.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.6690000000001,
        "end": 59.936000000000035,
        "average": 66.30250000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.853130042552948,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation, but timestamps are inaccurate (E1 ~10s early, E2 ~58s early), it omits end times, and it misplaces E2 relative to the reference interval, so it lacks temporal accuracy and completeness."
      }
    },
    {
      "question_id": "001",
      "question": "Once the girl on the right asks if she did Columbia, when does the girl on the left respond?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 703.8,
        "end": 704.0
      },
      "pred_interval": {
        "start": 786.1,
        "end": 804.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.30000000000007,
        "end": 100.5,
        "average": 91.40000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.08450704225352113,
        "text_similarity": 0.25397253036499023,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is factually incorrect: the correct response occurs at ~703.8\u2013704.0s immediately after the anchor, whereas the prediction places it much later (786.1\u2013804.5s) and adds unsupported details, so it does not match."
      }
    },
    {
      "question_id": "002",
      "question": "Once the girl on the left states the acceptance rate, when does she start explaining what scattergrams are?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 767.6,
        "end": 781.7
      },
      "pred_interval": {
        "start": 855.6,
        "end": 873.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.0,
        "end": 91.29999999999995,
        "average": 89.64999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869565,
        "text_similarity": 0.45630961656570435,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the correct event starts at ~767.6s and ends at 781.7s, whereas the prediction gives much later timestamps (855.6\u2013873.0s) and adds unsupported details about Columbia/GPA-SAT, so it fails on timing and includes likely hallucinations."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman on the right says \"That Sydney wasn't feeling well\", when does she say \"He's doing well\"?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 917.7,
        "end": 918.6
      },
      "pred_interval": {
        "start": 39.7,
        "end": 42.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 878.0,
        "end": 875.8000000000001,
        "average": 876.9000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529411,
        "text_similarity": 0.6997315287590027,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction identifies a different anchor utterance and entirely different timestamps, and it misstates the temporal relation; it fails to match the key factual elements of the ground-truth answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the girl on the left finishes listing application platforms, when does she say \"Just word of advice, just start\"?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1086.0,
        "end": 1089.5
      },
      "pred_interval": {
        "start": 10.5,
        "end": 30.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1075.5,
        "end": 1059.2,
        "average": 1067.35
      },
      "rationale_metrics": {
        "rouge_l": 0.32098765432098764,
        "text_similarity": 0.588731050491333,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps and misidentifies the anchor segment, mislabels the relation ('after' vs once_finished) and adds an unsupported FAFSA detail, so it does not match the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the girl on the right asks \"what type of law?\", when does the girl on the left reply saying she'd probably go into civil law?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1119.5,
        "end": 1120.9
      },
      "pred_interval": {
        "start": 99.4,
        "end": 109.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1020.1,
        "end": 1011.7,
        "average": 1015.9000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3953488372093023,
        "text_similarity": 0.731273889541626,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relation 'once_finished' matches, the predicted timestamps and the identified target utterance do not correspond to the correct event (wrong start/end times and wrong quoted speech), so the prediction fails to locate or describe the true reply."
      }
    },
    {
      "question_id": "001",
      "question": "After the mother says \"a small school too\", when does the daughter stretch her arms up?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1240.741,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 15.5,
        "end": 16.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1225.241,
        "end": 1225.6,
        "average": 1225.4205
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.39441412687301636,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states the temporal relation (the daughter stretches after the mother's line) but omits the precise timing details and interval endpoints provided in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "While the mother is explaining how teachers can adjust grades, when does the daughter adjust her body position?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.0,
        "end": 1281.0
      },
      "pred_interval": {
        "start": 55.6,
        "end": 57.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1222.4,
        "end": 1223.8,
        "average": 1223.1
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869568,
        "text_similarity": 0.3952152132987976,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the daughter's adjustment occurs during the mother's explanation, matching the reference relation; it preserves the original meaning despite omitting explicit timestamps."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks her daughter what she was doing, when does the daughter respond 'You was yelling her name'?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1410.0,
        "end": 1455.304
      },
      "gt_interval": {
        "start": 1423.817,
        "end": 1429.817
      },
      "pred_interval": {
        "start": 18.0,
        "end": 25.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1405.817,
        "end": 1404.417,
        "average": 1405.117
      },
      "rationale_metrics": {
        "rouge_l": 0.1639344262295082,
        "text_similarity": 0.4107478857040405,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely disagrees with the reference: times and speakers are incorrect and the anchor/target intervals do not match the specified timestamps; it only vaguely references yelling a name, so it is almost entirely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the daughter finishes saying 'The front door', when does the woman on the right begin responding and laughing?",
      "video_id": "aIgDLOYEj50",
      "video_number": "004",
      "segment": {
        "start": 1410.0,
        "end": 1455.304
      },
      "gt_interval": {
        "start": 1428.184,
        "end": 1432.184
      },
      "pred_interval": {
        "start": 31.4,
        "end": 35.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1396.7839999999999,
        "end": 1396.484,
        "average": 1396.634
      },
      "rationale_metrics": {
        "rouge_l": 0.2033898305084746,
        "text_similarity": 0.5851807594299316,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the daughter's utterance timing but gives the woman's time as 35.7s (the end of her statement) instead of the correct start ~31.77s; thus it is partially correct but the key start-time relation is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After Susan asks the audience to click on the subscribe button, when does she mention that talks will happen every two weeks?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.621,
        "end": 26.608
      },
      "pred_interval": {
        "start": 13.286621307893522,
        "end": 15.734634465928835
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.334378692106476,
        "end": 10.873365534071166,
        "average": 10.603872113088821
      },
      "rationale_metrics": {
        "rouge_l": 0.21505376344086022,
        "text_similarity": 0.8254076242446899,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the event timestamps are substantially incorrect and the predicted target includes an extra detail ('for the remainder of the school year') not present in the ground truth, so it is largely factually mismatched."
      }
    },
    {
      "question_id": "002",
      "question": "Once Susan introduces Nick Prollins, when does Nick greet Susan?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 83.186,
        "end": 83.986
      },
      "pred_interval": {
        "start": 48.924426123617586,
        "end": 51.50727806972752
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.26157387638242,
        "end": 32.478721930272485,
        "average": 33.37014790332745
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.9154475927352905,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely contradicts the reference: it gives very different timestamps, misattributes the greeting to E1 instead of E2, and reverses event roles; only the coarse 'after' relation matches. Key factual elements are incorrect or omitted."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he had a class of 40 grade six boys, when does the woman touch her face in surprise?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 195.0,
        "end": 197.51
      },
      "pred_interval": {
        "start": 234.4,
        "end": 241.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.400000000000006,
        "end": 44.29000000000002,
        "average": 41.84500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.1627906976744186,
        "text_similarity": 0.5071264505386353,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different events and timestamps and reverses the temporal order (man after woman) contrary to the ground truth (woman touches face after the man\u2019s '40 grade six boys' remark), so it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says he learned mostly about behavior management, when does the woman state that everyone needs classroom management tips?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 354.3,
        "end": 356.5
      },
      "pred_interval": {
        "start": 47.5,
        "end": 51.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 306.8,
        "end": 305.1,
        "average": 305.95000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413793,
        "text_similarity": 0.6427695155143738,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction misidentifies the anchor event (uses the woman instead of the man's statement), gives completely incorrect timestamps, and fails to locate the woman's utterance after the man's remark as in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying they are digressing, when does the woman state that it is connected to what they will talk about?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 392.9,
        "end": 395.5
      },
      "pred_interval": {
        "start": 137.3,
        "end": 141.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 255.59999999999997,
        "end": 254.0,
        "average": 254.79999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2549019607843137,
        "text_similarity": 0.60645991563797,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the relative order (woman speaks after the man) but provides incorrect absolute timestamps and misattributes/changes the quoted lines, so it fails to match the correct events and times."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman asks about the relationship between families and schools, when does the man describe his previous role at a bilingual school?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 460.0,
        "end": 468.923
      },
      "pred_interval": {
        "start": 206.6,
        "end": 210.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 253.4,
        "end": 258.123,
        "average": 255.7615
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.5758969783782959,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mislabels the anchor event (uses the man's line as E1 instead of the woman's question) and gives entirely incorrect timestamps (206.6s/208.8s vs the reference ~442.959s and 460\u2013468.923s), so it fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes asking about the common denominators in the parent-teacher relationship, when does the man say it's a 'really great question'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 583.804,
        "end": 584.905
      },
      "pred_interval": {
        "start": 519.1666666666666,
        "end": 524.4166666666666
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.63733333333334,
        "end": 60.488333333333344,
        "average": 62.562833333333344
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.6403315663337708,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and the relative ordering ('after'), but it gives a wildly incorrect timestamp for E2 (524.417s vs the correct ~583.804\u2013584.905s), contradicting the ground truth and omitting the precise timings\u2014hence major factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes explaining that there was an initial impression among groups that their problems were unique, when does he start describing what they were actually saying?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 610.142,
        "end": 620.738
      },
      "pred_interval": {
        "start": 603.3333333333334,
        "end": 611.9166666666666
      },
      "iou": 0.10196499023249937,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.8086666666666815,
        "end": 8.821333333333428,
        "average": 7.815000000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.6196211576461792,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and event boundaries conflict with the reference (times are much earlier and E2 ends before the correct E1 finish), and the relation 'after' contradicts the correct 'once_finished'; substantial factual errors render it almost entirely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the man talks about parents wanting their children to succeed academically, when does he mention that many parents are unsure how to support their child?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.2,
        "end": 724.5
      },
      "pred_interval": {
        "start": 147.5,
        "end": 157.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 566.7,
        "end": 567.4,
        "average": 567.05
      },
      "rationale_metrics": {
        "rouge_l": 0.2028985507246377,
        "text_similarity": 0.6834254264831543,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely mismatches the reference: timestamps and event boundaries are completely different (\u2248150s vs \u2248704\u2013724s), the anchor/event descriptions are inconsistent, and it fails to preserve the correct temporal relation; only the notion of parental uncertainty partially overlaps."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes asking what 'the best' means in the context of raising a child, when does the man explain that it looks different in different contexts?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 762.4,
        "end": 767.8
      },
      "pred_interval": {
        "start": 228.1,
        "end": 237.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 534.3,
        "end": 530.5,
        "average": 532.4
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.6712149381637573,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic content (woman asks about 'best' and the man says it varies by context) but the timestamps are incorrect and event boundaries are mislabelled, so it fails to match the correct timing/continuity (immediate response)."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman describes a picture of a parent and infant looking at each other, when does she demonstrate with her phone a shift in parental focus?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 822.7,
        "end": 825.8
      },
      "pred_interval": {
        "start": 445.5,
        "end": 471.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 377.20000000000005,
        "end": 354.4,
        "average": 365.8
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.8399783372879028,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction recognizes a description and a later phone-based shift but gives substantially different and incorrect timestamps and segment boundaries (hundreds of seconds off) and omits the explicit visual 'instead of this is this' demonstration, so it earns minimal credit."
      }
    },
    {
      "question_id": "001",
      "question": "After the man states he doesn't have children himself, when does he explain how he has spent his career working with children and families?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 878.7,
        "end": 880.4
      },
      "pred_interval": {
        "start": 4.4,
        "end": 7.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 874.3000000000001,
        "end": 873.1999999999999,
        "average": 873.75
      },
      "rationale_metrics": {
        "rouge_l": 0.21875,
        "text_similarity": 0.17677389085292816,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely captures that the speaker links a disclaimer about not having children to discussing career work with children, but it fails to provide the required timing, exact utterance, or the 'after' temporal relation and adds unsupported framing about addressing the audience."
      }
    },
    {
      "question_id": "002",
      "question": "After the man asks 'Tell me what matters to you?', when does the woman ask if he asks this question to parents and teachers?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 902.9,
        "end": 909.0
      },
      "pred_interval": {
        "start": 79.2,
        "end": 84.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 823.6999999999999,
        "end": 825.0,
        "average": 824.3499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1714285714285714,
        "text_similarity": 0.30689895153045654,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes the woman asks about parents and teachers but omits the required timestamps and the 'after' temporal relation, and it adds an unsupported claim about linking to his career experience."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman says she has been on a 'triangle' of experiences, when does she list her roles as a parent, teacher, and tutor?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1003.2,
        "end": 1017.677
      },
      "pred_interval": {
        "start": 97.0,
        "end": 100.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 906.2,
        "end": 917.077,
        "average": 911.6385
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.32751885056495667,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the gist that she lists parent/teacher/tutor roles but omits critical details from the correct answer\u2014no timestamps, event boundaries, or the 'once_finished' temporal relationship\u2014so it's incomplete. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the male speaker finishes saying 'Really great strategy, really great strategy', when does he begin talking about clarifying something?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1101.703,
        "end": 1108.353
      },
      "pred_interval": {
        "start": 115.75,
        "end": 117.75
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 985.953,
        "end": 990.6030000000001,
        "average": 988.278
      },
      "rationale_metrics": {
        "rouge_l": 0.1111111111111111,
        "text_similarity": 0.31462594866752625,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted start time (115.75s) is vastly different from the correct start time (1101.703s) and omits the correct interval, so it is incorrect and does not match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker describes most parents wanting their children and teachers to thrive, when does he start talking about the 1%?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1146.304,
        "end": 1154.554
      },
      "pred_interval": {
        "start": 70.625,
        "end": 73.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1075.679,
        "end": 1081.554,
        "average": 1078.6165
      },
      "rationale_metrics": {
        "rouge_l": 0.1276595744680851,
        "text_similarity": 0.26337987184524536,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (70.625s after the introduction) contradicts the correct timing (E2 begins at 1146.304s), omitting and misstating the actual start time for the 1% segment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man asks 'who trains us to deal with these situations?', when does the woman respond 'No one'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1256.9,
        "end": 1257.3
      },
      "pred_interval": {
        "start": 50.0,
        "end": 57.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1206.9,
        "end": 1199.5,
        "average": 1203.2
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6644355654716492,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterances but the timestamps are far off and the temporal relation is incorrect (predicted 'after' vs. correct 'once_finished' with immediate reply), so it fails on key factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes describing demanding parents, when does the woman say she's 'starting to break out in hives'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1302.8,
        "end": 1306.5
      },
      "pred_interval": {
        "start": 59.6,
        "end": 67.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1243.2,
        "end": 1239.3,
        "average": 1241.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3541666666666667,
        "text_similarity": 0.6978070735931396,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events (man finishing and woman saying she\u2019s breaking out in hives) and their order, but the timestamps are markedly incorrect and the relation is labeled as generic 'after' rather than the immediate 'once_finished', reducing factual precision."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says she would love to know more about that, when does the man state that the ABCD trust model is not specifically for schools or parents?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1423.0,
        "end": 1434.135
      },
      "pred_interval": {
        "start": 36.0,
        "end": 40.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1387.0,
        "end": 1394.135,
        "average": 1390.5675
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.22996588051319122,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the man's remark occurs after the woman's question, but it omits the key factual timing details (E1 end at 1421.11s; E2 from 1423s to 1434.135s) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "When is the next time the man introduces a letter of the ABCD trust model after he explains 'A is for ability'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1543.182,
        "end": 1544.983
      },
      "pred_interval": {
        "start": 57.1,
        "end": 59.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1486.082,
        "end": 1485.0829999999999,
        "average": 1485.5825
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.39166998863220215,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the next letter is 'B' and its topic, but it fails to provide the required timing information (the timestamp/relative time when 'B' is introduced), omitting the key factual element from the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes inviting teachers to move beyond the 'us against them' mindset, when does he introduce the idea of 'family engagement'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1496.349,
        "end": 1501.218
      },
      "pred_interval": {
        "start": 102.1,
        "end": 105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1394.249,
        "end": 1396.218,
        "average": 1395.2335
      },
      "rationale_metrics": {
        "rouge_l": 0.21917808219178084,
        "text_similarity": 0.2832232713699341,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the idea is introduced after the invitation and signals a shift to collaboration, but it omits the precise timing/timestamps and adds minimal unfounded phrasing ('solution', 'team parent/team school'), so it lacks key factual details from the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the man states 'believability', when does he ask if you will do what you say you're going to do?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1659.124,
        "end": 1661.589
      },
      "pred_interval": {
        "start": 60.96666666666666,
        "end": 63.25555555555556
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1598.1573333333333,
        "end": 1598.3334444444445,
        "average": 1598.245388888889
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.6051146984100342,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the relation as 'after' and the event types, it gives completely incorrect timestamps and durations for both events (major factual mismatch with the ground truth), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman asks for an example, when does the man describe the advice 'under promise and over deliver'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1691.205,
        "end": 1694.03
      },
      "pred_interval": {
        "start": 69.84444444444443,
        "end": 72.84444444444443
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1621.3605555555555,
        "end": 1621.1855555555555,
        "average": 1621.2730555555554
      },
      "rationale_metrics": {
        "rouge_l": 0.45,
        "text_similarity": 0.6989222168922424,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies both events and the relation 'after', but the timestamps differ substantially from the reference and the predicted answer omits E1's end time, so key temporal details are incorrect or incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes explaining why schools might not always follow through on promises, when does the woman summarize the advice as 'under promise and over deliver'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1742.844,
        "end": 1747.709
      },
      "pred_interval": {
        "start": 79.24444444444444,
        "end": 80.8111111111111
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1663.5995555555555,
        "end": 1666.897888888889,
        "average": 1665.248722222222
      },
      "rationale_metrics": {
        "rouge_l": 0.22784810126582278,
        "text_similarity": 0.5688891410827637,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their temporal relation ('after'); it captures the key semantic ordering (woman's summary follows the man's explanation) despite differing absolute timestamps."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks about initiatives, when does he ask if they have parent representative councils?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1820.1,
        "end": 1826.2
      },
      "pred_interval": {
        "start": 105.0,
        "end": 112.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1715.1,
        "end": 1714.2,
        "average": 1714.65
      },
      "rationale_metrics": {
        "rouge_l": 0.1891891891891892,
        "text_similarity": 0.46614745259284973,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the sequential relationship (the specific question follows the general initiatives question) but gives completely incorrect timestamps and omits the precise start/end times from the reference, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker says the reading breakfast was 'such a cool thing', when does the female speaker react with wide eyes?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1848.0,
        "end": 1849.0
      },
      "pred_interval": {
        "start": 168.5,
        "end": 171.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1679.5,
        "end": 1677.5,
        "average": 1678.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1764705882352941,
        "text_similarity": 0.5246859788894653,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' (reaction follows the remark) but gives wholly incorrect and inconsistent timestamps (far from 1845s/1848s) and omits key timing details, so it is largely factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions the connection with character strengths, when does she give examples of these strengths?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1957.0,
        "end": 1963.8
      },
      "pred_interval": {
        "start": 11.84375,
        "end": 38.4375
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1945.15625,
        "end": 1925.3625,
        "average": 1935.259375
      },
      "rationale_metrics": {
        "rouge_l": 0.163265306122449,
        "text_similarity": 0.07242758572101593,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the examples occur after the mention but the timestamps are completely different from the reference (far off in absolute time) and it lacks the precise anchor/target intervals, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female speaker talks about opportunities for 'loose ties communications between school staff and families', when does the male speaker agree?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2008.0,
        "end": 2009.5
      },
      "pred_interval": {
        "start": 27.708333333333332,
        "end": 28.4375
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1980.2916666666667,
        "end": 1981.0625,
        "average": 1980.6770833333335
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.3189443349838257,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly conveys that the male speaker agrees immediately after the female, but the provided timestamps are wildly incorrect compared to the reference (27\u201328s vs ~1999\u20132009s), so it fails on the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the female speaker introduces 'dependability', when does the male speaker explain it as 'being consistent'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2050.0,
        "end": 2054.5
      },
      "pred_interval": {
        "start": 53.90625,
        "end": 56.71875
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1996.09375,
        "end": 1997.78125,
        "average": 1996.9375
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.3372964859008789,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly reflects that the male explanation follows the female introduction and gives a plausible relative start time, but it does not match the reference's absolute timestamps or provide the full start\u2013end interval for the target segment."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks 'what do I believe?', when does he state that having a school that has done the thinking makes a teacher feel more supported?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2142.885,
        "end": 2150.675
      },
      "pred_interval": {
        "start": 12.066666666666666,
        "end": 23.666666666666668
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2130.8183333333336,
        "end": 2127.0083333333337,
        "average": 2128.913333333334
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.63687664270401,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels the relation as 'after' and roughly identifies the man's supportive statement, but it gives incorrect timestamps, misidentifies E1 (woman vs man), and omits/contradicts the precise temporal boundaries from the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says she can do her part but asks about the parents' part, when does the man explicitly state they are not talking about a wall regarding boundaries?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2228.336,
        "end": 2229.739
      },
      "pred_interval": {
        "start": 48.56666666666667,
        "end": 51.366666666666674
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2179.769333333333,
        "end": 2178.3723333333332,
        "average": 2179.070833333333
      },
      "rationale_metrics": {
        "rouge_l": 0.345679012345679,
        "text_similarity": 0.7203950881958008,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and quotes the man's line, but it mislabels and describes E1 (gesture rather than the woman's question) and the provided timestamps for both events are far from the reference times, making it largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes saying he got schooled on something he didn't realize, when does he explain his naive assumption about WhatsApp?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2246.965,
        "end": 2299.733
      },
      "pred_interval": {
        "start": 115.96666666666667,
        "end": 121.26666666666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2130.9983333333334,
        "end": 2178.4663333333333,
        "average": 2154.7323333333334
      },
      "rationale_metrics": {
        "rouge_l": 0.16279069767441862,
        "text_similarity": 0.7601426839828491,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it swaps the two events (the WhatsApp explanation is E2 in the reference but is labeled E1 in the prediction), uses completely different timestamps, and gives a different relation; thus it fails to match the reference timing and event alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes describing the participant's WhatsApp call from a mother during her holiday, when does he state that this behavior must stop?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2339.8,
        "end": 2341.0
      },
      "pred_interval": {
        "start": 1791.3,
        "end": 1822.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 548.5000000000002,
        "end": 518.0999999999999,
        "average": 533.3000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.20895522388059704,
        "text_similarity": 0.5852400660514832,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the anchor and target and that the target comes after the anchor, but the timestamps are substantially incorrect (off by several minutes) and it fails to capture that the target immediately follows the anchor as in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man asks if anyone else would be treated like that in any other industry, when does the woman reply 'No'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2381.0,
        "end": 2381.3
      },
      "pred_interval": {
        "start": 1926.6,
        "end": 1938.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 454.4000000000001,
        "end": 442.60000000000014,
        "average": 448.5000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6237307190895081,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction records both events at completely wrong timestamps (\u22481938.7s vs correct \u22482381.0s), so the woman's reply timing is incorrect; although it correctly indicates the reply comes after, it fails to match the required immediate timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes talking about the school's duty to create boundaries, when does he suggest working towards positive change?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2566.2,
        "end": 2575.5
      },
      "pred_interval": {
        "start": 35.96666739327567,
        "end": 40.14444529215495
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2530.233332606724,
        "end": 2535.355554707845,
        "average": 2532.7944436572843
      },
      "rationale_metrics": {
        "rouge_l": 0.1473684210526316,
        "text_similarity": 0.2080288827419281,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes he discusses duty then proposes a framework, but it gives incorrect/ inconsistent timestamps and fails to match the correct interval (2566.2\u20132575.5s) or the immediately-after timing, so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman says 'That's right', when does she start talking about teachers having problems with school leadership regarding boundaries?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2600.5,
        "end": 2606.0
      },
      "pred_interval": {
        "start": 53.08888985770089,
        "end": 62.1999998637608
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2547.411110142299,
        "end": 2543.8000001362393,
        "average": 2545.605555139269
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.5980733036994934,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps do not match the reference (2600.5s start, 2606.0s end); it gives two inconsistent times (\u224853.09s and 62.20s) and omits the correct end time while adding extraneous detail about the man finishing a sentence, so it fails to accurately match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks 'What's in and what's out for me?', when is the next time he asks 'What are my red lines?'",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2697.043,
        "end": 2698.184
      },
      "pred_interval": {
        "start": 68.46666739327569,
        "end": 72.06666675063121
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2628.5763326067245,
        "end": 2626.117333249369,
        "average": 2627.3468329280468
      },
      "rationale_metrics": {
        "rouge_l": 0.4117647058823529,
        "text_similarity": 0.5644065141677856,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the question but gives a completely incorrect timestamp (68.47s vs. 2697.043\u20132698.184s) and adds unrelated/incorrect details about the woman speaking and anchor event timing, contradicting the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker talks about reflecting on professional boundaries, when does he ask about boundaries around communication with parents and colleagues?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2677.99,
        "end": 2692.02
      },
      "pred_interval": {
        "start": 30.8,
        "end": 32.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2647.1899999999996,
        "end": 2659.72,
        "average": 2653.455
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.5457574725151062,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relationship, but it gives different (and incomplete) timestamps and omits end times, so it doesn't fully match the precise temporal annotations in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks if teachers are okay with answering WhatsApp messages from colleagues after school hours, when does he ask if they are comfortable sharing their personal mobile number with a parent?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2727.77,
        "end": 2736.23
      },
      "pred_interval": {
        "start": 34.5,
        "end": 37.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2693.27,
        "end": 2699.23,
        "average": 2696.25
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.5551291108131409,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the relative order ('after') and the two question types, but the provided timestamps are completely different from the ground truth and it omits the correct end times, so key factual timing information is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the female speaker talks about being clear about boundaries and communicating them with 'whole heart', when does the male speaker discuss the comfort derived from setting clear boundaries?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2840.24,
        "end": 2850.66
      },
      "pred_interval": {
        "start": 37.0,
        "end": 39.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2803.24,
        "end": 2811.16,
        "average": 2807.2
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935483,
        "text_similarity": 0.6844769716262817,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the event order ('after') and that the male discusses comfort from boundaries, but the timestamps are drastically incorrect compared to the reference (off by ~2756s) and it omits precise end times and alignment details, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says, 'I want this to be practical', when does he say, 'You have no idea'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2960.516,
        "end": 2964.8
      },
      "pred_interval": {
        "start": 2891.2587023881024,
        "end": 2907.948594664454
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.25729761189768,
        "end": 56.85140533554613,
        "average": 63.05435147372191
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.8541292548179626,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after'), but both anchor and target timestamps deviate substantially from the ground-truth intervals, making the answer factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man asks, 'Tell me what matters to you right now', when does the woman make a thumbs-up gesture?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3024.99,
        "end": 3026.99
      },
      "pred_interval": {
        "start": 3009.7637887739247,
        "end": 3018.1475614208493
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.226211226075065,
        "end": 8.84243857915044,
        "average": 12.034324902612752
      },
      "rationale_metrics": {
        "rouge_l": 0.32432432432432434,
        "text_similarity": 0.7695029973983765,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the thumbs-up occurs after the man's question, but the provided timestamps for both anchor and target deviate substantially from the ground truth and the claim of 'immediately after' overstates the relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes saying, 'Thank you for sharing that information with me', when does he immediately advise to 'take note of that teacher'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3046.646,
        "end": 3049.969
      },
      "pred_interval": {
        "start": 3055.5040104978675,
        "end": 3071.9490464100254
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.858010497867326,
        "end": 21.980046410025352,
        "average": 15.41902845394634
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.7845010161399841,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives timestamps that are ~11\u201316 seconds off and labels the relation as merely 'after' rather than the immediate follow indicated in the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes suggesting to invite families to propose solutions, when does the woman react with a wide-eyed expression?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3069.0,
        "end": 3070.0
      },
      "pred_interval": {
        "start": 77.5,
        "end": 84.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2991.5,
        "end": 2985.5,
        "average": 2988.5
      },
      "rationale_metrics": {
        "rouge_l": 0.13114754098360656,
        "text_similarity": 0.27263343334198,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timing (84.5s after the suggestion) contradicts the ground truth (reaction begins ~0.2s after), and it adds unsupported descriptive details, so it is factually incorrect and misaligned."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes warning against creating a 'cycle of learned helplessness', when does he ask how they would approach the problem?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3136.766,
        "end": 3138.327
      },
      "pred_interval": {
        "start": 66.7,
        "end": 70.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3070.0660000000003,
        "end": 3068.027,
        "average": 3069.0465000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.15873015873015872,
        "text_similarity": 0.41588088870048523,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is factually incorrect about timing (it claims 70.3s delay while the correct timing shows the question begins ~1.04s after the warning) and introduces unsupported detail about the speaker's tone."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman first states that the advice is about 'a way of being', when does she re-emphasize that 'This is about a way of being'?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3178.623,
        "end": 3181.496
      },
      "pred_interval": {
        "start": 76.1,
        "end": 80.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3102.523,
        "end": 3100.996,
        "average": 3101.7595
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.20819130539894104,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the re-emphasis occurs after the first statement but gives a completely incorrect relative timing (80.5s vs ~7\u201312s based on the provided timestamps) and adds unsupported details about hand gestures, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman suggests to 'crack a window open', when does she ask 'what would happen if?'",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3213.0,
        "end": 3217.0
      },
      "pred_interval": {
        "start": 30.2,
        "end": 32.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3182.8,
        "end": 3184.2,
        "average": 3183.5
      },
      "rationale_metrics": {
        "rouge_l": 0.13513513513513511,
        "text_similarity": 0.6759777665138245,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the target utterance, but it gives incorrect/ inconsistent timestamps, misidentifies the anchor (speaker introduction vs. 'crack a window open'), and adds irrelevant visual cues\u2014failing to match key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman asks for the name of the course, when does she state the name herself?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3391.0,
        "end": 3393.0
      },
      "pred_interval": {
        "start": 54.9,
        "end": 58.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3336.1,
        "end": 3334.7,
        "average": 3335.3999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.14705882352941177,
        "text_similarity": 0.6488434076309204,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but provides incorrect timestamps that do not match the reference and introduces an unsupported visual-cue detail, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the man confirms 'building bridges', when does the woman ask him about traveling?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3390.0,
        "end": 3482.7169999999996
      },
      "gt_interval": {
        "start": 3401.076,
        "end": 3404.512
      },
      "pred_interval": {
        "start": 12.1,
        "end": 16.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3388.976,
        "end": 3388.212,
        "average": 3388.594
      },
      "rationale_metrics": {
        "rouge_l": 0.2711864406779661,
        "text_similarity": 0.5871407985687256,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both state the temporal relation as 'after', the predicted answer's timestamps and event identifications do not match the reference (wrong anchor event and vastly different times), so it fails on factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the man says his website is on the screen, when does he mention having other tips and resources?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3390.0,
        "end": 3482.7169999999996
      },
      "gt_interval": {
        "start": 3425.376,
        "end": 3431.514
      },
      "pred_interval": {
        "start": 21.5,
        "end": 24.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3403.876,
        "end": 3407.014,
        "average": 3405.445
      },
      "rationale_metrics": {
        "rouge_l": 0.27692307692307694,
        "text_similarity": 0.6808563470840454,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the tips/resources occur afterward, but the timestamps differ noticeably from the reference (E2 start ~1s early and E2 end ~5.5s early) and the relation is less specific than 'once_finished', so it is only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman asks viewers to click the subscribe button, when does she mention inviting Nick back?",
      "video_id": "PQF4Fe1EQdk",
      "video_number": "005",
      "segment": {
        "start": 3390.0,
        "end": 3482.7169999999996
      },
      "gt_interval": {
        "start": 3471.07,
        "end": 3474.532
      },
      "pred_interval": {
        "start": 28.8,
        "end": 31.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3442.27,
        "end": 3443.532,
        "average": 3442.901
      },
      "rationale_metrics": {
        "rouge_l": 0.3548387096774193,
        "text_similarity": 0.7858963012695312,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the two events and their order (inviting Nick back occurs shortly after the subscribe prompt) and the relative timing is consistent, but the numeric timestamps and the reported duration of E2 differ from the reference and the relation phrasing ('after' vs 'once_finished') is slightly less precise."
      }
    },
    {
      "question_id": "001",
      "question": "While the speaker explains that she will show what to do before, during, and after parent-teacher conferences with five tips, when does the '5' graphic appear on screen?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 31.258,
        "end": 37.358
      },
      "pred_interval": {
        "start": 21.783332824707028,
        "end": 22.083333219800682
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.474667175292971,
        "end": 15.274666780199315,
        "average": 12.374666977746143
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.5708675384521484,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a substantially incorrect timestamp (21.78s vs. the reference 31.258s\u201337.358s) and thus contradicts the correct timing; it also omits the correct duration the '5' graphic remains on screen."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker introduces herself as a special education advocate, when do the visual overlays 'IEPs' and '504 Plans' appear?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 47.954,
        "end": 51.274
      },
      "pred_interval": {
        "start": 47.12666601755547,
        "end": 47.66666601755547
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8273339824445287,
        "end": 3.60733398244453,
        "average": 2.2173339824445293
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333326,
        "text_similarity": 0.6536595821380615,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states the overlays appear during her introduction, but the appearance time is off (47.12s vs 47.954s) and it omits the disappearance time (~51.274s), so it is incomplete and slightly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what one needs to do to prepare to help their child, when does the 'How Do You Prepare?' graphic appear?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 192.8,
        "end": 195.0
      },
      "pred_interval": {
        "start": 38.5,
        "end": 57.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 154.3,
        "end": 137.4,
        "average": 145.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.43409377336502075,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect timestamps (00:39 and 00:54) that do not match the ground-truth times (speaker ~189.0\u2013192.1s, graphic at 192.8\u2013195.0s); while the predicted order is the same, the timing is substantially wrong and it fails to note the relation to the anchor speech ending."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker mentions her guide 'The Ten Keys to Communication', when is the guide's cover displayed on screen?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 238.0,
        "end": 242.0
      },
      "pred_interval": {
        "start": 21.8,
        "end": 27.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 216.2,
        "end": 214.5,
        "average": 215.35
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.4919161796569824,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted times (00:23 and 00:30) do not match the ground-truth intervals (~234.97\u2013247.03s and 238.0\u2013242.0s); the answer is largely incorrect on timing and misses the correct overlap/context."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's introduction of 'tip two', when does the 'Tip 2' graphic appear on screen?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 294.0,
        "end": 298.0
      },
      "pred_interval": {
        "start": 27.6,
        "end": 31.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 266.4,
        "end": 266.7,
        "average": 266.54999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.4067796610169491,
        "text_similarity": 0.6759167909622192,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps (00:28/00:32) do not match the correct times (293\u2013299s for the introduction and 294\u2013298s for the graphic), and it omits the graphic disappearance and the full intro interval, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what the difference is between IEP and 504 plans, when does the graphic with the number '10' appear?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 380.8,
        "end": 383.86
      },
      "pred_interval": {
        "start": 38.1,
        "end": 42.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 342.7,
        "end": 341.26,
        "average": 341.98
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.32504183053970337,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly states the temporal relation (after) but the timestamps are substantially incorrect compared to the ground truth (both anchor and target times differ by large amounts) and it omits the graphic's disappearance time."
      }
    },
    {
      "question_id": "002",
      "question": "After the text overlay 'Academic Adjustments' appears, when does the text overlay 'Accommodations' appear?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 450.722,
        "end": 451.722
      },
      "pred_interval": {
        "start": 57.4,
        "end": 58.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 393.322,
        "end": 393.222,
        "average": 393.272
      },
      "rationale_metrics": {
        "rouge_l": 0.4067796610169491,
        "text_similarity": 0.6515254974365234,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') but the provided timestamps are far from the ground-truth values and it omits disappearance times; thus it contains major factual inaccuracies despite the correct ordering."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker describes where children might be struggling by listing areas, when do icons representing different areas of need appear?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 455.722,
        "end": 460.135
      },
      "pred_interval": {
        "start": 66.1,
        "end": 67.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 389.62199999999996,
        "end": 392.635,
        "average": 391.1285
      },
      "rationale_metrics": {
        "rouge_l": 0.14925373134328357,
        "text_similarity": 0.4924464225769043,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect timestamps that do not match the reference (66s/67.5s vs ~452\u2013460s), misstates the temporal relation, and even contradicts itself, so it fails to match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces question two, when does she first ask what the teacher is doing to support the child?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 516.831,
        "end": 519.018
      },
      "pred_interval": {
        "start": 17.583333333333332,
        "end": 20.833333333333336
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 499.2476666666667,
        "end": 498.1846666666667,
        "average": 498.7161666666667
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.6999565362930298,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the target follows the anchor, but the timestamps do not match the ground truth (absolute vs relative mismatch) and it omits the noted overlap in speech; therefore it is largely incorrect on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "During the discussion about the Meeting Toolkit, when does the speaker highlight how it provides clarity on accommodations versus modifications?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 609.57,
        "end": 615.0
      },
      "pred_interval": {
        "start": 67.58333333333333,
        "end": 73.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 541.9866666666667,
        "end": 541.75,
        "average": 541.8683333333333
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317073,
        "text_similarity": 0.8578243255615234,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the content (Meeting Toolkit and the accommodations vs modifications point) but the timestamps are substantially different from the reference, the anchor end time is omitted, and the temporal relation is wrong (E2 should occur within E1, not 'after')."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker talking about implementing new supports and services and wanting to follow up, when does the 'Follow Up' graphic appear on screen?",
      "video_id": "ANz32spFUqQ",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 887.283
      },
      "gt_interval": {
        "start": 707.435,
        "end": 710.155
      },
      "pred_interval": {
        "start": 13.9,
        "end": 21.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 693.535,
        "end": 688.555,
        "average": 691.045
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951808,
        "text_similarity": 0.3638540804386139,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys that the graphic appears while the speaker discusses implementing supports, but it omits the precise timing (graphic 707.435s\u2013710.155s and speaker span 703.194s\u2013710.155s) and does not state the exact 'during' relation or absolute-to-relative timing required."
      }
    },
    {
      "question_id": "001",
      "question": "Once Margaret finishes introducing herself, when does she introduce her husband Marco and sister Mary?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 9.58,
        "end": 12.102
      },
      "pred_interval": {
        "start": 21.833333333333332,
        "end": 24.066666666666663
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.253333333333332,
        "end": 11.964666666666663,
        "average": 12.108999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.7373947501182556,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the high-level relation ('after') but gets the anchor/target timestamps and segment content wrong (incorrect start/end times, misidentified events), so it largely fails to match the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After Margaret states they are speaking to primary and secondary teachers specifically, when does she say that the information can more broadly benefit students?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 50.9,
        "end": 55.67
      },
      "pred_interval": {
        "start": 43.13333333333333,
        "end": 45.666666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.766666666666666,
        "end": 10.003333333333337,
        "average": 8.885000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.609437108039856,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the relation ('after') but misaligns the temporal boundaries: E2's start/end times are far earlier and E1 lacks an end time, so the timestamps do not match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After Margaret tells interpreters and translators to email them for a certificate of attendance, when does she state that they are not producing certificates?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.787,
        "end": 114.451
      },
      "pred_interval": {
        "start": 59.333333333333336,
        "end": 61.66666666666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.45366666666667,
        "end": 52.78433333333332,
        "average": 49.619
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.7023349404335022,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the correct verbal content ('not producing certificates') but gives incorrect timestamps and swaps/labels the events (times ~59\u201361s vs correct ~91\u2013114s) and thus misrepresents the temporal relation; key factual timing/ordering is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker touches on budgets and administrators, when does she mention pursuing things career-wise?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 167.0,
        "end": 172.462
      },
      "pred_interval": {
        "start": 16.9,
        "end": 18.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 150.1,
        "end": 154.262,
        "average": 152.18099999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3673469387755102,
        "text_similarity": 0.5443464517593384,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly conveys the key temporal relation that the mention of pursuing things career-wise comes after the discussion of budgets and administrators, preserving the original meaning without adding incorrect details."
      }
    },
    {
      "question_id": "002",
      "question": "After Marco says they are all 'language geeks', when does he describe his experience teaching in a bilingual program on the Mexican border?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 278.303,
        "end": 309.735
      },
      "pred_interval": {
        "start": 58.1,
        "end": 66.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 220.203,
        "end": 243.335,
        "average": 231.769
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.5444127321243286,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the bilingual-program remark occurs after the 'language geeks' line, but it omits the required timestamps and is vague/partly self-contradictory about mentioning 'bilingual program' as something that precedes itself, so it fails to match the detailed ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After Marco describes his experience of having to translate English handouts into Spanish, when does Margaret share a similar experience with campus newsletters?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 331.826,
        "end": 355.983
      },
      "pred_interval": {
        "start": 231.5,
        "end": 234.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.32600000000002,
        "end": 121.483,
        "average": 110.90450000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.13953488372093023,
        "text_similarity": 0.270815908908844,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys that Margaret's mention comes after Marco's, but it misstates key facts (wrong referent/pronoun and replaces 'English handouts' with 'special ed meetings') and omits the precise timing details, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions working on the campus newsletter all week for a Friday release, when does she describe being asked to translate it on Thursday afternoon?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 354.383,
        "end": 355.983
      },
      "pred_interval": {
        "start": 419.5,
        "end": 424.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.11700000000002,
        "end": 68.81700000000001,
        "average": 66.96700000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21176470588235294,
        "text_similarity": 0.4855690896511078,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps conflict with the reference (337.2s and 419.5s vs the correct ~353\u2013356s window) and misstates the temporal relation\u2014the translation request does not follow immediately as the reference indicates, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes her point about being asked to translate due to bilingualism, when does the second speaker (Mary Lamb) introduce herself?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 413.696,
        "end": 416.456
      },
      "pred_interval": {
        "start": 426.1,
        "end": 444.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.403999999999996,
        "end": 27.543999999999983,
        "average": 19.97399999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17647058823529413,
        "text_similarity": 0.4010908603668213,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies Mary Lamb introducing herself and even quotes her line, but it gives a substantially incorrect timestamp (426.1s vs ~413.7\u2013416.5s) and misstates the temporal context, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines 'language access', when does she start talking about 'LEP'?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 705.6,
        "end": 710.0
      },
      "pred_interval": {
        "start": 690.0,
        "end": 728.0
      },
      "iou": 0.11578947368420993,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.600000000000023,
        "end": 18.0,
        "average": 16.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.587597131729126,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (definition before LEP) but the timestamps are substantially incorrect (ground truth: definition ends 699.0s and LEP starts ~705.6s; prediction: 690.0s and 728.0s), so it contradicts the true timing and is factually wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker clarifies that they are 'not talking about politics today', when does she explain that language comes with 'cultural and emotional baggage'?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 757.048,
        "end": 780.333
      },
      "pred_interval": {
        "start": 796.0,
        "end": 828.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.952,
        "end": 47.66700000000003,
        "average": 43.309500000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.18918918918918917,
        "text_similarity": 0.4224838316440582,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the correct order (clarification then discussion of baggage) but the timestamps are significantly inaccurate\u2014off by ~40\u201370 seconds\u2014so it does not align with the reference timings."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male voice finishes inviting participants to the chat, when does the female voice add to the chat invitation?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 822.133,
        "end": 827.458
      },
      "pred_interval": {
        "start": 900.0,
        "end": 901.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.86699999999996,
        "end": 73.84199999999998,
        "average": 75.85449999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.5363856554031372,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction preserves the ordering (female speaks after male) but gives entirely incorrect timestamps and omits the female's end time and the note that the target occurs once the anchor finishes, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains 'back translating' through DeepL, when does she say they will show an example?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 938.629,
        "end": 940.511
      },
      "pred_interval": {
        "start": 1053.284880087499,
        "end": 1065.6354798842915
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 114.65588008749899,
        "end": 125.12447988429153,
        "average": 119.89017998589526
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.6400576829910278,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and events that do not match the ground-truth intervals or the 'once_finished' relation; it introduces unrelated details and is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker discusses 'a letter referring to a letter jacket', when does she suggest changing the vocabulary for accurate translation?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1107.0,
        "end": 1150.0
      },
      "pred_interval": {
        "start": 7.958333333333333,
        "end": 11.458333333333334
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1099.0416666666667,
        "end": 1138.5416666666667,
        "average": 1118.7916666666667
      },
      "rationale_metrics": {
        "rouge_l": 0.13114754098360656,
        "text_similarity": 0.3207361698150635,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the gist that the speaker recommends changing vocabulary but fails to provide the required timing/temporal relation details (the anchor/target timestamps and that the suggestion occurs after the referenced mention), so it is incomplete for the question."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that Google Translate and DeepL are almost identical and accurate for the student handbook, when does she highlight a minor difference in DeepL's translation?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1151.0,
        "end": 1161.0
      },
      "pred_interval": {
        "start": 63.421875,
        "end": 70.64583333333334
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1087.578125,
        "end": 1090.3541666666667,
        "average": 1088.9661458333335
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.0676858201622963,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the nature of the minor translation difference but omits the required temporal anchors and the 'after' relation, and it introduces a specific phrase ('educational policies') not stated in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining why AI machine translations struggle with literary texts, when does she provide the first example of a mistranslated Russian literary text?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.0,
        "end": 1204.0
      },
      "pred_interval": {
        "start": 115.51041666666666,
        "end": 131.84583333333333
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1084.4895833333333,
        "end": 1072.1541666666667,
        "average": 1078.321875
      },
      "rationale_metrics": {
        "rouge_l": 0.11428571428571428,
        "text_similarity": 0.27577701210975647,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only gives a vague summary that she transitions to an example about colloquial phrases and omits the required timing, anchor/target boundaries, and relation; it fails to match the specific timestamps and details in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the Spanish-speaking father begins his first statement, when does the interpreter finish translating it to English?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1467.348,
        "end": 1510.677
      },
      "pred_interval": {
        "start": 154.8684861319497,
        "end": 159.41490036310853
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1312.4795138680502,
        "end": 1351.2620996368914,
        "average": 1331.8708067524708
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.65465247631073,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps, durations, and described utterances do not match the ground truth (entirely different times and content); only the temporal relation 'after' coincides, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lecturer asks what's hard about consecutive interpretation, when does the interpreter finish explaining her challenges?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1509.179,
        "end": 1519.19
      },
      "pred_interval": {
        "start": 207.22222282772972,
        "end": 210.5925925925926
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1301.9567771722704,
        "end": 1308.5974074074074,
        "average": 1305.277092289839
      },
      "rationale_metrics": {
        "rouge_l": 0.20000000000000004,
        "text_similarity": 0.6752853989601135,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: timestamps and end time are incorrect, the relation ('after') does not match 'once_finished', and the predicted utterance/content does not capture the interpreter completing her explanation at 1519.19."
      }
    },
    {
      "question_id": "003",
      "question": "After the lecturer introduces sight translation, when does the interpreter begin reading the handwritten note aloud?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1556.54,
        "end": 1558.452
      },
      "pred_interval": {
        "start": 242.66666666666666,
        "end": 245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1313.8733333333332,
        "end": 1313.452,
        "average": 1313.6626666666666
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.7344297766685486,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events (lecturer introduction and interpreter reading) but gives entirely different timestamps and incorrectly labels the relation as 'at the same time' instead of 'after', misrepresenting the temporal ordering and durations."
      }
    },
    {
      "question_id": "001",
      "question": "After the main presenter mentions a side exercise on the next slide, when does he begin discussing the importance of preparation for interpreting?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.359,
        "end": 1621.799
      },
      "pred_interval": {
        "start": 54.42551345012267,
        "end": 55.51345012266714
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1560.9334865498772,
        "end": 1566.2855498773329,
        "average": 1563.609518213605
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117646,
        "text_similarity": 0.31451231241226196,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the prediction correctly states the discussion occurs after the side-exercise mention, it gives a drastically incorrect timestamp (~54.43s versus the correct start at 1615.359s) and omits the target's duration, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the main presenter describes an interpreter as a 'traffic cop', when does he describe the typical reactions people have to being instructed by an interpreter?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1703.019,
        "end": 1712.776
      },
      "pred_interval": {
        "start": 182.94065847533375,
        "end": 197.8494623655914
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1520.0783415246663,
        "end": 1514.9265376344088,
        "average": 1517.5024395795376
      },
      "rationale_metrics": {
        "rouge_l": 0.14492753623188406,
        "text_similarity": 0.263203501701355,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single, incorrect timestamp (182.94s) that contradicts the correct anchor/target times (~1681s and ~1703\u20131713s) and omits the quoted target wording and duration, so it fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says he will see if the AI can understand him, when does he start speaking in Russian?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1813.0,
        "end": 1829.8
      },
      "pred_interval": {
        "start": 75.63333333333334,
        "end": 78.93333333333332
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1737.3666666666666,
        "end": 1750.8666666666666,
        "average": 1744.1166666666666
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.5692830085754395,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: timestamps and quoted content do not match the reference (75.6\u201381.9s vs 1810.2\u20131829.8s), and the temporal relation ('after') contradicts the correct 'once_finished' immediate-follow relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes training attorneys and judges about simultaneous interpretation, when does he explain the purpose of this for non-bilingual people?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1959.703,
        "end": 1964.21
      },
      "pred_interval": {
        "start": 3.0,
        "end": 7.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1956.703,
        "end": 1956.31,
        "average": 1956.5065
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.38493812084198,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation right (the explanation comes after the training description) but the timestamps are completely incorrect and it adds specific content about cognitive load not present in the reference, so it is only partially aligned."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker says it's hard to hear over the sound of her own voice, when does the male speaker comment about being distracted by reading?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2146.342,
        "end": 2148.204
      },
      "pred_interval": {
        "start": 5.3,
        "end": 30.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2141.042,
        "end": 2118.0040000000004,
        "average": 2129.523
      },
      "rationale_metrics": {
        "rouge_l": 0.1891891891891892,
        "text_similarity": 0.6349942088127136,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies E1's content and timestamps and provides incorrect timing for E2, so it fails to match the reference segments; it only correctly states that the male comment occurs after the female remark."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'Interpreting' slide, when does the female speaker state that teaching and interpreting are two different jobs?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2219.994,
        "end": 2241.697
      },
      "pred_interval": {
        "start": 13.3,
        "end": 28.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2206.694,
        "end": 2213.4970000000003,
        "average": 2210.0955000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322035,
        "text_similarity": 0.7262437343597412,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different timestamps and content (13.3s/26.9s about reading) and labels the relation as 'after', which contradicts the ground truth that the statement occurs at 2219.994\u20132241.697s while the slide is still displayed."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker explains that doing the interpretation exercise makes people more willing to speak slowly and make pauses, what is the next action he suggests?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2169.782,
        "end": 2173.68
      },
      "pred_interval": {
        "start": 19.7,
        "end": 29.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2150.0820000000003,
        "end": 2144.68,
        "average": 2147.3810000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.15,
        "text_similarity": 0.45346468687057495,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets only the temporal relation ('after') right but misstates both timestamps and the content of the next action (claims 'outsourcing' at 28.9s), whereas the correct next actions are giving written info or hiring an interpreter at 2169.782\u20132173.680, so it largely disagrees with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the other speaker states that translating 100 pages would take longer than a weekend, when does the main speaker agree and say it would probably take a month?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2348.812,
        "end": 2349.894
      },
      "pred_interval": {
        "start": 49.4,
        "end": 53.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2299.412,
        "end": 2296.694,
        "average": 2298.053
      },
      "rationale_metrics": {
        "rouge_l": 0.12195121951219512,
        "text_similarity": 0.35268256068229675,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction notes the speaker's agreement and even quotes the line, but it fails to provide the specific timestamps and the immediate once_finished relation given in the correct answer, and it introduces an unsupported detail about translation costs."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses the hourly rates for interpreters, when does he mention there is often a two-hour minimum?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2378.997,
        "end": 2380.078
      },
      "pred_interval": {
        "start": 138.8,
        "end": 143.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2240.1969999999997,
        "end": 2236.278,
        "average": 2238.2374999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.6647650599479675,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the two-hour minimum is mentioned after the hourly rates, but it omits the key factual detail requested\u2014the precise timing/timestamps (around 2378.997\u20132380.078s)\u2014making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions being aware of the difference between freelancers and agencies, when does he explain that most translators and interpreters are freelancers who work for agencies?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2403.56,
        "end": 2407.963
      },
      "pred_interval": {
        "start": 195.5,
        "end": 205.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2208.06,
        "end": 2202.463,
        "average": 2205.2615
      },
      "rationale_metrics": {
        "rouge_l": 0.061538461538461535,
        "text_similarity": 0.008330386132001877,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the explanation comes after the discussion of agencies, but it is vague and omits the precise timing and the immediate-follow relationship specified in the correct answer (exact timestamps and anchor/event framing)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions working for an agency, when does she mention working as a freelancer?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2686.2,
        "end": 2687.5
      },
      "pred_interval": {
        "start": 50.33333333333333,
        "end": 52.666666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2635.8666666666663,
        "end": 2634.8333333333335,
        "average": 2635.35
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384617,
        "text_similarity": 0.2382844090461731,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that freelancing is mentioned after the agency, but the absolute timestamps and the reported interval are wildly incorrect (correct times are ~2684.8\u20132687.5s with a ~0.2s gap), so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'we can dare to dream', when does Marco start talking about his Spanish interpreting skills?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2733.4,
        "end": 2738.4
      },
      "pred_interval": {
        "start": 46.916666666666664,
        "end": 48.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2686.4833333333336,
        "end": 2690.4,
        "average": 2688.4416666666666
      },
      "rationale_metrics": {
        "rouge_l": 0.16129032258064516,
        "text_similarity": 0.3286876678466797,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps (46.0s/47.0s vs 2732.7s/2733.4s) and asserts a 1-second gap, contradicting the reference that the target immediately follows; thus it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Marco describes how teaching Spanish improved his interpreting skills, when does the speaker (Margaret) share her similar experience about teaching young children?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2749.5,
        "end": 2752.9
      },
      "pred_interval": {
        "start": 56.416666666666664,
        "end": 59.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2693.0833333333335,
        "end": 2693.9,
        "average": 2693.491666666667
      },
      "rationale_metrics": {
        "rouge_l": 0.13157894736842105,
        "text_similarity": 0.2484840452671051,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's timestamps (55s/56s) do not match the correct timestamps (~2738.6\u20132752.9s) and thus mislocate the event and sequence; it contradicts the ground truth and is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the female speaker finishes listing language populations like Vietnamese, German, and French, when does she mention Arabic?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2857.835,
        "end": 2858.836
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 2854.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.835000000000036,
        "end": 4.5359999999996035,
        "average": 6.18549999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.041666666666666664,
        "text_similarity": 0.14462795853614807,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes Arabic is mentioned after listing other languages but gives a timestamp (2854.3s) that contradicts the reference, which places the Arabic mention in the target speech at 2857.835\u20132858.836 immediately after the anchor; thus the timing is incorrect and key temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the male speaker asks if an agency can provide a price for an interpreter scenario, when does he state that it's not a trade secret?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2915.411,
        "end": 2918.755
      },
      "pred_interval": {
        "start": 2921.0,
        "end": 2924.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.588999999999942,
        "end": 5.545000000000073,
        "average": 5.567000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.05128205128205129,
        "text_similarity": 0.23992657661437988,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it cites 'mystery shopping' at 2924.3s, which neither matches the correct time window (2913.789\u20132918.755s) nor the content about not being a trade secret."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker invites participants to unmute and ask questions, when does he mention that an email with a video link will be sent next week?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2969.806,
        "end": 2975.771
      },
      "pred_interval": {
        "start": 3005.3,
        "end": 3011.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.49400000000014,
        "end": 35.528999999999996,
        "average": 35.51150000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.041666666666666664,
        "text_similarity": 0.24123184382915497,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: it gives a different utterance ('stop sharing') at a wrong timestamp (3011.3s) and does not identify the email-with-video-link speech at ~2969.8\u20132975.8s as in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman on the top left panel says \"I would go to that school as a starting point\", how long does she continue explaining the process of approaching a school?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3031.4,
        "end": 3049.5
      },
      "pred_interval": {
        "start": 37.7,
        "end": 69.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2993.7000000000003,
        "end": 2980.2,
        "average": 2986.95
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.5806237459182739,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different start/end timestamps and an incorrect duration (6s vs the correct 18.1s) despite noting an 'after' relation; it therefore fails to match the reference timing and span."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man on the top left panel finishes mentioning PTA meetings or board meetings, when does the woman on the top left panel start talking about who they want to know?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3059.8,
        "end": 3062.05
      },
      "pred_interval": {
        "start": 38.3,
        "end": 54.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3021.5,
        "end": 3007.75,
        "average": 3014.625
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315788,
        "text_similarity": 0.7039921283721924,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (E1 then E2) but is largely incorrect: it reports wrong timestamps (starts vs finishes), mismatched absolute times, and mislabels the relation as 'after' instead of the correct immediate/direct follow-up."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the top left panel finishes explaining how to use YouTube videos for practice, when does the woman on the top left panel begin mentioning volunteering through church connections?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3110.298,
        "end": 3113.563
      },
      "pred_interval": {
        "start": 54.7,
        "end": 58.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3055.598,
        "end": 3054.8630000000003,
        "average": 3055.2305
      },
      "rationale_metrics": {
        "rouge_l": 0.23809523809523808,
        "text_similarity": 0.6561639308929443,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') correct but the timestamps are wildly incorrect and do not match the reference (predicted times in the 50\u201360s range vs. reference ~3110s), and it omits the anchor's correct end time, so it fails to match the ground truth. "
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes asking her main question about recommendations for remote interpreting in educational settings, when does the man (Jesse Thompson) ask if she'll be working directly for the school or an agency?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3259.94,
        "end": 3264.367
      },
      "pred_interval": {
        "start": 97.5,
        "end": 100.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3162.44,
        "end": 3163.6670000000004,
        "average": 3163.0535
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.6161361932754517,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely mismatches the reference: timestamps are completely different, the events are misidentified (predicted E2 is the woman speaking rather than the man asking about school vs. agency), and the relation/temporal alignment does not match the once_finished relation in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions she has been doing IEP meetings since January, when does she state that 80% of them were virtual?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3419.487,
        "end": 3423.472
      },
      "pred_interval": {
        "start": 46.2,
        "end": 56.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3373.2870000000003,
        "end": 3367.472,
        "average": 3370.3795
      },
      "rationale_metrics": {
        "rouge_l": 0.36585365853658536,
        "text_similarity": 0.7428469657897949,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the timestamps and event boundaries (46.2s/56.0s) do not match the ground-truth (3411.114\u20133423.472s), and it misidentifies the anchor/target and their relation. It only partially overlaps by mentioning the '80% were virtual' phrase, but the temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker describes IEPs as not straightforward, when does another person define an IEP as an individualized education plan under special education?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3472.184,
        "end": 3478.875
      },
      "pred_interval": {
        "start": 103.1,
        "end": 115.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3369.0840000000003,
        "end": 3362.975,
        "average": 3366.0295
      },
      "rationale_metrics": {
        "rouge_l": 0.3043478260869565,
        "text_similarity": 0.7456361055374146,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mismatches the reference timestamps and temporal relation: the correct answer has the target starting immediately after the anchor (anchor ends at 3472.184s, target begins then), whereas the prediction gives entirely different times and claims the events start simultaneously, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that agencies don't get the information, when does she give examples like a brief summary or an IP packet?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3616.2,
        "end": 3624.0
      },
      "pred_interval": {
        "start": 24.0,
        "end": 48.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3592.2,
        "end": 3576.0,
        "average": 3584.1
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.4168039858341217,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the topical content (agencies not getting information and that examples follow), but it gives completely incorrect timestamps and adds unrelated details (shift to budgeting), so it fails to match the correct temporal annotations and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says that doing the budget is the hardest part, when does she mention 'numbers, the dates, and all this'?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3631.3,
        "end": 3633.3
      },
      "pred_interval": {
        "start": 54.0,
        "end": 72.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3577.3,
        "end": 3561.3,
        "average": 3569.3
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.57640540599823,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives an incorrect timestamp (54.0s vs ~3631s), adds unrelated content about managing assignments, and fails to preserve the temporal relation that the 'numbers, the dates...' remark occurs after the budget comment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker suggests asking people to share their screen, when does she suggest asking them to scroll as they read?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3669.7,
        "end": 3672.2
      },
      "pred_interval": {
        "start": 148.0,
        "end": 172.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3521.7,
        "end": 3500.2,
        "average": 3510.95
      },
      "rationale_metrics": {
        "rouge_l": 0.2564102564102564,
        "text_similarity": 0.47685641050338745,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly paraphrases the suggestion to have participants scroll while reading but provides entirely incorrect timestamps (148\u2013172s vs the correct ~3669.7\u20133672.2s) and omits the accurate share-screen timing and temporal relation, making it largely factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first female speaker asks about inquiring with the school for contacts with various specialists for special education IEPs, when does the second female speaker state that it's a great idea for background information?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3750.0,
        "end": 3853.87
      },
      "gt_interval": {
        "start": 3772.54,
        "end": 3777.487
      },
      "pred_interval": {
        "start": 11.3625,
        "end": 12.8625
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3761.1775,
        "end": 3764.6245,
        "average": 3762.901
      },
      "rationale_metrics": {
        "rouge_l": 0.26262626262626265,
        "text_similarity": 0.49866968393325806,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') between the question and the response, but the provided timestamps are inconsistent with the reference absolute times, reduce intervals to single points, and do not match the correct timing offsets. Therefore the ordering is right but the timing details are incorrect/incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second female speaker finishes asking what professionals usually talk about, when does she ask about the terminologies they use?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3750.0,
        "end": 3853.87
      },
      "gt_interval": {
        "start": 3782.973,
        "end": 3786.917
      },
      "pred_interval": {
        "start": 17.5375,
        "end": 18.6125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3765.4355,
        "end": 3768.3044999999997,
        "average": 3766.87
      },
      "rationale_metrics": {
        "rouge_l": 0.3010752688172043,
        "text_similarity": 0.4827060103416443,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives an incorrect/implausible timestamp (17.5375s vs the correct ~3781\u20133786s range), fails to provide the target event's actual timings, and contains vague/contradictory phrasing about event boundaries; the relation label is similar in intent but the factual errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male speaker finishes saying that interpreters are friendly and help each other, when does the second female speaker explain that it's because it's a small community?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 3750.0,
        "end": 3853.87
      },
      "gt_interval": {
        "start": 3843.755,
        "end": 3850.083
      },
      "pred_interval": {
        "start": 26.9875,
        "end": 28.5375
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3816.7675,
        "end": 3821.5455,
        "average": 3819.1565
      },
      "rationale_metrics": {
        "rouge_l": 0.12121212121212122,
        "text_similarity": 0.5565882921218872,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the second speaker follows the male speaker, but the timestamps are completely different from the reference and the relation is weakened to 'after' instead of the immediate 'once_finished'; the predicted event timing/description is therefore largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker introduces the topic of a parent letter, when does she mention that emotional language is fraught with danger for AI?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1264.183,
        "end": 1268.577
      },
      "pred_interval": {
        "start": 53.2,
        "end": 53.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1210.983,
        "end": 1214.777,
        "average": 1212.88
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.6521000862121582,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives the wrong timestamp (53.2s vs 1245.195s/1264.183\u20131268.577s) and incorrectly states the mention coincides with the parent-letter introduction rather than occurring afterward."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker reads the ambiguous phrase 'I would have raised my hand on her child', when does she begin questioning its meaning?",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1297.182,
        "end": 1306.16
      },
      "pred_interval": {
        "start": 59.5,
        "end": 62.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1237.682,
        "end": 1243.76,
        "average": 1240.721
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.24141599237918854,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (59.5s) is far from the reference start time (1297.182s) and omits the end time and relation; it therefore contradicts the correct answer and is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the first speaker finishes saying that something 'doesn't make a lot of sense', when does the second speaker say 'Over to me?'",
      "video_id": "6Phe1ALDZSk",
      "video_number": "007",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1340.99,
        "end": 1341.351
      },
      "pred_interval": {
        "start": 64.0,
        "end": 64.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1276.99,
        "end": 1276.651,
        "average": 1276.8205
      },
      "rationale_metrics": {
        "rouge_l": 0.1904761904761905,
        "text_similarity": 0.2050894796848297,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps (around 64s) and a different quoted phrase, which contradicts the correct timing (~1340s) and context; it does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what she is talking about, when does she begin to explain it?",
      "video_id": "y9bwM3YYMd0",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 23.383
      },
      "gt_interval": {
        "start": 1.974,
        "end": 5.3
      },
      "pred_interval": {
        "start": 4.5,
        "end": 5.3
      },
      "iou": 0.24052916416115452,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.526,
        "end": 0.0,
        "average": 1.263
      },
      "rationale_metrics": {
        "rouge_l": 0.10344827586206896,
        "text_similarity": 0.365719199180603,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction (4.5s) does not match the correct start time (1.974s) for when she begins explaining; although 4.5s falls within the overall explanation window, it fails to identify the actual beginning."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises to document evidence and justify a request, when does she mention that 'all of this can be documented in writing'?",
      "video_id": "y9bwM3YYMd0",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 23.383
      },
      "gt_interval": {
        "start": 11.189,
        "end": 14.2
      },
      "pred_interval": {
        "start": 14.0,
        "end": 16.4
      },
      "iou": 0.03838034926117815,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.811,
        "end": 2.1999999999999993,
        "average": 2.5054999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.09090909090909093,
        "text_similarity": 0.2979353070259094,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly locates the phrase within the correct time window (14.0s falls inside 11.189\u201314.2s) but is less precise than the reference, giving a single timestamp instead of the full annotated span and omitting the anchor interval."
      }
    },
    {
      "question_id": "001",
      "question": "After the male student in the plaid shirt says that they were 'invalidated', when does he mention 'mental health'?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 10.6,
        "end": 11.8
      },
      "pred_interval": {
        "start": 41.254343182750596,
        "end": 42.417496828757145
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.654343182750594,
        "end": 30.617496828757144,
        "average": 30.635920005753867
      },
      "rationale_metrics": {
        "rouge_l": 0.3132530120481928,
        "text_similarity": 0.69124835729599,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly labels the relation as 'after' and that the target mentions 'mental health', its timestamps are entirely inconsistent with the reference (0.03\u20132.0s and 10.6\u201311.8s vs ~41\u201343s), so it is factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the female student in the black coat talks about 'basic respect', when does the female student in the purple hoodie question why a first-grader incident is being brought up?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 59.8,
        "end": 64.7
      },
      "pred_interval": {
        "start": 110.92690615031601,
        "end": 112.04215069940508
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.12690615031602,
        "end": 47.34215069940508,
        "average": 49.23452842486055
      },
      "rationale_metrics": {
        "rouge_l": 0.2524271844660194,
        "text_similarity": 0.6975767612457275,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the speakers, the questioning event, and the 'after' relation, but the provided timestamps are vastly different from the ground-truth times, so it is not temporally accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the female student in the black hijab asserts that she 'did put a conclusion', when does the female student in the white hoodie explain the 'trauma-informed approach'?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 118.0,
        "end": 123.6
      },
      "pred_interval": {
        "start": 150.29046767364252,
        "end": 152.72094011384564
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.290467673642524,
        "end": 29.12094011384565,
        "average": 30.705703893744086
      },
      "rationale_metrics": {
        "rouge_l": 0.32608695652173914,
        "text_similarity": 0.6830400824546814,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and their ordering, but the timestamps are significantly incorrect for both the anchor (predicted 150.29s vs ground truth 75.6s) and the target (predicted 152.72\u2013153.34s vs ground truth 118.0\u2013123.6s), so it is factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first student finishes speaking about seeing their point of view, when does the man in the black vest and backpack start speaking?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 180.8,
        "end": 184.6
      },
      "pred_interval": {
        "start": 30.125,
        "end": 36.625
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 150.675,
        "end": 147.975,
        "average": 149.325
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.6961817741394043,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the 'after' relation, but the timestamps are wildly incorrect and it omits the E2 end time and the correct E1 finish time, so key factual details are missing."
      }
    },
    {
      "question_id": "001",
      "question": "After the girl in the purple hoodie finishes her statement, when does the man with the microphone ask if anyone else wants to speak?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 384.801
      },
      "gt_interval": {
        "start": 335.3,
        "end": 337.5
      },
      "pred_interval": {
        "start": 33.3,
        "end": 33.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 302.0,
        "end": 303.7,
        "average": 302.85
      },
      "rationale_metrics": {
        "rouge_l": 0.19607843137254902,
        "text_similarity": 0.4119088053703308,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') between the girl's statement and the man's question but omits the precise timestamps (334.2s and 335.3\u2013337.5s) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the girl in the grey hoodie finishes her explanation about introverts, when does the male student in the blue jacket start speaking?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 384.801
      },
      "gt_interval": {
        "start": 363.5,
        "end": 372.7
      },
      "pred_interval": {
        "start": 36.3,
        "end": 37.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 327.2,
        "end": 335.3,
        "average": 331.25
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131145,
        "text_similarity": 0.4560282826423645,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the male student speaks after the girl, but it omits the key factual timestamps (e.g., start at 363.5s) given in the ground truth, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man with the microphone asks the 'brother' if he wants to say anything, when does the man in the black vest respond?",
      "video_id": "f6q4KLKM5l4",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 384.801
      },
      "gt_interval": {
        "start": 378.0,
        "end": 383.6
      },
      "pred_interval": {
        "start": 45.3,
        "end": 53.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 332.7,
        "end": 330.3,
        "average": 331.5
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.41403713822364807,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the man in the black vest responds after the question, but it omits the precise timing (start 378.0s, end 383.6s) and the immediate 'once_finished' relation given in the reference, so is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that the cat comes in whenever it's showtime, when does she ask 'How do you know?'",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 58.579,
        "end": 60.615
      },
      "pred_interval": {
        "start": 47.6,
        "end": 50.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.979,
        "end": 9.715000000000003,
        "average": 10.347000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.27999999999999997,
        "text_similarity": 0.3188919723033905,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the question 'How do you know?' is asked, but gives a substantially incorrect timestamp (50.9s vs the correct ~58.6\u201360.6s), omitting the accurate interval and thus failing on factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'We are going live', when does she welcome the friends?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.063,
        "end": 104.724
      },
      "pred_interval": {
        "start": 70.1,
        "end": 74.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.96300000000001,
        "end": 30.524,
        "average": 32.243500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.18604651162790697,
        "text_similarity": 0.37807464599609375,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (74.2s) contradicts the ground truth: 'We are going live' occurs at 86.9\u201390.745s and 'Welcome friends' occurs at 104.063\u2013104.724s, so the prediction is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker introduces the Summer Stride Tuesday Night Author Series, when does she mention that it's summertime for adults?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 198.433,
        "end": 202.298
      },
      "pred_interval": {
        "start": 148.2,
        "end": 154.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.233000000000004,
        "end": 47.897999999999996,
        "average": 49.0655
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307693,
        "text_similarity": 0.4946298599243164,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly timestamps the moment at 154.4s, while the reference places the introduction at ~186.2\u2013191.6s and the line 'it's summertime for adults' at ~198.4\u2013202.3s; the temporal claim is therefore wrong despite noting it occurs after the introduction."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the 'Summer Stride Tuesday Night Author Series', when does she describe the Summer Stride reading challenge for adults?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 203.9,
        "end": 254.0
      },
      "pred_interval": {
        "start": 69.67479079741835,
        "end": 72.62075255822187
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 134.22520920258165,
        "end": 181.37924744177815,
        "average": 157.8022283221799
      },
      "rationale_metrics": {
        "rouge_l": 0.16216216216216217,
        "text_similarity": 0.30038321018218994,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' but both timestamps are incorrect (anchor is not at 0:00:00 and the target does not occur at 0:20:11) and it omits the target's actual start/end times and content details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"And it's really fun.\", when does she mention Malaka Garib doing a zine?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 342.37,
        "end": 345.33
      },
      "pred_interval": {
        "start": 12.4375,
        "end": 15.4375
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 329.9325,
        "end": 329.8925,
        "average": 329.9125
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5990695953369141,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives completely different timestamps and omits end times, so the temporal positioning is largely incorrect and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces Rhodessa Jones, when is the next time she says \"So please come check that out.\"?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 388.074,
        "end": 389.334
      },
      "pred_interval": {
        "start": 44.4375,
        "end": 45.5625
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 343.6365,
        "end": 343.7715,
        "average": 343.704
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.5667786598205566,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps do not match the ground truth (predicted starts ~44\u201345s vs ground truth ~379\u2013389s) and it omits the end times; therefore it is largely incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "After Angela mentions that any question is open for the chat, when does she hold up the physical copy of 'The Overly Honest Teacher'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 563.7,
        "end": 567.0
      },
      "pred_interval": {
        "start": 510.94444444444446,
        "end": 513.0222222222222
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.75555555555559,
        "end": 53.97777777777776,
        "average": 53.366666666666674
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.7662633657455444,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their ordering (target occurs after the anchor), but the provided timestamps are substantially different from the ground truth, so the timing information is inaccurate and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once Angela finishes saying she will turn it over to Meredith, when does Meredith begin speaking?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 590.0,
        "end": 594.0
      },
      "pred_interval": {
        "start": 541.9444444444445,
        "end": 543.7222222222223
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.05555555555554,
        "end": 50.277777777777715,
        "average": 49.16666666666663
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962025,
        "text_similarity": 0.7276346683502197,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (Meredith speaks immediately after Angela) but gives an incorrect absolute start time (543.72s vs. the reference 590.0s), so the key requested timestamp is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Meredith introduces herself as the author, when does she start talking about what everyone has gone through in the last 18 months regarding education?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 625.0,
        "end": 679.0
      },
      "pred_interval": {
        "start": 584.5222222222222,
        "end": 589.4444444444445
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.47777777777776,
        "end": 89.55555555555554,
        "average": 65.01666666666665
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.7924450635910034,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although the predicted answer correctly identifies the temporal relation ('after'), the anchor and target timestamps and quoted content do not match the ground truth (both events are placed much earlier and the anchor quote is incorrect), so the prediction is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the main topic slide, when does the first specific point, 'Consistency', appear on screen?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 727.0,
        "end": 727.9
      },
      "pred_interval": {
        "start": 22.568290333939235,
        "end": 24.804804944550487
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 704.4317096660608,
        "end": 703.0951950554495,
        "average": 703.7634523607551
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.5195783972740173,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation, but the timestamps do not match the reference scale and are internally inconsistent (two different start times for E2), so it is largely incorrect on timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the last point, 'Autonomy', appears on screen, when does the speaker begin to talk about her past teaching experience and starting the year with the definition of autonomy?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.5,
        "end": 751.9
      },
      "pred_interval": {
        "start": 690.8407191259104,
        "end": 726.6330718900672
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.65928087408963,
        "end": 25.266928109932792,
        "average": 41.96310449201121
      },
      "rationale_metrics": {
        "rouge_l": 0.14736842105263157,
        "text_similarity": 0.4604581296443939,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and event assignments contradict the ground truth (different start times, wrong event identification, and incorrect 'at the same time' relation instead of 'after'), so it fails to match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker uses the example of traffic lights to explain consistency, when does she give the example of microwave popcorn instructions?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 822.0,
        "end": 831.0
      },
      "pred_interval": {
        "start": 788.6401132050235,
        "end": 800.3010919164968
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.359886794976546,
        "end": 30.69890808350317,
        "average": 32.02939743923986
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.6507518887519836,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially different timestamps and an incorrect relation (says events are simultaneous) compared to the ground truth where popcorn follows traffic lights; it also contains internal inconsistencies about the target start time. These errors omit the key temporal ordering."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions setting meal times as important, when does she elaborate on discussing meals further during the evening?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 883.0,
        "end": 891.0
      },
      "pred_interval": {
        "start": 19.652777777777775,
        "end": 22.589583333333334
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 863.3472222222222,
        "end": 868.4104166666667,
        "average": 865.8788194444444
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.4002678096294403,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps (\u224819.65s and 22.59s versus 878s and 883s), misidentifies the target cue (mentions a book rather than the quoted line about meals), and does not correctly reflect the 'after' relation\u2014thus mostly incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the slide lists 'Daily schedule of classes', 'Set meal times', etc., when does the speaker elaborate on students crashing and burning due to lack of food?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 900.2,
        "end": 911.0
      },
      "pred_interval": {
        "start": 38.65277777777778,
        "end": 41.65277777777778
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 861.5472222222222,
        "end": 869.3472222222222,
        "average": 865.4472222222222
      },
      "rationale_metrics": {
        "rouge_l": 0.1111111111111111,
        "text_similarity": 0.475091814994812,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and events do not match the reference (900.2\u2013911.0s) and introduce unrelated/hallucinated times (38.65s, 41.65s, 1:32 PM, 9:45 PM); therefore it is factually incorrect and misaligned with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying that all listed things can be implemented at home, when does the slide update to reveal the 'Having a schedule and routine at home mimics...' text?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 964.9,
        "end": 965.0
      },
      "pred_interval": {
        "start": 28.65277777777778,
        "end": 30.370833333333334
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 936.2472222222221,
        "end": 934.6291666666667,
        "average": 935.4381944444444
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.4500225782394409,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the reported timestamps do not match the reference (completely different times), the relation is labeled only as 'after' rather than the immediate 'once_finished', and the predicted anchor/target descriptions do not align with the reference events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes talking about morning hygiene routines, when does she start talking about knowing the time to leave the house?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1059.9,
        "end": 1067.9
      },
      "pred_interval": {
        "start": 4.111111111111111,
        "end": 4.944444444444444
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1055.788888888889,
        "end": 1062.9555555555557,
        "average": 1059.3722222222223
      },
      "rationale_metrics": {
        "rouge_l": 0.061855670103092786,
        "text_similarity": 0.1701807975769043,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the content cue (the quoted line) marking the transition, but gives an incorrect timestamp (4.11s vs. the reference 1059.9s) and omits the end time (1067.9s), so it is only partially matching."
      }
    },
    {
      "question_id": "002",
      "question": "While the 'After school' list of activities is displayed, when does the speaker mention 'making time for dinner'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1104.2,
        "end": 1107.0
      },
      "pred_interval": {
        "start": 57.111111111111114,
        "end": 57.72222222222223
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1047.088888888889,
        "end": 1049.2777777777778,
        "average": 1048.1833333333334
      },
      "rationale_metrics": {
        "rouge_l": 0.12765957446808512,
        "text_similarity": 0.22083723545074463,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the event occurring during the 'After school' slide and even quotes the utterance, but it gives a completely incorrect absolute timestamp (57.11s) instead of the correct ~1104.2\u20131107.0s within the 1070.8\u20131172.0s slide period, so the timing is factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes being consistent, when does she mention that parents might sometimes feel like 'the heavy'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1140.4,
        "end": 1145.7
      },
      "pred_interval": {
        "start": 67.00000000000001,
        "end": 67.22222222222223
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1073.4,
        "end": 1078.4777777777779,
        "average": 1075.9388888888889
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.2641071677207947,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction references the theme of consistency but gives a completely incorrect timestamp (67.00s vs ~1140s) and wrong segment cues, so it fails to match the correct temporal location despite partial topical overlap."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes discussing how consistency helps with sibling rivalry, when is the 'ACCOUNTABILITY' slide fully displayed?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1246.0,
        "end": 1247.5
      },
      "pred_interval": {
        "start": 19.8125,
        "end": 23.8125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1226.1875,
        "end": 1223.6875,
        "average": 1224.9375
      },
      "rationale_metrics": {
        "rouge_l": 0.326530612244898,
        "text_similarity": 0.6969273090362549,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the slide appears after the speaker finishes), but it omits the key factual details\u2014exact timestamps and duration\u2014provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker emphasizes the importance of repeatedly stressing accountability, when does the second panel describing accountability appear on the slide?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1285.7,
        "end": 1286.2
      },
      "pred_interval": {
        "start": 40.25,
        "end": 43.125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1245.45,
        "end": 1243.075,
        "average": 1244.2625
      },
      "rationale_metrics": {
        "rouge_l": 0.21739130434782608,
        "text_similarity": 0.4038570821285248,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') but omits the key factual details (the exact timestamps when the speaker finishes and when the second panel appears/is fully animated), making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that accountability is the baseline foundation for a school community, when does she mention 'due dates for homework assignments'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1329.7,
        "end": 1331.9
      },
      "pred_interval": {
        "start": 42.75,
        "end": 46.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1286.95,
        "end": 1285.65,
        "average": 1286.3000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.32653061224489793,
        "text_similarity": 0.5453585386276245,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly captures the temporal relation\u2014the phrase about due dates occurs after the baseline accountability statement\u2014matching the reference (timestamps omitted but relative timing is preserved)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"If there's one thing you take away to put in your toolbox tonight, I hope it is this\", when does she explain what students had to write if they said something negative?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1442.41,
        "end": 1454.99
      },
      "pred_interval": {
        "start": 30.9,
        "end": 38.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1411.51,
        "end": 1416.29,
        "average": 1413.9
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.4309217035770416,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives wrong and reversed timestamps, a nonsensical time-difference calculation, and does not match the correct timing or order of the explanation about the 25 positive adjectives."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes talking about reminding themselves of positive thoughts, when does she introduce the topic of 'Restorative practice when community has been broken'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1486.37,
        "end": 1488.95
      },
      "pred_interval": {
        "start": 18.8,
        "end": 25.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1467.57,
        "end": 1463.95,
        "average": 1465.76
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.47934621572494507,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the ordering (introduced after the positive-thoughts segment) but gives a completely incorrect timestamp (25.5s vs. the correct ~1486.37s) and thus fails on the key factual timing detail."
      }
    },
    {
      "question_id": "003",
      "question": "While the green slide titled \"Holding everyone accountable for their own choices and responsibilities\" is displayed, when does the speaker give the example of parents getting a dog for their kids?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1524.93,
        "end": 1577.73
      },
      "pred_interval": {
        "start": 32.9,
        "end": 36.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1492.03,
        "end": 1540.93,
        "average": 1516.48
      },
      "rationale_metrics": {
        "rouge_l": 0.19047619047619044,
        "text_similarity": 0.4165083169937134,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives entirely incorrect and inconsistent timestamps (e.g., ~36s vs correct ~1525s), misidentifies anchor/target times, and includes nonsensical calculations, so it does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that 'we have to equip them with their ability to be able to overcome obstacles', when does she explain what saying 'no' does?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1785.657,
        "end": 1788.202
      },
      "pred_interval": {
        "start": 2.3,
        "end": 34.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1783.357,
        "end": 1754.002,
        "average": 1768.6795
      },
      "rationale_metrics": {
        "rouge_l": 0.1142857142857143,
        "text_similarity": 0.11343896389007568,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction fails to match the required timestamps and temporal relationship (1781\u20131788s, immediate transition) and introduces incorrect durations and times (2.3s, 10.2s\u201334.2s). It also adds unfounded details about length and continuation, so it is largely incorrect despite vaguely similar content."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises to 'help and not hinder their development', when does she suggest brainstorming for an essay instead of writing it for them?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1881.94,
        "end": 1883.282
      },
      "pred_interval": {
        "start": 61.2,
        "end": 77.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1820.74,
        "end": 1805.3819999999998,
        "average": 1813.061
      },
      "rationale_metrics": {
        "rouge_l": 0.048192771084337345,
        "text_similarity": 0.0133381187915802,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely matches the general theme of not doing work for students but gives incorrect timestamps, lacks the specific note about brainstorming an essay instead of writing it for them, and adds unrelated details about resilience and duration, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the slide transitions to 'Collaboration Form Two', when does the speaker say that teachers want to work with parents and guardians?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1934.726,
        "end": 1943.0
      },
      "pred_interval": {
        "start": 153.3,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1781.4260000000002,
        "end": 1763.0,
        "average": 1772.2130000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.06122448979591837,
        "text_similarity": 0.13580560684204102,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer fails to provide the correct timestamps or the specified 'after' temporal relation (it gives vague/mistimed info and an incorrect 180.0s), and adds unwarranted narrative detail not present in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"It is so important so that we can get a hold of you when we need to be able to talk,\" when do the bullet points on the slide disappear?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2163.0,
        "end": 2163.5
      },
      "pred_interval": {
        "start": 203.448275862069,
        "end": 222.00377197941907
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1959.551724137931,
        "end": 1941.496228020581,
        "average": 1950.523976079256
      },
      "rationale_metrics": {
        "rouge_l": 0.10169491525423728,
        "text_similarity": 0.3559229373931885,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is vague and interpretive rather than providing the required timing: it omits the precise moment (immediately after the speech at ~2163.0\u20132163.5s) and adds unsupported commentary about a shift in focus, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a harmonious journey with unicorns and rainbows, when does she say, \"It's not always going to be perfect. And there's going to be times where we are going to continue to butt heads\"?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2176.5,
        "end": 2181.0
      },
      "pred_interval": {
        "start": 242.48032888191742,
        "end": 253.39641591664514
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1934.0196711180827,
        "end": 1927.603584083355,
        "average": 1930.8116276007188
      },
      "rationale_metrics": {
        "rouge_l": 0.03333333333333333,
        "text_similarity": 0.1351381093263626,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction paraphrases the speaker's content but fails to provide the required timestamps and relation (E1 ends at 2176.5s, E2 2176.5\u20132181.0, once_finished) and adds an unfounded detail about scheduling a follow-up, so it does not match the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker suggests saying, \"I don't think this is going the way that either of us intended,\" when does she suggest scheduling time later in the week for a phone call?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2219.5,
        "end": 2226.5
      },
      "pred_interval": {
        "start": 272.4939949021078,
        "end": 285.29485596707815
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1947.0060050978923,
        "end": 1941.205144032922,
        "average": 1944.105574565407
      },
      "rationale_metrics": {
        "rouge_l": 0.03636363636363636,
        "text_similarity": 0.2176184058189392,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is a vague paraphrase and omits all required temporal details and the once_finished relation (E1/E2 timings and judge conversion), so it fails to match the correct, specific answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that most classrooms will have a homework board, when does she suggest that students should come to school equipped with a planner?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2495.0,
        "end": 2500.0
      },
      "pred_interval": {
        "start": 49.71892145011006,
        "end": 52.390686618875115
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2445.28107854989,
        "end": 2447.609313381125,
        "average": 2446.4451959655075
      },
      "rationale_metrics": {
        "rouge_l": 0.3697478991596639,
        "text_similarity": 0.6942392587661743,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the anchor/target content and that the target occurs after the anchor, but the provided timestamps are inconsistent with the reference (predicted times ~50\u201353s vs reference ~2490\u20132500s), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker suggests having accessible snacks that can be accessed independently at home, when does she mention tasking students with making their own lunch or breakfast?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2538.8,
        "end": 2542.8
      },
      "pred_interval": {
        "start": 63.00327730009801,
        "end": 65.01460849642945
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2475.796722699902,
        "end": 2477.7853915035707,
        "average": 2476.7910571017364
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.6188552379608154,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their 'after' relation and preserves the core wording, but the timestamps use a different scale/format and the event durations/times do not match the reference absolute times."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker suggests having students set out their clothes the night before, when does she mention that these actions will help ease up on hectic morning routines?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2604.5,
        "end": 2608.0
      },
      "pred_interval": {
        "start": 69.56442428708748,
        "end": 72.13622905585252
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2534.9355757129124,
        "end": 2535.8637709441473,
        "average": 2535.39967332853
      },
      "rationale_metrics": {
        "rouge_l": 0.21621621621621623,
        "text_similarity": 0.7372586131095886,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and mentions setting out clothes, but it mislabels anchor/target events, provides incorrect/invalid timestamps, and thus fails to match the reference timing and event mapping."
      }
    },
    {
      "question_id": "001",
      "question": "After Meredith asks Angela if there are any questions, when does Angela respond about pushing her buttons?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2698.0,
        "end": 2701.0
      },
      "pred_interval": {
        "start": 2.2,
        "end": 4.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2695.8,
        "end": 2696.2,
        "average": 2696.0
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.5856576561927795,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the coarse temporal ordering (Angela responds after Meredith) but provides completely different timestamps (relative 2.2s/4.8s vs. correct 2691\u20132693s and 2698\u20132701s) and adds an unfounded detail about a subsequent 'pushing' event, so it is largely incorrect and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once Angela finishes her question about kindergartener and eighth grader interactions on cell phones, when does Meredith begin to answer?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2771.5,
        "end": 2773.5
      },
      "pred_interval": {
        "start": 48.4,
        "end": 54.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2723.1,
        "end": 2719.2,
        "average": 2721.1499999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2388059701492537,
        "text_similarity": 0.5216665863990784,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives different timestamps and mislabels the speaker (saying Meredith asked the question) which contradicts the reference (Angela finishes at 2770.5s and Meredith starts at 2771.5s); therefore it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Meredith encourages parents to follow their students on social media, when does she explain how to approach conversations offline?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2717.3,
        "end": 2729.9
      },
      "pred_interval": {
        "start": 74.4,
        "end": 78.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2642.9,
        "end": 2651.4,
        "average": 2647.15
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.6876426339149475,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the two events and that the explanation occurs after the encouragement, giving a plausible start time for E2, but it uses a different time scale than the reference and omits E2's end time, so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about establishing very baseline questions to begin with around social media with kids, when does she mention having family meetings around responsibility?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2893.0,
        "end": 2896.0
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.014285714285714285,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.0,
        "end": 164.0,
        "average": 103.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.577412486076355,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer gets the relative relation ('after') correct, both event timestamps are substantially inaccurate compared to the ground truth (anchor and especially target times are wrong by large margins), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses having to change how you talk in terms of voice projection for different age groups in the classroom, when does she advise being careful about the words you're choosing to use?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2921.0,
        "end": 2923.0
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.009523809523809525,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.0,
        "end": 137.0,
        "average": 104.0
      },
      "rationale_metrics": {
        "rouge_l": 0.23404255319148937,
        "text_similarity": 0.591750979423523,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly captures the qualitative 'after' relationship, but both anchor and target timestamps are substantially incorrect and do not match the provided correct segments, so it fails to identify the next related piece of advice accurately."
      }
    },
    {
      "question_id": "001",
      "question": "After Meredith discusses students possibly feeling social anxiety and insecurity when re-entering school, when does she suggest getting them together with friends and classmates ahead of time?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3092.1,
        "end": 3098.5
      },
      "pred_interval": {
        "start": 33.1,
        "end": 42.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3059.0,
        "end": 3055.8,
        "average": 3057.4
      },
      "rationale_metrics": {
        "rouge_l": 0.1714285714285714,
        "text_similarity": 0.6065248250961304,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the suggestion phrase and that it occurs afterward, but the timestamps are inconsistent with the reference and the anchor event is misidentified/shifted, so the temporal alignment and event correspondence are largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Meredith Esau states that social and emotional learning doesn't have to exist in its own silo, when does she talk about teachers weaving social-emotional learning into debate?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3227.397,
        "end": 3232.508
      },
      "pred_interval": {
        "start": 11.2,
        "end": 17.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3216.197,
        "end": 3214.908,
        "average": 3215.5525
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.6433706879615784,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has the correct relation ('after') but the event timings and anchor/target assignments are incorrect and do not match the precise timestamps given in the reference, so it largely fails on factual accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "During Meredith Esau's explanation about enabling students to see their unique talents, when does she mention students wanting to code instead of playing football?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3269.041,
        "end": 3312.931
      },
      "pred_interval": {
        "start": 31.6,
        "end": 37.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3237.4410000000003,
        "end": 3275.731,
        "average": 3256.5860000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.17721518987341775,
        "text_similarity": 0.6438919901847839,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the football vs. coding example but misassigns timestamps, conflates anchor/target segments, and states the relation as 'after' rather than the correct 'during', so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Angela finishes asking about parents in conflict regarding screen time, when does Meredith start her response by saying \"Absolutely\"?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3400.93,
        "end": 3402.12
      },
      "pred_interval": {
        "start": 25.9,
        "end": 55.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3375.0299999999997,
        "end": 3346.42,
        "average": 3360.725
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6209713220596313,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly labels the temporal relation as 'after' but misidentifies both anchor and target events and provides completely different timestamps and utterances than the ground truth, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After Meredith states that you should \"never put your student in the middle\", when does she suggest helping them develop their own sense of compromise?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3462.72,
        "end": 3469.49
      },
      "pred_interval": {
        "start": 64.2,
        "end": 74.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3398.52,
        "end": 3394.5899999999997,
        "average": 3396.555
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.5383070707321167,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target event and the 'after' relationship, but it mislabels the anchor event (different wording) and gives inconsistent timestamps, omitting the precise anchor phrasing and accurate timing from the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After Angela lists several places where Meredith's book is available, when does she specifically mention Green Apple Books?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3556.06,
        "end": 3559.12
      },
      "pred_interval": {
        "start": 95.3,
        "end": 118.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3460.7599999999998,
        "end": 3441.12,
        "average": 3450.9399999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.581932783126831,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is entirely mismatched: it identifies different events, speakers, and timestamps (Meredith's raffle question and a yes reply at ~118s) rather than Angela's bookstore list and the Green Apple Books mention at ~3534\u20133559s, and it gives the wrong temporal relation."
      }
    },
    {
      "question_id": "001",
      "question": "After Angela asks if librarians buy digital books, when does Anissa confirm they do?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3595.925,
        "end": 3600.567
      },
      "pred_interval": {
        "start": 5.467129676281815,
        "end": 5.663943692044391
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3590.4578703237185,
        "end": 3594.9030563079555,
        "average": 3592.680463315837
      },
      "rationale_metrics": {
        "rouge_l": 0.17241379310344826,
        "text_similarity": 0.5051072835922241,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that Anissa affirms Angela's question, but the timestamps do not match the reference (incorrect conversion/anchor) and it omits the end times and span alignment required by the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once Anissa finishes asking Angela if she wants to take the YouTube viewer's question, when does Angela say 'Sure, yeah'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3617.561,
        "end": 3621.102
      },
      "pred_interval": {
        "start": 18.930349566149495,
        "end": 19.02857179187593
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3598.630650433851,
        "end": 3602.073428208124,
        "average": 3600.3520393209874
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322037,
        "text_similarity": 0.4588237404823303,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that a 'Sure, yeah' follows the question quickly, but it misidentifies the speakers (saying Angela asks and Meredith responds) and gives different timestamps, omitting the key factual elements about who asked and who replied."
      }
    },
    {
      "question_id": "003",
      "question": "After Meredith begins answering the question about schools helping children cope with COVID deaths, when does Angela next speak?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3720.052,
        "end": 3723.137
      },
      "pred_interval": {
        "start": 34.97523590396114,
        "end": 36.63271043498716
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3685.076764096039,
        "end": 3686.504289565013,
        "average": 3685.790526830526
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.5466516613960266,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps do not match the reference (large timing discrepancy/incorrect scaling) and it mischaracterizes Angela's turn\u2014omitting her exact utterance and implying an overlapping back-and-forth instead of speaking after Meredith finishes."
      }
    },
    {
      "question_id": "001",
      "question": "After Angela finishes stating that they need to pick one more winner who will get a digital code, when does she clarify that it's an actual digital ebook?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3778.3,
        "end": 3780.3
      },
      "pred_interval": {
        "start": 10.866666666666665,
        "end": 13.666666666666666
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3767.4333333333334,
        "end": 3766.6333333333337,
        "average": 3767.0333333333338
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.6705267429351807,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct ordering (E2 occurs after E1) but the timestamps are not aligned with the reference (large discrepancies in start times and missing/end bounds), so it fails on factual timing accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "Once Angela finishes stating her intention to put her email in the chat, when do her fingers come into view as she appears to type?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.5,
        "end": 3840.4
      },
      "pred_interval": {
        "start": 36.45555555555556,
        "end": 41.15555555555556
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3797.0444444444443,
        "end": 3799.2444444444445,
        "average": 3798.144444444444
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.5615540742874146,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies E1 (apology vs saying she'll put her email in the chat) and gives entirely different timestamps; although E2 is after E1, the key event and timing information are incorrect, so the answer is essentially wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman at the bottom finishes describing the usefulness of the book for specific tips, when does she start reading the quote about teaching children?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 3930.0,
        "end": 3989.9829999999997
      },
      "gt_interval": {
        "start": 3945.123,
        "end": 3950.606
      },
      "pred_interval": {
        "start": 17.0,
        "end": 21.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3928.123,
        "end": 3929.4060000000004,
        "average": 3928.7645
      },
      "rationale_metrics": {
        "rouge_l": 0.14634146341463417,
        "text_similarity": 0.42824822664260864,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the reading event (E2) and a verbal cue for its start, but it fails to provide the required timestamps, misstates E1 timing (saying E1 starts instead of giving the correct finish time), and adds an unsupported on-screen phrase, so it does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'we want to be your allies', when does she talk about parents and teachers wanting the best for the kids?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1953.971,
        "end": 1964.847
      },
      "pred_interval": {
        "start": 37.9,
        "end": 40.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1916.071,
        "end": 1924.747,
        "average": 1920.409
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.26660820841789246,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gives the correct approximate start time (37.9s) corresponding to the ground-truth interval's start, but it omits the end of the interval and does not state the temporal relation to the 'we want to be your allies' utterance, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes discussing the scaffolding teachers have in place, when does she mention bringing students to meetings?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2058.427,
        "end": 2061.512
      },
      "pred_interval": {
        "start": 102.4,
        "end": 107.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1956.027,
        "end": 1953.8120000000001,
        "average": 1954.9195
      },
      "rationale_metrics": {
        "rouge_l": 0.15151515151515152,
        "text_similarity": 0.5207036733627319,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction provides a single timestamp (102.4s) but the correct mention occurs around 108\u2013111s (1:48\u20131:51), so the timing is inaccurate and the answer omits the quoted phrasing; it is only loosely related."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker describes her parents attending a parent-teacher conference without her, when does she explain how her presence could have helped?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2110.911,
        "end": 2135.889
      },
      "pred_interval": {
        "start": 191.4,
        "end": 199.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1919.511,
        "end": 1935.989,
        "average": 1927.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3614457831325302,
        "text_similarity": 0.5632597208023071,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the relative order (the second moment is after the first) but the timestamps are wildly incorrect compared to the ground truth (predicted ~191\u2013200s vs. correct ~2096\u20132136s), so it is largely factually wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining how collaborating with parents increases involvement on a holistic level in the academic environment, when does she start talking about identifying unique talents and abilities?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2348.835
      },
      "pred_interval": {
        "start": 33.6,
        "end": 39.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2306.4,
        "end": 2308.935,
        "average": 2307.6675
      },
      "rationale_metrics": {
        "rouge_l": 0.17543859649122806,
        "text_similarity": 0.22735090553760529,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures a vague 'after' relation but gives completely different timestamps and misses the immediate adjacency (2340.9s follows 2340.5s) and precise timing in the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says it's our duty to instill independence, resilience, and self-reliance in students, when does she state that 'autonomy is action when we...'?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2407.49,
        "end": 2412.5
      },
      "pred_interval": {
        "start": 57.0,
        "end": 61.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2350.49,
        "end": 2350.6,
        "average": 2350.545
      },
      "rationale_metrics": {
        "rouge_l": 0.1081081081081081,
        "text_similarity": 0.1614111363887787,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation ('after'), its timestamps are drastically wrong compared to the reference and it omits the anchor's key content about 'instill[ing] independence, resilience, and self-reliance,' so it fails to match the target timing and context."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions students experiencing a gamut of emotions within a calendar day, when does she begin discussing their ability to be allies and advocates for others?",
      "video_id": "g33LDzFuj7k",
      "video_number": "011",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2429.86,
        "end": 2438.809
      },
      "pred_interval": {
        "start": 72.8,
        "end": 76.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2357.06,
        "end": 2362.009,
        "average": 2359.5344999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.125,
        "text_similarity": 0.2517770230770111,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly indicates the target occurs 'after' the anchor, the timestamps are wildly incorrect compared to the ground truth and it omits the brief pause/transition and end time, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes stating that this is 'practice dialogue number one', when does the English speaker (teacher) begin her first segment?",
      "video_id": "pzuzJ9H-4jw",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 81.412,
        "end": 86.516
      },
      "pred_interval": {
        "start": 0.0,
        "end": 210.0
      },
      "iou": 0.024304761904761902,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.412,
        "end": 123.484,
        "average": 102.44800000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680848,
        "text_similarity": 0.4023427367210388,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely notes the segment occurs 'after' the phrase but incorrectly states it is at the beginning of the video and omits the precise timings (81.412\u201386.516s), contradicting the ground truth; only the 'after' relation is captured."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes reading the English segment for Segment 5, when does she start reading the Telugu segment for Segment 6?",
      "video_id": "pzuzJ9H-4jw",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 233.917
      },
      "gt_interval": {
        "start": 158.981,
        "end": 170.016
      },
      "pred_interval": {
        "start": 49.7,
        "end": 57.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 109.28099999999999,
        "end": 112.31599999999999,
        "average": 110.79849999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.8256967663764954,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect\u2014timestamps and segment labels do not match the reference (predicted ~49.7s/57.7s vs. correct 154.706s/158.981\u2013170.016s), and it omits the beep detail; only the 'after' relation coincides. This constitutes a near-complete mismatch."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes reading the English segment for Segment 7, when does she start reading the Telugu segment for Segment 8?",
      "video_id": "pzuzJ9H-4jw",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 233.917
      },
      "gt_interval": {
        "start": 187.957,
        "end": 202.515
      },
      "pred_interval": {
        "start": 56.3,
        "end": 57.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 131.65699999999998,
        "end": 144.915,
        "average": 138.286
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7610009908676147,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mostly contradicts the reference: it gives completely different segment labels and timestamps (56\u201357s vs. 184\u2013202s) and incorrectly states the target ends at its start; only the 'after' relation matches, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "While the 'Thank You!' screen is displayed, when does the speaker begin her concluding remarks about the dialogue?",
      "video_id": "pzuzJ9H-4jw",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 233.917
      },
      "gt_interval": {
        "start": 209.544,
        "end": 233.917
      },
      "pred_interval": {
        "start": 233.9,
        "end": 234.1
      },
      "iou": 0.0006922951620783482,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.355999999999995,
        "end": 0.18299999999999272,
        "average": 12.269499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.5931057333946228,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it places both events around 233.9\u2013234.1s and labels the relation 'after', whereas the reference has the 'Thank You!' screen at 207.801s and the concluding remarks from 209.544s\u2013233.917s with relation 'during', so timings and relation are contradicted."
      }
    },
    {
      "question_id": "001",
      "question": "After the introductory speaker says, 'And I will turn it over to you,' when does Megan start speaking her introductory remarks?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 88.123,
        "end": 97.103
      },
      "pred_interval": {
        "start": 206.83333333333334,
        "end": 221.83333333333334
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 118.71033333333334,
        "end": 124.73033333333335,
        "average": 121.72033333333334
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7311983108520508,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relationship as 'after' but incorrectly reports all timestamps and misassigns the start of Megan's remarks (it marks the handover as E2 start); therefore it fails to match the key temporal details in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After Megan displays the 'Pandemic Parenting Principles' slide, when does she first mention graduating from Fairfield in 2010?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 101.112,
        "end": 102.697
      },
      "pred_interval": {
        "start": 101.83333333333334,
        "end": 111.16666666666666
      },
      "iou": 0.08589709587587789,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.721333333333348,
        "end": 8.469666666666654,
        "average": 4.595500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.5830334424972534,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies both events and their timings/content (E1 timing and slide vs. speaker intro; E2 not Megan's Fairfield comment), only the temporal relation 'after' coincidentally matches, so it fails to capture the correct details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions she got her master's in early childhood from BC, when does she start talking about teaching in Newton and Wellesley?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 168.6,
        "end": 174.5
      },
      "pred_interval": {
        "start": 31.25452367569043,
        "end": 43.61004114368508
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.34547632430957,
        "end": 130.88995885631493,
        "average": 134.11771759031225
      },
      "rationale_metrics": {
        "rouge_l": 0.16,
        "text_similarity": 0.3792380392551422,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps and misreports the anchor timing (starts vs correct finish), so it fails to match the correct E2 start at 168.6s; only the vague 'after' relation is consistent."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is talking about the Jesuit ideals instilled during her time at Fairfield, when does she mention the ability to reflect on where she was and wanted to go?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 355.5
      },
      "pred_interval": {
        "start": 96.5757129986467,
        "end": 101.33042647365642
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 255.42428700135332,
        "end": 254.16957352634358,
        "average": 254.79693026384845
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237114,
        "text_similarity": 0.7010375261306763,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the quoted phrase and the temporal relation (it follows the Jesuit-ideals remark), but the timestamps differ substantially from the reference (wrong time scale), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions offering strategies to help motivate your child, when does she introduce the topic of handwriting?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 351.8,
        "end": 361.0
      },
      "pred_interval": {
        "start": 46.0,
        "end": 103.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 305.8,
        "end": 258.0,
        "average": 281.9
      },
      "rationale_metrics": {
        "rouge_l": 0.12307692307692307,
        "text_similarity": 0.28193923830986023,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates handwriting is introduced after the strategies remark, but it omits the specific timestamps given in the reference and adds an unverified detail about examples of children's work, making it incomplete and partly hallucinated."
      }
    },
    {
      "question_id": "002",
      "question": "While the slide displays '2-5 minutes of handwriting practice adds up!', when does the speaker explain the importance of modeling numbers for children?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 396.0,
        "end": 413.0
      },
      "pred_interval": {
        "start": 104.7,
        "end": 111.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 291.3,
        "end": 301.3,
        "average": 296.3
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.2878183126449585,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction notes the speaker discusses modeling numbers in the same topical context but says it occurs 'after' the handwriting remark and omits the specified timestamps; the correct answer states the explanation happens during the slide (396\u2013413s), so the temporal relation and detail are incorrect/missing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes talking about using magnetic letters for word building, when does she start discussing ordering numbers?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 455.216,
        "end": 464.045
      },
      "pred_interval": {
        "start": 112.3,
        "end": 118.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 342.916,
        "end": 345.745,
        "average": 344.33050000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.5176306366920471,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes a transition to 'ordering numbers' but fails to provide the requested timing information and adds an unsubstantiated detail about repetitive practice not present in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker describes the specific elements a child included on their glowfish poster, when does she start to explain the general benefits of using big poster boards?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 532.8,
        "end": 542.0
      },
      "pred_interval": {
        "start": 512.4,
        "end": 602.0
      },
      "iou": 0.10267857142857191,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.399999999999977,
        "end": 60.0,
        "average": 40.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2894736842105263,
        "text_similarity": 0.8187285661697388,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the correct events but gives substantially incorrect start times for both E1 and E2 and wrongly labels the relation as 'after' rather than the reference's immediate 'once_finished', so it fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes showing the second poster example about the solar system, when does she explicitly list the general benefits of posters?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 576.835,
        "end": 593.004
      },
      "pred_interval": {
        "start": 604.8,
        "end": 677.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.964999999999918,
        "end": 83.99599999999998,
        "average": 55.98049999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.824651837348938,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events but gives substantially incorrect timestamps for both E1 and E2 (604.8s and 677.0s vs. 523\u2013576s and 576.835\u2013593.004s) and mischaracterizes the relation as merely 'after' rather than the immediate 'once_finished', so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker makes a general statement about children hesitating to write about things they can't draw, when does she provide a personal example of her nephew?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 672.0,
        "end": 690.0
      },
      "pred_interval": {
        "start": 689.4,
        "end": 730.0
      },
      "iou": 0.010344827586207288,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.399999999999977,
        "end": 40.0,
        "average": 28.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.6985557079315186,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after', but the timestamps are significantly inaccurate\u2014shifting both events much later and losing the immediacy of the nephew example\u2014so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions a 'how-to drawing type book', when does she explain that drawing is a big piece of practice in addition to writing?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 699.375,
        "end": 704.155
      },
      "pred_interval": {
        "start": 32.6875,
        "end": 35.3125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 666.6875,
        "end": 668.8425,
        "average": 667.765
      },
      "rationale_metrics": {
        "rouge_l": 0.3207547169811321,
        "text_similarity": 0.6229404211044312,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the provided timestamps are factually incorrect (off by several minutes/seconds compared with the reference), so the key factual details about timing are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker transitions to the 'MAKE YOUR OWN MATH GAMES' slide, when does she specifically describe making green and pink number cards?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 744.49,
        "end": 748.575
      },
      "pred_interval": {
        "start": 47.0625,
        "end": 50.3125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 697.4275,
        "end": 698.2625,
        "average": 697.845
      },
      "rationale_metrics": {
        "rouge_l": 0.2828282828282829,
        "text_similarity": 0.6563221216201782,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and their temporal relation ('after'), but the provided timestamps are significantly incorrect compared to the ground truth, so key factual timing information is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a collection of 'just right' books is key to helping children learn to read, when does she show an example of a Scholastic reader?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 911.4,
        "end": 915.0
      },
      "pred_interval": {
        "start": 20.629629533203126,
        "end": 36.20370368109806
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 890.7703704667969,
        "end": 878.7962963189019,
        "average": 884.7833333928494
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.5196731090545654,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the event order (the Scholastic reader is shown after the 'just right' mention) but the reported timestamps are vastly different from the reference (20s vs ~890\u2013915s) and it omits the end-time ranges, so it is largely incorrect and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "During the explanation of the '5 Finger Test', when does the speaker describe what constitutes a 'just right' book?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.5,
        "end": 970.0
      },
      "pred_interval": {
        "start": 32.38181818181818,
        "end": 42.70370368109806
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 954.1181818181818,
        "end": 927.2962963189019,
        "average": 940.7072390685419
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.7005616426467896,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct sequence (explanation then 'just right' example) but the provided timestamps do not match the reference intervals (off by large, inconsistent amounts and not a clear absolute\u2192relative conversion), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "While the images of the cardboard clocks are displayed, when does the speaker mention a child drawing a puppy on one of them?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1058.2,
        "end": 1060.95
      },
      "pred_interval": {
        "start": 15.4,
        "end": 16.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1042.8,
        "end": 1044.3500000000001,
        "average": 1043.575
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.6286463141441345,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the mention occurs while the clock image is displayed, but it gives an incorrect single timestamp (15.4s vs. the 1058.2\u20131060.95s interval) and includes unsupported details about left/right placement, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "While the 'Weekend News' template is shown on the screen, when does the speaker say that kids would be excited to share their news?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.3,
        "end": 1138.0
      },
      "pred_interval": {
        "start": 44.2,
        "end": 45.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1087.1,
        "end": 1092.2,
        "average": 1089.65
      },
      "rationale_metrics": {
        "rouge_l": 0.20224719101123598,
        "text_similarity": 0.5554628372192383,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect timestamps and asserts the mention occurs 'after' the visual, whereas the ground truth places the speaker's remark (1131.3\u20131138.0s) during the 'Weekend News' display (1039.8\u20131155.5s); the predicted answer contradicts the correct temporal relation and introduces wrong times."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining the 'Hide & Go Seek' sight word game, when does she start explaining the 'Memory' sight word game?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1204.2,
        "end": 1205.5
      },
      "pred_interval": {
        "start": 127.6,
        "end": 129.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1076.6000000000001,
        "end": 1076.4,
        "average": 1076.5
      },
      "rationale_metrics": {
        "rouge_l": 0.23655913978494622,
        "text_similarity": 0.6817887425422668,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (Memory follows Hide & Go Seek) but the timestamps are incorrect and do not match the reference (1204.2s start); it thus hallucinates wrong absolute times and omits the precise alignment given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes recommending non-fiction books if the home collection is mostly storybooks, when does she start talking about having visuals to practice multiplication?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1250.14,
        "end": 1251.01
      },
      "pred_interval": {
        "start": 37.4,
        "end": 39.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1212.74,
        "end": 1211.91,
        "average": 1212.325
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.5739418268203735,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that a transition occurs but omits the required timing details (start/end timestamps), so it fails to answer 'when' despite being directionally correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes explaining that dry erase boards help with handwriting consistency, when does she start talking about having math tools for exploration?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1322.321,
        "end": 1324.997
      },
      "pred_interval": {
        "start": 62.6,
        "end": 64.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1259.721,
        "end": 1260.6970000000001,
        "average": 1260.209
      },
      "rationale_metrics": {
        "rouge_l": 0.3809523809523809,
        "text_similarity": 0.6704767346382141,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the speaker transitions from handwriting consistency to math tools, but it omits the specific timing information (start at 1319.993s and start/end of the math-tools segment at 1322.321s\u20131324.997s), making it incomplete for the asked 'when' question."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes mentioning having sharpened pencils around for home setup, when does she display and introduce the word wall for visual reinforcement?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1425.637,
        "end": 1450.0
      },
      "pred_interval": {
        "start": 119.6,
        "end": 121.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1306.037,
        "end": 1328.8,
        "average": 1317.4185
      },
      "rationale_metrics": {
        "rouge_l": 0.13953488372093023,
        "text_similarity": 0.31110960245132446,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates she moves from home setup remarks to the word wall, but it omits the specific timestamps and duration given in the reference (1422.916 to ~1450s), so it is incomplete for the asked 'when'."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions having an alphabet chart or number line for easy accessibility, when does she introduce the word wall?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1425.2,
        "end": 1427.5
      },
      "pred_interval": {
        "start": 19.7,
        "end": 53.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1405.5,
        "end": 1374.0,
        "average": 1389.75
      },
      "rationale_metrics": {
        "rouge_l": 0.1558441558441558,
        "text_similarity": 0.33773231506347656,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the relation ('after') but fails to provide the required timing details (timestamps) and adds unverified visual descriptions, omitting key factual elements from the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker suggests parents type the words for a book while the child illustrates, when does she suggest having the child practice typing?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1506.0,
        "end": 1508.0
      },
      "pred_interval": {
        "start": 56.3,
        "end": 62.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1449.7,
        "end": 1445.1,
        "average": 1447.4
      },
      "rationale_metrics": {
        "rouge_l": 0.1411764705882353,
        "text_similarity": 0.293905109167099,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the child practices typing after the parent-typing suggestion, but it gives an incorrect timestamp and irrelevant slide timing (56.3s) instead of the precise target timestamps (1506.0\u20131508.0s), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that games help build skills, when does she mention 'problem solving'?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1617.701,
        "end": 1618.281
      },
      "pred_interval": {
        "start": 24.02439024390244,
        "end": 48.81203007518797
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1593.6766097560976,
        "end": 1569.468969924812,
        "average": 1581.5727898404548
      },
      "rationale_metrics": {
        "rouge_l": 0.5396825396825397,
        "text_similarity": 0.7242194414138794,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation 'after' and locates the 'problem solving' mention, but it misidentifies the anchor (uses 'games' instead of 'skills like turn-taking') and provides timestamps that are far from the reference, so key details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker compares reading less than a minute a day to 20 minutes a day, when does she state that 1.8 million words are exposed per year?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1683.016,
        "end": 1684.426
      },
      "pred_interval": {
        "start": 64.86486486486486,
        "end": 73.88819875776397
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1618.1511351351353,
        "end": 1610.537801242236,
        "average": 1614.3444681886856
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.8128692507743835,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different timestamps and quotes (mentioning ~8k and ~282k words) and does not identify the 1.8 million-word statement at ~1683s, so it fails to match the correct event."
      }
    },
    {
      "question_id": "001",
      "question": "Once the 'Q&A' slide is displayed, when does the slide asking 'COVID and religious education?' appear?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1797.8,
        "end": 1803.9
      },
      "pred_interval": {
        "start": 24.1,
        "end": 25.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1773.7,
        "end": 1778.0,
        "average": 1775.85
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.7036497592926025,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that E2 follows E1, but the reported timestamps are completely different from the reference (and no end times/duration are given), and the relation 'after' is less precise than the ground-truth 'once_finished'; overall the timing and detail are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker concludes her statement about music being important for religious education, when does she start talking about children learning about saints?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1840.4,
        "end": 1844.9
      },
      "pred_interval": {
        "start": 31.9,
        "end": 33.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1808.5,
        "end": 1811.3000000000002,
        "average": 1809.9
      },
      "rationale_metrics": {
        "rouge_l": 0.30434782608695654,
        "text_similarity": 0.7924242615699768,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target topic (children learning about saints) and the 'after' relation, but it misidentifies the anchor phrase and gives entirely different/incorrect timestamps (no anchor end time), so key timing and anchor details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes asking how schools can assist with the transition to in-person learning, when does the slide listing 'clear expectations' appear?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1879.3,
        "end": 1940.0
      },
      "pred_interval": {
        "start": 54.4,
        "end": 55.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1824.8999999999999,
        "end": 1884.2,
        "average": 1854.55
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.7089599370956421,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer largely contradicts the ground truth\u2014times and event descriptions are incorrect and inconsistent with the reference; only the temporal relation ('after') matches, so it gets a very low score."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks, 'How to ensure children are not being distracted by non-school work when online?', when does she start listing tips?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1959.342,
        "end": 1963.495
      },
      "pred_interval": {
        "start": 14.9,
        "end": 54.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1944.442,
        "end": 1909.395,
        "average": 1926.9185
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.318637490272522,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the tips occur after the question, but the timestamps are drastically incorrect (14.9\u201354.1s vs 1959.342\u20131963.495s) and it introduces specific strategies not supported by the reference, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes giving tips on staying on task, when does she offer to answer questions?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1997.897,
        "end": 2003.182
      },
      "pred_interval": {
        "start": 54.9,
        "end": 58.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1942.9969999999998,
        "end": 1945.082,
        "average": 1944.0394999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.08,
        "text_similarity": 0.16205835342407227,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it cites a different event and wrong timestamp (54.9s) and does not match the correct timing (~1997.9\u20132003.2s) or wording, thus contradicting the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the host says 'Great. I have two more. Thank you for that.', when does she ask the question about kindergarten?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2083.534,
        "end": 2093.534
      },
      "pred_interval": {
        "start": 130.1,
        "end": 136.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1953.4340000000002,
        "end": 1957.4340000000002,
        "average": 1955.4340000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.12244897959183673,
        "text_similarity": 0.34585440158843994,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives a completely different timestamp (130.1s) and adds unrelated context, failing to match the correct timestamps (start at 2083.534s) or the described relation; it is factually incorrect and contains hallucinated detail."
      }
    },
    {
      "question_id": "001",
      "question": "Once the female speaker on the right finishes asking the question about the emotional side of entering a new grade, when does the female speaker on the left begin to respond?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2297.683
      },
      "gt_interval": {
        "start": 2181.936,
        "end": 2182.54
      },
      "pred_interval": {
        "start": 68.4,
        "end": 70.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2113.536,
        "end": 2112.04,
        "average": 2112.788
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7192521095275879,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mostly fails: timestamps and event boundaries (start vs completion) conflict with the reference and the numerical times are entirely different; the relation 'after' is a vague mismatch with the specific 'once_finished' relation. There is only a minimal semantic overlap that the target follows the anchor, so a very low score is warranted."
      }
    },
    {
      "question_id": "002",
      "question": "When is the next time the female speaker on the left mentions a way students are socializing online after she describes a student's weekly Zoom calls?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2297.683
      },
      "gt_interval": {
        "start": 2204.249,
        "end": 2210.952
      },
      "pred_interval": {
        "start": 72.0,
        "end": 72.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2132.249,
        "end": 2138.052,
        "average": 2135.1504999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.6940226554870605,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and segments do not match the correct (much later) times and do not identify the same target utterance; the temporal relation and event locations are incorrect, so the answer is essentially wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the female speaker on the left finishes talking about an online newspaper, when does she begin to introduce the 'Outschool' platform?",
      "video_id": "kQpSkCpDE4g",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2297.683
      },
      "gt_interval": {
        "start": 2227.882,
        "end": 2232.043
      },
      "pred_interval": {
        "start": 74.4,
        "end": 76.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2153.482,
        "end": 2155.543,
        "average": 2154.5125
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7263885736465454,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the timestamps and described content do not match the reference (completely different absolute times and anchor/topic descriptions), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'Number four', when does the text 'Parents don't get trained' appear on screen?",
      "video_id": "J-upF-lwWvg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 46.400000000000006
      },
      "gt_interval": {
        "start": 2.02,
        "end": 3.73
      },
      "pred_interval": {
        "start": 4.183597511302455,
        "end": 8.901650975355585
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.1635975113024553,
        "end": 5.171650975355584,
        "average": 3.66762424332902
      },
      "rationale_metrics": {
        "rouge_l": 0.4666666666666667,
        "text_similarity": 0.8009161949157715,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' but both timestamp values for E1 and E2 are substantially incorrect compared to the ground truth, so the key factual timing information is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'communicating at the IEP table', when does she continue her explanation about parents at the IEP table?",
      "video_id": "J-upF-lwWvg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 46.400000000000006
      },
      "gt_interval": {
        "start": 9.85,
        "end": 12.35
      },
      "pred_interval": {
        "start": 19.976120291088247,
        "end": 27.705743521910545
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.126120291088247,
        "end": 15.355743521910545,
        "average": 12.740931906499396
      },
      "rationale_metrics": {
        "rouge_l": 0.4523809523809524,
        "text_similarity": 0.7766097784042358,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction reproduces similar phrasing but has substantially incorrect timestamps and event boundaries, misidentifies the start/end times, and labels the relation as 'after' instead of the correct 'once_finished', so it fails on key factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'And number five', when does the text 'you don't have the playbook' appear?",
      "video_id": "J-upF-lwWvg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 46.400000000000006
      },
      "gt_interval": {
        "start": 24.8,
        "end": 26.0
      },
      "pred_interval": {
        "start": 39.13383285971394,
        "end": 45.56483833830364
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.33383285971394,
        "end": 19.564838338303637,
        "average": 16.94933559900879
      },
      "rationale_metrics": {
        "rouge_l": 0.5666666666666667,
        "text_similarity": 0.8479888439178467,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps for both E1 and E2 are substantially incorrect compared to the reference, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions they have three presentations, when does she introduce the first presenter, Khadija Mohamed?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.767,
        "end": 45.473
      },
      "pred_interval": {
        "start": 37.0523429414165,
        "end": 37.518980633391564
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.714657058583505,
        "end": 7.954019366608435,
        "average": 5.83433821259597
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272727,
        "text_similarity": 0.6140862107276917,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives a single timestamp (37.05s) that contradicts the correct introduction window (40.767\u201345.473s) and omits the E1 timing and the required 'after' relation, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions revealing the hidden curriculum, when does the slide change to display 'Teaching Scotland's Future'?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 201.8,
        "end": 202.8
      },
      "pred_interval": {
        "start": 5.6,
        "end": 62.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 196.20000000000002,
        "end": 140.8,
        "average": 168.5
      },
      "rationale_metrics": {
        "rouge_l": 0.33333333333333337,
        "text_similarity": 0.6725262403488159,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the reported timestamps are completely wrong (predicted E1 at 5.6s vs correct 166.5\u2013169.8s; predicted E2 ~58.8\u201362.0s vs correct ~201.8\u2013202.8s), so it fails on factual timing."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing teacher educators, when does a blue speech bubble with the question 'Is the team all right?' appear on the screen?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 245.5,
        "end": 258.7
      },
      "pred_interval": {
        "start": 62.9,
        "end": 65.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 182.6,
        "end": 193.39999999999998,
        "average": 188.0
      },
      "rationale_metrics": {
        "rouge_l": 0.23999999999999996,
        "text_similarity": 0.5253287553787231,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys a simultaneous/at-the-same-time relationship, but the timestamps and element types are largely incorrect (completely different start/end times, wrong description of E1/E2) and it omits the correct disappearance time, so it fails to match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes discussing the challenges faced by collectivist teachers, when does she state the research question about how minority ethnic teachers use their cultural, religious, and linguistic skills?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 390.6,
        "end": 394.78
      },
      "pred_interval": {
        "start": 58.0,
        "end": 62.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 332.6,
        "end": 332.78,
        "average": 332.69
      },
      "rationale_metrics": {
        "rouge_l": 0.12903225806451615,
        "text_similarity": 0.3908255994319916,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (58.0s \u2192 62.0s) are completely inconsistent with the correct timestamps (E1 ends 390.5s, E2 390.6\u2013394.78s), so it is factually incorrect and misrepresents the relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker quotes Ladson-Billings about Critical Race Theory, when does she explain what Critical Race Theory helps to do?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.0,
        "end": 427.3
      },
      "pred_interval": {
        "start": 113.8,
        "end": 120.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 306.2,
        "end": 306.5,
        "average": 306.35
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.32359519600868225,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the explanation occurs after the quote but gives completely different timestamps (113.8/120.8s vs. reference 419.6/420.0s) and misrepresents the near-immediate transition in the ground truth, so the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the 'racial microaggression framework', when does she mention the time period 'the early 1970s to the 1990s'?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 517.1,
        "end": 519.1
      },
      "pred_interval": {
        "start": 16.3,
        "end": 22.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 500.8,
        "end": 497.1,
        "average": 498.95000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951808,
        "text_similarity": 0.5033460855484009,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly states that the time period is mentioned after the discussion of the racial microaggression framework; its additional note about historical context does not contradict the reference."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker shares discussions from her study, when does the first speech bubble graphic appear on the 'Cultural and Linguistic experiences' slide?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 574.2,
        "end": 574.9
      },
      "pred_interval": {
        "start": 119.5,
        "end": 137.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 454.70000000000005,
        "end": 437.9,
        "average": 446.3
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.4989423155784607,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the speech bubble appears while the speaker discusses her study (matching the core timing), but it omits the precise timestamps given in the reference and adds unsupported details about teacher\u2013student interaction and a thematic shift that are not in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions a teacher in Edinburgh trying to talk about the slave trade issue with her class, when does the speaker state that the discussion 'went terribly wrong'?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 721.0,
        "end": 722.451
      },
      "pred_interval": {
        "start": 34.451851851851856,
        "end": 40.77777777777778
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 686.5481481481481,
        "end": 681.6732222222222,
        "average": 684.1106851851852
      },
      "rationale_metrics": {
        "rouge_l": 0.27956989247311825,
        "text_similarity": 0.6357866525650024,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly preserves the 'after' relation (the 'went terribly wrong' remark follows the Edinburgh-teacher mention), but the provided timestamps/durations are entirely inconsistent with the reference and key timing details are missing, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the teacher asks Muslim children to draw an image of what they think God looks like, when does the speaker describe the Muslim kids just sitting there and staring?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.761,
        "end": 793.029
      },
      "pred_interval": {
        "start": 45.592592592592595,
        "end": 50.592592592592595
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 701.1684074074074,
        "end": 742.4364074074074,
        "average": 721.8024074074074
      },
      "rationale_metrics": {
        "rouge_l": 0.19780219780219782,
        "text_similarity": 0.6008633375167847,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target event describing the kids sitting and staring, but it misstates both event timings and the anchor event content, adds an unfounded quote, and gives an incorrect relation label, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the teacher asks what's happening in Burma, when do the children ask why it's not in the news?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 897.0,
        "end": 899.8
      },
      "pred_interval": {
        "start": 60.92748899599809,
        "end": 63.87636289443797
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 836.0725110040019,
        "end": 835.923637105562,
        "average": 835.9980740547819
      },
      "rationale_metrics": {
        "rouge_l": 0.24489795918367346,
        "text_similarity": 0.5047352313995361,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the children ask after the teacher, but the reported interval (~60.93s) is far from the actual relative gap (~4\u20137s based on the provided timestamps), so the timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the 'Using a microaggression framework' slide, when does the speaker list the instances of lessons that negate, nullify, exclude, or marginalize?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 950.0,
        "end": 951.0
      },
      "pred_interval": {
        "start": 65.44987945876396,
        "end": 67.02731792484605
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 884.5501205412361,
        "end": 883.9726820751539,
        "average": 884.261401308195
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.3140472173690796,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly describes that the speaker lists the instances, but the provided timestamps (\u224865.45\u201367.03s) do not match the reference times (950\u2013951s within a slide shown 943\u2013969s), so the timing is incorrect and inconsistent."
      }
    },
    {
      "question_id": "001",
      "question": "Once Stella finishes inviting Jacqueline to introduce herself, when does Jacqueline begin speaking?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1145.3,
        "end": 1146.2
      },
      "pred_interval": {
        "start": 249.4,
        "end": 255.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 895.9,
        "end": 891.0,
        "average": 893.45
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.8124016523361206,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts key facts: the timestamps differ drastically and the temporal relation is given as 'after' rather than the immediate 'once_finished'. It also adds an unsupported visual cue, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Khadija finishes her presentation and apology, when does Stella begin thanking her?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1119.9,
        "end": 1121.2
      },
      "pred_interval": {
        "start": 601.2,
        "end": 605.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 518.7,
        "end": 516.0,
        "average": 517.35
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.7037183046340942,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the reference on both timestamps and speaker identities (wrong times and speakers) and introduces an unfounded visual-cue claim; it does not match the correct events."
      }
    },
    {
      "question_id": "003",
      "question": "After Jacqueline says, 'Let me just see if I can share these slides with you', when does her presentation's title slide appear in full screen?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1177.8,
        "end": 1182.0
      },
      "pred_interval": {
        "start": 729.8,
        "end": 736.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 448.0,
        "end": 445.79999999999995,
        "average": 446.9
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.759429931640625,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies both events and their timestamps (major time and event-type mismatches) and only matches the 'after' relation; it does not align with the correct answer's events or times."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes talking about how students of color were feeling left out at the secondary school, when does she transition to the slide about 'Race in Scotland'?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1260.0,
        "end": 1269.32
      },
      "pred_interval": {
        "start": 31.3,
        "end": 40.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1228.7,
        "end": 1228.52,
        "average": 1228.6100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2413793103448276,
        "text_similarity": 0.5245530605316162,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the transition occurs after that discussion, but it omits the precise timing details (1259s/1260s and the subsequent phrase ending at 1269.32s) given in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that only 40% of Scottish people surveyed agreed that immigrants make Scotland a better place, when does she mention the number of hate crimes reported in Scotland?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1312.001,
        "end": 1323.455
      },
      "pred_interval": {
        "start": 60.8,
        "end": 72.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1251.201,
        "end": 1251.355,
        "average": 1251.278
      },
      "rationale_metrics": {
        "rouge_l": 0.14545454545454545,
        "text_similarity": 0.4983443319797516,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer is vague and does not provide the timing information requested; the correct answer specifies precise timestamps (speaker mentions hate crimes from 1312.001s and finishes at 1323.455s), which the prediction omits. The prediction is only loosely related and misses key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that colourblind racism leads to the omission and avoidance of race issues in the classroom, when does she mention that teachers fear talking about race?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1376.326,
        "end": 1379.45
      },
      "pred_interval": {
        "start": 114.5,
        "end": 129.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1261.826,
        "end": 1250.15,
        "average": 1255.988
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.6477302312850952,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the comment occurs afterwards, but it is vague and misidentifies what was just discussed (omission/avoidance of race in classrooms vs 'issues faced by students of color') and omits the specific timing relationship given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that Critical Race Theory \"really underpins everything that I do when it comes to research\", when does she start discussing the \"Methods\" of their research?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1453.971,
        "end": 1486.746
      },
      "pred_interval": {
        "start": 11.2,
        "end": 11.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1442.771,
        "end": 1475.1460000000002,
        "average": 1458.9585000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.11363636363636362,
        "text_similarity": 0.16905297338962555,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction notes a topical shift to 'Methods' but fails to provide the required timestamps, anchor/target boundaries, or quoted phrases; it also asserts a slide transition not specified in the reference, so key factual details are missing and some content is unsupported."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes the first tenet of Critical Race Theory as the assertion that \"racism is normal, it's ordinary, it's systemic\", when does she describe the next tenet?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1440.328,
        "end": 1450.256
      },
      "pred_interval": {
        "start": 44.8,
        "end": 46.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1395.528,
        "end": 1404.056,
        "average": 1399.792
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.1391541212797165,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives no timestamps and wrongly claims a shift to research methods rather than identifying the immediate following segment that describes the second tenet, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that students wholeheartedly disagreed with the idea of equal chance regardless of race/ethnicity, when does she introduce Omar's quote about teachers trying to motivate them?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1613.0,
        "end": 1618.0
      },
      "pred_interval": {
        "start": 44.43333333333334,
        "end": 53.611111111111114
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1568.5666666666666,
        "end": 1564.388888888889,
        "average": 1566.4777777777776
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.5239441394805908,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the quote occurs after the prior statement but gives a non-matching time (44.43s vs. 1613.0s\u20131618.0s) and adds an unsupported detail about a slide, so it is largely incorrect and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says she emailed 31 schools, when does she state how many responses she received?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1782.3,
        "end": 1783.5
      },
      "pred_interval": {
        "start": 25.371553515010604,
        "end": 30.285387703701094
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1756.9284464849893,
        "end": 1753.2146122962988,
        "average": 1755.0715293906442
      },
      "rationale_metrics": {
        "rouge_l": 0.12000000000000001,
        "text_similarity": 0.09362612664699554,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states she received one response but fails to answer the asked 'when' (no timestamps or anchor/target relation provided) and adds an interpretive comment about participation, making it incomplete relative to the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that one of the secured schools dropped out before recruiting, when does she mention that the second school dropped out a day before the focus groups began?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1818.0,
        "end": 1825.4
      },
      "pred_interval": {
        "start": 47.665051232187395,
        "end": 52.025907374903376
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1770.3349487678126,
        "end": 1773.3740926250966,
        "average": 1771.8545206964545
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.34341880679130554,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes two schools dropped out but contradicts the reference timing (saying both dropped out before recruiting) and adds an unsupported reason ('lack of interest') while omitting the specific timestamps and that the second dropout occurred a day before focus groups."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the gatekeeper's reason for not including Black students, when does she state that the gatekeeper didn't ask them to participate?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1907.394,
        "end": 1909.8
      },
      "pred_interval": {
        "start": 67.79245073316729,
        "end": 71.016938305302
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1839.6015492668328,
        "end": 1838.783061694698,
        "average": 1839.1923054807653
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.35492897033691406,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the causal/temporal relation that the statement follows the gatekeeper's explanation, but it omits the precise timestamps and the explicit note that the non-participation statement immediately follows as a direct consequence."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says teachers avoided the topic because they didn't want to be seen as racist, when does she explain that a lack of interest convergence led to inaction?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1959.768,
        "end": 1967.955
      },
      "pred_interval": {
        "start": 45.8,
        "end": 62.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1913.968,
        "end": 1905.455,
        "average": 1909.7115
      },
      "rationale_metrics": {
        "rouge_l": 0.21951219512195122,
        "text_similarity": 0.6234240531921387,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the explanation occurs after the comment about teachers, but it gives incorrect timestamps (45.8\u201362.5s) that contradict the reference segments (~1971\u20131979s), so it is factually inaccurate despite capturing the order."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the slide titled 'The Normalcy of Racism', when does the speaker explain why whiteness is considered neutral?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1991.32,
        "end": 2002.951
      },
      "pred_interval": {
        "start": 70.0,
        "end": 74.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1921.32,
        "end": 1928.951,
        "average": 1925.1354999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.7045504450798035,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the explanation occurs during the 'The Normalcy of Racism' slide, but the provided timestamps (70.0\u201374.0s) are completely inconsistent with the ground-truth interval (1991.320\u20132002.951s) and thus factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining Rashida's reasoning for not wearing a hijab, when does she mention Omar's comment about jokes hurting?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2082.096,
        "end": 2085.039
      },
      "pred_interval": {
        "start": 79.9,
        "end": 84.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2002.196,
        "end": 2000.3390000000002,
        "average": 2001.2675
      },
      "rationale_metrics": {
        "rouge_l": 0.15873015873015875,
        "text_similarity": 0.4878086745738983,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states Omar's comment occurs after Rashida's point, but the provided timestamps (79.9\u201384.7s) do not match the reference times (E2 at 2082.096\u20132085.039s), so the timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker discusses students internalizing their experience of 'othering' and exclusion, when does she mention that teachers would mostly ignore comments or engage in biased language?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2148.8,
        "end": 2166.4
      },
      "pred_interval": {
        "start": 48.5,
        "end": 51.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2100.3,
        "end": 2114.8,
        "average": 2107.55
      },
      "rationale_metrics": {
        "rouge_l": 0.3658536585365853,
        "text_similarity": 0.6336497068405151,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys the relative order (after) but the timestamps are wildly incorrect compared to the ground truth and it fails to provide the correct interval for the teachers' comments, making it factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states she is glad to have been part of the study, when does she say that the students did not feel heard or supported?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2205.5,
        "end": 2210.0
      },
      "pred_interval": {
        "start": 161.1,
        "end": 165.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2044.4,
        "end": 2044.5,
        "average": 2044.45
      },
      "rationale_metrics": {
        "rouge_l": 0.21978021978021978,
        "text_similarity": 0.464352011680603,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly identifies the content (students not feeling heard) and a subsequent recommendations transition, but the timing is substantially incorrect compared to the reference (161.1s vs. 2205.5s+) and thus fails to match the key temporal facts; overall largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes her statement about acknowledging bias being scary but important, when does the slide transition to show the 'Schools can:' recommendations?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2333.0,
        "end": 2333.5
      },
      "pred_interval": {
        "start": 197.5,
        "end": 202.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2135.5,
        "end": 2131.0,
        "average": 2133.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285714,
        "text_similarity": 0.6399441957473755,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer preserves the sequence (speaker then slide) but the timestamps are completely incorrect compared to the ground truth (2328.0s and 2333.0\u20132333.5s vs 197.5s and 202.5s), so it fails on factual correctness."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces implementing curriculum that explicitly includes race talk and counter narratives, when does she explain how teachers can learn to put this into practice?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2345.1,
        "end": 2369.5
      },
      "pred_interval": {
        "start": 49.0,
        "end": 198.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2296.1,
        "end": 2171.5,
        "average": 2233.8
      },
      "rationale_metrics": {
        "rouge_l": 0.0930232558139535,
        "text_similarity": 0.2430620789527893,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely incorrect timestamps and adds unsupported phrasing; it does not match the reference E1/E2 time spans or quoted segments. Any loose similarity (that teachers are addressed later) is insufficient given the major factual errors."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker mentions that the racial equality framework from a couple of years ago is 'very general,' when does she suggest specific, practical training would be more helpful?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2453.2,
        "end": 2458.5
      },
      "pred_interval": {
        "start": 199.0,
        "end": 252.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2254.2,
        "end": 2206.5,
        "average": 2230.35
      },
      "rationale_metrics": {
        "rouge_l": 0.1388888888888889,
        "text_similarity": 0.2189697027206421,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps are completely different from the correct ones (off by a large margin) and do not match the annotated start/end times or quoted segment, so the answer is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker says 'Thank you', when does the title slide for the presentation appear on screen and is acknowledged?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2535.7,
        "end": 2536.7
      },
      "pred_interval": {
        "start": 25.8,
        "end": 26.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2509.8999999999996,
        "end": 2510.3999999999996,
        "average": 2510.1499999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.40972355008125305,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: times differ drastically from the reference, the acknowledged speaker/name is wrong, and it contradicts the correct timing and acknowledgement details (including the 'Yes, we can see it' event)."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'Outline' slide, when does Dr. Nighet Riaz state that racism is normalized in everyday interactions?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2593.7,
        "end": 2602.4
      },
      "pred_interval": {
        "start": 38.1,
        "end": 44.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2555.6,
        "end": 2558.3,
        "average": 2556.95
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.141926571726799,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly states the event occurs 'during' the Outline slide but gives a vastly incorrect timestamp (38.1s vs the correct 2593.7\u20132602.4s within 2581.9\u20132627.0s), so the key temporal information is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Nighet Riaz states her name, when does she explain her roles at the University of the West of Scotland and Advance HE?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2550.6,
        "end": 2567.6
      },
      "pred_interval": {
        "start": 30.5,
        "end": 35.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2520.1,
        "end": 2532.5,
        "average": 2526.3
      },
      "rationale_metrics": {
        "rouge_l": 0.1282051282051282,
        "text_similarity": 0.3362647294998169,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the roles (Associate Lecturer and a role with Advance(d) HE) and that they occur after her name, but the timestamps are substantially wrong and it omits the mention of Khadijah Muhammad and the correct event timing, so it is only partially accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions disrupting the narrative, when does she start discussing global citizenship through the lens of the beneficent other?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2700.74,
        "end": 2707.407
      },
      "pred_interval": {
        "start": 484.3333333333333,
        "end": 508.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2216.4066666666663,
        "end": 2199.407,
        "average": 2207.9068333333335
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.4273609519004822,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (484.3s / ~508.0s) are wildly inconsistent with the reference segment (2700.079\u20132707.407s), so it fails to correctly locate the start of the 'beneficent other' discussion."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing global citizenship and Scotland's curricular intention to imbricate it through the curriculum, when does she specifically mention it presenting a cross-curricular and whole school approach?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2790.853,
        "end": 2797.969
      },
      "pred_interval": {
        "start": 664.0,
        "end": 687.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2126.853,
        "end": 2110.969,
        "average": 2118.911
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.4292462468147278,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (664s and 687s) do not match the correct timestamps (~2783s\u20132798s), and it adds a visual slide claim not present in the reference\u2014thus it is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the slide changes to 'Decolonising the Curriculum', when does the speaker begin talking about that specific term?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2851.15,
        "end": 2854.233
      },
      "pred_interval": {
        "start": 820.6666666666667,
        "end": 841.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2030.4833333333333,
        "end": 2012.7330000000002,
        "average": 2021.6081666666669
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.6118270754814148,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted times (820.6s slide change, 841.5s speech) conflict strongly with the reference times (~2850.5s slide fully visible, speech starting ~2851.15s\u20132854.233s), so it is incorrect and not semantically aligned."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that decolonisation moves out of a western framework, when does she start mentioning Sophia Kell?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2904.8,
        "end": 2905.4
      },
      "pred_interval": {
        "start": 17.56278502148452,
        "end": 19.581534345920083
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2887.2372149785156,
        "end": 2885.8184656540802,
        "average": 2886.527840316298
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.3434412479400635,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that Sophia Kell is mentioned after the decolonisation remark, but it gives an incorrect timestamp (01:04 vs. ~48:24) and omits the precise immediate succession and exact time range specified in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes her journey from compulsory to higher education, when does she identify internalized racism as a large part of her imposter syndrome?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3077.4,
        "end": 3085.5
      },
      "pred_interval": {
        "start": 22.8,
        "end": 29.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3054.6,
        "end": 3056.5,
        "average": 3055.55
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.40777453780174255,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted times correspond to the reference events: 22.8s matches the journey segment and 29.0s falls within the interval where she identifies internalized racism, preserving the 'after' relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker recounts a SAMI colleague advising her to channel frustration into something positive, when does she mention the planning of the project called Humari Pehchan?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3060.6,
        "end": 3064.5
      },
      "pred_interval": {
        "start": 31.3,
        "end": 45.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3029.2999999999997,
        "end": 3019.2,
        "average": 3024.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2424242424242424,
        "text_similarity": 0.3924465775489807,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly preserves the order (after) and names the project, and its first timestamp matches the reference when converted to relative time, but the predicted time for the project planning (45.3s) does not align with the reference interval (approx. 48.4\u201352.3s) and the answer omits the specified time ranges."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker outlines the project's aim to bring families, schools, and community organizations together for storytelling, when does she state that the project is a call for action and a provocation?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3121.9,
        "end": 3125.0
      },
      "pred_interval": {
        "start": 57.2,
        "end": 61.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3064.7000000000003,
        "end": 3063.3,
        "average": 3064.0
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.5455495119094849,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies that the 'call for action and a provocation' comes after the aims and includes both terms, giving a matching relative timestamp; it slightly differs by providing a single relative second rather than the full time interval in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the research methodology's approach to power imbalances, when does she introduce the project's aim for children and parents?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3288.427,
        "end": 3304.244
      },
      "pred_interval": {
        "start": 5.5,
        "end": 59.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3282.927,
        "end": 3245.0440000000003,
        "average": 3263.9855000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.22602578997612,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation (the aim is introduced after the methodology discussion) but omits the key factual details in the reference\u2014specifically the precise start and end timestamps\u2014so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker concludes the description of story development using heritage language, when does she start listing the project participants?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3305.525,
        "end": 3316.596
      },
      "pred_interval": {
        "start": 11.8,
        "end": 59.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3293.725,
        "end": 3257.396,
        "average": 3275.5605
      },
      "rationale_metrics": {
        "rouge_l": 0.1276595744680851,
        "text_similarity": 0.33060818910598755,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (participants are listed after the heritage-language discussion) but omits all the required precise timing information and extent of the list, so it is incomplete relative to the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions the project was postponed to September, when does she state the duration of the project and its output?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3369.8,
        "end": 3378.21
      },
      "pred_interval": {
        "start": 59.5,
        "end": 60.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3310.3,
        "end": 3317.71,
        "average": 3314.005
      },
      "rationale_metrics": {
        "rouge_l": 0.32558139534883723,
        "text_similarity": 0.608767032623291,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that duration and output follow the postponement but omits all required timestamps (3369.800s\u20133378.210s) and specific temporal relation details, so it fails to provide the key factual elements. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (Niget) finishes saying 'thank you', when does the other speaker (Katerina) start thanking her?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.516,
        "end": 3415.117
      },
      "pred_interval": {
        "start": 35.2,
        "end": 37.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3378.3160000000003,
        "end": 3377.717,
        "average": 3378.0165
      },
      "rationale_metrics": {
        "rouge_l": 0.15584415584415587,
        "text_similarity": 0.4664165675640106,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction refers to different speakers, times, and events (Diane/Sophie at ~35s) and adds visual details, which contradicts the correct answer about Niget/Katerina at ~3413s and an immediate once_finished relation; it is therefore incorrect and hallucinated."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker in the bottom left talks about extreme prevent agendas, when does she talk about overt acts of racism in schools?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3579.7,
        "end": 3585.6
      },
      "pred_interval": {
        "start": 47.1,
        "end": 50.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3532.6,
        "end": 3535.2999999999997,
        "average": 3533.95
      },
      "rationale_metrics": {
        "rouge_l": 0.3863636363636364,
        "text_similarity": 0.7956299781799316,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both the anchor and target events and that the target occurs after the anchor; timestamps appear to use a different (relative) timebase and the anchor end time is omitted, so minor timing/format differences warrant a small deduction."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker in the bottom right discusses the concept of 'racism without racists', when does she explain that the system itself is inherently racist?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3628.4,
        "end": 3675.8
      },
      "pred_interval": {
        "start": 50.5,
        "end": 54.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3577.9,
        "end": 3621.7000000000003,
        "average": 3599.8
      },
      "rationale_metrics": {
        "rouge_l": 0.3695652173913044,
        "text_similarity": 0.8557450771331787,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies the relative relation ('after') but the absolute timestamps are completely inaccurate compared to the reference, so it fails to match the anchor/target timings provided."
      }
    },
    {
      "question_id": "001",
      "question": "After the host thanks the speakers, when does she introduce a question for Jackie?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3771.2,
        "end": 3772.9
      },
      "pred_interval": {
        "start": 458.0,
        "end": 532.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3313.2,
        "end": 3240.9,
        "average": 3277.05
      },
      "rationale_metrics": {
        "rouge_l": 0.3389830508474576,
        "text_similarity": 0.6915847063064575,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their order, but the timestamps are grossly inaccurate compared to the reference (off by thousands of seconds) and it omits the end times, so it fails on factual correctness."
      }
    },
    {
      "question_id": "002",
      "question": "After Jackie states that the US has been using CRT in anti-discrimination training, when does she mention that it hasn't been implemented in Scotland?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3861.8,
        "end": 3866.4
      },
      "pred_interval": {
        "start": 638.0,
        "end": 675.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3223.8,
        "end": 3191.4,
        "average": 3207.6000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.12658227848101267,
        "text_similarity": 0.6115111112594604,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the sequence (US then Scotland) but misattributes the speaker (says 'host' instead of Jackie), gives incorrect timestamps (completely different from the reference) and omits end times, so it only partially matches."
      }
    },
    {
      "question_id": "003",
      "question": "After Jackie concludes her answer about diversity being everyone's issue, when does the host pick up on Greg's statement and question?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3918.884,
        "end": 3925.172
      },
      "pred_interval": {
        "start": 860.0,
        "end": 898.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3058.884,
        "end": 3027.172,
        "average": 3043.0280000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.6253971457481384,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (E2 after E1) but the timestamps are far off from the ground truth and it misattributes Jackie\u2019s answer to the host; key factual elements (correct absolute times and speaker) are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman in the top-right finishes her sentence about normalising provocation, when does the woman in the top-left start speaking about a national campaign?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3954.0,
        "end": 4036.3
      },
      "pred_interval": {
        "start": 3941.0,
        "end": 4080.7
      },
      "iou": 0.5891195418754495,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 44.399999999999636,
        "average": 28.699999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.6229800581932068,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both answers label the relation as 'after', the predicted timings and content are largely incorrect and include hallucinated details (wrong start/end times and topic); it thus fails to match the reference's key temporal and semantic facts."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman in the top-right finishes saying they have 10 minutes before wrapping up, when does the woman in the bottom-left start speaking?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4129.3,
        "end": 4130.2
      },
      "pred_interval": {
        "start": 25.911111111111115,
        "end": 37.01111111111111
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4103.388888888889,
        "end": 4093.188888888889,
        "average": 4098.288888888888
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.6154344081878662,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: timestamps and event assignments are incorrect and it hallucinates quoted speech; although both state E2 occurs after E1, the temporal details and relation ('once_finished' vs generic 'after') do not match."
      }
    },
    {
      "question_id": "002",
      "question": "After Lisa (woman in bottom-left) asks if anyone would like to put their hand up, when is the next time an unidentified woman speaks to assist her?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4214.2,
        "end": 4215.3
      },
      "pred_interval": {
        "start": 47.41111111111111,
        "end": 56.41111111111111
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4166.788888888888,
        "end": 4158.888888888889,
        "average": 4162.8388888888885
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.542197048664093,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures the coarse 'after' relation; its absolute/relative timestamps are substantially different from the ground truth and it mislabels speakers and utterances, so it fails to match the correct timing and speaker details."
      }
    },
    {
      "question_id": "003",
      "question": "After Lisa (woman in bottom-left) says 'You have the floor' to Ken, when does Ken (man in bottom-right) start asking his question?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4227.7,
        "end": 4304.7
      },
      "pred_interval": {
        "start": 56.31111111111112,
        "end": 60.111111111111114
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4171.388888888889,
        "end": 4244.5888888888885,
        "average": 4207.988888888889
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.5679754018783569,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both answers agree the relation is 'after', the predicted timestamps and event descriptions do not match the reference (wrong E1 timing/description, incorrect E2 start/end and missing the long question completion), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (top right) says she was hoping to be told who the next Education Minister was, when does the speaker (top left) say she was excited for an announcement about a new education minister?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4405.248,
        "end": 4411.436
      },
      "pred_interval": {
        "start": 400.0824856557803,
        "end": 410.39795653625123
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4005.1655143442194,
        "end": 4001.0380434637486,
        "average": 4003.1017789039843
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131148,
        "text_similarity": 0.5498830676078796,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative ordering (target follows anchor) but gives entirely wrong timestamps and incorrect event boundaries/names, so it is largely misaligned with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (top right) says there are many voices missing within policy construction, when does the speaker (top left) start talking about Kokab Stewart?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4411.577,
        "end": 4421.13
      },
      "pred_interval": {
        "start": 213.84691794650175,
        "end": 222.65285546265596
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4197.730082053498,
        "end": 4198.477144537344,
        "average": 4198.103613295421
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5798141956329346,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies speakers and provides completely different timestamps and event descriptions than the reference; the only tenuous match is the implied order (target after anchor), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (bottom left) says 'I am a lowly teacher more so than a policy expert', when does she start talking about mandated teacher training in anti-racism?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4463.676,
        "end": 4476.028
      },
      "pred_interval": {
        "start": 382.55892481266045,
        "end": 403.86787263914863
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4081.11707518734,
        "end": 4072.1601273608517,
        "average": 4076.638601274096
      },
      "rationale_metrics": {
        "rouge_l": 0.16470588235294117,
        "text_similarity": 0.5041635632514954,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the anti-racism discussion follows the self-introduction, but the provided timestamps and event boundaries are wildly inaccurate compared to the reference (off by thousands of seconds and mislabeling E1/E2), so it fails to match the correct timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that a policy won't translate into practice without teacher buy-in, when does she start talking about specific and mandated training for teachers?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4487.6,
        "end": 4494.5
      },
      "pred_interval": {
        "start": 20.4,
        "end": 29.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4467.200000000001,
        "end": 4464.7,
        "average": 4465.950000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.6708551645278931,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect: its anchor and target timestamps differ drastically from the reference, the target occurs much later than specified, and the relation ('after') fails to capture the immediate 'once_finished' adjacency; it only matches the topic phrase but not the required timings or relation."
      }
    },
    {
      "question_id": "002",
      "question": "After David finishes asking if it's appropriate to apply critical race theory to Irish racism in Scotland, when does Jackie respond?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4674.198,
        "end": 4676.359
      },
      "pred_interval": {
        "start": 4589.9,
        "end": 4605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 84.29800000000068,
        "end": 71.35900000000038,
        "average": 77.82850000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.7019700407981873,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets only the temporal relation ('after') right but the anchor/target timestamps and quoted utterances are completely different from the ground truth (wrong time ranges and missing the target 'yes'), so it largely fails to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once Jackie finishes saying she will stop before making a political boo-boo, when does the speaker (top right) begin to discuss critical race theory's applicability beyond color racial issues?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4576.21,
        "end": 4586.726
      },
      "pred_interval": {
        "start": 4618.4,
        "end": 4638.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.1899999999996,
        "end": 51.67399999999998,
        "average": 46.93199999999979
      },
      "rationale_metrics": {
        "rouge_l": 0.17142857142857143,
        "text_similarity": 0.7028084993362427,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect: both E1 and E2 timestamps differ substantially from the ground truth, the quoted line appears hallucinated, and the temporal relation ('after') does not match the specified 'once_finished' immediate relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once the top-left female speaker finishes stating she is looking forward to inviting Nicola to present her work, when does she explain the reason for the invitation?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4663.9,
        "end": 4672.2
      },
      "pred_interval": {
        "start": 4650.0,
        "end": 4654.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.899999999999636,
        "end": 18.0,
        "average": 15.949999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680854,
        "text_similarity": 0.6056075096130371,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the general order (explanation follows the invitation) but the timestamps are incorrect by many seconds and it introduces unsupported details (e.g., 'young people' and 'critical race theory'), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the top-left female speaker asks if there are any more questions, when does the top-right female speaker begin to answer a question from 'Sarah Khan'?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4697.9,
        "end": 4703.6
      },
      "pred_interval": {
        "start": 4690.7,
        "end": 4732.0
      },
      "iou": 0.13801452784505333,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.199999999999818,
        "end": 28.399999999999636,
        "average": 17.799999999999727
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962028,
        "text_similarity": 0.47965553402900696,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction provides substantially incorrect timings and adds unrelated detail (inviting Nicola); it places the start at 4690.7s (during the anchor) and ends at 4732.0s, contradicting the reference start at 4697.9s and the shorter target interval."
      }
    },
    {
      "question_id": "003",
      "question": "After the top-right female speaker mentions that UWS has included a clear statement in placement handbooks, when does she describe the challenge of students having the confidence to speak up about discrimination?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4738.4,
        "end": 4750.8
      },
      "pred_interval": {
        "start": 5332.0,
        "end": 5397.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 593.6000000000004,
        "end": 646.1999999999998,
        "average": 619.9000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777776,
        "text_similarity": 0.4600417912006378,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct event sequence and content (the speaker describing students' reluctance), but the timestamps are substantially incorrect and the duration is exaggerated, so it fails to match the ground-truth timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the top-left woman finishes speaking about BAME students experiencing racism and leaving, when does the bottom-right woman begin speaking about the important message?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4841.8,
        "end": 4845.0
      },
      "pred_interval": {
        "start": 51.1,
        "end": 53.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4790.7,
        "end": 4792.0,
        "average": 4791.35
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.4967007040977478,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'after' relation but gives an incorrect start time (51.1s vs the correct 4841.8s absolute / ~4.8s after the top-left finish) and omits the bottom-right finish time, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the bottom-right woman states that including an explicit statement in handbooks can make a difference in a student's life, when does she mention that they only have four minutes left?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4880.8,
        "end": 4897.0
      },
      "pred_interval": {
        "start": 57.2,
        "end": 58.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4823.6,
        "end": 4838.4,
        "average": 4831.0
      },
      "rationale_metrics": {
        "rouge_l": 0.15789473684210525,
        "text_similarity": 0.45128464698791504,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single relative time (57.2s) that does not match the correct timestamps (4880.8\u20134897.0s) and omits the precise begin/end times; therefore it is essentially incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the bottom-right woman finishes describing the story from the book 'Whistling Vivaldi', when does she mention that there are many other examples in that book?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4986.36,
        "end": 4989.384
      },
      "pred_interval": {
        "start": 62.7,
        "end": 64.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4923.66,
        "end": 4924.684,
        "average": 4924.1720000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.3771350383758545,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly locates the mention within the segment (62.7s), which corresponds closely to the correct interval start (4986.360s \u2192 ~62.46s relative) and matches the described relation (occurs immediately after finishing)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker expresses her upset about the schools' reluctance to participate, when does she mention Negat having difficulties with recruitment?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5023.8,
        "end": 5035.8
      },
      "pred_interval": {
        "start": 33.3,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4990.5,
        "end": 4999.2,
        "average": 4994.85
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.4818267226219177,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation right but fails on key details: E1/E2 timestamps are incorrect or inconsistent (mixing absolute and relative times), and E2 is misidentified as the schools' reluctance rather than Negat's recruitment difficulties, so it does not match the reference events."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'there is no neutral position' for the first time following Jackie's quote, when does she say it again?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5084.475,
        "end": 5085.8
      },
      "pred_interval": {
        "start": 62.6,
        "end": 65.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5021.875,
        "end": 5019.900000000001,
        "average": 5020.887500000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6065020561218262,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely mislabels and mis-times the events (timestamps 5010.0s and 62.6s vs. correct ~5080\u20135085s) and fails to identify the two specific successive occurrences; the relation 'after' is only a loose match to 'next' but does not compensate for the factual errors."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes quoting the first Martin Luther King line, when does she start quoting the second one about accepting evil?",
      "video_id": "0S2MoSiyLiw",
      "video_number": "015",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5105.5,
        "end": 5114.1
      },
      "pred_interval": {
        "start": 71.4,
        "end": 74.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5034.1,
        "end": 5039.400000000001,
        "average": 5036.75
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.553714394569397,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are largely incorrect and inconsistent with the reference (e.g., E1 given as a start at 5010.0s vs the correct E1 finishing at 5104.7s, and E2 erroneously at 71.4s), and it misrepresents the event boundaries; while 'after' is directionally similar to 'once_finished', the prediction fails on the key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that the team is going to do a quick introduction, when does Ashley Satri introduce herself?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.156,
        "end": 102.754
      },
      "pred_interval": {
        "start": 48.95833333333333,
        "end": 51.651785714285715
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.19766666666668,
        "end": 51.10221428571429,
        "average": 43.14994047619048
      },
      "rationale_metrics": {
        "rouge_l": 0.15053763440860213,
        "text_similarity": 0.7355999946594238,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the temporal relationship ('after') but misidentifies the anchor (wrong speaker/timing) and gives incorrect timestamps for both events (E1 and E2), failing to match the reference intervals and key details."
      }
    },
    {
      "question_id": "002",
      "question": "Next, after Ashley Satri finishes introducing herself, when does Carly Thibodeau introduce herself?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 106.54,
        "end": 116.471
      },
      "pred_interval": {
        "start": 65.11309523809524,
        "end": 68.1483630952381
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.426904761904765,
        "end": 48.32263690476191,
        "average": 44.874770833333336
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.831049382686615,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the ground truth: it mislabels the anchor, provides entirely different timestamps (71s range vs. 102.754s and 106.54\u2013116.471s), and fails to identify Carly's correct introduction window."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the \"IDEA Supervision, Monitoring, and Support team\", when does she begin listing other teams within that department?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 184.641,
        "end": 188.788
      },
      "pred_interval": {
        "start": 123.625,
        "end": 158.625
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.01599999999999,
        "end": 30.16300000000001,
        "average": 45.5895
      },
      "rationale_metrics": {
        "rouge_l": 0.0759493670886076,
        "text_similarity": 0.288327693939209,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps (123.6\u2013158.6s) conflict with the correct times (anchor ends 184.622s, target starts 184.641s), and the prediction introduces team names not specified in the reference; thus it is largely incorrect. "
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is explaining the purpose of an IEP, when does she highlight the phrase \"prepare them\" on the slide?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 285.569,
        "end": 304.375
      },
      "pred_interval": {
        "start": 106.75,
        "end": 132.75
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 178.81900000000002,
        "end": 171.625,
        "average": 175.222
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.5320320129394531,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the highlight occurs during the explanation, but the provided time interval (106.7\u2013132.7s) is far from the reference interval (285.569\u2013304.375s), so the timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes saying \"So please feel free to reach out\", when does the slide transition to \"The IEP Decision Making Process\"?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 324.55,
        "end": 326.4
      },
      "pred_interval": {
        "start": 200.75,
        "end": 202.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.80000000000001,
        "end": 124.14999999999998,
        "average": 123.975
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.48446959257125854,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives times (200.7\u2013202.2s) that directly contradict the correct timestamps (~324.55\u2013326.4s) and omits the anchor timing, so it is factually incorrect and misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks what the IEP meeting is, when does she define it as a communication vehicle?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 348.721,
        "end": 354.0
      },
      "pred_interval": {
        "start": 49.5,
        "end": 58.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 299.221,
        "end": 295.5,
        "average": 297.3605
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142855,
        "text_similarity": 0.6744755506515503,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives a wrong timestamp (49.5s vs ~348\u2013354s) and misstates the relation/anchor, contradicting the ground truth timing and sequence."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the SAU has the ultimate responsibility for FAPE, when does she mention that they will discuss disagreement on the next slide?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.242,
        "end": 409.428
      },
      "pred_interval": {
        "start": 58.9,
        "end": 62.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 345.34200000000004,
        "end": 346.528,
        "average": 345.93500000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.20338983050847456,
        "text_similarity": 0.6182876229286194,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly implies the SAU mention occurs before the discussion of disagreement, but it gives an incorrect timestamp (58.9s vs 400.6\u2013403.8s), omits the second event's timing, and misstates details, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Recap' slide appears, when does the speaker state that the IEP meeting serves as a communication vehicle?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 529.94,
        "end": 533.504
      },
      "pred_interval": {
        "start": 332.6,
        "end": 340.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 197.34000000000003,
        "end": 193.404,
        "average": 195.372
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7658262252807617,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives a wrong timestamp (332.6s) and omits the correct interval (529.94\u2013533.504s); only the qualitative 'after recap' relation matches, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that the SAU has ultimate responsibility for FAPE, when does she mention that dispute resolution options are available?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 544.0,
        "end": 547.0
      },
      "pred_interval": {
        "start": 17.69047619047619,
        "end": 25.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 526.3095238095239,
        "end": 522.0,
        "average": 524.1547619047619
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.6476082801818848,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but fails to match or preserve the event time spans (wrong timestamps and only gives a start for E1 and an end for E2), so it is largely misaligned with the reference timing details."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'Required Participants for IEP Meetings' slide, when does the speaker describe the qualifications of a representative from the SAU?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 600.0
      },
      "pred_interval": {
        "start": 31.30952380952381,
        "end": 34.166666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 552.6904761904761,
        "end": 565.8333333333334,
        "average": 559.2619047619048
      },
      "rationale_metrics": {
        "rouge_l": 0.14925373134328357,
        "text_similarity": 0.6484748721122742,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely mismatched: it gives entirely different timestamps (31.3\u201334.1s vs 521\u2013700s and 584\u2013600s), reverses/mislabels the events, and states the relation is 'after' rather than the correct 'during'."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the special education director, IP coordinator, and assistant principal as part of the team, when does she state that the child must be invited?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.0,
        "end": 713.5
      },
      "pred_interval": {
        "start": 330.2315877859435,
        "end": 370.3814713896459
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 369.7684122140565,
        "end": 343.1185286103541,
        "average": 356.44347041220533
      },
      "rationale_metrics": {
        "rouge_l": 0.29213483146067415,
        "text_similarity": 0.7586656212806702,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps and event boundaries are completely misaligned (predicted ~330\u2013370s vs ground truth 690\u2013713.5s), the event ordering and content do not match the reference, and only the vague mention of 'the child' overlaps."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining the conditions for when a team member's attendance is not necessary, when does she begin discussing the conditions for an excusal?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 769.0,
        "end": 776.0
      },
      "pred_interval": {
        "start": 330.90035804462116,
        "end": 370.9880504580843
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 438.09964195537884,
        "end": 405.0119495419157,
        "average": 421.55579574864726
      },
      "rationale_metrics": {
        "rouge_l": 0.2376237623762376,
        "text_similarity": 0.6827049851417542,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it gives entirely different timestamps (\u2248330s\u2013371s) and wrong event relationships (starts coincide) whereas the reference specifies E1 ending at 755.0s and E2 beginning at 769.0s and ending at 776.0s; the prediction thus contradicts the correct answer and omits key timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks 'What if the parent can't attend the IEP meeting?', when does she begin explaining the requirements for public agencies?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 909.0,
        "end": 924.0
      },
      "pred_interval": {
        "start": 36.3,
        "end": 58.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 872.7,
        "end": 865.3,
        "average": 869.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2298850574712644,
        "text_similarity": 0.6423947215080261,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('after') right but the timestamps are wildly incorrect and it omits the precise end times; it fails to match the key factual timing (870.39s) from the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker discusses offering other methods for parents to attend IEP meetings, when does she explain that a meeting might be conducted without a parent?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 950.0,
        "end": 965.0
      },
      "pred_interval": {
        "start": 282.1,
        "end": 303.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 667.9,
        "end": 661.5,
        "average": 664.7
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.7048623561859131,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies an explanation about conducting a meeting without a parent and that it follows the anchor, but the timestamps and anchor content do not match the reference (predicted ~282\u2013303s vs correct 936\u2013965s) and the relation label differs, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Ashley suggests recording attempts to contact parents in the written notice, when does Carly agree with this recommendation?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1076.3,
        "end": 1077.4
      },
      "pred_interval": {
        "start": 687.9444444444443,
        "end": 717.4444444444443
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 388.3555555555556,
        "end": 359.95555555555575,
        "average": 374.1555555555557
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.5318042635917664,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the relation as 'after,' it mislabels events, uses entirely different and inconsistent timestamps, and even misidentifies the speaker; these major factual errors make it largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ashley finishes posing the question about holding an IEP meeting with only an 18-year-old student, when does Carly offer her initial thought on the matter?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1117.7,
        "end": 1122.7
      },
      "pred_interval": {
        "start": 806.1777777777778,
        "end": 835.7777777777777
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 311.52222222222224,
        "end": 286.92222222222233,
        "average": 299.2222222222223
      },
      "rationale_metrics": {
        "rouge_l": 0.22471910112359553,
        "text_similarity": 0.6089777946472168,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps for both E1 and E2 differ drastically from the ground truth and the temporal relation is mislabeled, so the prediction fails to match the correct events or their relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'determining those present levels', when does she mention the 'Office Hours Archives \u2013 Data Collection Modules' link?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.32,
        "end": 1234.36
      },
      "pred_interval": {
        "start": 18.698412622724263,
        "end": 32.73809523809524
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1214.6215873772758,
        "end": 1201.6219047619047,
        "average": 1208.1217460695902
      },
      "rationale_metrics": {
        "rouge_l": 0.40963855421686746,
        "text_similarity": 0.7820647954940796,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') and provides start times, but the timestamps do not match the reference (and are inconsistently scaled), and it omits the end times given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'determining modifications and/or accommodations', when does she mention the 'MTSS office'?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.054,
        "end": 1294.278
      },
      "pred_interval": {
        "start": 31.13095238095238,
        "end": 44.047619047619044
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1259.9230476190478,
        "end": 1250.230380952381,
        "average": 1255.0767142857144
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7775552272796631,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') and identifies both anchor and target, but it gives very different timestamps and mislabels the target as a visual link appearing rather than the speaker mentioning 'MTSS office', so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'This is an IEP checklist that I think Carly developed', when does she describe what the checklist tells you to do?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1366.516,
        "end": 1375.503
      },
      "pred_interval": {
        "start": 44.047619047619044,
        "end": 53.57142857142857
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1322.468380952381,
        "end": 1321.9315714285713,
        "average": 1322.1999761904763
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.7476991415023804,
        "llm_judge_score": 1,
        "llm_judge_justification": "While both state the temporal relation is 'after', the prediction misidentifies the anchor phrase, the target event, and all timestamps (using 44\u201350s vs correct 1354\u20131375s) and thus fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"number five\", when does the \"Amendments\" slide fully appear on screen?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1443.918,
        "end": 1444.018
      },
      "pred_interval": {
        "start": 17.2,
        "end": 32.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1426.7179999999998,
        "end": 1411.218,
        "average": 1418.9679999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.42105263157894735,
        "text_similarity": 0.6865541934967041,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misreports the key timestamps (gives 17.2s vs correct 1439.347s and 1443.918/1444.018s) and omits the stable appearance time; it also implies the slide appears immediately rather than ~4.6s after the speaker, so it is largely incorrect though it attempts to state a temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the \"IEP Meeting Timelines\" slide appears on screen, when does the speaker state that timelines can be a little bit confusing?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1517.804,
        "end": 1519.454
      },
      "pred_interval": {
        "start": 54.3,
        "end": 57.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1463.5040000000001,
        "end": 1461.954,
        "average": 1462.729
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7638694047927856,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives different timestamps, the wrong quoted phrase, and reverses the order (predicting the speaker before the slide) contrary to the reference where the slide appears first and the speaker says 'they can be a little bit confusing' afterward."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that an advanced written notice needs to go out seven days prior to the IEP meeting, when do they state that parents must sign to waive this 7-day advanced written notice?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1560.813,
        "end": 1570.038
      },
      "pred_interval": {
        "start": 68.9,
        "end": 74.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1491.913,
        "end": 1495.638,
        "average": 1493.7755
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.6708061099052429,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly restates that parents must sign to waive the notice (the 'after' relation) but gives a single, incorrect timestamp (68.9s) and omits the correct detailed time ranges (\u22481536\u20131544s and 1560\u20131570s), so it's largely inaccurate. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains the 60-calendar-day and 45-school-day evaluation timelines, when does she explain that evaluation reports must be provided to the parent at least three days prior to the IEP meeting?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1633.362,
        "end": 1641.913
      },
      "pred_interval": {
        "start": 15.7,
        "end": 39.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1617.662,
        "end": 1602.413,
        "average": 1610.0375
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.7911285161972046,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but completely misidentifies both anchor and target time spans and their content (wrong timestamps and segment descriptions versus the correct 1582\u20131598s and 1633\u20131642s windows), so it largely fails to match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide changes to 'Requirement that Program be in Effect', when does the speaker clarify that the 30-day timeline applies to both IEP development and implementation?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1719.674,
        "end": 1731.971
      },
      "pred_interval": {
        "start": 152.3,
        "end": 163.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1567.374,
        "end": 1568.871,
        "average": 1568.1225
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.7454885244369507,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both anchor and target are substantially incorrect compared to the ground truth, and the temporal relation ('at' vs correct 'after') is wrong; it therefore fails to match the correct timing and relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states the annual meeting date for the student as January 6th, 2022, when does she state when the next annual meeting must be held?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1785.48,
        "end": 1793.83
      },
      "pred_interval": {
        "start": 174.1,
        "end": 180.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1611.38,
        "end": 1613.53,
        "average": 1612.455
      },
      "rationale_metrics": {
        "rouge_l": 0.2916666666666667,
        "text_similarity": 0.7351661920547485,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction fails to match the reference: timestamps and quoted content differ, it hallucinates a specific 2023 date, and it asserts a 'before' relation instead of the correct 'once_finished' relation\u2014no semantic alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states the annual meeting date of January 6, 2022, when does she mention that the duration of the IEP begins on January 16?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1873.116,
        "end": 1878.561
      },
      "pred_interval": {
        "start": 51.733333333333334,
        "end": 56.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1821.3826666666666,
        "end": 1821.961,
        "average": 1821.6718333333333
      },
      "rationale_metrics": {
        "rouge_l": 0.27906976744186046,
        "text_similarity": 0.7715893983840942,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states that the IEP duration begins on Jan 16 after the Jan 6 annual meeting and matches the dates, but it omits the key timestamp details (1778.790s and 1873.116\u20131878.561s) provided in the correct answer and adds an unnecessary remark about a diagram."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'Annual IEP & Duration of IEP' diagram, when does the speaker explain the 7-day notice period?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1901.094,
        "end": 1909.493
      },
      "pred_interval": {
        "start": 72.16666666666667,
        "end": 79.46666666666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1828.9273333333333,
        "end": 1830.0263333333332,
        "average": 1829.4768333333332
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.5548223257064819,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly places the explanation during the Duration of IEP diagram but fails to give the referenced timestamps and wrongly states the 7-day notice spans 1/16/22\u20131/15/23 (a year), which contradicts and omits the precise start/end times in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that there are two sets of 364-day timelines, when does she next discuss parents waiving the 7-day notice?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1896.915,
        "end": 1901.942
      },
      "pred_interval": {
        "start": 135.86666666666667,
        "end": 142.46666666666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1761.0483333333332,
        "end": 1759.4753333333333,
        "average": 1760.2618333333332
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237117,
        "text_similarity": 0.7258815765380859,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps (135.8\u2013142.4s vs the reference ~1901.9\u20131980.0s) and adds unrelated timeline diagram details, so it does not match the correct timing or content."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if there are any other questions about timelines, when does she say they are 'good for now'?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.362,
        "end": 2131.025
      },
      "pred_interval": {
        "start": 2130.3333333333335,
        "end": 2133.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.028666666666595,
        "end": 2.474999999999909,
        "average": 32.25183333333325
      },
      "rationale_metrics": {
        "rouge_l": 0.3272727272727273,
        "text_similarity": 0.5041875839233398,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the phrase occurs after the timelines discussion, but it omits the specific timestamps and adds an unsupported detail about 'compliance questions.' It is vague ('very end') rather than matching the precise timing in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the procedural manual, when does she introduce the Maine Unified Special Education Regulations (MUSER)?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2219.043,
        "end": 2229.826
      },
      "pred_interval": {
        "start": 2135.5555555555557,
        "end": 2139.222222222222
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 83.48744444444446,
        "end": 90.60377777777785,
        "average": 87.04561111111116
      },
      "rationale_metrics": {
        "rouge_l": 0.1568627450980392,
        "text_similarity": 0.44011664390563965,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the relative ordering (MUSER is introduced after the procedural manual) but omits the key factual details\u2014explicit time intervals (2201.096s\u20132203.659s and 2219.043s\u20132229.826s) and precise 'once_finished' temporal relation\u2014so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the 'Special Education Laws and Regulations', when does the '2024-25 Professional Development Schedule' slide appear?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2361.5,
        "end": 2421.5
      },
      "pred_interval": {
        "start": 5.5,
        "end": 14.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2356.0,
        "end": 2407.4,
        "average": 2381.7
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6389485597610474,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different timestamps and events (6.3s/7.1s and a calendar) that contradict the reference (laws at 2347.5\u20132349.8 and the 2024\u201325 slide at 2361.5\u20132421.5), omitting the correct slide timing and introducing incorrect details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes encouraging viewers to reach out to them, when does the speaker say, 'I think that is it'?",
      "video_id": "eIt2t9mkSZA",
      "video_number": "016",
      "segment": {
        "start": 2490.0,
        "end": 2538.75
      },
      "gt_interval": {
        "start": 2514.0,
        "end": 2516.5
      },
      "pred_interval": {
        "start": 43.125,
        "end": 47.55
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2470.875,
        "end": 2468.95,
        "average": 2469.9125
      },
      "rationale_metrics": {
        "rouge_l": 0.3478260869565218,
        "text_similarity": 0.42167025804519653,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right (encouragement then 'I think that is it') but the timestamps are massively different from the reference (predicted times are far off and no end times/durations are given), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the host introduces Stephen McKinney, when does Stephen McKinney start talking about the dramatic impact of the pandemic?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 101.9,
        "end": 107.8
      },
      "pred_interval": {
        "start": 101.70833333333333,
        "end": 103.34325396825398
      },
      "iou": 0.23692267604716322,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.1916666666666771,
        "end": 4.456746031746022,
        "average": 2.3242063492063494
      },
      "rationale_metrics": {
        "rouge_l": 0.358974358974359,
        "text_similarity": 0.9121330976486206,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the overall relation ('after') correct but misstates the anchor timing (101.7s vs 80.8s), slightly misreports the target start (103.3s vs 101.9s), and omits the key detail that Stephen's initial attempt was interrupted\u2014thus it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Stephen McKinney mentions that social problems like child mental health pre-existed COVID-19 and were exacerbated, when does he bring up the Carers Scotland Act 2016?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 160.326,
        "end": 171.144
      },
      "pred_interval": {
        "start": 103.34325396825398,
        "end": 104.84325396825398
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.98274603174602,
        "end": 66.30074603174603,
        "average": 61.641746031746024
      },
      "rationale_metrics": {
        "rouge_l": 0.4096385542168675,
        "text_similarity": 0.9177538156509399,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the predicted timestamps are significantly inaccurate compared to the ground truth (anchor ~132s and target ~160s vs predicted ~103s and ~104.8s), so the answer fails to correctly locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states the Carers Scotland Act 2016 was published in 2018 by the Scottish Government, when does he state that the Act applies to adult and young carers?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 168.5,
        "end": 171.0
      },
      "pred_interval": {
        "start": 173.91666666666666,
        "end": 179.91666666666666
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.416666666666657,
        "end": 8.916666666666657,
        "average": 7.166666666666657
      },
      "rationale_metrics": {
        "rouge_l": 0.14159292035398233,
        "text_similarity": 0.40819454193115234,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general content (the Act applies to adult and young carers) but gives incorrect and inconsistent time stamps and temporal relation\u2014the provided spans (173.9\u2013179.9s) and 'after' relation conflict with the reference (anchor at 167.5s, target 168.5\u2013171.0s, immediately following)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker defines a young carer as a person under 18 with caring responsibilities, when does he provide an additional condition for defining a young carer?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 217.0,
        "end": 222.5
      },
      "pred_interval": {
        "start": 271.3958333333333,
        "end": 277.1958333333333
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.395833333333314,
        "end": 54.695833333333326,
        "average": 54.54583333333332
      },
      "rationale_metrics": {
        "rouge_l": 0.24719101123595508,
        "text_similarity": 0.6701877117156982,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets only the vague 'after' relation right but misstates both anchor and target timestamps (off by ~54s) and introduces different content; thus it fails to match the correct temporal locations and key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker cites the 2011 census data about the age distribution of young carers in the UK, when does he mention that Scottish Government guidance acknowledges very young carers?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 236.0,
        "end": 244.5
      },
      "pred_interval": {
        "start": 338.7440476190476,
        "end": 343.9440476190476
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 102.74404761904759,
        "end": 99.44404761904758,
        "average": 101.09404761904759
      },
      "rationale_metrics": {
        "rouge_l": 0.24719101123595505,
        "text_similarity": 0.5828556418418884,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps and temporal relation contradict the reference (expected ~235\u2013244s vs predicted ~338\u2013344s and inconsistent relation), and while both mention very young carers, the predicted content does not match the cited Scottish Government guidance\u2014major temporal and semantic mismatches."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker says he just wanted to introduce the topic, when does he refer to 'invisible children'?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 424.5,
        "end": 426.1
      },
      "pred_interval": {
        "start": 91.125,
        "end": 100.958
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 333.375,
        "end": 325.14200000000005,
        "average": 329.2585
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7134756445884705,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the semantic relation ('after') and the events' content, but the timestamps are completely different from the ground truth (major temporal mismatches and missing anchor end time), so it largely fails to match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After John says 'Many thanks, Stephen', when is the next time he speaks to introduce the next presentation?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 461.0,
        "end": 465.0
      },
      "pred_interval": {
        "start": 414.042,
        "end": 430.292
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.95800000000003,
        "end": 34.70800000000003,
        "average": 40.83300000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.275,
        "text_similarity": 0.7310296297073364,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it gives completely different anchor and target timestamps and quotes different utterances, and fails to identify the introduction of Katie's presentation; only the generic 'after' relation is correct."
      }
    },
    {
      "question_id": "003",
      "question": "While Katie's first slide 'Widening the gap? The challenges for equitable music education in Scotland' is displayed, when does she mention Leo Moscardini?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 490.7,
        "end": 504.0
      },
      "pred_interval": {
        "start": 440.375,
        "end": 450.958
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.32499999999999,
        "end": 53.04199999999997,
        "average": 51.68349999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2888888888888889,
        "text_similarity": 0.7539560794830322,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the ground truth (off by ~40\u201360s) and place the mention outside the slide visibility, incorrectly labeling the relation as 'after' rather than occurring while the slide is displayed, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes listing what the presentation will cover, when does she say 'Okay'?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 526.04,
        "end": 527.9
      },
      "pred_interval": {
        "start": 519.6724137931035,
        "end": 522.7660294536058
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.367586206896476,
        "end": 5.133970546394153,
        "average": 5.750778376645314
      },
      "rationale_metrics": {
        "rouge_l": 0.24137931034482757,
        "text_similarity": 0.6852477788925171,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only preserves the coarse temporal relation ('after') but the timestamps and segment boundaries are substantially incorrect and the anchor/target intervals are misidentified, so it fails to match the correct answer's key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing Case Study B, when does she begin describing Case Study C?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 561.5,
        "end": 567.5
      },
      "pred_interval": {
        "start": 592.6158038147139,
        "end": 601.5331582135464
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.1158038147139,
        "end": 34.03315821354636,
        "average": 32.57448101413013
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.5476704239845276,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps contradict the reference (predicted E2 starts at ~601.5s vs. correct 561.5s) and misidentifies anchor timing; the temporal relationship and key timing are incorrect, so it provides essentially the wrong answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions the number of pupils taking Advanced Higher music in Case Study A, when does she mention the number of pupils taking qualifications in Case Study B?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 627.5,
        "end": 634.2
      },
      "pred_interval": {
        "start": 708.1520682173496,
        "end": 720.9380880081949
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.65206821734955,
        "end": 86.73808800819484,
        "average": 83.6950781127722
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.6001573801040649,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps and mislabels the anchor/target compared to the reference (correct E2 is 627.5\u2013634.2s), so it is largely incorrect despite noting an 'after' relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that inequality in music education was beginning in primary schools and persisting, when does she explain that the focus on performance privileges middle-class pupils?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 728.2,
        "end": 740.8
      },
      "pred_interval": {
        "start": 37.2,
        "end": 47.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 691.0,
        "end": 693.0,
        "average": 692.0
      },
      "rationale_metrics": {
        "rouge_l": 0.19178082191780824,
        "text_similarity": 0.4917447865009308,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after') but omits the precise timestamps provided in the reference and introduces an unsupported detail about 'access to resources,' so it is only partially aligned with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "While the slide titled 'What this means?' is displayed, when does the speaker state that working-class, poorer households, disabled children, and those with additional support needs are effectively excluded?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 763.3,
        "end": 771.2
      },
      "pred_interval": {
        "start": 51.0,
        "end": 63.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 712.3,
        "end": 708.0,
        "average": 710.15
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117643,
        "text_similarity": 0.2771337032318115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely repeats that the speaker highlights those groups' exclusion but omits the precise timing and fails to state that this occurs during the 'What this means?' slide; its phrasing is also circular and non-informative."
      }
    },
    {
      "question_id": "001",
      "question": "Once Katie finishes saying 'Thanks very much', when does John begin thanking her?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 882.288,
        "end": 883.549
      },
      "pred_interval": {
        "start": 18.3,
        "end": 23.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 863.988,
        "end": 859.949,
        "average": 861.9685
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.5226643681526184,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the order (John speaks after Katie) but the timestamps differ substantially from the reference (predicted gap 5.3s vs actual 0.78s), and it omits John's end time and the specified relation, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the first slide of Lindsay's presentation appears on screen, when does Lindsay begin to introduce her project?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 921.97,
        "end": 989.732
      },
      "pred_interval": {
        "start": 43.3,
        "end": 49.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 878.6700000000001,
        "end": 940.5319999999999,
        "average": 909.601
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.5822200775146484,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction (43.3s after) contradicts the ground truth start time (about 8.54s after the first slide) and omits the introduction end time; it is therefore largely incorrect though it does state a relative timing."
      }
    },
    {
      "question_id": "003",
      "question": "During the 'Background to study' slide, when does Lindsay state that poverty is detrimental to academic attainment?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 964.634,
        "end": 969.402
      },
      "pred_interval": {
        "start": 45.8,
        "end": 50.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 918.8340000000001,
        "end": 919.1020000000001,
        "average": 918.9680000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.5524822473526001,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the event occurs during the 'Background to study' slide but gives a clearly incorrect timestamp (45.8s) versus the actual times (absolute 964.634\u2013969.402s; relative \u224812.8\u201317.6s), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes explaining the 'Study design' slide, when does she start discussing the 'Research Participants and School Profiles'?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1085.64,
        "end": 1103.0
      },
      "pred_interval": {
        "start": 100.3,
        "end": 105.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 985.3400000000001,
        "end": 997.3,
        "average": 991.32
      },
      "rationale_metrics": {
        "rouge_l": 0.2432432432432432,
        "text_similarity": 0.5860562324523926,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the qualitative relation (the discussion begins after the study design explanation) but omits the crucial timestamps and duration provided in the reference, making it incomplete for this timing-based question."
      }
    },
    {
      "question_id": "001",
      "question": "After the female presenter finishes speaking about the previous research, when does she transition to the 'Reflective questions' slide?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1249.4,
        "end": 1250.5
      },
      "pred_interval": {
        "start": 31.688888888888886,
        "end": 36.666666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1217.7111111111112,
        "end": 1213.8333333333333,
        "average": 1215.7722222222224
      },
      "rationale_metrics": {
        "rouge_l": 0.23853211009174308,
        "text_similarity": 0.7418460249900818,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the female presenter finishing, the 'Reflective questions' slide start/end, and the temporal relation 'after'; the only minor issue is that the timestamps are given in a different (relative) timebase than the absolute times in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female presenter finishes talking about the reflective questions, when does the male presenter start speaking?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1272.5,
        "end": 1273.0
      },
      "pred_interval": {
        "start": 54.63333333333333,
        "end": 55.62222222222223
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1217.8666666666666,
        "end": 1217.3777777777777,
        "average": 1217.6222222222223
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.6910576224327087,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction preserves the core temporal relation (female finishes, male speaks afterward) and provides corresponding timestamps on a different scale, but it mislabels event boundaries (E1 as a start), gives slightly different durations (male first-word end), and uses a less precise relation ('after' vs 'once_finished')."
      }
    },
    {
      "question_id": "003",
      "question": "While the first male presenter is speaking about digital exclusion, when does he mention the conditions that education systems need to meet to be successful?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1343.9,
        "end": 1346.5
      },
      "pred_interval": {
        "start": 73.09722222222221,
        "end": 76.75555555555556
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1270.802777777778,
        "end": 1269.7444444444445,
        "average": 1270.2736111111112
      },
      "rationale_metrics": {
        "rouge_l": 0.17218543046357615,
        "text_similarity": 0.566156268119812,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives entirely different timestamps and a different quoted utterance, and wrongly labels the relation as 'after' instead of the correct 'during'."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker says 'There you go', when does the screen transition to the presentation slide with the second speaker in a small window?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.2,
        "end": 1426.5
      },
      "pred_interval": {
        "start": 49.87878787878787,
        "end": 53.03703703703704
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1376.3212121212123,
        "end": 1373.462962962963,
        "average": 1374.8920875420877
      },
      "rationale_metrics": {
        "rouge_l": 0.18666666666666665,
        "text_similarity": 0.6850331425666809,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer only matches the relation ('after') but gives completely incorrect timestamps and event descriptions that do not align with the ground truth timing or content, so it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker states that online lessons could be quite useful, when does he discuss gaining insights into pupils' lives and building better relationships with parents?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1637.5,
        "end": 1645.0
      },
      "pred_interval": {
        "start": 27.627713544076485,
        "end": 39.257985626652356
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1609.8722864559236,
        "end": 1605.7420143733477,
        "average": 1607.8071504146355
      },
      "rationale_metrics": {
        "rouge_l": 0.3555555555555555,
        "text_similarity": 0.30717670917510986,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the discussion occurs afterward but omits the precise timestamps (1617.5s, 1637.5\u20131645.0s) and introduces unsupported details (Kevin/staff), making it incomplete and partly inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes asking Kevin to pick up some stuff, when does Kevin begin to talk about the work being done to provide technology to young people?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1660.0,
        "end": 1676.0
      },
      "pred_interval": {
        "start": 27.627713544076485,
        "end": 39.257985626652356
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1632.3722864559236,
        "end": 1636.7420143733477,
        "average": 1634.5571504146355
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.6103085875511169,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: it reverses roles (saying Kevin asked the first speaker), introduces unrelated details about insights/parents not in the reference, and omits the specific timing and 'once_finished' relation given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning digital inclusion, when does she begin discussing finances and support?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1787.8,
        "end": 1790.0
      },
      "pred_interval": {
        "start": 48.0,
        "end": 52.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1739.8,
        "end": 1737.9,
        "average": 1738.85
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.6219791173934937,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly captures the temporal relation ('after') but the timestamps are factually incorrect and inconsistent with the reference (and it omits the phrase completion time), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces the 'Key findings' section, when does she mention parents on low incomes being more concerned about money?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1815.4,
        "end": 1829.0
      },
      "pred_interval": {
        "start": 84.8,
        "end": 89.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1730.6000000000001,
        "end": 1740.0,
        "average": 1735.3000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4788732394366197,
        "text_similarity": 0.5643316507339478,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps (84.8s/89.0s) are completely inconsistent with the reference (1814.4s/1815.4s\u20131829.0s), indicating incorrect temporal identification and likely fabrication; the relation 'after' does not accurately capture the reference's 'once_finished' alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that 'progress clearly being made' regarding digital inclusion, when does she mention that children on free meals are more likely to share devices?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1952.9,
        "end": 1958.5
      },
      "pred_interval": {
        "start": 36.3,
        "end": 40.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1916.6000000000001,
        "end": 1917.6,
        "average": 1917.1
      },
      "rationale_metrics": {
        "rouge_l": 0.19780219780219782,
        "text_similarity": 0.4285399913787842,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative order (the device-sharing comment comes after the progress remark) but gives an incorrect timestamp (36.3s vs. ~1950\u20131955s) and adds an unverified claim about on-screen text, omitting the required precise timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces 'Free meal replacements', when does she discuss the importance of choice and dignity in food provision?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2003.8,
        "end": 2010.5
      },
      "pred_interval": {
        "start": 78.7,
        "end": 83.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1925.1,
        "end": 1927.4,
        "average": 1926.25
      },
      "rationale_metrics": {
        "rouge_l": 0.21505376344086022,
        "text_similarity": 0.580342710018158,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly identifies that the discussion of choice and dignity occurs after the free meal replacements, but it gives incorrect/ inconsistent timestamps for the target event and adds unverified on-screen text, so it fails to match the reference details."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"cash first approaches\", when does the male speaker begin speaking?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2167.9,
        "end": 2172.6
      },
      "pred_interval": {
        "start": 219.73333333333332,
        "end": 231.46666666666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1948.1666666666667,
        "end": 1941.1333333333332,
        "average": 1944.65
      },
      "rationale_metrics": {
        "rouge_l": 0.27848101265822783,
        "text_similarity": 0.7458868026733398,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps and segment boundaries are substantially inaccurate and do not match the reference; key factual timing details are therefore incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks Angela Japp's question about creative approaches, when does Katie start her answer by mentioning 'the digital'?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2235.835,
        "end": 2259.242
      },
      "pred_interval": {
        "start": 249.23333333333332,
        "end": 257.73333333333335
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1986.6016666666667,
        "end": 2001.5086666666668,
        "average": 1994.0551666666668
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.7221947312355042,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is incorrect: it gives entirely different timestamps, speaker identity, and quoted utterance (no mention of Katie or 'the digital') and thus does not match the reference, despite both labeling the relation as 'after'."
      }
    },
    {
      "question_id": "003",
      "question": "Once Katie finishes discussing culturally valued aspects in schools, when does the male speaker thank her?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2297.355,
        "end": 2298.476
      },
      "pred_interval": {
        "start": 298.73333333333335,
        "end": 300.73333333333335
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1998.6216666666667,
        "end": 1997.7426666666668,
        "average": 1998.1821666666667
      },
      "rationale_metrics": {
        "rouge_l": 0.31707317073170727,
        "text_similarity": 0.6556421518325806,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely different from the ground truth: it gives entirely different timestamps, a different utterance and speaker content, and does not identify the male speaker thanking Katie as in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After John asks whether initiatives like 'Big Noise' would impact music provision generally if targeted at deprived areas, when does Alistair start speaking?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2390.458,
        "end": 2391.922
      },
      "pred_interval": {
        "start": 56.285714285714285,
        "end": 57.35714285714286
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2334.172285714286,
        "end": 2334.5648571428574,
        "average": 2334.3685714285716
      },
      "rationale_metrics": {
        "rouge_l": 0.08888888888888889,
        "text_similarity": 0.5097646713256836,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely different timestamps (~56\u201357s vs ~2390s), misidentifies the quoted utterance, and does not place Alistair at the correct segment, so it fails to match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After Alistair says that some programs are 'hugely expensive', when does he advise treating them with caution?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2406.7,
        "end": 2414.2
      },
      "pred_interval": {
        "start": 81.57142857142857,
        "end": 82.11904761904762
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2325.1285714285714,
        "end": 2332.080952380952,
        "average": 2328.6047619047617
      },
      "rationale_metrics": {
        "rouge_l": 0.11494252873563218,
        "text_similarity": 0.4468672275543213,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer cites completely different timestamps and a different utterance that does not correspond to Alistair's 'hugely expensive' remark or his subsequent caution about multinational companies, so it fails to match the reference events despite labeling the relation 'after.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker states that recovery plans were produced by the Scottish and English governments, when does he explain what those recovery plans are about?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2499.117,
        "end": 2513.724
      },
      "pred_interval": {
        "start": 15.1,
        "end": 52.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2484.0170000000003,
        "end": 2461.224,
        "average": 2472.6205
      },
      "rationale_metrics": {
        "rouge_l": 0.2830188679245283,
        "text_similarity": 0.7102717161178589,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is incorrect\u2014timestamps and described content do not match the reference (different anchor/target times and quoted content), so it fails to identify the correct segment or content."
      }
    },
    {
      "question_id": "002",
      "question": "After Sarah mentions that digital literacy for parents wasn't something they specifically looked at, when does she explain the challenges parents faced with remote learning?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2558.855,
        "end": 2578.0
      },
      "pred_interval": {
        "start": 102.0,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2456.855,
        "end": 2368.0,
        "average": 2412.4275
      },
      "rationale_metrics": {
        "rouge_l": 0.3561643835616438,
        "text_similarity": 0.7480620741844177,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and segment boundaries differ drastically from the reference (anchor/target times off by thousands of seconds and target duration incorrect); only the qualitative relation ('after') matches, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the male host mentions a question from Katharine Reid, when does Sarah laugh?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2740.86,
        "end": 2741.43
      },
      "pred_interval": {
        "start": 63.77777777777778,
        "end": 65.11111111111111
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2677.0822222222223,
        "end": 2676.3188888888885,
        "average": 2676.700555555555
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677422,
        "text_similarity": 0.5991608500480652,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the temporal relation ('after') but misidentifies both events (wrong speaker for the laugh and incorrect description of the anchor) and gives completely different timestamps, so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Sarah finishes saying 'Yes', when does she begin to explain how more money for families leads to better outcomes for children?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2744.15,
        "end": 2772.16
      },
      "pred_interval": {
        "start": 69.0,
        "end": 73.11111111111111
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2675.15,
        "end": 2699.0488888888885,
        "average": 2687.099444444444
      },
      "rationale_metrics": {
        "rouge_l": 0.13559322033898305,
        "text_similarity": 0.5523996949195862,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely mismatched: different speaker, vastly different timestamps/durations, and incorrect event boundaries, so it fails to align with the reference timing and events."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male host mentions people working in rural areas, when does he ask Lindsay for her thoughts?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2828.315,
        "end": 2829.617
      },
      "pred_interval": {
        "start": 81.55555555555556,
        "end": 82.22222222222221
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2746.7594444444444,
        "end": 2747.394777777778,
        "average": 2747.077111111111
      },
      "rationale_metrics": {
        "rouge_l": 0.1515151515151515,
        "text_similarity": 0.5071294903755188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly captures that the ask follows the mention, but it mislabels which event is E1/E2, gives entirely different timestamps, and uses a vague 'after' relation instead of the specific 'once_finished', so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states that all schools must be very mindful to poverty-proof themselves, when does she mention that teaching pedagogies and strategies should be transparent and inclusive?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2858.72,
        "end": 2868.63
      },
      "pred_interval": {
        "start": 2856.3141890172774,
        "end": 2872.2420634499085
      },
      "iou": 0.6221796914532347,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4058109827224143,
        "end": 3.612063449908419,
        "average": 3.0089372163154167
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.6833881139755249,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target events and the target follows the anchor, but the timestamps differ from the reference (anchor start/end and target start/end are inaccurate or omitted) and it fails to state the immediate adjacency specified in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman suggests giving probationers and undergraduates more practical tools before they go into probation, when does she say that a lot can be learned from autism education?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2896.13,
        "end": 2901.89
      },
      "pred_interval": {
        "start": 2877.225744686649,
        "end": 2901.0031746792806
      },
      "iou": 0.19758045063061758,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.904255313351314,
        "end": 0.8868253207192538,
        "average": 9.895540317035284
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.6634124517440796,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly identifies both events and their temporal relation: the anchor start falls within the reference anchor interval and the predicted target start lies within the reference target interval, with the target occurring after the anchor; differences are within acceptable tolerance."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man reminds people about the PACT project launch, when does he describe it as a professional learning project funded by the Scottish Government?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2951.199,
        "end": 2960.769
      },
      "pred_interval": {
        "start": 2929.708679479438,
        "end": 2951.2972194716026
      },
      "iou": 0.003162216936477298,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.49032052056191,
        "end": 9.471780528397176,
        "average": 15.481050524479542
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.8052191734313965,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the qualitative relation ('after'), but the timestamps are substantially incorrect (anchor given as 2850.0s vs correct 2943.532s; target start 2929.709s vs correct 2951.199s) and it omits end times, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first speaker puts in an advert for John McKendrick, when does he praise the work of the Caledonian Club?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3049.4,
        "end": 3053.3
      },
      "pred_interval": {
        "start": 50.8506944922769,
        "end": 53.3736944922769
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2998.549305507723,
        "end": 2999.926305507723,
        "average": 2999.2378055077234
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666667,
        "text_similarity": 0.7755006551742554,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely mismatches the reference: it mislabels E1 (says he praises McKendrick rather than introduces him), gives different timestamps and omits E1's end time, and states the relation as 'after' instead of the immediate 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes asking Mary a question, when does the second speaker (John) appear and begin to speak?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3079.0,
        "end": 3082.0
      },
      "pred_interval": {
        "start": 80.69449227690137,
        "end": 83.81449227690138
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2998.305507723099,
        "end": 2998.1855077230985,
        "average": 2998.245507723099
      },
      "rationale_metrics": {
        "rouge_l": 0.38461538461538464,
        "text_similarity": 0.69508957862854,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: times are massively different and inconsistent with the reference (different scale and wrong end time), and the relation 'after' and event timestamps do not match the precise 'once_finished' timing in the ground truth; only the vague ordering (second speaks after first) is preserved."
      }
    },
    {
      "question_id": "003",
      "question": "After Mary finishes discussing how to best support students, when does John appear on screen and comment on her questions?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3152.3,
        "end": 3159.5
      },
      "pred_interval": {
        "start": 124.09227690137295,
        "end": 130.09227690137294
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3028.2077230986274,
        "end": 3029.4077230986272,
        "average": 3028.8077230986273
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.6713564395904541,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation, but the event timestamps, durations, and event labels are substantially different from the ground truth (misaligned and inaccurate), so it fails to match the reference details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first speaker asks if something happened and how it is going, when does he state that he will put the question to the Glasgow team first?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3213.51,
        "end": 3214.09
      },
      "pred_interval": {
        "start": 10.4,
        "end": 12.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3203.11,
        "end": 3201.79,
        "average": 3202.45
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6628292798995972,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the target utterance but gives completely different timestamps and omits the anchor end time; it also uses a less specific relation ('after' vs 'once_finished'), so it is largely inconsistent with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker with the beard begins speaking, when does he mention the rollout of 50,000 plus iPads?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3232.12,
        "end": 3237.0
      },
      "pred_interval": {
        "start": 19.4,
        "end": 25.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3212.72,
        "end": 3212.0,
        "average": 3212.3599999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3561643835616438,
        "text_similarity": 0.6759883761405945,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation and mentions the 50,000+ iPads, but the provided timestamps are wildly inconsistent with the ground-truth times, a major factual error."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states that getting information out to all families about support is available, when does she finish her statement that this is really critical?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3390.0,
        "end": 3482.1099999999997
      },
      "gt_interval": {
        "start": 3396.5,
        "end": 3398.0
      },
      "pred_interval": {
        "start": 3390.0,
        "end": 3482.1
      },
      "iou": 0.016286644951140083,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.5,
        "end": 84.09999999999991,
        "average": 45.299999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.18666666666666665,
        "text_similarity": 0.5259172320365906,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a significantly incorrect end timestamp (3482.1s) that contradicts the correct finish time (3396.5s, concluding 3398.0s) and fails to match the required E1/E2 alignment."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states that the presenters were exemplary in their timekeeping, when does he continue to say he will be exemplary in his timekeeping?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3390.0,
        "end": 3482.1099999999997
      },
      "gt_interval": {
        "start": 3427.0,
        "end": 3431.0
      },
      "pred_interval": {
        "start": 3390.0,
        "end": 3482.1
      },
      "iou": 0.043431053203040214,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 51.09999999999991,
        "average": 44.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.16901408450704225,
        "text_similarity": 0.5184325575828552,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes he says he'll be exemplary, but provides a wrong timestamp (3482.1s vs the correct ~3427.0\u20133431.0s) and omits the start/end intervals, so it is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the man asks to virtually clap, when does he physically clap his hands?",
      "video_id": "pO8HTf-ntyc",
      "video_number": "017",
      "segment": {
        "start": 3390.0,
        "end": 3482.1099999999997
      },
      "gt_interval": {
        "start": 3476.0,
        "end": 3478.0
      },
      "pred_interval": {
        "start": 3390.0,
        "end": 3482.1
      },
      "iou": 0.021715526601520107,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.0,
        "end": 4.099999999999909,
        "average": 45.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.5467433929443359,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the clap occurs after the virtual clap but gives an incorrect timestamp (3482.1s vs. 3476\u20133478s) and omits the virtual-clap timing and clap finish time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "During the time the 'Strategic Priority 3' slide is displayed, when does the speaker mention the Alliance's 2021 to 2025 strategy?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 22.0,
        "end": 27.7
      },
      "pred_interval": {
        "start": 6.844444444444445,
        "end": 18.944444444444446
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.155555555555555,
        "end": 8.755555555555553,
        "average": 11.955555555555554
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.39762258529663086,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the mention occurs during the slide, but it wrongly claims it happens in the \"first few seconds\" (actual is 22.0\u201327.7s) and adds an unsupported detail about \"Objective 3,\" so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the NFDHR logo and name are fully displayed, when does the text indicating its establishment appear?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 165.0,
        "end": 168.0
      },
      "pred_interval": {
        "start": 16.98888888888889,
        "end": 20.87229166666667
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 148.01111111111112,
        "end": 147.12770833333332,
        "average": 147.56940972222222
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5456842184066772,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys that the establishment text appears after the logo, but it omits the precise timing information, adds an unsupported detail about a close-up and the logo 'fading in,' and thus fails to match the factual specifics in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the statistics for Education, Food Security, and Health & Nutrition programs are fully displayed, when do the statistics for WASH, Peace Building, Protection & Gender, and Shelter & CCCM programs appear?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.0,
        "end": 211.0
      },
      "pred_interval": {
        "start": 39.98888888888889,
        "end": 44.87229166666666
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 167.01111111111112,
        "end": 166.12770833333335,
        "average": 166.56940972222225
      },
      "rationale_metrics": {
        "rouge_l": 0.11267605633802816,
        "text_similarity": 0.25966280698776245,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the order (second group appears after the first) but omits the key timing details (196.0\u2013200.0s for initial stats and 207.0\u2013211.0s for the remaining) and the sequential 'once_finished' relation, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "During the display of the main descriptive text for the 'Education Overview 2022' slide, when do the icons and numerical statistics appear?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 237.0,
        "end": 243.0
      },
      "pred_interval": {
        "start": 64.09444444444443,
        "end": 70.70277777777777
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 172.90555555555557,
        "end": 172.29722222222222,
        "average": 172.6013888888889
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666669,
        "text_similarity": 0.4059511125087738,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely captures that some icons/statistics appear during the Education slide, but it introduces unsupported program-specific claims and incorrectly states the timing/order (many stats said to appear after the Education section), contradicting the precise 'during' timing and timestamps given in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that the Syrian crisis created a fragmented society unable to benefit from its expertise, when does she mention that they started working together from 2018?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 351.3,
        "end": 364.9
      },
      "pred_interval": {
        "start": 4.3,
        "end": 53.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 347.0,
        "end": 311.4,
        "average": 329.2
      },
      "rationale_metrics": {
        "rouge_l": 0.17307692307692307,
        "text_similarity": 0.5813349485397339,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the order and that the speaker mentions 2018, but the anchor and target timestamps (4.3s and 53.5\u201357.8s) substantially mismatch the correct timings (350.9s and 351.3\u2013364.9s), so key factual elements are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that 2 million Syrian children are out of education and tens of thousands are in North Syrian camps, when does she begin to list the specific numbers of camps and schools?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 418.5,
        "end": 427.8
      },
      "pred_interval": {
        "start": 57.5,
        "end": 60.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 361.0,
        "end": 366.90000000000003,
        "average": 363.95000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6284193396568298,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the listing occurs after the 2 million children remark, but the timestamps and event boundaries are completely different from the ground truth (and E1 is mis-specified as a start rather than the finish time), so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that IRC has invested a lot in research and social and emotional learning, when does she explain how these learnings can be used for out-of-school and in-school children?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 551.0,
        "end": 568.9
      },
      "pred_interval": {
        "start": 510.0,
        "end": 525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 43.89999999999998,
        "average": 42.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23376623376623376,
        "text_similarity": 0.619757890701294,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives substantially incorrect timestamps (E2 starts at 525.0s vs correct 551.0s and omits the end), mislabels segments, and fails to capture the correct temporal boundaries of the explanation, so it does not accurately match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces the PRIEST project as one of IRC's flagship programs, when does she state the countries where it is implemented?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.9,
        "end": 627.597
      },
      "pred_interval": {
        "start": 716.5,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.60000000000002,
        "end": 92.40300000000002,
        "average": 93.00150000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.8048642873764038,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the anchor and target but gives times that are ~100s later than the reference (622.0s/622.9s vs 716.5s/720.0s) and only states 'after' rather than 'immediately follows', so it is largely incorrect on timing and relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces Ahlam Ahmed as the Education Programme Manager, when does Ahlam Ahmed greet the speaker?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 777.178,
        "end": 778.34
      },
      "pred_interval": {
        "start": 23.666666666666668,
        "end": 25.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 753.5113333333334,
        "end": 753.09,
        "average": 753.3006666666668
      },
      "rationale_metrics": {
        "rouge_l": 0.12499999999999997,
        "text_similarity": 0.46199023723602295,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect and contradictory: it describes Ahlam asking Amanda to move slides at ~23.67s rather than Ahlam greeting Amanda immediately after being introduced around 777s, so it omits and hallucinates key facts."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks to move to the next slide, when does the slide visually change?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 962.9,
        "end": 963.4
      },
      "pred_interval": {
        "start": 937.5,
        "end": 940.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.399999999999977,
        "end": 22.899999999999977,
        "average": 24.149999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.43778595328330994,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives a completely incorrect timestamp (937.5s) and wrong trigger, contradicting the ground truth that the slide change happens around 962.9\u2013963.4s immediately after the request; it omits the correct timing and adds unfounded details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states the project's duration and donor, when does she mention the number of targeted children?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1031.1,
        "end": 1039.3
      },
      "pred_interval": {
        "start": 945.0,
        "end": 952.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.09999999999991,
        "end": 87.29999999999995,
        "average": 86.69999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.12244897959183673,
        "text_similarity": 0.24040481448173523,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (945.0s) contradicts the ground truth (target at 1031.1\u20131039.3s) and misrepresents the temporal order; it is factually incorrect and omits the correct timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'with ALP classrooms', when does she begin to introduce 'the second activity or the second intervention'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1104.5,
        "end": 1109.5
      },
      "pred_interval": {
        "start": 905.2083333333333,
        "end": 1034.5833333333335
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 199.29166666666674,
        "end": 74.91666666666652,
        "average": 137.10416666666663
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.7487757205963135,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation 'after' right but the event timings are largely incorrect: the anchor is given as a start time (905.2s) instead of the correct end at 1054.5s, and the target is placed at 1034.5s (with no duration) rather than the correct 1104.5\u20131109.5s."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes talking about 'on the safe school protocols', when does she describe how 'Temporary learning spaces will also be provided with wash facilities and essential cleaning hygiene materials'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1138.5,
        "end": 1148.0
      },
      "pred_interval": {
        "start": 832.2916666666667,
        "end": 1041.375
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 306.20833333333326,
        "end": 106.625,
        "average": 206.41666666666663
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.7592959403991699,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies the anchor and target timings (predicting start times of 832.2s and 1041.3s vs. the ground-truth anchor end 1102.5s and target 1138.5\u20131148.0s), and gives an incorrect/zero-duration target; although both label the relation 'after', the temporal annotations are incorrect and contradictory to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions community sensitization, when does she describe the creation of a community-based support system for children?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1246.823,
        "end": 1274.838
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1440.0
      },
      "iou": 0.1334047619047613,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.823000000000093,
        "end": 165.16200000000003,
        "average": 90.99250000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.31460674157303375,
        "text_similarity": 0.8226786851882935,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') but major timestamp errors and invented boundaries (anchor start/end and target start/end) contradict the precise times in the reference and imply overlap; thus it is largely inaccurate. "
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes mentioning integration into the formal learning system, when does she state the overall objective of the program?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1339.066,
        "end": 1350.221
      },
      "pred_interval": {
        "start": 1440.0,
        "end": 1481.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.93399999999997,
        "end": 130.779,
        "average": 115.85649999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.30588235294117644,
        "text_similarity": 0.8346346616744995,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the order (the objective comes after the integration mention) but the timestamps are substantially incorrect (off by ~100+ seconds) and the immediacy of the follow-up is lost, so it fails to match the reference timing and extent."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing school referrals and observation capacity, when does she mention that guidelines for pathways and referral will be revised?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1389.248,
        "end": 1400.984
      },
      "pred_interval": {
        "start": 1481.0,
        "end": 1521.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 91.75199999999995,
        "end": 120.01600000000008,
        "average": 105.88400000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.31818181818181823,
        "text_similarity": 0.8369473814964294,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the revision is mentioned after the referral discussion, but the timestamps and event durations are substantially incorrect (off by ~100s) and it fails to reflect that the target directly follows the anchor as in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker finishes mentioning case management and referral pathways, when does she start listing additional skills training in the capacity building package?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1471.0,
        "end": 1480.5
      },
      "pred_interval": {
        "start": 1415.5555555555554,
        "end": 1418.2222222222222
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.44444444444457,
        "end": 62.27777777777783,
        "average": 58.8611111111112
      },
      "rationale_metrics": {
        "rouge_l": 0.10714285714285714,
        "text_similarity": 0.2927209138870239,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated and incorrect: it provides different events and timestamps (1415.5s/1418.2s) that do not match the reference timings (1470.8s\u20131471.0s\u20131480.5s) and fails to identify the listing of additional skills training."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker says \"Yeah, next slide, please\" for the first time, when does the green box with the English text for the \"Commitment indicator\" appear on screen?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1634.9,
        "end": 1720.9
      },
      "pred_interval": {
        "start": 15.958333333333332,
        "end": 37.666666666666664
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1618.9416666666668,
        "end": 1683.2333333333333,
        "average": 1651.0875
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.6402338743209839,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps and durations are entirely different from the reference (off by large factors) and the anchor/event descriptions don't match the correct times/content; only the 'after' relation aligns."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male speaker finishes asking to move to the next slide, when does the slide actually change?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1785.53,
        "end": 1785.6
      },
      "pred_interval": {
        "start": 1.8666666666666667,
        "end": 3.0555555555555554
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1783.6633333333334,
        "end": 1782.5444444444443,
        "average": 1783.1038888888888
      },
      "rationale_metrics": {
        "rouge_l": 0.25974025974025977,
        "text_similarity": 0.6732642650604248,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps are grossly different, the slide-change timing is wrong (it does not start simultaneously with the speaker), and the relation 'start' contradicts the correct 'once_finished' sequence; only the general notion of a speaker request is retained."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the child's achievement in the first case study, when does she ask for the next slide?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1909.926,
        "end": 1911.04
      },
      "pred_interval": {
        "start": 43.98888888888889,
        "end": 46.17777777777778
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1865.937111111111,
        "end": 1864.8622222222223,
        "average": 1865.3996666666667
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.654091477394104,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is fundamentally incorrect: timestamps and event boundaries do not match the reference, the anchor/target roles and times are wrong, and the temporal relation ('start') contradicts the correct 'once_finished' relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks for the next slide, when does the slide transition to the domestic violence case study?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1915.72,
        "end": 1921.8
      },
      "pred_interval": {
        "start": 22.122222222222224,
        "end": 25.222222222222225
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1893.5977777777778,
        "end": 1896.5777777777778,
        "average": 1895.0877777777778
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.6119592785835266,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the speaker request and a slide change but gives entirely different timestamps and an incorrect relation (says they start together) versus the ground truth where the slide begins after the speaker finishes and completes later; major factual discrepancies. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes her detailed explanation of how they help children facing domestic violence, when does she say 'Next slide'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2003.389,
        "end": 2004.0
      },
      "pred_interval": {
        "start": 104.9,
        "end": 106.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1898.4889999999998,
        "end": 1897.2,
        "average": 1897.8445
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.561794638633728,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly indicates the 'after completion' relation but the timing is wildly inaccurate (predicts 105.0s after versus the ground-truth ~1.12s after), and it also fails to match the referenced time interval."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Arabic speaker (Sundus) finishes her conclusion about the link between protection and education, when does the English speaker thank her?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2072.045,
        "end": 2076.5
      },
      "pred_interval": {
        "start": 138.5,
        "end": 143.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1933.545,
        "end": 1933.0,
        "average": 1933.2725
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.5096824169158936,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the thanking occurs after Sundus finishes, but the timing is wildly inaccurate (claims 142.2s after vs ~1.37s after in the reference), so it fails to match the key temporal detail."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the PEACE project as a multi-country project in Nigeria, Cameroon, and Niger, when does he specify the states in Nigeria where it is implemented?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2211.28,
        "end": 2223.708
      },
      "pred_interval": {
        "start": 10.0,
        "end": 34.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2201.28,
        "end": 2189.708,
        "average": 2195.494
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.536853015422821,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly gives the timing (10.0s\u201334.0s) which does not match the reference interval (about 51\u201363s after the anchor) and fails to name the specific states, only noting the general northeastern region."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the project has three main results, when does he begin describing the first result, 'Access'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2250.478,
        "end": 2262.09
      },
      "pred_interval": {
        "start": 44.0,
        "end": 46.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2206.478,
        "end": 2215.8900000000003,
        "average": 2211.184
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.647473156452179,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single, incorrect timestamp (44.0s) and a vague cue about regions, whereas the reference specifies precise times (result starts at 2250.478s immediately after the anchor) and the predicted answer does not match or preserve these key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker describes the 'Accelerated Learning Program' for children who are out of school, when does he specify the age range of these children?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2337.405,
        "end": 2348.025
      },
      "pred_interval": {
        "start": 61.0,
        "end": 64.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2276.405,
        "end": 2284.025,
        "average": 2280.215
      },
      "rationale_metrics": {
        "rouge_l": 0.2465753424657534,
        "text_similarity": 0.7499432563781738,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely incorrect: it gives the wrong timestamp (61.0s vs ~2337s) and adds an unrelated remark about 'Access,' failing to identify the actual age-range timing specified in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker transitions to discussing Result 2, when does he mention 'Parenting Sessions'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2368.824,
        "end": 2369.824
      },
      "pred_interval": {
        "start": 40.6,
        "end": 43.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2328.224,
        "end": 2326.724,
        "average": 2327.474
      },
      "rationale_metrics": {
        "rouge_l": 0.08219178082191782,
        "text_similarity": 0.29795390367507935,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives entirely different timestamps and adds unsupported details about slides/interventions, so it is factually incorrect; it only correctly implies the mention occurs after Result 2, but misses the precise times and key elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"Next slide,\" when does he begin asking about integrating child protection and education activities?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2755.7,
        "end": 2766.6
      },
      "pred_interval": {
        "start": 2670.0,
        "end": 2701.7175795232147
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.69999999999982,
        "end": 64.88242047678523,
        "average": 75.29121023839252
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6126353740692139,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the content of the E2 question but gets both events' timings drastically wrong (off by ~80s), mislabels E1 (not the 'Next slide' utterance), and gives an imprecise temporal relation ('after' vs once_finished)."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker (Kunja) finishes explaining that child protection activities require more funding, when does Amanda thank him?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2899.8,
        "end": 2900.8
      },
      "pred_interval": {
        "start": 10.753743981543318,
        "end": 13.298080235125125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2889.046256018457,
        "end": 2887.501919764875,
        "average": 2888.2740878916657
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.7586382627487183,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the temporal relation right but misidentifies who thanks whom (says Kunja thanks Amanda instead of Amanda thanking Kunja) and gives incorrect timestamps/confident hallucinated reasoning, so it fails on key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "Once Amanda finishes introducing Mike and hands over to him, when does Mike thank Amanda?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2938.9,
        "end": 2940.5
      },
      "pred_interval": {
        "start": 12.748432087561367,
        "end": 13.298080235125125
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2926.1515679124386,
        "end": 2927.2019197648747,
        "average": 2926.676743838657
      },
      "rationale_metrics": {
        "rouge_l": 0.2588235294117647,
        "text_similarity": 0.6676228046417236,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures that Mike thanks Amanda immediately after her handover and includes the quoted words, and the relative timing aligns with the reference; it omits the anchor's end vs. start distinction and the target's end time, so it's not a perfect match."
      }
    },
    {
      "question_id": "001",
      "question": "While the male speaker introduces the first question about project findings, when does he ask the panelists to be brief?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3039.0,
        "end": 3046.5
      },
      "pred_interval": {
        "start": 10.4,
        "end": 12.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3028.6,
        "end": 3034.4,
        "average": 3031.5
      },
      "rationale_metrics": {
        "rouge_l": 0.16470588235294117,
        "text_similarity": 0.5840895175933838,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the event order and short temporal gap (1.7s vs ~1.5s) but fails to match the provided absolute timestamps and event spans, giving single short-scale times rather than the correct 3030\u20133046s range."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker thanks Ahlam for her input, when does he introduce Sundus?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3153.0,
        "end": 3159.0
      },
      "pred_interval": {
        "start": 12.4,
        "end": 14.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3140.6,
        "end": 3144.5,
        "average": 3142.55
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753088,
        "text_similarity": 0.685616672039032,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and the reported 2.1s gap are completely inconsistent with the ground-truth timestamps (~3130.5\u20133132.3s and 3153.0\u20133159.0s, a ~21\u201322s gap) and it omits the intervening remark noted in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After Sundus finishes speaking in Arabic, when does Paul ask 'Can you say that again?'",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3234.7,
        "end": 3235.7
      },
      "pred_interval": {
        "start": 24.9,
        "end": 27.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3209.7999999999997,
        "end": 3207.8999999999996,
        "average": 3208.8499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1395348837209302,
        "text_similarity": 0.5186554789543152,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the order (male introduction then Paul's request) but gives substantially incorrect timestamps and time difference (predicted 24.9\u219227.8s, \u03942.9s vs. ground truth 3215.0\u21923234.7s, \u0394\u224819.7s) and omits the explicit link to Sundus finishing speaking."
      }
    },
    {
      "question_id": "001",
      "question": "After Sundus finishes speaking, when does Mike thank her for her input?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3217.7,
        "end": 3221.9
      },
      "pred_interval": {
        "start": 20.0,
        "end": 24.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3197.7,
        "end": 3197.4,
        "average": 3197.55
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.6947940587997437,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and event order, but the timestamps are materially incorrect (20.0/23.8s vs. 3215.5/3217.7\u20133221.9s), it omits E2's end time, and adds an unsupported quotation, so it fails to match the reference facts."
      }
    },
    {
      "question_id": "002",
      "question": "After Mike asks Paul to share a main finding from his project, when does Paul ask Mike to repeat the question?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3234.3,
        "end": 3235.5
      },
      "pred_interval": {
        "start": 32.3,
        "end": 36.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3202.0,
        "end": 3198.6,
        "average": 3200.3
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7045634984970093,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and that Paul\u2019s question occurs after Mike\u2019s, but it gives completely incorrect timestamps (and omits end times), so key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes explaining how the integrated approach helps children, when does Mike thank him for his insights?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3306.5,
        "end": 3307.4
      },
      "pred_interval": {
        "start": 53.0,
        "end": 56.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3253.5,
        "end": 3250.9,
        "average": 3252.2
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.5498472452163696,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (Mike thanks immediately after Paul) and the verbal cue, but it provides different timestamps (likely relative) than the reference absolute times and omits the E2 end time, so it is only partially aligned."
      }
    },
    {
      "question_id": "003",
      "question": "Once Sarah finishes explaining that multi-year programs allow for transition and protection support, when does she mention Niger's innovation of a cash intervention?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3520.695,
        "end": 3536.3
      },
      "pred_interval": {
        "start": 3429.4,
        "end": 3451.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 91.29500000000007,
        "end": 85.0,
        "average": 88.14750000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.28070175438596495,
        "text_similarity": 0.661107063293457,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relative relation ('after') and the target event (Niger's cash intervention), but the timestamps are substantially incorrect (both E1 and E2 start times differ by ~80\u201370+ seconds from ground truth) and the predicted answer omits the correct end time for E2."
      }
    },
    {
      "question_id": "001",
      "question": "After Ahlam states that the first lesson involves integrating child protection with educational activities, when does she explain how child protection creates a safe environment for children?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3626.1,
        "end": 3634.8
      },
      "pred_interval": {
        "start": 1.1832927913015625,
        "end": 1.6712853023486087
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3624.9167072086984,
        "end": 3633.1287146976515,
        "average": 3629.022710953175
      },
      "rationale_metrics": {
        "rouge_l": 0.14814814814814817,
        "text_similarity": 0.6703565120697021,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but misidentifies both events and gives completely different timestamps and event descriptions than the reference, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ahlam finishes speaking and says 'Thanks Mike. Over to you.', when does the host thank Ahlam for her insights?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3679.3,
        "end": 3687.5
      },
      "pred_interval": {
        "start": 31.260219752750775,
        "end": 31.949912185340946
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3648.0397802472494,
        "end": 3655.550087814659,
        "average": 3651.794934030954
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.6851202249526978,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the host thanks Ahlam after her turn, but the event timestamps greatly mismatch the reference (predicted times do not align with the given absolute times) and the temporal relation labeling is less precise, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the host asks Sundus about the lessons learned from the monitoring system in Northern Syria, when does Sundus provide her initial lessons learned?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3704.2,
        "end": 3724.0
      },
      "pred_interval": {
        "start": 64.86160965457279,
        "end": 65.44129620110404
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3639.338390345427,
        "end": 3658.5587037988957,
        "average": 3648.948547072161
      },
      "rationale_metrics": {
        "rouge_l": 0.20224719101123595,
        "text_similarity": 0.7101322412490845,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation (host then Sundus), but the provided timestamps do not match the reference timings (significantly different numerical values), so the answer is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once Sundus finishes asking Mike to repeat the question, when does Mike start repeating the question?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3785.8,
        "end": 3796.2
      },
      "pred_interval": {
        "start": 3752.6886851341196,
        "end": 3830.589108910891
      },
      "iou": 0.13350376667784905,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.11131486588056,
        "end": 34.38910891089108,
        "average": 33.75021188838582
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7513929605484009,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal ordering right (Mike repeats after Sundus) but the provided timestamps are substantially off from the reference (E1 ~28s early, E2 ~45s late), so it fails to match the precise timing required."
      }
    },
    {
      "question_id": "002",
      "question": "After Sundus finishes providing her answer, when does Mike thank her for her input?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3851.7,
        "end": 3853.6
      },
      "pred_interval": {
        "start": 3830.589108910891,
        "end": 3898.9796296296295
      },
      "iou": 0.02778162792200386,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.110891089108918,
        "end": 45.37962962962956,
        "average": 33.24526035936924
      },
      "rationale_metrics": {
        "rouge_l": 0.29999999999999993,
        "text_similarity": 0.6123434901237488,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the timestamps are substantially different from the reference (predicted ~3899/3903s vs reference ~3850/3851.7s) and it omits Mike's end time, so it is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After Mike says he will ask Ahlam the same question, when does Ahlam ask for clarification on the question?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3864.0,
        "end": 3872.7
      },
      "pred_interval": {
        "start": 3903.5156250000005,
        "end": 3946.7361111111113
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.515625000000455,
        "end": 74.0361111111115,
        "average": 56.775868055555975
      },
      "rationale_metrics": {
        "rouge_l": 0.29508196721311475,
        "text_similarity": 0.6872023940086365,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (Ahlam asks after Mike) but the absolute timestamps are off by ~87\u201390s, it mischaracterizes E1's phrasing (starting vs finishing) and omits Ahlam's completion time, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman in the bottom left finishes explaining how all staff were working together, when does the male host in the top left thank Ahlam?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3937.666,
        "end": 3939.047
      },
      "pred_interval": {
        "start": 1.0444437662760417,
        "end": 3.0444437662760415
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3936.6215562337243,
        "end": 3936.002556233724,
        "average": 3936.3120562337244
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131145,
        "text_similarity": 0.4438086748123169,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: it claims the thanks occurs at the very beginning of the video, contradicting the ground-truth timestamps showing the male host thanks Ahlam around 3937.666\u20133939.047s after Ahlam's speech ends."
      }
    },
    {
      "question_id": "002",
      "question": "Once the male host finishes asking what type of evidence ECHO asked for, when does the female speaker (Sarah) in the top middle-right start to respond?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3981.063,
        "end": 3984.022
      },
      "pred_interval": {
        "start": 15.866665940057665,
        "end": 16.766665940057663
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3965.1963340599423,
        "end": 3967.2553340599425,
        "average": 3966.2258340599424
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.4554538130760193,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that Sarah begins responding shortly after the host, but it omits the precise timing details given in the reference (E1 ends at 3979.308s; E2 starts at 3981.063s and ends at 3984.022s), so it lacks required specificity."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes explaining how learning is put into action and fits into different contexts, when does he say 'Thank you'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4068.612,
        "end": 4069.152
      },
      "pred_interval": {
        "start": 92.13333260672432,
        "end": 94.63333260672432
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3976.478667393276,
        "end": 3974.518667393276,
        "average": 3975.498667393276
      },
      "rationale_metrics": {
        "rouge_l": 0.163265306122449,
        "text_similarity": 0.48371025919914246,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures that Paul says 'Thank you' immediately after finishing his explanation, but it omits the precise timestamps provided in the correct answer (4068.251s \u2192 4068.612\u20134069.152s), so it is incomplete. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the host says \"go ahead Paul, sorry\", when does Paul begin his response?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4123.162,
        "end": 4123.743
      },
      "pred_interval": {
        "start": 4160.0,
        "end": 4230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.83799999999974,
        "end": 106.2569999999996,
        "average": 71.54749999999967
      },
      "rationale_metrics": {
        "rouge_l": 0.125,
        "text_similarity": 0.5974380373954773,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted times and temporal relation are incorrect: it wrongly places the host and Paul much later and with a long delay, whereas the ground truth shows Paul speaking immediately after the host (once_finished) around 4123s. This contradicts the reference and omits the correct timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Paul finishes his feedback and says \"Thank you\", when does the host respond?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4179.089,
        "end": 4183.52
      },
      "pred_interval": {
        "start": 4290.0,
        "end": 4320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 110.91100000000006,
        "end": 136.47999999999956,
        "average": 123.69549999999981
      },
      "rationale_metrics": {
        "rouge_l": 0.12698412698412698,
        "text_similarity": 0.51756751537323,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely misaligned: it gives different times (~4290\u20134320s vs ~4187\u20134189s), the wrong responder (Kunja vs the host), and indicates a delayed response rather than the immediate reply described in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the host explicitly says \"So thank you for that\" to Kunja, when does she transition to Lynn's question?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4298.586,
        "end": 4300.0
      },
      "pred_interval": {
        "start": 4290.0,
        "end": 4350.0
      },
      "iou": 0.023566666666662666,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.58600000000024,
        "end": 50.0,
        "average": 29.29300000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.20779220779220778,
        "text_similarity": 0.44396457076072693,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect and inconsistent timestamps (saying 4290.0s and 4350.0s) compared to the reference (~4295s and ~4298.6\u20134300s) and thus misstates the transition timing; it only correctly notes a generic 'after' relationship, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker thanks the previous participant, when does he introduce a question from Lynn?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4296.586,
        "end": 4299.158
      },
      "pred_interval": {
        "start": 4306.315129770027,
        "end": 4311.629569260699
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.729129770026702,
        "end": 12.471569260698743,
        "average": 11.100349515362723
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6445517539978027,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer mislabels and mis-times both events (timestamps are ~10\u201315s off and event boundaries swapped), so it does not match the reference; only the 'after' relation is correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes reading Lynn's question, when does he ask Sundus for her opinion?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4324.216,
        "end": 4328.945
      },
      "pred_interval": {
        "start": 4370.229443874398,
        "end": 4409.795069852811
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.013443874398035,
        "end": 80.85006985281143,
        "average": 63.43175686360473
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6676871180534363,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after', but the provided timestamps are substantially different from the ground truth and it reports E1's start rather than the correct E1 end time; thus the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Sundus finishes stating that violence is not the only challenge, when does the speaker confirm that she answered part of the question?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4415.375,
        "end": 4425.293
      },
      "pred_interval": {
        "start": 4527.179527963351,
        "end": 4532.345522521653
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.80452796335067,
        "end": 107.05252252165337,
        "average": 109.42852524250202
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.6264278292655945,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the event timestamps and boundaries are significantly misaligned with the ground truth (wrong start/end times and missing E1 end), so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the host asks if the panelist agrees with his statement, when does Ahlam reply with 'Yes, yes, Mike'?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4479.36,
        "end": 4480.3
      },
      "pred_interval": {
        "start": 4480.906926427543,
        "end": 4482.106926427543
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5469264275434398,
        "end": 1.8069264275427486,
        "average": 1.6769264275430942
      },
      "rationale_metrics": {
        "rouge_l": 0.2641509433962264,
        "text_similarity": 0.5159440636634827,
        "llm_judge_score": 5,
        "llm_judge_justification": "The predicted timestamp (4480.907s) is close but about 0.6s after the correct end time (4480.3s) and falls outside the correct start\u2013end interval (4479.36\u20134480.3s); it provides a single, slightly late time instead of the precise start and end, so it's partially correct but inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking who would like to answer the question, when does Paul Bagambe begin to speak?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4707.194,
        "end": 4709.296
      },
      "pred_interval": {
        "start": 4776.0,
        "end": 4786.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.80599999999959,
        "end": 76.70399999999972,
        "average": 72.75499999999965
      },
      "rationale_metrics": {
        "rouge_l": 0.4126984126984127,
        "text_similarity": 0.7958571314811707,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly places Paul speaking after the host, but the absolute timestamps are wrong by ~70\u201380s and it incorrectly states there is no significant delay (predicted 10s gap vs. actual ~1.5s), so it contradicts key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Paul Bagambe mentions 'praise singers', when does he elaborate on what they do?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4717.127,
        "end": 4732.251
      },
      "pred_interval": {
        "start": 4785.5,
        "end": 4806.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.37299999999959,
        "end": 74.54899999999998,
        "average": 71.46099999999979
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.5730494260787964,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies that Paul elaborates, but gives a significantly incorrect timestamp (4785.5s vs. the correct 4717.127\u20134732.251s) and adds an unsupported detail ('initiative'); thus it fails on key factual timing and includes hallucination."
      }
    },
    {
      "question_id": "003",
      "question": "After the host thanks Paul, when does Sindus (woman in bottom-left video) start speaking?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4827.512,
        "end": 4830.216
      },
      "pred_interval": {
        "start": 4765.8,
        "end": 4800.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.711999999999534,
        "end": 29.816000000000713,
        "average": 45.764000000000124
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.5770654678344727,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the ordering (Sindus speaks after the host) but gives incorrect absolute timestamps (both events are ~40s earlier than the ground truth), so the timing is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker (Sundus) finishes her concluding remarks in Arabic, when does the moderator thank her and explain the fast wrap-up session?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4897.7,
        "end": 4916.5
      },
      "pred_interval": {
        "start": 4830.7,
        "end": 4835.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.0,
        "end": 81.19999999999982,
        "average": 74.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.24000000000000005,
        "text_similarity": 0.3620986044406891,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only states that the moderator thanks Sundus after her remarks, which matches part of the relation, but it omits the precise timing details and incorrectly says the moderator introduces the next speaker instead of a fast wrap-up, so it is largely incomplete and partially inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During Sarah's key message, when does she mention strengthening the existing system and training teachers?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4936.0,
        "end": 4942.9
      },
      "pred_interval": {
        "start": 5041.5,
        "end": 5046.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.5,
        "end": 103.70000000000073,
        "average": 104.60000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.5344685912132263,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the relation that the mention occurs during Sarah's key message, but it omits the critical timing details (start/end timestamps and the specific interval 4936.0\u20134942.9) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks Sundus for her key takeaway message, when does Sundus begin speaking in Arabic?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 4985.7,
        "end": 5026.77
      },
      "pred_interval": {
        "start": 5019.0,
        "end": 5020.6
      },
      "iou": 0.03895787679572291,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.30000000000018,
        "end": 6.170000000000073,
        "average": 19.735000000000127
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.5403072834014893,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mostly mismatches the reference: timestamps, speaker identity, utterance content, and durations are incorrect; only the temporal relation ('after') coincidentally matches. Significant factual elements from the correct answer are omitted or wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the host thanks Paul, when does Amanda, the next speaker, start speaking?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 5190.0,
        "end": 5356.2699999999995
      },
      "gt_interval": {
        "start": 5251.699,
        "end": 5254.524
      },
      "pred_interval": {
        "start": 1.7,
        "end": 6.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5249.999,
        "end": 5248.224,
        "average": 5249.1115
      },
      "rationale_metrics": {
        "rouge_l": 0.3018867924528302,
        "text_similarity": 0.4744632840156555,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly states Amanda begins 1.7s after the host, whereas the ground truth shows she begins at 5251.699s (about 45.36s after the host finishes at 5206.342s); the timing is substantially wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once Amanda says 'Over to you, David', when does David start speaking?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 5190.0,
        "end": 5356.2699999999995
      },
      "gt_interval": {
        "start": 5287.087,
        "end": 5288.53
      },
      "pred_interval": {
        "start": 54.0,
        "end": 54.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5233.087,
        "end": 5234.33,
        "average": 5233.708500000001
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.4862406253814697,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly notes David speaks after Amanda, but the provided timestamp (54.0s) is far from the correct start time (~5287.087s) and omits the precise timing details, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After David finishes explaining about the feedback form, when does he start giving instructions about the FILO page and WELO coffee lounge?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 5190.0,
        "end": 5356.2699999999995
      },
      "gt_interval": {
        "start": 5309.965,
        "end": 5315.734
      },
      "pred_interval": {
        "start": 57.6,
        "end": 58.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5252.365,
        "end": 5257.734,
        "average": 5255.0495
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.5817893147468567,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies that FILO/WELO instructions follow the feedback form but gives incorrect timestamps (57.6s\u219258.0s vs reference 5304.217s\u21925309.965s) and omits the E2 end time, so it fails to match the correct timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the expectations for IRC Child Protection Team Members, when does he finish describing the expectations for IRC Education Team Members?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2490.27,
        "end": 2535.335
      },
      "pred_interval": {
        "start": 2494.3,
        "end": 2570.9
      },
      "iou": 0.508929678779608,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0300000000002,
        "end": 35.565000000000055,
        "average": 19.797500000000127
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.7255814075469971,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect: timestamps and event boundaries do not match the ground truth and it introduces hallucinated content, with only the high-level 'after' relationship coinciding with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says that 'cross-sector collaboration is really needed to achieve peace expectation', when does he start talking about the integration across the four education models?",
      "video_id": "ASodCu91spY",
      "video_number": "018",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2555.112,
        "end": 2569.771
      },
      "pred_interval": {
        "start": 2761.3,
        "end": 2769.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 206.1880000000001,
        "end": 199.92899999999963,
        "average": 203.05849999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6984121799468994,
        "llm_judge_score": 2,
        "llm_judge_justification": "While both state the relation as 'after', the predicted answer misidentifies and mis-times the anchor/target segments (wrong start/end times and segment labels) and fails to locate when the four-model integration is described, so it is largely incorrect."
      }
    }
  ]
}