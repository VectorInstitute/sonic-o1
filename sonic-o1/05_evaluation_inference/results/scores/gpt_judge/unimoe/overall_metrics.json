{
  "model": "unimoe",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.23205799779498643,
            "rouge_l_std": 0.052929056254944415,
            "text_similarity_mean": 0.7638907246291637,
            "text_similarity_std": 0.06181663158758067,
            "llm_judge_score_mean": 6.125,
            "llm_judge_score_std": 1.3169567191065923
          },
          "short": {
            "rouge_l_mean": 0.22672983845423555,
            "rouge_l_std": 0.07317698959324745,
            "text_similarity_mean": 0.6891876235604286,
            "text_similarity_std": 0.09750560254398513,
            "llm_judge_score_mean": 5.875,
            "llm_judge_score_std": 1.79843682124227
          },
          "cider": {
            "cider_detailed": 0.005244367793653308,
            "cider_short": 6.48412438448617e-07
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.21046951819355414,
            "rouge_l_std": 0.03385526580372614,
            "text_similarity_mean": 0.7505876507077899,
            "text_similarity_std": 0.06845747221597669,
            "llm_judge_score_mean": 6.761904761904762,
            "llm_judge_score_std": 1.6876233473018432
          },
          "short": {
            "rouge_l_mean": 0.1748841905178781,
            "rouge_l_std": 0.054370899269919136,
            "text_similarity_mean": 0.614263748838788,
            "text_similarity_std": 0.12188123440087073,
            "llm_judge_score_mean": 5.761904761904762,
            "llm_judge_score_std": 1.743143354068869
          },
          "cider": {
            "cider_detailed": 0.04977754087792087,
            "cider_short": 0.030449444022400057
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.20059980736813487,
            "rouge_l_std": 0.037412620093667366,
            "text_similarity_mean": 0.7257437838448418,
            "text_similarity_std": 0.08173086915647064,
            "llm_judge_score_mean": 5.722222222222222,
            "llm_judge_score_std": 2.129307544081867
          },
          "short": {
            "rouge_l_mean": 0.15475635462551526,
            "rouge_l_std": 0.07307895476850965,
            "text_similarity_mean": 0.6531608932548099,
            "text_similarity_std": 0.1302523312107001,
            "llm_judge_score_mean": 5.0,
            "llm_judge_score_std": 2.0548046676563256
          },
          "cider": {
            "cider_detailed": 0.025368019588273113,
            "cider_short": 1.516432298275909e-05
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.20555960726597702,
            "rouge_l_std": 0.0402217461203944,
            "text_similarity_mean": 0.6899367411931355,
            "text_similarity_std": 0.09732997915283206,
            "llm_judge_score_mean": 4.333333333333333,
            "llm_judge_score_std": 1.6599866130651642
          },
          "short": {
            "rouge_l_mean": 0.1529737203044325,
            "rouge_l_std": 0.07029203547792764,
            "text_similarity_mean": 0.6243795533974965,
            "text_similarity_std": 0.0984605778692951,
            "llm_judge_score_mean": 3.6,
            "llm_judge_score_std": 1.4047538337136987
          },
          "cider": {
            "cider_detailed": 0.023523842689629162,
            "cider_short": 0.0016253167737730926
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.2111359595656093,
            "rouge_l_std": 0.05745774399500875,
            "text_similarity_mean": 0.6673833292264205,
            "text_similarity_std": 0.15437065919700235,
            "llm_judge_score_mean": 4.076923076923077,
            "llm_judge_score_std": 1.8589301497837805
          },
          "short": {
            "rouge_l_mean": 0.11777491160543214,
            "rouge_l_std": 0.05448062928569607,
            "text_similarity_mean": 0.5256015543754284,
            "text_similarity_std": 0.13111239460317045,
            "llm_judge_score_mean": 2.8461538461538463,
            "llm_judge_score_std": 1.2307692307692308
          },
          "cider": {
            "cider_detailed": 6.071854988459292e-07,
            "cider_short": 1.6960532859859468e-06
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.20437006199978386,
            "rouge_l_std": 0.030356059262286312,
            "text_similarity_mean": 0.6402034282684326,
            "text_similarity_std": 0.10398683573164537,
            "llm_judge_score_mean": 3.9,
            "llm_judge_score_std": 1.997498435543818
          },
          "short": {
            "rouge_l_mean": 0.17809003905042026,
            "rouge_l_std": 0.08470151370851421,
            "text_similarity_mean": 0.5588995203375816,
            "text_similarity_std": 0.14204609231570023,
            "llm_judge_score_mean": 3.95,
            "llm_judge_score_std": 1.7741194999210173
          },
          "cider": {
            "cider_detailed": 0.03113284736880424,
            "cider_short": 0.003927313717710438
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.19516407723355741,
            "rouge_l_std": 0.03416247030740117,
            "text_similarity_mean": 0.6865883810179574,
            "text_similarity_std": 0.08741817307002356,
            "llm_judge_score_mean": 3.2857142857142856,
            "llm_judge_score_std": 1.1605769149479943
          },
          "short": {
            "rouge_l_mean": 0.14047416765609438,
            "rouge_l_std": 0.05995872787386157,
            "text_similarity_mean": 0.5677473821810314,
            "text_similarity_std": 0.11229845327769948,
            "llm_judge_score_mean": 3.5714285714285716,
            "llm_judge_score_std": 1.635074734608514
          },
          "cider": {
            "cider_detailed": 7.840296955215881e-44,
            "cider_short": 5.143593035089264e-09
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.1915365045445472,
            "rouge_l_std": 0.03220377775581154,
            "text_similarity_mean": 0.6887648701667786,
            "text_similarity_std": 0.07845376791104182,
            "llm_judge_score_mean": 4.75,
            "llm_judge_score_std": 1.6393596310755
          },
          "short": {
            "rouge_l_mean": 0.1565076670435034,
            "rouge_l_std": 0.05245600716736276,
            "text_similarity_mean": 0.6348784640431404,
            "text_similarity_std": 0.11811993007339687,
            "llm_judge_score_mean": 4.75,
            "llm_judge_score_std": 1.920286436967152
          },
          "cider": {
            "cider_detailed": 0.003731881030619297,
            "cider_short": 0.0012091533066846878
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.2200278789505371,
            "rouge_l_std": 0.05283031965365445,
            "text_similarity_mean": 0.6677896703282992,
            "text_similarity_std": 0.10937829001714733,
            "llm_judge_score_mean": 4.375,
            "llm_judge_score_std": 1.6282019735483269
          },
          "short": {
            "rouge_l_mean": 0.16720732404422023,
            "rouge_l_std": 0.08198097656010851,
            "text_similarity_mean": 0.5572268900771936,
            "text_similarity_std": 0.1326189063636255,
            "llm_judge_score_mean": 3.9166666666666665,
            "llm_judge_score_std": 1.288302069478359
          },
          "cider": {
            "cider_detailed": 0.0013907439357862383,
            "cider_short": 0.03238595934833502
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.19835151629122635,
            "rouge_l_std": 0.041713756033234885,
            "text_similarity_mean": 0.6736340380233267,
            "text_similarity_std": 0.14869945808076349,
            "llm_judge_score_mean": 4.913043478260869,
            "llm_judge_score_std": 2.2246258792114832
          },
          "short": {
            "rouge_l_mean": 0.16484393050893062,
            "rouge_l_std": 0.06261741457947453,
            "text_similarity_mean": 0.6026382932196492,
            "text_similarity_std": 0.15150327994121876,
            "llm_judge_score_mean": 4.521739130434782,
            "llm_judge_score_std": 2.082422611274443
          },
          "cider": {
            "cider_detailed": 0.02746795714336151,
            "cider_short": 0.05889716050434529
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.2517736940014398,
            "rouge_l_std": 0.09372938713797148,
            "text_similarity_mean": 0.791451830130357,
            "text_similarity_std": 0.07583536067094031,
            "llm_judge_score_mean": 6.6923076923076925,
            "llm_judge_score_std": 1.4876215081395163
          },
          "short": {
            "rouge_l_mean": 0.2073054577223779,
            "rouge_l_std": 0.07392987788621916,
            "text_similarity_mean": 0.7238241250698383,
            "text_similarity_std": 0.1007831282190631,
            "llm_judge_score_mean": 6.3076923076923075,
            "llm_judge_score_std": 1.7709022204955904
          },
          "cider": {
            "cider_detailed": 0.030368428719053462,
            "cider_short": 0.10990472605796348
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.19752053232988132,
            "rouge_l_std": 0.05848699360943165,
            "text_similarity_mean": 0.6470536986986796,
            "text_similarity_std": 0.08863334643405883,
            "llm_judge_score_mean": 4.333333333333333,
            "llm_judge_score_std": 2.0816659994661326
          },
          "short": {
            "rouge_l_mean": 0.15160555778234136,
            "rouge_l_std": 0.05966422388480219,
            "text_similarity_mean": 0.5956516050630145,
            "text_similarity_std": 0.1161213440155161,
            "llm_judge_score_mean": 4.0,
            "llm_judge_score_std": 1.6666666666666667
          },
          "cider": {
            "cider_detailed": 1.6382955001427197e-09,
            "cider_short": 1.2300077428873693e-05
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.18735391655239605,
            "rouge_l_std": 0.038184911980816805,
            "text_similarity_mean": 0.701644355836122,
            "text_similarity_std": 0.08689252318111587,
            "llm_judge_score_mean": 1.9565217391304348,
            "llm_judge_score_std": 1.0825999650424987
          },
          "short": {
            "rouge_l_mean": 0.1733828052312299,
            "rouge_l_std": 0.0674755968492768,
            "text_similarity_mean": 0.6450128529382788,
            "text_similarity_std": 0.10593780629646578,
            "llm_judge_score_mean": 2.0,
            "llm_judge_score_std": 0.8846517369293828
          },
          "cider": {
            "cider_detailed": 0.06407578526964927,
            "cider_short": 0.023741009252726784
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.2081477747762793,
          "text_similarity_mean": 0.6995901924670235,
          "llm_judge_score_mean": 4.709638763317693
        },
        "short": {
          "rouge_l_mean": 0.16665661265743165,
          "text_similarity_mean": 0.6148055774120522,
          "llm_judge_score_mean": 4.31542963725238
        },
        "cider": {
          "cider_detailed_mean": 0.020160155633888063,
          "cider_short_mean": 0.020166915153359073
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9215686274509803,
          "correct": 94,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.29630275798603045,
            "rouge_l_std": 0.10171748690269589,
            "text_similarity_mean": 0.75837336100784,
            "text_similarity_std": 0.11371606216747807,
            "llm_judge_score_mean": 8.970588235294118,
            "llm_judge_score_std": 2.1211164390709167
          },
          "rationale_cider": 0.16939215383099182
        },
        "02_Job_Interviews": {
          "accuracy": 0.94,
          "correct": 94,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.28421213459805844,
            "rouge_l_std": 0.08242853810338306,
            "text_similarity_mean": 0.730211708843708,
            "text_similarity_std": 0.1069106617061222,
            "llm_judge_score_mean": 9.21,
            "llm_judge_score_std": 1.7452506983238827
          },
          "rationale_cider": 0.13790155894194908
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.9260869565217391,
          "correct": 213,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.2770361191482646,
            "rouge_l_std": 0.08024776600176116,
            "text_similarity_mean": 0.7455360334852468,
            "text_similarity_std": 0.10616127977811535,
            "llm_judge_score_mean": 8.921739130434782,
            "llm_judge_score_std": 1.8726768847176398
          },
          "rationale_cider": 0.19786981984307056
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.7692307692307693,
          "correct": 30,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.26728575804498617,
            "rouge_l_std": 0.0622887807195076,
            "text_similarity_mean": 0.7431815946713473,
            "text_similarity_std": 0.10002386830051103,
            "llm_judge_score_mean": 7.0256410256410255,
            "llm_judge_score_std": 2.9827467936853065
          },
          "rationale_cider": 0.22507864660190066
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9217391304347826,
          "correct": 106,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.27312070467825855,
            "rouge_l_std": 0.08137769473892915,
            "text_similarity_mean": 0.7357872009601282,
            "text_similarity_std": 0.1299908146660411,
            "llm_judge_score_mean": 8.530434782608696,
            "llm_judge_score_std": 2.363938273049905
          },
          "rationale_cider": 0.13491021415386917
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.8505747126436781,
          "correct": 74,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.24004132285928312,
            "rouge_l_std": 0.08654578015578462,
            "text_similarity_mean": 0.6930184569316863,
            "text_similarity_std": 0.18052854124720383,
            "llm_judge_score_mean": 7.528735632183908,
            "llm_judge_score_std": 3.2829727223397542
          },
          "rationale_cider": 0.16142041633106619
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.8823529411764706,
          "correct": 45,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.24167137699255958,
            "rouge_l_std": 0.0848084932566305,
            "text_similarity_mean": 0.7000797262378767,
            "text_similarity_std": 0.16888949535340314,
            "llm_judge_score_mean": 7.509803921568627,
            "llm_judge_score_std": 3.2801993544128543
          },
          "rationale_cider": 0.22476276056395916
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 0.9104477611940298,
          "correct": 61,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.2581117662528101,
            "rouge_l_std": 0.07791964435016617,
            "text_similarity_mean": 0.7259564975193188,
            "text_similarity_std": 0.15401040192299406,
            "llm_judge_score_mean": 8.701492537313433,
            "llm_judge_score_std": 2.2791391761524054
          },
          "rationale_cider": 0.208703577885787
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.8527131782945736,
          "correct": 110,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.2750869821471507,
            "rouge_l_std": 0.08830774110696775,
            "text_similarity_mean": 0.7253406010856924,
            "text_similarity_std": 0.13893586284855766,
            "llm_judge_score_mean": 7.821705426356589,
            "llm_judge_score_std": 2.897338025525603
          },
          "rationale_cider": 0.20528080602694912
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.8,
          "correct": 72,
          "total": 90,
          "rationale": {
            "rouge_l_mean": 0.27529497475429027,
            "rouge_l_std": 0.087900168986472,
            "text_similarity_mean": 0.7499105475842953,
            "text_similarity_std": 0.1535711534273915,
            "llm_judge_score_mean": 7.5,
            "llm_judge_score_std": 3.0120129850686603
          },
          "rationale_cider": 0.2403912430080877
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 0.9384615384615385,
          "correct": 61,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.24623397695529958,
            "rouge_l_std": 0.08590647365819619,
            "text_similarity_mean": 0.7251196470398169,
            "text_similarity_std": 0.15450920823101583,
            "llm_judge_score_mean": 8.707692307692307,
            "llm_judge_score_std": 2.237602267009364
          },
          "rationale_cider": 0.09420567502814112
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.9365079365079365,
          "correct": 177,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.2836956040453261,
            "rouge_l_std": 0.10567585969206258,
            "text_similarity_mean": 0.7323189926870838,
            "text_similarity_std": 0.12492961093680491,
            "llm_judge_score_mean": 8.529100529100528,
            "llm_judge_score_std": 2.1493076861111438
          },
          "rationale_cider": 0.17565488603137172
        },
        "13_Olympics": {
          "accuracy": 0.8115942028985508,
          "correct": 56,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.22289186626699517,
            "rouge_l_std": 0.08761815685773681,
            "text_similarity_mean": 0.6874268229888834,
            "text_similarity_std": 0.19049060779278695,
            "llm_judge_score_mean": 6.63768115942029,
            "llm_judge_score_std": 3.5589080498147654
          },
          "rationale_cider": 0.14922295080229464
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8816367503703884,
        "rationale": {
          "rouge_l_mean": 0.2646911803637933,
          "text_similarity_mean": 0.7270970146956095,
          "llm_judge_score_mean": 8.122662668278023
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.019601818780079737,
          "std_iou": 0.1027069727653942,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.022813688212927757,
            "count": 6,
            "total": 263
          },
          "R@0.5": {
            "recall": 0.019011406844106463,
            "count": 5,
            "total": 263
          },
          "R@0.7": {
            "recall": 0.011406844106463879,
            "count": 3,
            "total": 263
          },
          "mae": {
            "start_mean": 1049.7055670094423,
            "end_mean": 4604.455889339761,
            "average_mean": 2827.0807281746006
          },
          "rationale": {
            "rouge_l_mean": 0.2405715567101078,
            "rouge_l_std": 0.09244631881477205,
            "text_similarity_mean": 0.5387881605358291,
            "text_similarity_std": 0.1830448510415869,
            "llm_judge_score_mean": 2.102661596958175,
            "llm_judge_score_std": 1.9579497083399062
          },
          "rationale_cider": 0.29517855392378756
        },
        "02_Job_Interviews": {
          "mean_iou": 0.024183957551942404,
          "std_iou": 0.10159476915589971,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.044534412955465584,
            "count": 11,
            "total": 247
          },
          "R@0.5": {
            "recall": 0.012145748987854251,
            "count": 3,
            "total": 247
          },
          "R@0.7": {
            "recall": 0.004048582995951417,
            "count": 1,
            "total": 247
          },
          "mae": {
            "start_mean": 622.4792576852092,
            "end_mean": 616.1231116446794,
            "average_mean": 619.3011846649446
          },
          "rationale": {
            "rouge_l_mean": 0.2287280085058594,
            "rouge_l_std": 0.09618815766409554,
            "text_similarity_mean": 0.501050373682609,
            "text_similarity_std": 0.18955266615312813,
            "llm_judge_score_mean": 2.133603238866397,
            "llm_judge_score_std": 1.91583122531087
          },
          "rationale_cider": 0.2934449098308956
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.009391988803267996,
          "std_iou": 0.05966607037735632,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.009505703422053232,
            "count": 5,
            "total": 526
          },
          "R@0.5": {
            "recall": 0.009505703422053232,
            "count": 5,
            "total": 526
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 526
          },
          "mae": {
            "start_mean": 1422.3177863908475,
            "end_mean": 1423.7050874388665,
            "average_mean": 1423.011436914857
          },
          "rationale": {
            "rouge_l_mean": 0.23102684343083774,
            "rouge_l_std": 0.08522560306235304,
            "text_similarity_mean": 0.5421381343807671,
            "text_similarity_std": 0.18205652539658343,
            "llm_judge_score_mean": 2.214828897338403,
            "llm_judge_score_std": 1.9277463420577496
          },
          "rationale_cider": 0.2077791409046163
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.030893280750059243,
          "std_iou": 0.10614071921111799,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05217391304347826,
            "count": 6,
            "total": 115
          },
          "R@0.5": {
            "recall": 0.008695652173913044,
            "count": 1,
            "total": 115
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 115
          },
          "mae": {
            "start_mean": 315.766462603932,
            "end_mean": 315.7717191107363,
            "average_mean": 315.76909085733405
          },
          "rationale": {
            "rouge_l_mean": 0.2449242897530197,
            "rouge_l_std": 0.10099725148687407,
            "text_similarity_mean": 0.5434802907964458,
            "text_similarity_std": 0.16611618188428873,
            "llm_judge_score_mean": 1.9391304347826086,
            "llm_judge_score_std": 1.7707132265877652
          },
          "rationale_cider": 0.35740523309649425
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.010753309944818221,
          "std_iou": 0.07118991303041931,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.011730205278592375,
            "count": 4,
            "total": 341
          },
          "R@0.5": {
            "recall": 0.005865102639296188,
            "count": 2,
            "total": 341
          },
          "R@0.7": {
            "recall": 0.005865102639296188,
            "count": 2,
            "total": 341
          },
          "mae": {
            "start_mean": 1532.1841002574558,
            "end_mean": 1534.9679958448023,
            "average_mean": 1533.576048051129
          },
          "rationale": {
            "rouge_l_mean": 0.23433471939094616,
            "rouge_l_std": 0.10064004118197004,
            "text_similarity_mean": 0.535396126101662,
            "text_similarity_std": 0.21554936853658865,
            "llm_judge_score_mean": 2.002932551319648,
            "llm_judge_score_std": 1.6487772661753135
          },
          "rationale_cider": 0.20185737419947947
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.013934191362621173,
          "std_iou": 0.07540403685586106,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015873015873015872,
            "count": 4,
            "total": 252
          },
          "R@0.5": {
            "recall": 0.003968253968253968,
            "count": 1,
            "total": 252
          },
          "R@0.7": {
            "recall": 0.003968253968253968,
            "count": 1,
            "total": 252
          },
          "mae": {
            "start_mean": 478.39169658083307,
            "end_mean": 510.5966912559845,
            "average_mean": 494.49419391840877
          },
          "rationale": {
            "rouge_l_mean": 0.26698955998531854,
            "rouge_l_std": 0.085104361072729,
            "text_similarity_mean": 0.639622200842178,
            "text_similarity_std": 0.15189800319280622,
            "llm_judge_score_mean": 1.8888888888888888,
            "llm_judge_score_std": 1.3197054879615786
          },
          "rationale_cider": 0.1892177198432015
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.01595950015576815,
          "std_iou": 0.08049579132882523,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.028846153846153848,
            "count": 3,
            "total": 104
          },
          "R@0.5": {
            "recall": 0.009615384615384616,
            "count": 1,
            "total": 104
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 104
          },
          "mae": {
            "start_mean": 550.317590710423,
            "end_mean": 550.3454439424772,
            "average_mean": 550.3315173264501
          },
          "rationale": {
            "rouge_l_mean": 0.23316199023680376,
            "rouge_l_std": 0.06778979005322135,
            "text_similarity_mean": 0.588800042294539,
            "text_similarity_std": 0.13294041958593905,
            "llm_judge_score_mean": 2.4134615384615383,
            "llm_judge_score_std": 2.0550621220400584
          },
          "rationale_cider": 0.11499366697048187
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.048037910444916745,
          "std_iou": 0.14072778673041783,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.06578947368421052,
            "count": 10,
            "total": 152
          },
          "R@0.5": {
            "recall": 0.03289473684210526,
            "count": 5,
            "total": 152
          },
          "R@0.7": {
            "recall": 0.013157894736842105,
            "count": 2,
            "total": 152
          },
          "mae": {
            "start_mean": 394.7536260324287,
            "end_mean": 482.91824887725926,
            "average_mean": 438.83593745484404
          },
          "rationale": {
            "rouge_l_mean": 0.24841589490378008,
            "rouge_l_std": 0.0989873654657104,
            "text_similarity_mean": 0.5449419151991606,
            "text_similarity_std": 0.18568845209008508,
            "llm_judge_score_mean": 1.9605263157894737,
            "llm_judge_score_std": 1.5427315954576377
          },
          "rationale_cider": 0.2923729189007497
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.014344854610834007,
          "std_iou": 0.07968419945152835,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015584415584415584,
            "count": 6,
            "total": 385
          },
          "R@0.5": {
            "recall": 0.007792207792207792,
            "count": 3,
            "total": 385
          },
          "R@0.7": {
            "recall": 0.0025974025974025974,
            "count": 1,
            "total": 385
          },
          "mae": {
            "start_mean": 401.5192623261589,
            "end_mean": 401.84135280590317,
            "average_mean": 401.68030756603105
          },
          "rationale": {
            "rouge_l_mean": 0.24245043745005365,
            "rouge_l_std": 0.09663996705834697,
            "text_similarity_mean": 0.5224883722552618,
            "text_similarity_std": 0.18533403819216682,
            "llm_judge_score_mean": 2.1298701298701297,
            "llm_judge_score_std": 1.8258650278749704
          },
          "rationale_cider": 0.26502084848726853
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.022040922950479405,
          "std_iou": 0.0904035985380321,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02702702702702703,
            "count": 5,
            "total": 185
          },
          "R@0.5": {
            "recall": 0.005405405405405406,
            "count": 1,
            "total": 185
          },
          "R@0.7": {
            "recall": 0.005405405405405406,
            "count": 1,
            "total": 185
          },
          "mae": {
            "start_mean": 429.23114572325005,
            "end_mean": 457.1326851947063,
            "average_mean": 443.18191545897815
          },
          "rationale": {
            "rouge_l_mean": 0.2249717321047221,
            "rouge_l_std": 0.0797125876275155,
            "text_similarity_mean": 0.5173793099971639,
            "text_similarity_std": 0.18447177425736408,
            "llm_judge_score_mean": 2.443243243243243,
            "llm_judge_score_std": 2.0396794316941005
          },
          "rationale_cider": 0.21870105005616278
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.009423842161031154,
          "std_iou": 0.06225686545622377,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.006944444444444444,
            "count": 1,
            "total": 144
          },
          "R@0.5": {
            "recall": 0.006944444444444444,
            "count": 1,
            "total": 144
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 144
          },
          "mae": {
            "start_mean": 430.42181365134064,
            "end_mean": 427.6735250395213,
            "average_mean": 429.04766934543096
          },
          "rationale": {
            "rouge_l_mean": 0.22083739008509817,
            "rouge_l_std": 0.07950608491878204,
            "text_similarity_mean": 0.5694793925310174,
            "text_similarity_std": 0.1909401419087139,
            "llm_judge_score_mean": 1.8333333333333333,
            "llm_judge_score_std": 1.6414763002993509
          },
          "rationale_cider": 0.15422305179756415
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.004428381933890594,
          "std_iou": 0.035699877502251835,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0022271714922048997,
            "count": 1,
            "total": 449
          },
          "R@0.5": {
            "recall": 0.0022271714922048997,
            "count": 1,
            "total": 449
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 449
          },
          "mae": {
            "start_mean": 1133.1642131335923,
            "end_mean": 1090.0231427039414,
            "average_mean": 1111.5936779187668
          },
          "rationale": {
            "rouge_l_mean": 0.22700185294655686,
            "rouge_l_std": 0.10139461324611777,
            "text_similarity_mean": 0.5134564352767306,
            "text_similarity_std": 0.20835098405289584,
            "llm_judge_score_mean": 2.202672605790646,
            "llm_judge_score_std": 2.0246569138673687
          },
          "rationale_cider": 0.18441355436391366
        },
        "13_Olympics": {
          "mean_iou": 0.01295280908725486,
          "std_iou": 0.08493444691031046,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.011627906976744186,
            "count": 1,
            "total": 86
          },
          "R@0.5": {
            "recall": 0.011627906976744186,
            "count": 1,
            "total": 86
          },
          "R@0.7": {
            "recall": 0.011627906976744186,
            "count": 1,
            "total": 86
          },
          "mae": {
            "start_mean": 185.27734249649995,
            "end_mean": 183.80959203857208,
            "average_mean": 184.543467267536
          },
          "rationale": {
            "rouge_l_mean": 0.23674836695554174,
            "rouge_l_std": 0.09620951673354798,
            "text_similarity_mean": 0.5769761629229369,
            "text_similarity_std": 0.1751788626176434,
            "llm_judge_score_mean": 1.9069767441860466,
            "llm_judge_score_std": 1.552401792398013
          },
          "rationale_cider": 0.24870007529312235
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.018149751425920284,
        "mae_average": 828.6497826861008,
        "R@0.3": 0.024205963987748737,
        "R@0.5": 0.01043839427722875,
        "R@0.7": 0.004467491802027672,
        "rationale": {
          "rouge_l_mean": 0.2369355878814343,
          "text_similarity_mean": 0.5487689936012539,
          "llm_judge_score_mean": 2.0901638091406562
        }
      }
    }
  }
}