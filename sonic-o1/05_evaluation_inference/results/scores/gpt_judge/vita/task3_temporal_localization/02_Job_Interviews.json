{
  "topic_id": 2,
  "topic_name": "Job Interviews",
  "num_evaluated": 250,
  "aggregated_metrics": {
    "mean_iou": 0.026085090170237288,
    "std_iou": 0.1036035070015144,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.04,
      "count": 10,
      "total": 250
    },
    "R@0.5": {
      "recall": 0.012,
      "count": 3,
      "total": 250
    },
    "R@0.7": {
      "recall": 0.008,
      "count": 2,
      "total": 250
    },
    "mae": {
      "start_mean": 121.25368399999999,
      "end_mean": 123.71622,
      "average_mean": 122.48495199999999
    },
    "rationale": {
      "rouge_l_mean": 0.22341265181941883,
      "rouge_l_std": 0.10206172287185152,
      "text_similarity_mean": 0.4104600582420826,
      "text_similarity_std": 0.17732432144475568,
      "llm_judge_score_mean": 3.768,
      "llm_judge_score_std": 2.8344622064864438
    },
    "rationale_cider": 0.2807422932394935
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the man asks the woman to sell him the pen, when does the woman start describing the pen as fantastic and smooth?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 3.47,
        "end": 8.757
      },
      "pred_interval": {
        "start": 59.6,
        "end": 60.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.13,
        "end": 51.243,
        "average": 53.6865
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.3418215811252594,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is vague and incorrect about timing\u2014saying 'end of the video' contradicts the precise 3.47s\u20138.757s interval in the correct answer; it only loosely implies the event occurs later but omits key timestamp details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes asking why the man needs the pen, when does the man reply, 'Ah, exactly. Now, this is the point...'?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 24.55,
        "end": 30.536
      },
      "pred_interval": {
        "start": 58.4,
        "end": 58.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.849999999999994,
        "end": 28.164,
        "average": 31.006999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.5561016798019409,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer correctly captures the key relation that the man's reply occurs immediately after the woman's question; it preserves the semantics of the reference (the target follows the anchor) despite omitting timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "After the man says 'You are not there just to carry the product to the customer', when does he list reasons like color or thick writing for wanting a pen?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 39.244,
        "end": 50.436
      },
      "pred_interval": {
        "start": 58.7,
        "end": 59.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.456000000000003,
        "end": 8.564,
        "average": 14.010000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.5137330293655396,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states that the man lists reasons for wanting a pen after the quoted remark, preserving the temporal relationship; it omits the exact timestamps and slightly simplifies the timing wording ('right after'), so it is nearly but not perfectly specific."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces herself as a former visa officer, when does she explain what American officials expect during an interview?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 34.481,
        "end": 40.61
      },
      "pred_interval": {
        "start": 1.0,
        "end": 2.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.481,
        "end": 38.61,
        "average": 36.045500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.0851063829787234,
        "text_similarity": 0.05216221511363983,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is incorrect and incomplete: it only states she introduces herself (and inaccurately as 'beginning of the video') but gives no timestamps or when she explains what officials expect; it fails to address the target event occurring after the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman states that the visa officer is judging you, when does the text 'BE CONFIDENT!' appear on screen?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 106.12,
        "end": 111.935
      },
      "pred_interval": {
        "start": 14.0,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 92.12,
        "end": 96.935,
        "average": 94.5275
      },
      "rationale_metrics": {
        "rouge_l": 0.21739130434782608,
        "text_similarity": 0.10149842500686646,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes the text appears after the woman's line, but it omits the key factual details (the exact timestamps and how much later the target occurs: 106.12s\u2013111.935s), making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes her advice about maintaining general eye contact, when does she begin talking about a slight smile?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 149.259,
        "end": 151.34
      },
      "pred_interval": {
        "start": 16.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 133.259,
        "end": 134.34,
        "average": 133.7995
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777776,
        "text_similarity": 0.11332434415817261,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly conveys the 'once_finished' temporal relationship (she begins talking about a slight smile immediately after finishing the eye-contact advice) but omits the precise timestamps and duration provided in the reference answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes stating that the visa officer will not have a favorable impression, when does she mention that this could influence a refusal?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 155.0,
        "end": 156.5
      },
      "pred_interval": {
        "start": 152.0,
        "end": 154.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 2.5,
        "average": 2.75
      },
      "rationale_metrics": {
        "rouge_l": 0.03389830508474576,
        "text_similarity": 0.20118921995162964,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives an incorrect start time (152.0s) whereas the correct target speech begins at 155.0s immediately after the anchor (151.0\u2013155.0s); it also omits the anchor/target segmentation and the 'once_finished' relation, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman asks what to do to gain confidence, when does she state that practicing is first and foremost?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 159.766,
        "end": 161.729
      },
      "pred_interval": {
        "start": 160.0,
        "end": 162.0
      },
      "iou": 0.7739480752014354,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.23400000000000887,
        "end": 0.2709999999999866,
        "average": 0.2524999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.08695652173913043,
        "text_similarity": 0.2212991714477539,
        "llm_judge_score": 9,
        "llm_judge_justification": "The predicted start time (160.0s) matches the reference target start (159.766s) closely and correctly answers when she speaks; it omits the detailed anchor interval and the explicit 'after' relation but captures the essential timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes explaining how Argo Visa can help gain confidence for visa issuance, when does the screen transition to 'Follow us:'?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 187.512,
        "end": 200.0
      },
      "pred_interval": {
        "start": 198.0,
        "end": 199.0
      },
      "iou": 0.0800768737988469,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.488,
        "end": 1.0,
        "average": 5.744
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.43762338161468506,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives 198.0s, which contradicts the correct start time of 187.512s (though 198.0s is still within the 'Follow us:' on-screen interval until 200s); this is a significant timing error and omits the accurate transition timestamp."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the second house rule about turning on video, when does she state that without video the interview will not be conducted?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 29.731,
        "end": 32.777
      },
      "pred_interval": {
        "start": 25.0,
        "end": 26.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.731000000000002,
        "end": 6.777000000000001,
        "average": 5.754000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142858,
        "text_similarity": 0.04745670408010483,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the remark occurs after the second house rule introduction, but it omits the key factual details requested (the specific anchor and target timestamps and the precise end time of the statement)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how to use the chat icon, when does she start explaining how to use the raise hand icon?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 50.521,
        "end": 57.454
      },
      "pred_interval": {
        "start": 48.0,
        "end": 49.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.521000000000001,
        "end": 8.454,
        "average": 5.487500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.1694915254237288,
        "text_similarity": 0.13928431272506714,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the raise hand explanation follows the chat icon explanation but omits the key timing details and immediacy/next-rule information (specific timestamps and that it immediately follows), so it's incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing all the continents where TTEC is located, when does she state that TTEC India is located in Ahmedabad?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 84.04,
        "end": 88.665
      },
      "pred_interval": {
        "start": 75.0,
        "end": 76.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.040000000000006,
        "end": 12.665000000000006,
        "average": 10.852500000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.13114754098360656,
        "text_similarity": 0.49487075209617615,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly notes the statement comes after the continent listing but omits the requested timing details (specific timestamps and quoted segment), so it fails to provide the key factual information in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing the first reason for leaving a job, when does she start explaining the second reason?",
      "video_id": "2dgx53kiOBQ",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 46.0
      },
      "gt_interval": {
        "start": 10.023,
        "end": 15.169
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.0
      },
      "iou": 0.028275054375104646,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.977,
        "end": 0.8309999999999995,
        "average": 2.904
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.4926271438598633,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is incorrect: it claims the second reason starts at ~15.0s after the first, whereas the correct start is 10.023s (immediately after the first at 10.003s) and the prediction confuses the start and end times and omits the 'Number two' cue."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks 'So what can you say?', when does she provide the suggested response?",
      "video_id": "2dgx53kiOBQ",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 46.0
      },
      "gt_interval": {
        "start": 37.076,
        "end": 40.609
      },
      "pred_interval": {
        "start": 44.0,
        "end": 45.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.9239999999999995,
        "end": 4.390999999999998,
        "average": 5.657499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.08333333333333333,
        "text_similarity": 0.11632315814495087,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps (question at 44.0s, response at 45.0s) contradict the ground truth (question ~36.194\u201337.057s, response 37.076\u201340.609s), so it fails to match the correct timing and relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'you're on the hunt', when does he say '\u6211\u5728\u627e\u5de5\u4f5c'?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 3.0,
        "end": 4.9
      },
      "pred_interval": {
        "start": 1.0,
        "end": 3.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 1.9000000000000004,
        "average": 1.9500000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.46808510638297873,
        "text_similarity": 0.6850259304046631,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase but gives the wrong target phrase ('\u6211\u4eec' vs '\u6211') and incorrect timing (1.0\u20133.0s vs correct 3.0\u20134.9s), which reverses the temporal relation instead of 'after'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying '\u5c65\u5386\u8868' (CV), when does he say '\u5e94\u5f81\u5de5\u4f5c' (To apply for jobs)?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 15.5,
        "end": 16.5
      },
      "pred_interval": {
        "start": 7.0,
        "end": 9.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.5,
        "end": 7.5,
        "average": 8.0
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.3036300539970398,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the temporal relation (target follows anchor) but gives completely incorrect timestamps (anchor at 7.0s vs 13.0s; target at 8\u20139s vs 15.5\u201316.5), so it is largely factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'when's the interview?', when does he ask about the salary for the position in Mandarin?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 33.0,
        "end": 36.4
      },
      "pred_interval": {
        "start": 14.0,
        "end": 16.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 20.4,
        "average": 19.7
      },
      "rationale_metrics": {
        "rouge_l": 0.339622641509434,
        "text_similarity": 0.4890315532684326,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps contradict the ground truth by a large margin (anchor 23.821s vs predicted 14.0s; target 33.0\u201336.4s vs predicted 15.0\u201316.0s), so despite identifying a salary question it is factually incorrect about timing and relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the first tip about dressing to impress, when does she start giving the second tip?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 11.708,
        "end": 14.471
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.3619254433586681,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.2919999999999998,
        "end": 1.471,
        "average": 0.8815
      },
      "rationale_metrics": {
        "rouge_l": 0.4102564102564102,
        "text_similarity": 0.650209903717041,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives incorrect timestamps (12.0s vs correct 11.147s finish and 11.708s start) and wrongly states the second tip starts immediately; it misrepresents both timing and the temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the three quick tips for a job fair, when does she explain the second tip about doing research?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 14.811,
        "end": 20.96
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.0
      },
      "iou": 0.16262806960481377,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.18900000000000006,
        "end": 4.960000000000001,
        "average": 2.5745000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.5101685523986816,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the relative order (the second tip comes after the first), but it omits the requested timing details (the specific start/end timestamps for the second tip) and thus is incomplete compared to the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Nothing else will make you stand out more than your personality', when does she say 'Be sure to use that'?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 28.591,
        "end": 29.734
      },
      "pred_interval": {
        "start": 31.0,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.408999999999999,
        "end": 2.2659999999999982,
        "average": 2.3374999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.32142857142857145,
        "text_similarity": 0.39654290676116943,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct order (the second utterance follows the first) but the reported timestamps are off by about 2.6\u20133 seconds compared to the reference and it omits the precise start/end times, so it's imprecise for temporal localization."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking \"Tell me about yourself\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 10.0,
        "end": 16.993
      },
      "pred_interval": {
        "start": 0.0,
        "end": 2.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 14.992999999999999,
        "average": 12.4965
      },
      "rationale_metrics": {
        "rouge_l": 0.22641509433962262,
        "text_similarity": 0.38666126132011414,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect and irrelevant: it claims the question starts at the video beginning and provides no timing for the green answer text, contradicting the provided timestamps and omitting key details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes asking \"Why should we hire you?\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 29.937,
        "end": 39.249
      },
      "pred_interval": {
        "start": 163.0,
        "end": 165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 133.063,
        "end": 125.751,
        "average": 129.40699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.29090909090909095,
        "text_similarity": 0.418268084526062,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the relation (the text appears after the speaker finishes) but omits the key factual details provided in the correct answer\u2014the precise start time (29.937s), end time (39.249s), and full-display duration\u2014so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes announcing \"Let's check your pronunciation. Listen and repeat.\", when does he start repeating the first smart answer?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 121.718,
        "end": 126.147
      },
      "pred_interval": {
        "start": 165.0,
        "end": 165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.282,
        "end": 38.852999999999994,
        "average": 41.067499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.21739130434782605,
        "text_similarity": 0.46589213609695435,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation (he begins repeating right after finishing the announcement), but it omits the precise start time (121.718s) and slightly generalizes the small 0.48s gap as 'immediately.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of the virtual interview, when does she list the three things to set the right first impression?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 13.846,
        "end": 19.861
      },
      "pred_interval": {
        "start": 25.0,
        "end": 26.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.154,
        "end": 6.138999999999999,
        "average": 8.6465
      },
      "rationale_metrics": {
        "rouge_l": 0.2666666666666666,
        "text_similarity": 0.3309946358203888,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after') but omits the key factual elements\u2014the specific timestamps (E1 at 3.557s and E2 from 13.846s to 19.861s)\u2014so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to use a virtual background or blur it, when does she start talking about sound and internet connection?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 40.176,
        "end": 42.981
      },
      "pred_interval": {
        "start": 37.0,
        "end": 40.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.176000000000002,
        "end": 2.9810000000000016,
        "average": 3.078500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4074074074074074,
        "text_similarity": 0.5265116691589355,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a start time of 37.0s, which contradicts the reference (speaker finishes background at 39.594s and begins discussing sound/internet at 40.176s); the timestamp is incorrect and thus fails to match the correct relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker recommends connecting laptops to an ethernet cable, when is the next piece of advice she gives regarding avoiding distractions?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 50.012,
        "end": 59.987
      },
      "pred_interval": {
        "start": 41.0,
        "end": 42.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.012,
        "end": 17.987000000000002,
        "average": 13.499500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.5680148601531982,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same general topic (avoiding distractions) but gives an incorrect timestamp (41.0s vs the correct 50.012s) and omits the specific advice to put the phone on Do Not Disturb, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the video's topic about crushing a sales job interview, when does the animated logo for 'The Elliott Group' appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.378,
        "end": 13.048
      },
      "pred_interval": {
        "start": 2.0,
        "end": 3.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.378,
        "end": 10.048,
        "average": 7.713
      },
      "rationale_metrics": {
        "rouge_l": 0.13953488372093023,
        "text_similarity": 0.4365094304084778,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that the logo appears after the speaker's introduction, but it omits the specific timing details given in the correct answer (start at 7.378s, end at 13.048s), so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that many applicants are 'unprepared' for sales jobs, when does the text overlay 'COME PREPARED' appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 55.459,
        "end": 56.559
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.459,
        "end": 40.559,
        "average": 40.509
      },
      "rationale_metrics": {
        "rouge_l": 0.19512195121951217,
        "text_similarity": 0.4574577212333679,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but omits all specific timing details (start/end timestamps and duration) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker describes looking 'unmanicured' for an interview, when does he make a specific hand gesture with both hands to emphasize his point?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 322.0,
        "end": 323.0
      },
      "pred_interval": {
        "start": 195.0,
        "end": 196.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 127.0,
        "end": 127.0,
        "average": 127.0
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869568,
        "text_similarity": 0.42750608921051025,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly indicates the gesture occurs during the 'unmanicured' description (matching the 'during' relation), but it omits the specific timestamps and the explicit mention of the two-handed emphasis given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that resumes are not needed, when does he ask what dealerships want instead?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 175.098,
        "end": 175.998
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.90199999999999,
        "end": 64.00200000000001,
        "average": 61.952
      },
      "rationale_metrics": {
        "rouge_l": 0.04878048780487805,
        "text_similarity": 0.16597308218479156,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (235.0s) contradicts the correct timing (anchor ~169.09\u2013171.19s with target ~175.10\u2013175.998s); it fails to identify the actual event window and is therefore incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's explanation of what 'the machine' is, when does a visual of a man and woman in a car showroom appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 307.098,
        "end": 311.098
      },
      "pred_interval": {
        "start": 260.0,
        "end": 265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.09800000000001,
        "end": 46.09800000000001,
        "average": 46.59800000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.12903225806451615,
        "text_similarity": 0.43030375242233276,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly timestamps the visual at 260.0s, while the ground truth places it around 307.1\u2013311.1s (within the anchor speech 307.92\u2013311.625s); this direct timing contradiction makes the answer essentially incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining that customers and managers will see you as someone who 'works at the mall' if you are dressed poorly, when does he directly advise to 'dress nice'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 273.401,
        "end": 274.923
      },
      "pred_interval": {
        "start": 285.0,
        "end": 290.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.59899999999999,
        "end": 15.076999999999998,
        "average": 13.337999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384614,
        "text_similarity": 0.3335716724395752,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the correct phrase but gives a substantially incorrect timestamp (285.0s) instead of ~273.4\u2013274.9s directly after the anchor, so it fails on the key temporal accuracy required."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if people are hiring someone average, when does he mention they are trying to hire a 'difference maker'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.877,
        "end": 375.04
      },
      "pred_interval": {
        "start": 395.0,
        "end": 400.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.12299999999999,
        "end": 24.95999999999998,
        "average": 24.541499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.24137931034482757,
        "text_similarity": 0.43047046661376953,
        "llm_judge_score": 7,
        "llm_judge_justification": "The predicted answer correctly captures the key relationship that the 'difference maker' comment follows the question about hiring someone average, but it omits the precise timestamps and detailed temporal boundaries given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he is 'very coachable', when does the text overlay 'I'M VERY COACHABLE' appear on screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 414.192,
        "end": 418.53
      },
      "pred_interval": {
        "start": 405.0,
        "end": 410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.192000000000007,
        "end": 8.529999999999973,
        "average": 8.86099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.13636363636363638,
        "text_similarity": 0.17379352450370789,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is broadly correct that the overlay appears after the spoken phrase, but it omits all precise timing details (start and end timestamps) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises to have 'eye contact', when does he directly look at the camera and say 'just like I'm looking at you in the camera'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 533.923,
        "end": 537.649
      },
      "pred_interval": {
        "start": 415.0,
        "end": 420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 118.923,
        "end": 117.649,
        "average": 118.286
      },
      "rationale_metrics": {
        "rouge_l": 0.13559322033898305,
        "text_similarity": 0.2838541269302368,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the speaker looks at the camera and says the line after advising 'eye contact', but it omits the precise timing and segment details given in the correct answer (specific start/end timestamps for E1 and E2)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"eye contact, write that down,\" when does he demonstrate eye contact with his hands?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 535.23,
        "end": 537.26
      },
      "pred_interval": {
        "start": 520.0,
        "end": 523.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.230000000000018,
        "end": 14.259999999999991,
        "average": 14.745000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2641509433962264,
        "text_similarity": 0.7195510268211365,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the demonstration occurs after the spoken instruction but gives a wrong timestamp (520.0s) and omits the correct demonstration interval (535.23\u2013537.26s), so it is factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"How stupid would that be?\" when does he say, \"Eye contact, look.\"",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 549.39,
        "end": 551.41
      },
      "pred_interval": {
        "start": 540.0,
        "end": 543.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.389999999999986,
        "end": 8.409999999999968,
        "average": 8.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.3642178773880005,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the target happens after the anchor, but it gives an incorrect timestamp (540.0s) and omits the target's actual time window (549.39\u2013551.41s), contradicting the ground-truth timing details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'll self-educate\" for the second time, when does the text overlay \"I'LL SELF EDUCATE\" appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 637.11,
        "end": 642.12
      },
      "pred_interval": {
        "start": 600.0,
        "end": 603.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.110000000000014,
        "end": 39.120000000000005,
        "average": 38.11500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.7762547135353088,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the overlay appears after the second utterance but gives the wrong timestamp (600.0s) and omits the correct interval; ground truth places the text at 637.11\u2013642.12, so the timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"We help you land your dream job\", when does the text \"TRAGIC ENDINGS\" appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.968,
        "end": 13.737
      },
      "pred_interval": {
        "start": 15.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.032,
        "end": 6.263,
        "average": 4.6475
      },
      "rationale_metrics": {
        "rouge_l": 0.3673469387755102,
        "text_similarity": 0.7009937763214111,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') but omits all the specific timing details (E1 end at 5.161s and E2 start/end at 11.968s/13.737s) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the client receiving an email stating 'Sorry, we're going with somebody else,' when does he say the client was devastated?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 176.5,
        "end": 177.7
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.5,
        "end": 62.30000000000001,
        "average": 60.400000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.2745098039215686,
        "text_similarity": 0.46885740756988525,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') and preserves the meaning, but it omits the precise timing information (the exact start and end timestamps) provided in the correct answer, so it's incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is saying 'you come across as sounding actually inexperienced', when does the word 'INEXPERIENCED' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 225.1,
        "end": 228.2
      },
      "pred_interval": {
        "start": 265.0,
        "end": 270.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.900000000000006,
        "end": 41.80000000000001,
        "average": 40.85000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.20833333333333331,
        "text_similarity": 0.53882896900177,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the text overlaps the spoken phrase), but it omits the specific timestamps and precise duration given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing the three tips for casual interviews, when does 'KEY TIP #1 NEVER TREAT AN INTERVIEW AS A CASUAL AFFAIR' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 270.6,
        "end": 275.0
      },
      "pred_interval": {
        "start": 315.0,
        "end": 320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.39999999999998,
        "end": 45.0,
        "average": 44.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.27532529830932617,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relation that the text appears once the speaker finishes (matching 'once_finished'), but it omits the key factual timing details from the reference (appearance at ~270.6s and duration until ~275.0s)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'So, those are my three tips', when does the text 'YOU NEED TO PREPARE THE SAME WAY' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 379.3,
        "end": 382.2
      },
      "pred_interval": {
        "start": 345.0,
        "end": 346.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.30000000000001,
        "end": 36.19999999999999,
        "average": 35.25
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.488348126411438,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the relation (the text appears after the spoken line) but omits the key timing details (text appears at 379.3s and remains until 382.2s) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining about the interview preparation videos, when does he start talking about his free ebook, 'My Practice Interview'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 401.4,
        "end": 409.8
      },
      "pred_interval": {
        "start": 409.0,
        "end": 410.0
      },
      "iou": 0.09302325581395457,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.600000000000023,
        "end": 0.19999999999998863,
        "average": 3.9000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.34615384615384615,
        "text_similarity": 0.5899966359138489,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states he begins discussing the ebook immediately after the videos (matching the 'once_finished' relation), but it omits the specific timestamps and the end time of that segment provided in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing his free ebook, when is the next time he mentions 'My Interview Accelerator Workshop'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 418.2,
        "end": 421.9
      },
      "pred_interval": {
        "start": 427.0,
        "end": 428.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.800000000000011,
        "end": 6.100000000000023,
        "average": 7.450000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680848,
        "text_similarity": 0.604892909526825,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly states that the workshop is mentioned immediately after the ebook, but it omits the precise timestamps (418.2s mention, 421.9s completion) given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces herself as a licensed hairdresser, when does she explain why she decided to take a break from the hair industry?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.03,
        "end": 28.03
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.030000000000001,
        "end": 15.030000000000001,
        "average": 13.030000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.4109589041095891,
        "text_similarity": 0.506547212600708,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the sequence (the explanation comes after the introduction) but is vague and omits the key timing details (the specific timestamps 23.03\u201328.03s and 5.66s) requested in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states she needs to get ready, when does she announce that her hair and makeup are done?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.66,
        "end": 113.61
      },
      "pred_interval": {
        "start": 14.0,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.66,
        "end": 98.61,
        "average": 97.63499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2622950819672132,
        "text_similarity": 0.5372910499572754,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the announcement comes after she says she needs to get ready, but it omits the specific timestamps and exact timing details present in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says she will try on outfits, when does she begin to show her chosen interview outfit in the mirror?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 277.7,
        "end": 279.6
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.69999999999999,
        "end": 39.60000000000002,
        "average": 41.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.17142857142857143,
        "text_similarity": 0.4809418320655823,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') but omits the key factual timestamps and onset details (first visible at 277.7s, fully shown by 279.6s), making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman declares she has finally 'got the outfit down', when does she describe the specific clothing items she is wearing?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 257.6,
        "end": 272.0
      },
      "pred_interval": {
        "start": 260.0,
        "end": 270.0
      },
      "iou": 0.6944444444444455,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3999999999999773,
        "end": 2.0,
        "average": 2.1999999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.4406779661016949,
        "text_similarity": 0.6172500848770142,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly indicates the description occurs after the declaration and gives a timestamp (260.0s) that falls within the true interval (257.6\u2013272.0s), but it omits the precise start and end times stated in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions her discount code, when does she start explaining Dossier's reward system?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 423.05,
        "end": 433.322
      },
      "pred_interval": {
        "start": 365.0,
        "end": 370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.05000000000001,
        "end": 63.322,
        "average": 60.68600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.3616909980773926,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the explanation occurs after the discount mention but gives a wrong timestamp (365.0s vs ~422.1s) and omits the precise start/finish times, so it is largely factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes spraying perfume on her neck/hair, when does she spray perfume on her wrist?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 365.341,
        "end": 366.421
      },
      "pred_interval": {
        "start": 400.0,
        "end": 405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.65899999999999,
        "end": 38.57900000000001,
        "average": 36.619
      },
      "rationale_metrics": {
        "rouge_l": 0.49056603773584906,
        "text_similarity": 0.7433909177780151,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a timestamp (400.0s) that contradicts the correct immediate timing (start ~365.341s, finish ~366.421s) and omits the actual start/finish details, so it is essentially incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker suggests bringing a resume, when does she explain why it's a good idea?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 440.04,
        "end": 452.824
      },
      "pred_interval": {
        "start": 420.0,
        "end": 425.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.04000000000002,
        "end": 27.824000000000012,
        "average": 23.932000000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.2295081967213115,
        "text_similarity": 0.39180558919906616,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single time (420.0s) that contradicts the ground truth (explanation begins at ~440.04s after the suggestion at 439.824s), so it is essentially incorrect and misses the correct temporal relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman advises writing down a list of questions, when does she suggest asking about work hours?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 537.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 510.0,
        "end": 512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.0,
        "end": 27.5,
        "average": 27.25
      },
      "rationale_metrics": {
        "rouge_l": 0.10169491525423728,
        "text_similarity": 0.29257941246032715,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect and incomplete: it gives a wrong timestamp (510.0s) for the advice and fails to identify when she asks about work hours (533\u2013539.5s), contradicting the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman suggests researching the salon's social media and website, when does she explain why this research is important?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 652.5,
        "end": 659.0
      },
      "pred_interval": {
        "start": 540.0,
        "end": 542.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 112.5,
        "end": 117.0,
        "average": 114.75
      },
      "rationale_metrics": {
        "rouge_l": 0.1176470588235294,
        "text_similarity": 0.2877315282821655,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives an incorrect timestamp (540.0s) and fails to identify the actual explanation point (E2 at 652.5\u2013659.0s); it contradicts the ground-truth timing and content."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman recommends having a portfolio ready to show, when does she emphasize the importance of social media in the salon world?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 702.0
      },
      "pred_interval": {
        "start": 570.0,
        "end": 572.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 126.0,
        "end": 130.0,
        "average": 128.0
      },
      "rationale_metrics": {
        "rouge_l": 0.16393442622950818,
        "text_similarity": 0.4360056221485138,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (570.0s) is far from the correct intervals (680.5\u2013684.0s and 696.0\u2013702.0s) and thus fails to identify when she discusses social media; it contradicts the provided timing and relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman talks about social media being a big thing in the salon world, when does she explain that social media marketing can bring more clients into the salon?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 733.4,
        "end": 797.5
      },
      "pred_interval": {
        "start": 725.0,
        "end": 735.0
      },
      "iou": 0.022068965517241693,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.399999999999977,
        "end": 62.5,
        "average": 35.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.4045206904411316,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative order (that the explanation comes after the earlier remark) but omits the key factual details (the specific time ranges/timestamps) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes saying that confidence and the way one presents themselves goes a long way, when does she talk about preferring a personable applicant?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 784.0,
        "end": 794.9
      },
      "pred_interval": {
        "start": 765.0,
        "end": 775.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 19.899999999999977,
        "average": 19.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.5343294143676758,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence (she speaks about preferring a personable applicant after the confidence comment) but omits the key temporal details and timestamps (784.0\u2013794.9s) required by the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman talks about waiting for the car's air conditioning to cool down, when does she advise giving enough time to arrive early for the interview?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 854.5,
        "end": 861.7
      },
      "pred_interval": {
        "start": 805.0,
        "end": 815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.5,
        "end": 46.700000000000045,
        "average": 48.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.34870094060897827,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that she advises arriving early (i.e., 'after'), but it omits the specific timestamps/intervals (E1 and E2) given in the correct answer, making it incomplete. There is no contradictory content, but key factual timing details are missing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'wish me luck', when does she say 'I'm back from the interview'?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1004.0
      },
      "gt_interval": {
        "start": 882.5,
        "end": 883.5
      },
      "pred_interval": {
        "start": 975.0,
        "end": 982.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 92.5,
        "end": 98.5,
        "average": 95.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1,
        "text_similarity": 0.26517826318740845,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction preserves the order (target after anchor) but the timestamps are substantially wrong (~97\u2013100s later) and it omits the precise intervals given in the correct answer, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the animated intro sequence concludes, when does the speaker greet the audience with \"Morning, everyone\"?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 51.533,
        "end": 52.234
      },
      "pred_interval": {
        "start": 21.0,
        "end": 23.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.533,
        "end": 29.234,
        "average": 29.8835
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.3459824323654175,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (21.0s) is completely incorrect and contradicts the reference, which places the greeting after the intro at 51.533\u201352.234 (intro ends at 50.512)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"First, context\", when does the text \"Design exercise \u2260 white boarding\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.396,
        "end": 101.982
      },
      "pred_interval": {
        "start": 156.0,
        "end": 157.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.604,
        "end": 55.018,
        "average": 77.311
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7618700861930847,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the text appears after the utterance but gives a vastly incorrect timestamp (156.0s vs the correct ~56.396s) and omits the correct end time, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that the next thing to get right is the deliverables, when does the text introduce action item number two?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 195.3,
        "end": 198.0
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.69999999999999,
        "end": 42.0,
        "average": 40.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.1111111111111111,
        "text_similarity": 0.37380021810531616,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the text appears after the speaker (the key ordering), but it omits the crucial timing details (192.6s, 195.3s\u2013198.0s) and other specifics from the reference, making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he clearly missed the biggest thing, when does the text describe the final deliverable for a take-home exercise?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 256.5,
        "end": 261.7
      },
      "pred_interval": {
        "start": 260.0,
        "end": 265.0
      },
      "iou": 0.19999999999999865,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5,
        "end": 3.3000000000000114,
        "average": 3.4000000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.1923076923076923,
        "text_similarity": 0.4170643091201782,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the relative order (the text appears after the speaker's remark) but omits the key timing details given in the reference (anchor completes at 254.8s; text appears 256.5\u2013261.7s), so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"action item number three\", when does the text overlay \"Use standard patterns\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 348.0,
        "end": 352.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 332.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 20.0,
        "average": 19.0
      },
      "rationale_metrics": {
        "rouge_l": 0.163265306122449,
        "text_similarity": 0.3624934256076813,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the overlay appears after the spoken cue but inaccurately describes it as 'immediately' and omits the precise timing and duration (it actually appears ~3.5s later at 348.0s and stays until 352.0s), so key factual details are missing/misstated."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is talking about looking at other apps for design inspiration, when does the text overlay \"By looking at Google MD, Apple HIG\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.0,
        "end": 378.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 347.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.0,
        "end": 31.0,
        "average": 28.0
      },
      "rationale_metrics": {
        "rouge_l": 0.20779220779220778,
        "text_similarity": 0.22254955768585205,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the overlay appears during the speaker's discussion about design inspiration, but it omits the key factual details\u2014the precise timings (text at 370.0\u2013378.0s and anchor speech 357.2\u2013378.0s)\u2014so it's incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"Action item number four\", when does the text overlay \"Pick the right prompt\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 382.7,
        "end": 386.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 362.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.69999999999999,
        "end": 24.0,
        "average": 23.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.44391751289367676,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that the text appears after the speaker finishes (the 'once_finished' relation), but it omits the documented short delay and precise timing (382.7s vs anchor ending at 379.3s), effectively implying immediate appearance and lacking key temporal detail."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"Now is the fun one, fun time,\" when does the text \"Hot Take\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 528.0,
        "end": 533.5
      },
      "pred_interval": {
        "start": 526.0,
        "end": 527.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 6.5,
        "average": 4.25
      },
      "rationale_metrics": {
        "rouge_l": 0.15999999999999998,
        "text_similarity": 0.17628519237041473,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the text appears after the speaker (matching the relative timing), but it omits the explicit timestamps and duration provided in the correct answer, making it incomplete for the asked 'when'."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions his 'cover letter video', when does the YouTube video thumbnail related to cover letters appear on the screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 566.5,
        "end": 615.0
      },
      "pred_interval": {
        "start": 534.0,
        "end": 539.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.5,
        "end": 76.0,
        "average": 54.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.36582106351852417,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the thumbnail appears after the mention, but it omits the key factual details (exact onset at 566.5s, duration until 615.0s and the anchor timing 562.0\u2013565.0s), so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker says \"smash the like button for the YouTube algorithm,\" when does he make a 'smashing' gesture with his arms?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 607.0,
        "end": 609.0
      },
      "pred_interval": {
        "start": 548.0,
        "end": 550.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.0,
        "end": 59.0,
        "average": 59.0
      },
      "rationale_metrics": {
        "rouge_l": 0.08333333333333333,
        "text_similarity": 0.34826967120170593,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the gesture occurs while saying the phrase, but it omits the key factual timing details (anchor speech 605.0\u2013608.0s; gesture 607.0\u2013609.0s) required by the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the host states that Mr. Syed Hassan is a top LinkedIn voice with good experience in recruitment and HR, when does Syed greet the host?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.826,
        "end": 23.329
      },
      "pred_interval": {
        "start": 2.0,
        "end": 3.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.826,
        "end": 20.329,
        "average": 20.0775
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.3429062068462372,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states that the greeting occurs after the host's introduction (matching the target event), but it omits the specific timestamp details provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes asking what applicants should consider before applying for jobs, when does Syed begin his answer by saying 'Well, that's a very good question'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 74.466,
        "end": 81.582
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.465999999999994,
        "end": 65.582,
        "average": 62.523999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.3665885031223297,
        "llm_judge_score": 10,
        "llm_judge_justification": "The predicted answer matches the correct answer's key fact that Syed begins speaking immediately after the host finishes; it preserves the timing relationship and contains no contradictions or extra errors."
      }
    },
    {
      "question_id": "003",
      "question": "After Syed states that many people apply for irrelevant jobs, when does he start explaining that most companies in Dubai use ATS systems?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.0,
        "end": 105.605
      },
      "pred_interval": {
        "start": 20.0,
        "end": 21.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 84.0,
        "end": 84.605,
        "average": 84.30250000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142855,
        "text_similarity": 0.48176342248916626,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the ATS explanation occurs after the anchor event but wrongly asserts it happens 'right after'\u2014the reference shows a significant time gap between the two events, so the timing detail is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first speaker finishes explaining that rejections in Dubai are automated, when does the second speaker give positive feedback?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 162.4,
        "end": 164.8
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.6,
        "end": 75.19999999999999,
        "average": 73.89999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.19607843137254902,
        "text_similarity": 0.5544260740280151,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the causal timing (speaker 2 responds immediately after speaker 1), but it omits the specific timestamps and duration details provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker mentions that developers are mostly in demand in the UAE, when does he start listing specific types of developers?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.1,
        "end": 255.2
      },
      "pred_interval": {
        "start": 245.0,
        "end": 250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.099999999999994,
        "end": 5.199999999999989,
        "average": 5.6499999999999915
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222227,
        "text_similarity": 0.1924847960472107,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the ordering (listing follows the mention) but omits the required precise timestamps and relation details from the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the left (Hassan) starts explaining his screening process for resumes, when does he first mention checking the candidate's years of experience?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 364.35,
        "end": 366.36
      },
      "pred_interval": {
        "start": 356.2,
        "end": 360.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.150000000000034,
        "end": 5.960000000000036,
        "average": 7.055000000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.2181818181818182,
        "text_similarity": 0.5998834371566772,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the relative relation (that the mention occurs after he starts explaining) but provides no timing details; it omits the specific timestamps (start at ~364.18s and mention at ~364.35\u2013366.36s) required to answer \"when,\" so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the left (Hassan) mentions asking for a screening call, when does he advise to check for red flags during that call?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 429.39,
        "end": 432.42
      },
      "pred_interval": {
        "start": 407.9,
        "end": 411.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.49000000000001,
        "end": 20.620000000000005,
        "average": 21.055000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.12307692307692307,
        "text_similarity": 0.46712586283683777,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that red flags should be checked during the screening call after he mentions arranging one, but it omits the specific timestamps and precise temporal bounds provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the left (Hassan) states they shortlist a candidate, when does he mention calling them to assess them in person?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.58,
        "end": 443.3
      },
      "pred_interval": {
        "start": 425.6,
        "end": 429.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.979999999999961,
        "end": 13.800000000000011,
        "average": 14.889999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.573058545589447,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction conveys the same temporal relation\u2014that the call to assess them in person occurs once they shortlist the candidate\u2014without adding incorrect details, so it matches the correct answer semantically."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the right finishes talking about the content being helpful for many working professionals, when does he mention sharing Mr. Hassan's profile?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 523.7,
        "end": 526.1
      },
      "pred_interval": {
        "start": 512.6,
        "end": 513.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.100000000000023,
        "end": 12.700000000000045,
        "average": 11.900000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.5070819854736328,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but omits the key temporal details (the specific event time intervals/timestamps) provided in the correct answer, so it is incomplete though not incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the right asks the audience if they have any questions, when does he tell them to write in the comments?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 542.0,
        "end": 543.5
      },
      "pred_interval": {
        "start": 527.9,
        "end": 528.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.100000000000023,
        "end": 15.299999999999955,
        "average": 14.699999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384614,
        "text_similarity": 0.4911448359489441,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys that the request to 'write in the comments' comes later, but it introduces an unsupported detail (mentioning Mr. Hassan reviewing/answering) and does not align with the correct event sequence which ties it specifically to the earlier 'any questions' prompt, so it is only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the right finishes stating that he and Mr. Hassan will review and answer questions, when does the man on the left say 'Definitely, definitely'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 546.5,
        "end": 547.5
      },
      "pred_interval": {
        "start": 545.0,
        "end": 545.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.5,
        "end": 2.1000000000000227,
        "average": 1.8000000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.29999999999999993,
        "text_similarity": 0.46823394298553467,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation that the left man speaks after the right man finishes, but it omits the precise timing and immediacy (the 546.5\u2013547.5s timestamp) given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says her first job interview was through LinkedIn, when does she mention the job tab on LinkedIn?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 112.525,
        "end": 116.189
      },
      "pred_interval": {
        "start": 1.0,
        "end": 2.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.525,
        "end": 114.189,
        "average": 112.857
      },
      "rationale_metrics": {
        "rouge_l": 0.0816326530612245,
        "text_similarity": 0.2645772695541382,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction asserts the job-tab mention occurred 'right after' the first-interview remark, which contradicts the reference timestamps showing the target event occurs much later (112.525s vs 45.771s). This temporal claim is therefore incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is demonstrating on her phone how to search for a job on LinkedIn, when does she verbally instruct to write 'architect or interior designer jobs'?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 146.179,
        "end": 148.622
      },
      "pred_interval": {
        "start": 15.0,
        "end": 16.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 131.179,
        "end": 132.622,
        "average": 131.90050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.10909090909090909,
        "text_similarity": 0.21741719543933868,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the instruction occurs during the phone demonstration but omits the key factual details (the visual anchor at 140.843s and the spoken interval 146.179s\u2013148.622s), so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'keywords plays a very important role', when does her finger scroll down the list of job posts on the phone screen?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 170.3
      },
      "pred_interval": {
        "start": 20.0,
        "end": 21.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 150.0,
        "end": 149.3,
        "average": 149.65
      },
      "rationale_metrics": {
        "rouge_l": 0.19230769230769232,
        "text_similarity": 0.26601842045783997,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the scroll happens after she finishes speaking but omits the specific timing and duration (170.0\u2013170.3s and anchor end at 166.902s) given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions finding multiple tabs when searching for architect jobs, when does she instruct the viewer to go to the 'posts' tab?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.4,
        "end": 158.9
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.6,
        "end": 81.1,
        "average": 79.35
      },
      "rationale_metrics": {
        "rouge_l": 0.2545454545454545,
        "text_similarity": 0.1638241708278656,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after') but omits the key factual details in the reference\u2014namely the precise timestamps and the exact phrasing\u2014making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises cross-checking company details before sending a CV, when does she suggest calling the company's mentioned number for verification?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 383.586,
        "end": 388.331
      },
      "pred_interval": {
        "start": 265.0,
        "end": 270.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 118.58600000000001,
        "end": 118.33100000000002,
        "average": 118.45850000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.17777777777777778,
        "text_similarity": 0.1616283655166626,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') but omits the key factual details (the specific timestamps and end time provided in the correct answer), so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says she went to the company's profile, when does she mention finding the number there?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 381.5,
        "end": 383.466
      },
      "pred_interval": {
        "start": 342.6,
        "end": 345.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.89999999999998,
        "end": 37.76600000000002,
        "average": 38.333
      },
      "rationale_metrics": {
        "rouge_l": 0.08163265306122448,
        "text_similarity": 0.22397705912590027,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys the temporal order (the mention occurs after she says she went to the company's profile) but omits key details from the reference\u2014namely the exact timestamps and the immediacy ('starts immediately after') relation\u2014so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the company asked her to share her CV via email, when does she say she actually shared it?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 401.369,
        "end": 404.314
      },
      "pred_interval": {
        "start": 359.8,
        "end": 361.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.56900000000002,
        "end": 42.81400000000002,
        "average": 42.19150000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463772,
        "text_similarity": 0.40024471282958984,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys that she shared the CV shortly after being asked, but it omits the precise temporal relation and exact timing information provided in the reference (start/end times and the once_finished relation)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says she gave the company a call to check on hiring, when does she confirm that she did call and they were looking for a candidate?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 388.572,
        "end": 395.942
      },
      "pred_interval": {
        "start": 364.2,
        "end": 366.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.372000000000014,
        "end": 29.841999999999985,
        "average": 27.107
      },
      "rationale_metrics": {
        "rouge_l": 0.1095890410958904,
        "text_similarity": 0.5114982724189758,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction restates that she confirms calling and that the company was looking, but fails to provide the required timing/timestamps or the explicit temporal relation; it omits key factual details from the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that not getting a job at a big company is not the case, when does she start introducing strategies for getting a job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 191.24,
        "end": 195.36
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.75999999999999,
        "end": 44.639999999999986,
        "average": 44.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24175824175824176,
        "text_similarity": 0.23851622641086578,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially incorrect (~49s later than the reference), it omits the target end time, and it fails to preserve the immediate temporal relation indicated in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the 'BEFORE INTERVIEW' text appears on screen, when does the 'DURING INTERVIEW (ONSITE & OFFSITE)' text appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.12,
        "end": 209.32
      },
      "pred_interval": {
        "start": 240.0,
        "end": 245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.879999999999995,
        "end": 35.68000000000001,
        "average": 37.78
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677416,
        "text_similarity": 0.41068893671035767,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are significantly different from the reference (240s/245s vs. 198s/200.12s\u2013209.32s) and omit the correct duration and ordering details, so it does not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that you should get ready technically, when does she start listing examples of how to get ready technically?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 338.44,
        "end": 347.64
      },
      "pred_interval": {
        "start": 345.0,
        "end": 346.0
      },
      "iou": 0.10869565217391318,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.560000000000002,
        "end": 1.6399999999999864,
        "average": 4.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.14035087719298245,
        "text_similarity": 0.21436679363250732,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states that the examples begin immediately after the prior remark, but it omits the precise timestamps (338.44s\u2013347.64s) and the detail that the subsequent segment covers the full list of technical prep activities."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says that the next topic is things to do during the interview, when does she state the first thing to do?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 405.02,
        "end": 415.34
      },
      "pred_interval": {
        "start": 350.0,
        "end": 351.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.01999999999998,
        "end": 64.33999999999997,
        "average": 59.67999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.12,
        "text_similarity": 0.10693168640136719,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that the first item occurs 'after' the topic cue and provides no timing or details; it omits the key timestamps and the brief pause noted in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker tells the audience to ask interviewers questions about themselves during the chat, when does she start giving an example related to infrastructure as code?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 470.16,
        "end": 495.68
      },
      "pred_interval": {
        "start": 355.0,
        "end": 356.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 115.16000000000003,
        "end": 139.68,
        "average": 127.42000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.14492753623188406,
        "text_similarity": 0.20171546936035156,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that the example comes after the prompt but omits the key factual detail\u2014the specific start time (\u2248470.16s) and duration\u2014so it fails to provide the critical timing information in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises to 'get to know your interviewer, build a relationship with the person', when does she state that doing so leaves an impression?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.0,
        "end": 532.52
      },
      "pred_interval": {
        "start": 512.0,
        "end": 514.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 18.519999999999982,
        "average": 17.75999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3773584905660377,
        "text_similarity": 0.3899012804031372,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction restates that the speaker says it 'leaves an impression' but omits the required temporal details (529.0\u2013532.52s) and the relation ('once_finished'), so it fails to answer the asked 'when'."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the point 'Be yourself', when does she explain the consequences of trying to be fake?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 544.33,
        "end": 584.4
      },
      "pred_interval": {
        "start": 536.0,
        "end": 538.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.330000000000041,
        "end": 46.39999999999998,
        "average": 27.36500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.61714768409729,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the key relation that the explanation comes after the 'Be yourself' point, but it omits the specific timestamps and duration details provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that interviews are typically 'peer programming sessions', when does she give an example of reducing latency from 100 milliseconds to 8 milliseconds?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 668.88,
        "end": 677.08
      },
      "pred_interval": {
        "start": 540.0,
        "end": 542.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 128.88,
        "end": 135.08000000000004,
        "average": 131.98000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.6156757473945618,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation (that the latency example occurs after the peer-programming statement) but omits the explicit timing details (start/end timestamps) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes advising to 'add numbers as well if they are numbers to add', when does she say that 'there are sometimes that there won't be numbers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 704.38,
        "end": 708.06
      },
      "pred_interval": {
        "start": 745.0,
        "end": 750.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.620000000000005,
        "end": 41.940000000000055,
        "average": 41.28000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.4644087553024292,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (the target follows the anchor) but the timestamps are substantially incorrect (off by ~41\u201346 seconds) and it fails to provide the correct interval for the target, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions an impact on '10 different teams', when does she talk about '10 million users or 10 million customers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 723.39,
        "end": 725.25
      },
      "pred_interval": {
        "start": 760.0,
        "end": 765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.610000000000014,
        "end": 39.75,
        "average": 38.18000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.5852552652359009,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly preserves the key relation\u2014the '10 million users or customers' remark occurs after the '10 different teams' remark\u2014so the essential (relative) timing is accurate despite different absolute timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "When does the next numbered text overlay appear after the overlay '6. Mention past achievements you can comfortably talk about.'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 796.43,
        "end": 800.39
      },
      "pred_interval": {
        "start": 800.0,
        "end": 805.0
      },
      "iou": 0.045507584597431046,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.57000000000005,
        "end": 4.610000000000014,
        "average": 4.090000000000032
      },
      "rationale_metrics": {
        "rouge_l": 0.2933333333333333,
        "text_similarity": 0.7518182992935181,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect and contradictory timestamps (795.0s and 800.0s) that do not match the correct start time of the next overlay (7) at 796.43s and its end at 800.39s, so it fails to identify the correct appearance timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions doing what you should be doing after the interview, when does the text overlay 'Sit back and focus on yourself.' appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 899.5,
        "end": 901.9
      },
      "pred_interval": {
        "start": 925.0,
        "end": 926.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.5,
        "end": 24.100000000000023,
        "average": 24.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.4805116653442383,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the overlay appears after the speaker, but it omits the precise timestamps and incorrectly implies it appears immediately ('right after') when the overlay actually appears ~10.1 seconds later and includes a disappearance time."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that it's either you get an offer or a rejection, when does she say 'If you get an offer, congratulations to you'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 917.6,
        "end": 919.6
      },
      "pred_interval": {
        "start": 930.0,
        "end": 931.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.399999999999977,
        "end": 11.399999999999977,
        "average": 11.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.537991464138031,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the quoted line but gives a substantially incorrect timestamp (930.0s vs. 917.6\u2013919.6s) and omits the referenced E1 timing and explicit relation, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's invitation for help on social media, when do her social media handles appear on screen?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 983.0,
        "end": 987.0
      },
      "pred_interval": {
        "start": 940.0,
        "end": 941.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.0,
        "end": 46.0,
        "average": 44.5
      },
      "rationale_metrics": {
        "rouge_l": 0.18749999999999997,
        "text_similarity": 0.5483623743057251,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the handles appear during the speaker's invitation (relation), but gives a wrong timestamp (940.0s vs ~983.5\u2013984.5s) and omits the actual handle on/off times (983.0\u2013987.0s), so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of '3 Things to know when INTERVIEWING', when does she start talking about people being chosen relative to other candidates?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 32.8,
        "end": 38.0
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.799999999999997,
        "end": 25.0,
        "average": 22.9
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.26459553837776184,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted start time (12.0s) is completely inconsistent with the reference (E2 starts at 32.8s, after E1 at 20.0\u201326.0s), so it is incorrect and contradicts the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker talks about false positives and negatives in terms of accuracy, when does she explain that companies care about not hiring bad talents?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 103.0,
        "end": 106.0
      },
      "pred_interval": {
        "start": 25.0,
        "end": 26.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.0,
        "end": 80.0,
        "average": 79.0
      },
      "rationale_metrics": {
        "rouge_l": 0.16,
        "text_similarity": 0.17873212695121765,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (25.0s) is far from the correct event (around 103.0s when she says 'Companies actually care more'), so it contradicts the correct temporal location and misses the described 'after' relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying what you should be doing after the interview, when does she mention that the video is about getting your dream job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1019.1500000000001
      },
      "gt_interval": {
        "start": 890.5,
        "end": 894.9
      },
      "pred_interval": {
        "start": 925.6,
        "end": 934.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.10000000000002,
        "end": 39.80000000000007,
        "average": 37.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.0851063829787234,
        "text_similarity": 0.21294063329696655,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (925.6s) contradicts the correct timing (target at 890.5\u2013894.9s immediately after 889.3s); it is factually incorrect and introduces a hallucinated reference to an 'interview' start."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman mentions the STAR method, when does she explain what it stands for?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 159.92,
        "end": 164.1
      },
      "pred_interval": {
        "start": 245.0,
        "end": 249.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.08000000000001,
        "end": 84.9,
        "average": 84.99000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222227,
        "text_similarity": 0.6612771153450012,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly timestamps the mention (245.0s vs 159.08s) and omits the precise explanation interval; while it notes the explanation follows immediately, the core timing is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the man introduces the topic of what not to say, when does the woman advise against bad-mouthing former employers?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 185.4,
        "end": 191.0
      },
      "pred_interval": {
        "start": 260.0,
        "end": 263.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.6,
        "end": 72.0,
        "average": 73.3
      },
      "rationale_metrics": {
        "rouge_l": 0.4406779661016949,
        "text_similarity": 0.569571852684021,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted time (260.0s) is far from the correct interval (185.4\u2013191.0s) and omits the end time and the included phrase 'Big red flag', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man states that preparation ties back to building skills, when does he explain what deep research involves?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 247.12,
        "end": 252.48
      },
      "pred_interval": {
        "start": 270.0,
        "end": 274.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.879999999999995,
        "end": 21.52000000000001,
        "average": 22.200000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.5295005440711975,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (270.0s) is inconsistent with the transcript evidence, which shows the 'Dig deeper' segment starts at 247.120s and ends at 252.480s; the prediction also omits the E1 time of 237.120s."
      }
    },
    {
      "question_id": "001",
      "question": "After the man sips his coffee, when does he say 'it builds skills'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 342.0,
        "end": 343.0
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 307.0,
        "end": 307.0,
        "average": 307.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.44160860776901245,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly conveys the relation that the man says 'it builds skills' immediately after finishing his coffee, matching the reference's 'after' timing (\u22480.8s later)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying 'every single time', when does he start saying 'You show up differently'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 347.5,
        "end": 348.9
      },
      "pred_interval": {
        "start": 36.0,
        "end": 37.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 311.5,
        "end": 311.9,
        "average": 311.7
      },
      "rationale_metrics": {
        "rouge_l": 0.40816326530612246,
        "text_similarity": 0.4541754722595215,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys that the second line begins immediately after the first, but it gives an incorrect timestamp for 'every single time' (36.0s vs 344.0s) and does not provide the correct start time for 'You show up differently' (347.5s)."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces the 'deep dive' into interview preparation, when does she mention covering 'surprising insights and steps'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 26.0,
        "end": 29.5
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.0,
        "end": 16.5,
        "average": 15.25
      },
      "rationale_metrics": {
        "rouge_l": 0.32653061224489793,
        "text_similarity": 0.4767981171607971,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the mention occurs after the deep-dive introduction, but it omits the specific timestamps (26.0s\u201329.5s) and wrongly implies it occurs immediately ('right after') rather than several seconds later, so it lacks key factual detail and precision."
      }
    },
    {
      "question_id": "002",
      "question": "During the man's explanation of how dressing formally applies to online interviews, when does he mention the concept of 'enclothed cognition'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 77.0,
        "end": 80.0
      },
      "pred_interval": {
        "start": 145.0,
        "end": 150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.0,
        "end": 70.0,
        "average": 69.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.493802547454834,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the mention occurs during his explanation (matches the relation) but fails to provide the key timing details (the specified timestamps 68.5s start and 77.0\u201380.0s mention), so it's incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying \"makes you stand out\", when does he say \"absolutely\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 335.4,
        "end": 336.0
      },
      "pred_interval": {
        "start": 345.6,
        "end": 346.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.200000000000045,
        "end": 10.199999999999989,
        "average": 10.200000000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.28250962495803833,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('absolutely' is said immediately after 'makes you stand out'), but it omits the precise timestamps and explicit timing details given in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the man finishes sipping from his white cup, when does he say \"it builds skills\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 343.0,
        "end": 343.6
      },
      "pred_interval": {
        "start": 357.8,
        "end": 359.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.800000000000011,
        "end": 15.599999999999966,
        "average": 15.199999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2790697674418604,
        "text_similarity": 0.4888766407966614,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') that the man says 'it builds skills' after sipping, but it omits the specific timestamps (343.0\u2013343.6s vs 340.9s) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions his parents advised him to always go on interviews, when does he explicitly state that interviews are practice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.064,
        "end": 43.554
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.064,
        "end": 30.554000000000002,
        "average": 28.809
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.22791385650634766,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (12.0s) contradicts the reference (39.064\u201343.554s) and omits the correct relation to the parents' advice ending at 22.242s, so it is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising not to overstate qualifications, when does he tell viewers to know their worth for negotiation?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.944,
        "end": 117.861
      },
      "pred_interval": {
        "start": 195.0,
        "end": 196.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.056,
        "end": 78.139,
        "average": 83.5975
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.3862833082675934,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (195.0s) contradicts the reference, which places the 'know your worth' advice starting at ~105.944s and ending at ~117.861s, so the prediction is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions strengths and weaknesses, when does he mention why you want this particular job?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 179.8,
        "end": 182.4
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.19999999999999,
        "end": 57.599999999999994,
        "average": 56.39999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.08695652173913043,
        "text_similarity": 0.23043496906757355,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative order right (target after anchor) but gives incorrect timestamps (235/240s vs correct ~175\u2013182s) and omits the interval ranges, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's discussion about having coaches being important for self-improvement, when does he mention Roger Wakefield?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 215.9,
        "end": 218.0
      },
      "pred_interval": {
        "start": 240.0,
        "end": 245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.099999999999994,
        "end": 27.0,
        "average": 25.549999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.0851063829787234,
        "text_similarity": 0.13470271229743958,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (240.0s) contradicts the ground truth, which places the mention at 215.9\u2013218.0s within the 213.2\u2013232.0s discussion, so the answer is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes elaborating on researching a company and asking about their projects, when does he transition to discussing common questions about training and education?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.3,
        "end": 314.7
      },
      "pred_interval": {
        "start": 245.0,
        "end": 250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.30000000000001,
        "end": 64.69999999999999,
        "average": 62.5
      },
      "rationale_metrics": {
        "rouge_l": 0.058823529411764705,
        "text_similarity": 0.22111491858959198,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (245.0s \u2192 250.0s) conflict with the correct intervals (anchor 289.0\u2013297.7s, target 305.3\u2013314.7s); it is therefore factually incorrect and does not preserve the temporal relation described."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what goals need to be achieved for more money, when does he say that the company will pay that money?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.5,
        "end": 340.9
      },
      "pred_interval": {
        "start": 395.0,
        "end": 402.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.5,
        "end": 61.10000000000002,
        "average": 58.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677416,
        "text_similarity": 0.47899720072746277,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the payment statement comes after the question, but it omits the specific timestamps and misrepresents the content by quoting an inaccurate/irrelevant phrase rather than stating the payment timing as in the reference, constituting a factual mismatch and omission of key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions going for a union job, when does he describe understanding the process of safety and hazard assessments?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 381.5
      },
      "pred_interval": {
        "start": 407.0,
        "end": 416.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.80000000000001,
        "end": 34.5,
        "average": 33.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.24137931034482757,
        "text_similarity": 0.5239279866218567,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the relative order (assessment described after the union job mention) but omits the specific timing details and timestamps given in the reference, which are key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about expressing passion for the job, when does he advise being a student of construction?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 531.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 512.0,
        "end": 514.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 25.5,
        "average": 22.25
      },
      "rationale_metrics": {
        "rouge_l": 0.339622641509434,
        "text_similarity": 0.4988565742969513,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the speaker discusses being a student of construction after talking about passion, matching the relative ordering in the correct answer without adding or omitting key information."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks about the key responsibilities of a construction worker, when does he list these responsibilities?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 588.0,
        "end": 610.0
      },
      "pred_interval": {
        "start": 516.0,
        "end": 518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.0,
        "end": 92.0,
        "average": 82.0
      },
      "rationale_metrics": {
        "rouge_l": 0.163265306122449,
        "text_similarity": 0.39022672176361084,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the sequence that the responsibilities are listed after the question, but it omits the key timestamp details and is vague about timing, making it incomplete compared to the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks about dealing with an unhappy supervisor, when does he advise owning up to your mistakes?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 705.0,
        "end": 711.0
      },
      "pred_interval": {
        "start": 520.0,
        "end": 522.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 185.0,
        "end": 189.0,
        "average": 187.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3181818181818182,
        "text_similarity": 0.40932372212409973,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction accurately captures the key temporal relation from the reference: the speaker advises owning up to mistakes after asking about dealing with an unhappy supervisor, with no contradictions or missing essential facts."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that the foreman will then explain to the general contractor, when does he start describing a similar dynamic between a journeyman and an apprentice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 744.67,
        "end": 790.86
      },
      "pred_interval": {
        "start": 725.0,
        "end": 730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.66999999999996,
        "end": 60.860000000000014,
        "average": 40.264999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.5879904627799988,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after' / once_finished) between the two explanations, but it omits the key timing details (start and end timestamps) and duration provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes reading the question about developing skill sets, when does he begin explaining his method of doing so?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 903.0
      },
      "pred_interval": {
        "start": 925.0,
        "end": 930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.0,
        "end": 27.0,
        "average": 30.0
      },
      "rationale_metrics": {
        "rouge_l": 0.20338983050847456,
        "text_similarity": 0.32693612575531006,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are far from the ground truth (892.0s) and claim a delayed start (930.0s) rather than immediately after finishing, so it contradicts the correct timing and relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining due diligence regarding unsafe acts, when does he start discussing strengths and weaknesses for an interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 960.4,
        "end": 976.0
      },
      "pred_interval": {
        "start": 945.0,
        "end": 950.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.399999999999977,
        "end": 26.0,
        "average": 20.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2028985507246377,
        "text_similarity": 0.44862961769104004,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relative ordering (second topic starts after the first), but both timestamps are factually incorrect \u2014 E1 should end around 939.0s and E2 begins around 960.4s \u2014 so it is not accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker highlights the 'Practice makes perfect' section, when does he advise being cool, collected, and confident for the upcoming interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1113.03,
        "end": 1118.08
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1050.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.02999999999997,
        "end": 68.07999999999993,
        "average": 65.55499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.5460610389709473,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the general sequence that the advice follows the 'Practice makes perfect' highlight, but it omits the precise timing and the explicit detail that the advice occurs after the anchor finishes, leaving ambiguity about whether it is simultaneous with the highlight."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker reads 'What is your main weakness?', when does he advise turning a weakness into a positive?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1213.307,
        "end": 1216.671
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1050.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 163.30700000000002,
        "end": 166.67100000000005,
        "average": 164.98900000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.6195329427719116,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the advice to turn a weakness into a positive occurs after the speaker reads the question, matching the key temporal relation in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'dress to impress', when does he read the sentence 'What you wear to an interview is very important'?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1258.4,
        "end": 1261.5
      },
      "pred_interval": {
        "start": 1236.0,
        "end": 1240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.40000000000009,
        "end": 21.5,
        "average": 21.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.4615384615384615,
        "text_similarity": 0.6481760740280151,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly conveys the relative ordering (the target occurs after the anchor), but it omits the precise timestamps and adds the qualifier 'immediately after,' a slight embellishment compared to the 0.9s gap in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker reads 'dress professionally, not casually', when does he start reading the specific advice about what women should not wear?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1272.8,
        "end": 1277.3
      },
      "pred_interval": {
        "start": 1245.0,
        "end": 1250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.799999999999955,
        "end": 27.299999999999955,
        "average": 27.549999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.375,
        "text_similarity": 0.5524801015853882,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is factually incorrect and contradicts the reference: the correct target starts at 1272.8s (after the anchor at 1262.0\u20131264.9s), whereas the prediction incorrectly gives 1245.0s (well before the anchor)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes reading the dressing advice for women, when does he read the dressing advice for men?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1277.7,
        "end": 1282.0
      },
      "pred_interval": {
        "start": 1250.0,
        "end": 1255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.700000000000045,
        "end": 27.0,
        "average": 27.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.5374739766120911,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (1250.0s) contradicts the correct timing (starts at ~1277.7s and ends at 1282.0s) and omits that the men's advice follows immediately after the women's; therefore it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker welcomes to the Job Search Preparation Resume Writing Workshop, when does she introduce herself as Cynthia White?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 9.61,
        "end": 15.95
      },
      "pred_interval": {
        "start": 2.0,
        "end": 3.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.609999999999999,
        "end": 12.95,
        "average": 10.28
      },
      "rationale_metrics": {
        "rouge_l": 0.05128205128205129,
        "text_similarity": 0.17855575680732727,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly states the introduction occurs at the beginning of the video, contradicting the correct temporal information that the self-introduction starts at 9.61s immediately after the welcome; it also omits the precise timing and relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that employers spend about 60 to 90 seconds reviewing a resume, when does she explain the purpose of a cover letter?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 93.56,
        "end": 100.47
      },
      "pred_interval": {
        "start": 165.0,
        "end": 170.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.44,
        "end": 69.53,
        "average": 70.485
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413793,
        "text_similarity": 0.6086775660514832,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but omits the key factual details (the specific timestamps: cover letter purpose starts at 93.56s and ends at 100.47s) requested by the question."
      }
    },
    {
      "question_id": "001",
      "question": "During the initial 'You will learn' slide, when does the speaker mention how to format an electronic resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 172.9
      },
      "pred_interval": {
        "start": 150.0,
        "end": 152.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 20.900000000000006,
        "average": 20.450000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.22950819672131148,
        "text_similarity": 0.5944006443023682,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the mention occurs during the initial 'You will learn' slide but omits the crucial timing details (170.0s\u2013172.9s within the slide), so it is incomplete for the question asked."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'Design a resume that is simple and clean', when is the next checklist item, 'Limit the resume to two pages maximum', mentioned?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 233.0,
        "end": 235.8
      },
      "pred_interval": {
        "start": 246.0,
        "end": 248.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 12.199999999999989,
        "average": 12.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7971466779708862,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction is true but uninformative: it only restates that the next item occurs after the anchor and omits the key timing details (233.0\u2013235.8s) and relative timing requested, so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing the tip 'Review the job announcement', when does she begin discussing 'Consider the employer's perspective'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 274.9,
        "end": 307.2
      },
      "pred_interval": {
        "start": 270.0,
        "end": 272.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.899999999999977,
        "end": 35.19999999999999,
        "average": 20.049999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7507247924804688,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the second topic begins immediately after the first, but it fails to provide the key factual timestamps (starts at 274.9s, ends at 307.2s) given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that 'one size does not fit all', when does she finish explaining the need to edit a basic resume for each job?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.18,
        "end": 330.25
      },
      "pred_interval": {
        "start": 345.0,
        "end": 360.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.819999999999993,
        "end": 29.75,
        "average": 22.284999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.48391589522361755,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives concrete timestamps but they are substantially different from the reference (ref finish 330.25s vs predicted 360.0s); while it preserves the 'after' relation, the timestamps are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that they need to be careful about concealing information, when does the 'Style: Combination Resume - Sample #1' title appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.5,
        "end": 519.3
      },
      "pred_interval": {
        "start": 510.0,
        "end": 520.0
      },
      "iou": 0.37999999999999545,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5,
        "end": 0.7000000000000455,
        "average": 3.1000000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.15873015873015875,
        "text_similarity": 0.36766788363456726,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation that the title appears shortly after the speaker finishes (matches 'once_finished'), but it omits the precise timestamps (514.3s, 515.5s, 519.3s) and the detail that the speaker only begins discussing the title later, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the 'Style: Combination Resume - Sample #2' title appears, when does the speaker start describing the benefits for job seekers like those changing industries or reentering the workforce?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 542.7,
        "end": 556.7
      },
      "pred_interval": {
        "start": 540.0,
        "end": 560.0
      },
      "iou": 0.7,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 2.7000000000000455,
        "end": 3.2999999999999545,
        "average": 3.0
      },
      "rationale_metrics": {
        "rouge_l": 0.22857142857142856,
        "text_similarity": 0.31006452441215515,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly indicates the explanation occurs after the title and roughly overlaps the correct interval, but its start (540.0s vs 542.7s) and end (560.0s vs 556.7s) timestamps are noticeably inaccurate and the predicted end extends beyond the actual finish."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes summarizing James' scenario, when does she state the recommended resume type for him?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 667.3,
        "end": 674.9
      },
      "pred_interval": {
        "start": 600.0,
        "end": 610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.29999999999995,
        "end": 64.89999999999998,
        "average": 66.09999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.5576000213623047,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (600.0\u2013610.0s) conflict entirely with the correct times (summary ends 664.9s; recommendation 667.3\u2013674.9s), so it is factually incorrect and misaligns with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes discussing how to choose the appropriate resume style, when does she start talking about the actual resume and its contents?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 877.86,
        "end": 884.43
      },
      "pred_interval": {
        "start": 870.0,
        "end": 930.0
      },
      "iou": 0.10949999999999895,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.860000000000014,
        "end": 45.57000000000005,
        "average": 26.715000000000032
      },
      "rationale_metrics": {
        "rouge_l": 0.1702127659574468,
        "text_similarity": 0.328973650932312,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timings contradict the ground truth: it gives 870.0s and 930.0s versus the correct 877.86s (end) and 877.86\u2013884.43s (start immediately after). The large timing errors and failure to indicate immediate continuation make it largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'name and contact information' as a basic category for a resume, when does she list the 'skills and accomplishments' category?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.09,
        "end": 922.44
      },
      "pred_interval": {
        "start": 930.0,
        "end": 960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.909999999999968,
        "end": 37.559999999999945,
        "average": 23.734999999999957
      },
      "rationale_metrics": {
        "rouge_l": 0.08163265306122448,
        "text_similarity": 0.2271847426891327,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps are widely inaccurate and contradict the ground truth: the correct answer states 'skills and accomplishments' directly follows at ~920.09\u2013922.44s, whereas the prediction places it much later (960.0s)."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions that many employers are beginning to use emails for communication with job candidates, when does she advise opening a new email address?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1011.0,
        "end": 1024.0
      },
      "pred_interval": {
        "start": 960.0,
        "end": 990.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.0,
        "end": 34.0,
        "average": 42.5
      },
      "rationale_metrics": {
        "rouge_l": 0.253968253968254,
        "text_similarity": 0.6029794216156006,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps (960\u2013990s) than the reference (1011\u20131024s), so it is factually incorrect about when the advice occurs despite matching the advice content. The timing mismatch contradicts the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Skills & Accomplishments' section, when does she suggest using mynextmove.org?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1121.7,
        "end": 1126.15
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1060.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.70000000000005,
        "end": 66.15000000000009,
        "average": 68.92500000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.32655778527259827,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the mention of mynextmove.org occurs after the introduction of the 'Skills & Accomplishments' section, matching the reference's relative timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes mentioning the onetonline.org website, when does the 'New Graduate' text appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1199.0,
        "end": 1199.5
      },
      "pred_interval": {
        "start": 1060.0,
        "end": 1070.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 139.0,
        "end": 129.5,
        "average": 134.25
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.6861419081687927,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the relative ordering (the 'New Graduate' text appears after the speaker mentions onetonline.org) but omits the crucial timing details (start at 1199.0s, fully displayed by 1199.5s) provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "When is the next time a new category of individual appears on screen after the 'New Graduate' text is displayed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1202.0,
        "end": 1202.5
      },
      "pred_interval": {
        "start": 1070.0,
        "end": 1080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 132.0,
        "end": 122.5,
        "average": 127.25
      },
      "rationale_metrics": {
        "rouge_l": 0.16949152542372883,
        "text_similarity": 0.6425955891609192,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: the correct next category appears starting at 1202.0s (fully by 1202.5s, 'Formerly Incarcerated'), whereas the prediction gives 1070.0s and does not identify the correct category."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about 'fragments in your work history', when does she introduce 'Summary Statements'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.3,
        "end": 1283.6
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.700000000000045,
        "end": 36.40000000000009,
        "average": 39.05000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.05,
        "text_similarity": 0.38066643476486206,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted time (1320.0s) is far from the correct interval where 'Summary Statements' are introduced (E2 starts at 1278.3s and ends at 1283.6s), so it is incorrect and omits the correct timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states the importance of including summary statements, when does she explain how they clearly communicate qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1341.0,
        "end": 1351.0
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.0,
        "end": 31.0,
        "average": 26.0
      },
      "rationale_metrics": {
        "rouge_l": 0.08955223880597014,
        "text_similarity": 0.4379046559333801,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (1320.0s) contradicts the reference, which places the explanation beginning at 1341.0s (after E1 ends at 1339.1s); it is therefore incorrect and omits the correct segment details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining how to list relevant skills using bullets, when does the 'Skills/Summary of Skills' section appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1430.0,
        "end": 1431.0
      },
      "pred_interval": {
        "start": 1415.0,
        "end": 1420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 11.0,
        "average": 13.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2388059701492537,
        "text_similarity": 0.5563610792160034,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the key relation that the section appears once the speaker finishes, but it omits the crucial timing details (speaker finishes at 1425.0s; box starts appearing at 1430.0s and is fully visible by 1431.0s)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'This is an accomplishment statement', when does the 'Example 4: Accomplishment Statement' text box appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1466.0,
        "end": 1466.5
      },
      "pred_interval": {
        "start": 1530.0,
        "end": 1535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.0,
        "end": 68.5,
        "average": 66.25
      },
      "rationale_metrics": {
        "rouge_l": 0.22641509433962262,
        "text_similarity": 0.6024408340454102,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only states that the box appears after the utterance, which is true but extremely vague; it omits the key timing details (start at 1466.0s, fully in place by 1466.5s) and thus fails to match the reference's specific temporal information."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing job duties and contributions, when does she start to explain how to list the most recent job first?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1599.24,
        "end": 1604.0
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1600.0
      },
      "iou": 0.05428571428571364,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.240000000000009,
        "end": 4.0,
        "average": 6.6200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.5523747801780701,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the ordering (finish then start) and roughly the start time right, but the reported finish time (1590.0s) is significantly off from 1597.95s and it omits the E2 end timestamp (1604.0s), so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the yellow hexagonal graphics for 'Education/Training' fully appear on the screen, when does the speaker begin to list the types of earned qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1622.7,
        "end": 1628.27
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.700000000000045,
        "end": 18.269999999999982,
        "average": 20.485000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.4261055290699005,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives timestamps that are substantially earlier than the reference (off by ~20.9s for the graphic and ~12.7s for the start), and it omits the finish time; although the temporal relation ('after') is preserved, the factual timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Body' section of the cover letter, when does she provide an example of an introduction?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1798.91,
        "end": 1805.84
      },
      "pred_interval": {
        "start": 17.0,
        "end": 23.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1781.91,
        "end": 1782.84,
        "average": 1782.375
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.5448908805847168,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the temporal relation (the example occurs after the 'Body' introduction) but the provided timestamps are vastly incorrect compared to the reference and it omits the finish times, so key temporal details are missing or wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide changes to display 'Sample Cover Letter', when does the speaker begin describing the included elements?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1893.78,
        "end": 1906.58
      },
      "pred_interval": {
        "start": 35.0,
        "end": 40.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1858.78,
        "end": 1866.58,
        "average": 1862.6799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068966,
        "text_similarity": 0.5609021782875061,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly captures the relative timing\u2014the speaker begins describing the elements 1 second after the slide appears (36.0s vs. 35.0s), matching the reference's relative offset; differences in absolute timebase and omission of the end time do not affect the correct start time."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes the cover letter tip about checking for errors, when does the slide transition to 'Electronic Resume'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1944.0,
        "end": 1944.99
      },
      "pred_interval": {
        "start": 58.0,
        "end": 60.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1886.0,
        "end": 1884.99,
        "average": 1885.495
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.617458701133728,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the relation that the slide follows once the speaker finishes, but the timestamps are substantially incorrect (58.0/59.0s vs 1943.92/1944.0\u20131944.99s) and it omits the transition end time, so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions applications and resumes being submitted online, when does she explain that an electronic resume will contain the same information as a standard resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1969.8,
        "end": 1974.8
      },
      "pred_interval": {
        "start": 205.0,
        "end": 206.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1764.8,
        "end": 1768.8,
        "average": 1766.8
      },
      "rationale_metrics": {
        "rouge_l": 0.0967741935483871,
        "text_similarity": 0.26016443967819214,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') that the explanation follows the mention of online submissions, but it omits the requested timing details (the specific timestamps provided in the correct answer)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that electronic information needs to be in plain text, when does she mention that bolded or underlined text needs to be removed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1980.1,
        "end": 1986.8
      },
      "pred_interval": {
        "start": 207.0,
        "end": 208.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1773.1,
        "end": 1778.8,
        "average": 1775.9499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.508170485496521,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction accurately conveys the same temporal relation (that removal is required once/immediately after the plain-text requirement is stated) and contains no incorrect or extra information."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Electronic Resume Tips' slide appears, when does the speaker advise to limit each line to 65 characters?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2027.3,
        "end": 2029.4
      },
      "pred_interval": {
        "start": 209.0,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1818.3,
        "end": 1819.4,
        "average": 1818.85
      },
      "rationale_metrics": {
        "rouge_l": 0.3404255319148936,
        "text_similarity": 0.6775224208831787,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (that the advice occurs after the slide), but it omits the key factual details\u2014the specific timestamps (2027.3s\u20132029.4s) provided in the correct answer\u2014so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating their website address, when does she state her contact information is again right there?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2148.0,
        "end": 2152.0
      },
      "pred_interval": {
        "start": 2145.0,
        "end": 2156.0
      },
      "iou": 0.36363636363636365,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 4.0,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869568,
        "text_similarity": 0.294249564409256,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction conveys the same meaning: the contact information is stated immediately after the speaker finishes the website address, with no contradictions or missing key elements."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes thanking the viewer for taking a look at the workshop, when does the screen transition to the Extension logo?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2160.1,
        "end": 2161.0
      },
      "pred_interval": {
        "start": 2158.0,
        "end": 2161.0
      },
      "iou": 0.3000000000000303,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.099999999999909,
        "end": 0.0,
        "average": 1.0499999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.48350435495376587,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the transition occurs after the speaker's thanks, but it omits all precise timing information (start time, full visibility time, and segment end) provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Britney' scenario, when does she explain that Britney should highlight skills from volunteering, sports, clubs, and other activities?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 729.63,
        "end": 736.05
      },
      "pred_interval": {
        "start": 690.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.629999999999995,
        "end": 16.049999999999955,
        "average": 27.839999999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.53125,
        "text_similarity": 0.5688233375549316,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor at 690.0s but gives an incorrect time for the explanation (705.0s) while the correct interval is 729.63\u2013736.05s, so it misses the key timing detail."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes Martha's current job as a teacher's assistant with three years of experience, when does the speaker state that Martha should use a skills-based resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 788.07,
        "end": 792.83
      },
      "pred_interval": {
        "start": 735.0,
        "end": 745.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.07000000000005,
        "end": 47.83000000000004,
        "average": 50.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.4556962025316456,
        "text_similarity": 0.6921956539154053,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the event types right but the timestamps are substantially incorrect (E1 off by ~6s, E2 off by ~48s) and reverses the order \u2014 the correct target occurs after the anchor, but the prediction places it before, so it fails temporal alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning contacting them for interactive sessions on various topics, when does she start talking about the website?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2140.17,
        "end": 2150.24
      },
      "pred_interval": {
        "start": 2154.6,
        "end": 2155.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.429999999999836,
        "end": 5.0600000000004,
        "average": 9.745000000000118
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.6433014869689941,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only preserves the ordering (website discussed after contact) but gives substantially incorrect timestamps (2154.6\u20132155.3s vs. reference start 2140.17s and end 2150.24s), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states her name, Cynthia White, when does she thank the viewers for watching the workshop?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2151.26,
        "end": 2155.3
      },
      "pred_interval": {
        "start": 2159.8,
        "end": 2160.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.539999999999964,
        "end": 5.5,
        "average": 7.019999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.3583372235298157,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states she thanks viewers after giving her name, but it gives substantially incorrect timestamps (2159.8\u20132160.8s vs. the true 2151.26\u20132155.3s), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After Michael Emery introduces himself as the Director of Human Resources at IOM, when does he explain that competency-based interviews are also referred to as behavioral or situational interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 17.414,
        "end": 23.021
      },
      "pred_interval": {
        "start": 2.0,
        "end": 5.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.414000000000001,
        "end": 18.021,
        "average": 16.7175
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605632,
        "text_similarity": 0.5648693442344666,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that the explanation comes after the introduction, but it is vague and omits the specific timestamps and precise timing provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'why do we use competency-based interviews?', when does he mention that a competency-based interview has a 0.51 predictor of success?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 91.2,
        "end": 97.969
      },
      "pred_interval": {
        "start": 13.0,
        "end": 16.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.2,
        "end": 81.969,
        "average": 80.08449999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.6337383985519409,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys that the mention comes after an explanatory segment (i.e., it follows the prior discussion), but it is vague and omits the specific anchor, the explicit timing details, and the precise reference to the 0.51 predictor as given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes listing examples of what to prepare for, when does he start giving the second piece of advice about saving the vacancy notice?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.8,
        "end": 158.0
      },
      "pred_interval": {
        "start": 235.0,
        "end": 240.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.19999999999999,
        "end": 82.0,
        "average": 82.1
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.27972081303596497,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general 'after' relation but gives a completely wrong timestamp (235.0s vs correct ~151.6s/152.8s) and inaccurately states it starts 'immediately after' despite a specific start at 152.8s; key timing details are incorrect or missing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes explaining why it's important to save the vacancy notice by saying 'it's gone off the web', when does he start talking about the structure of an interview?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.8,
        "end": 209.8
      },
      "pred_interval": {
        "start": 240.0,
        "end": 245.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.19999999999999,
        "end": 35.19999999999999,
        "average": 33.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.44926923513412476,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the correct ordering (after) but the timestamps are substantially wrong (predicted E1 at 240.0s vs correct 167.5s, predicted E2 at 245.0s vs correct 207.8s), so it fails the required temporal localization."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the fourth letter for the CAR principles, when does he explain what panels often ask about this fourth letter?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.34,
        "end": 330.41
      },
      "pred_interval": {
        "start": 345.0,
        "end": 350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.660000000000025,
        "end": 19.589999999999975,
        "average": 17.125
      },
      "rationale_metrics": {
        "rouge_l": 0.2247191011235955,
        "text_similarity": 0.2838034927845001,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect timing (saying the introduction is at 345.0s and the explanation occurs shortly after) whereas the reference anchors the introduction at ~300.28s and the explanation at ~330.34\u2013330.41s, so the answer is largely wrong and not aligned with the target interval."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks how long a response should be, when does he warn that candidates can be marked down?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 419.29,
        "end": 427.37
      },
      "pred_interval": {
        "start": 360.0,
        "end": 365.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.29000000000002,
        "end": 62.370000000000005,
        "average": 60.83000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.358974358974359,
        "text_similarity": 0.42468777298927307,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly paraphrases the warning content but gives an incorrect timestamp (360.0s vs. the correct ~419\u2013427s) and thus fails on key factual details and the relative ordering with the anchor."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises having a pre-prepared statement for questions to the panel, when does he mention good 'bog standard questions'?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 494.0,
        "end": 501.0
      },
      "pred_interval": {
        "start": 375.0,
        "end": 380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 119.0,
        "end": 121.0,
        "average": 120.0
      },
      "rationale_metrics": {
        "rouge_l": 0.16438356164383564,
        "text_similarity": 0.30361106991767883,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (375.0s) is far from the correct target interval (494.0s\u2013501.0s) and fails to identify the correct occurrence or relative ordering, so it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the panel assessing your answers, when does he first equate this to diving in the Olympics?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.94,
        "end": 530.52
      },
      "pred_interval": {
        "start": 512.0,
        "end": 536.0
      },
      "iou": 0.1908333333333303,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.940000000000055,
        "end": 5.480000000000018,
        "average": 9.710000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.39009666442871094,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is incorrect: the correct target begins at 525.94s (after the 518.24s anchor), whereas the prediction claims it starts at 512.0s, contradicting the true timing and omitting the end time and the 'after' relationship."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises choosing examples commensurate with the position level, when does he give the specific example of a candidate accused of cheating in high school?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 612.01,
        "end": 619.36
      },
      "pred_interval": {
        "start": 549.0,
        "end": 570.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.00999999999999,
        "end": 49.360000000000014,
        "average": 56.185
      },
      "rationale_metrics": {
        "rouge_l": 0.19178082191780824,
        "text_similarity": 0.48327770829200745,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the relation is 'after' but gives incorrect timing and boundaries (549.0\u2013570.0 vs the true 612.01\u2013619.36) and therefore fails to match the key factual elements of the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing the three different formats for interviews, when does the '3 types of interviews' graphic appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.1,
        "end": 710.8
      },
      "pred_interval": {
        "start": 690.0,
        "end": 705.0
      },
      "iou": 0.2355769230769225,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.100000000000023,
        "end": 5.7999999999999545,
        "average": 7.949999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307692,
        "text_similarity": 0.662358283996582,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the graphic appears once the speaker finishes (matching the causal timing) but omits the key factual details of the exact timestamp (700.1s) and the duration it remains visible (until 710.8s)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'There's telephone interviews', when does the advice '1. Stand up' appear on screen for telephone interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 717.2,
        "end": 807.3
      },
      "pred_interval": {
        "start": 705.0,
        "end": 715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.200000000000045,
        "end": 92.29999999999995,
        "average": 52.25
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.6543331742286682,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the relative ordering ('after') but omits the key factual details in the reference (exact timestamps, duration, and that other content occurs in between), so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises on making sure the background is tidy for video interviews, when does the visual text '3. Appropriate clothing' appear?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 815.0
      },
      "pred_interval": {
        "start": 840.0,
        "end": 845.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.0,
        "end": 30.0,
        "average": 35.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307692,
        "text_similarity": 0.48173198103904724,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states the 'after' temporal relationship, but it omits the specific timestamps and duration (800.0s\u2013815.0s) provided in the correct answer, so it's incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the importance of eye contact in a face-to-face interview, when does he explain how to involve the entire panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.8,
        "end": 897.0
      },
      "pred_interval": {
        "start": 872.0,
        "end": 875.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.799999999999955,
        "end": 22.0,
        "average": 17.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.4067796610169492,
        "text_similarity": 0.6569429636001587,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly states the panel explanation occurs at 872.0s, whereas the reference places it at 884.8\u2013897.0s after the eye-contact segment (872.0\u2013878.0s); this misstates the timing and ordering, so it's largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the young man's inappropriate attire during an interview, when does he advise to wear appropriate clothing?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 927.1,
        "end": 929.2
      },
      "pred_interval": {
        "start": 906.0,
        "end": 909.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.100000000000023,
        "end": 20.200000000000045,
        "average": 20.650000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.4522368013858795,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (906.0s) contradicts the ground truth (advice at 927.1\u2013929.2s immediately after the anecdote) and is therefore incorrect; it fails to match the correct timing and sequence."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says connecting with panel members on social media is a 'no-no', when does he say that sending a thank you is appropriate?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1087.0,
        "end": 1088.5
      },
      "pred_interval": {
        "start": 1056.0,
        "end": 1062.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.0,
        "end": 26.5,
        "average": 28.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068965,
        "text_similarity": 0.34177225828170776,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly conveys that the speaker forbids social-media connections and states a thank-you is appropriate afterward (after the interview), matching the correct relation and intent."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker gives the example question about a dysfunctional team, when does he ask about the three most important values?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1154.0,
        "end": 1158.0
      },
      "pred_interval": {
        "start": 1074.0,
        "end": 1080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.0,
        "end": 78.0,
        "average": 79.0
      },
      "rationale_metrics": {
        "rouge_l": 0.19607843137254902,
        "text_similarity": 0.4896589517593384,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single timestamp (1074.0s) that contradicts the correct timeline (E1: 1126\u20131133s; E2: 1154\u20131158s) and misstates the relation; it is factually incorrect and omits the correct interval."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes saying the phrase \"contrary evidence questions\", when does the specific text for \"5. Contrary-evidence questions\" appear on the blue panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1237.0,
        "end": 1257.7
      },
      "pred_interval": {
        "start": 1236.0,
        "end": 1240.0
      },
      "iou": 0.138248847926267,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 17.700000000000045,
        "average": 9.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.24,
        "text_similarity": 0.5819283723831177,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the text appears after the speaker's phrase, but it omits the key quantitative details (the exact timestamps 1237.0s appearance and 1257.7s end, and the 1.2s offset) required by the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining contrary-evidence questions, when does the \"Closing words\" slide appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1257.7,
        "end": 1259.0
      },
      "pred_interval": {
        "start": 1297.0,
        "end": 1302.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.299999999999955,
        "end": 43.0,
        "average": 41.14999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.13636363636363638,
        "text_similarity": 0.3711225986480713,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation (slide appears right after the speaker finishes) but omits the precise timestamps and mischaracterizes the speaker's content (contrary-evidence questions vs 'usefulness of the tutorial'), so it is only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes stating that he hopes the tutorial was useful, when does he recommend visiting the IOM website for jobs?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1275.9,
        "end": 1284.3
      },
      "pred_interval": {
        "start": 1300.0,
        "end": 1302.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.09999999999991,
        "end": 17.700000000000045,
        "average": 20.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.5024968385696411,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly states the recommendation occurs after the 'tutorial useful' remark, but saying 'right after' implies immediacy and omits the precise timing (there is about a 12.6s gap between the two events)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the session as 'interviewing prep 101', when does he state that the session will build on other career presentations?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 27.466,
        "end": 37.226
      },
      "pred_interval": {
        "start": 12.0,
        "end": 13.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.466000000000001,
        "end": 24.226,
        "average": 19.846
      },
      "rationale_metrics": {
        "rouge_l": 0.09302325581395349,
        "text_similarity": -0.0005464218556880951,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the statement comes after the 'interviewing prep 101' introduction but fails to provide the required timing details (specific timestamps and that the target directly follows the anchor), so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces himself as Daniel Moglen, when does he state where he works?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 66.316,
        "end": 68.83
      },
      "pred_interval": {
        "start": 14.0,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.316,
        "end": 53.83,
        "average": 53.073
      },
      "rationale_metrics": {
        "rouge_l": 0.057142857142857134,
        "text_similarity": -0.048692554235458374,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly indicates the speaker states where he works after introducing himself, but it omits the key timing details (specific timestamps and that the target immediately follows the anchor) present in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that the workshops are just the beginning, when does he encourage everyone to stay in touch?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 169.9,
        "end": 175.8
      },
      "pred_interval": {
        "start": 245.0,
        "end": 246.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.1,
        "end": 70.19999999999999,
        "average": 72.64999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2068965517241379,
        "text_similarity": 0.31367430090904236,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation that the encouragement comes after the 'just the beginning' remark, but it omits key specifics from the reference (speaker identity and precise timing/timestamps)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, cool', when does he welcome everyone?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 202.8,
        "end": 203.6
      },
      "pred_interval": {
        "start": 247.0,
        "end": 248.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.19999999999999,
        "end": 44.400000000000006,
        "average": 44.3
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.38984549045562744,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly conveys the key temporal relation\u2014he welcomes everyone immediately after saying 'All right, cool'\u2014which matches the ground truth 'once_finished' timing; the omission of exact timestamps is a permissible paraphrase."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker shares his screen, when does he ask the audience to reflect on job interviews?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 298.6,
        "end": 303.3
      },
      "pred_interval": {
        "start": 250.0,
        "end": 251.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.60000000000002,
        "end": 52.30000000000001,
        "average": 50.45000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.5146965980529785,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (he asks after sharing his screen) but omits the key factual details from the ground truth\u2014specific anchor/target timestamps and the slide context\u2014so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks which interview questions the audience is most concerned about, when does he instruct them to type those questions in the chatbox?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.283,
        "end": 336.694
      },
      "pred_interval": {
        "start": 395.0,
        "end": 396.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.716999999999985,
        "end": 59.30599999999998,
        "average": 60.011499999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.08695652173913043,
        "text_similarity": 0.12220347672700882,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is a vague paraphrase that omits the required timing information (the specific timestamps/relative placement immediately after E1) and thus fails to capture the key factual detail in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if there's anything he would have wanted to do differently in an interview, when does he ask if he said everything he wanted to say?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 520.97,
        "end": 525.55
      },
      "pred_interval": {
        "start": 510.0,
        "end": 512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.970000000000027,
        "end": 13.549999999999955,
        "average": 12.259999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.15625,
        "text_similarity": 0.2039887011051178,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction reverses the temporal order: the correct answer states the 'did I say everything' event immediately follows the 'regrets' question, but the prediction says the regrets question comes after asking if he said everything; it also omits the timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, so let's dive in here', when does the text 'DO NOT JUDGE YOUR PERFORMANCE - OR SELF WORTH - ON A JOB OFFER' appear on screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 570.09,
        "end": 574.39
      },
      "pred_interval": {
        "start": 540.0,
        "end": 543.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.090000000000032,
        "end": 31.389999999999986,
        "average": 30.74000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117646,
        "text_similarity": 0.40866947174072266,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the text appears after the speaker's line but fails to provide the requested timing details (start and full-display timestamps) given in the correct answer, making it an incomplete response."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker discusses applying for jobs that ask for too much experience, when does he state that getting interviews indicates a good resume and cover letter?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 606.28,
        "end": 616.41
      },
      "pred_interval": {
        "start": 600.0,
        "end": 602.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.279999999999973,
        "end": 14.409999999999968,
        "average": 10.34499999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.20895522388059704,
        "text_similarity": 0.46849459409713745,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly restates that the speaker links interviews to a good resume/cover letter after discussing overqualified jobs, but it fails to provide the requested timing details (the specific timestamps given in the reference)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing his initial application for a graduate writing specialist position at UCSB, when does he mention being a finalist but not getting the job?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.0,
        "end": 718.5
      },
      "pred_interval": {
        "start": 720.0,
        "end": 730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 11.5,
        "average": 8.75
      },
      "rationale_metrics": {
        "rouge_l": 0.10909090909090909,
        "text_similarity": 0.1280650496482849,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (that the finalist comment occurs after the initial description) but omits the crucial timing details (the specific start/end timestamps and the once_finished interval) required by the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining that he and his family were not ready to move to apply for the second position, when does he reiterate that not getting a job interview is not necessarily unsuccessful?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.248,
        "end": 773.62
      },
      "pred_interval": {
        "start": 750.0,
        "end": 760.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.248000000000047,
        "end": 13.620000000000005,
        "average": 15.934000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.09677419354838711,
        "text_similarity": 0.12674608826637268,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states that the reiteration occurs after the explanation, preserving the core relation, but it omits the required precise timestamps and relation labeling (E1/E2, once_finished) present in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks what the main thing hiring managers are looking for when interviewing people, when does he state their response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 874.4,
        "end": 883.6
      },
      "pred_interval": {
        "start": 840.0,
        "end": 850.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.39999999999998,
        "end": 33.60000000000002,
        "average": 34.0
      },
      "rationale_metrics": {
        "rouge_l": 0.06557377049180328,
        "text_similarity": 0.15610064566135406,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the speaker reveals the answer 'after' asking, but it omits the essential timing details (start/finish timestamps) and the noted short pause/comment, so it is incomplete relative to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks for thoughts on 'likability' and mentions its subjective nature, when does he read the comment 'Doesn't sound fair' from the chat box?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 897.4,
        "end": 898.3
      },
      "pred_interval": {
        "start": 870.0,
        "end": 872.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.399999999999977,
        "end": 26.299999999999955,
        "average": 26.849999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.2181818181818182,
        "text_similarity": 0.44829031825065613,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that the speaker asks about likability but provides no timing or when the 'Doesn't sound fair' comment is read (nor that it occurs after), omitting key factual details required by the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes reading the chat comment 'I'm disheartened by that', when does he describe his own reaction to the concept of 'likability'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 937.485,
        "end": 939.668
      },
      "pred_interval": {
        "start": 935.0,
        "end": 940.0
      },
      "iou": 0.43659999999999854,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4850000000000136,
        "end": 0.33199999999999363,
        "average": 1.4085000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.38761240243911743,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the sequence (he describes his reaction after reading the comment) but omits the key factual details from the reference\u2014specific timestamps and the note that the reaction occurs immediately after\u2014so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating that 'Interviewing is an inexact science', when does he elaborate on this by posing a rhetorical question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 974.6,
        "end": 985.7
      },
      "pred_interval": {
        "start": 960.0,
        "end": 965.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.600000000000023,
        "end": 20.700000000000045,
        "average": 17.650000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.44146037101745605,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the speaker elaborates via a rhetorical question, but it omits the required timing details (the start/end timestamps and that E2 occurs immediately after E1), so it is incomplete for the asked 'when'."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if something makes sense, when does he start talking about thinking about your audience?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1085.885,
        "end": 1093.694
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1052.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.88499999999999,
        "end": 41.69399999999996,
        "average": 38.789499999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.0909090909090909,
        "text_similarity": 0.1400083601474762,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that discussion of the audience immediately follows the question, matching the relative timing, but it omits the specific anchor/target timestamps and explicit temporal details given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces HR interview/phone screens, when does he refer to it as a 'gatekeeper'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1125.176,
        "end": 1128.0
      },
      "pred_interval": {
        "start": 1060.0,
        "end": 1062.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.17599999999993,
        "end": 66.0,
        "average": 65.58799999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.15789473684210525,
        "text_similarity": 0.29262399673461914,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states that the speaker calls HR interviews/phone screens a 'gatekeeper' after introducing them, but it omits the specific anchor/target timestamps (E1 at 1120.00-1125.0s and E2 at 1125.176s-1128.0s) and the explicit sequencing detail present in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's discussion of site visits, when does he describe the current form of these visits?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1176.087,
        "end": 1183.755
      },
      "pred_interval": {
        "start": 1070.0,
        "end": 1072.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 106.08699999999999,
        "end": 111.75500000000011,
        "average": 108.92100000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.2681155800819397,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction restates that the speaker describes the visits during the discussion but omits the required precise temporal localization and relation details (specific anchor/target timestamps and that the target elaborates while topic continues), so it is overly vague and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that panel interviews maintain fairness, when does he mention there's no feedback or response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1247.396,
        "end": 1252.49
      },
      "pred_interval": {
        "start": 1345.0,
        "end": 1350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.60400000000004,
        "end": 97.50999999999999,
        "average": 97.55700000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.22641509433962262,
        "text_similarity": 0.4656680226325989,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the relative order (that the comment comes after 'fairness') but omits the required timing details/timestamps and thus fails to provide the specific temporal answer requested."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker recommends being on a hiring committee, when does he share his personal experience as a grad student?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1287.554,
        "end": 1295.994
      },
      "pred_interval": {
        "start": 1395.0,
        "end": 1400.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 107.44599999999991,
        "end": 104.00600000000009,
        "average": 105.726
      },
      "rationale_metrics": {
        "rouge_l": 0.13114754098360656,
        "text_similarity": 0.3577020764350891,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the anecdote occurs after the recommendation, but it omits the key factual timing details (start/end timestamps and anchor info) given in the reference, making it incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions that the community is invited to sit on interviews for higher positions, when does he advise attending them to learn?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.58,
        "end": 1299.06
      },
      "pred_interval": {
        "start": 1415.0,
        "end": 1420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.42000000000007,
        "end": 120.94000000000005,
        "average": 122.18000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.09230769230769229,
        "text_similarity": 0.33967727422714233,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies that he advises attending when the community is invited to sit on interviews, but it omits the immediacy ('go to those too' right after the invitation) and the explicit purpose (to hear the types of questions asked)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to always save the job description as a PDF, when does he explain that the job posting is often removed after expiry?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1452.209,
        "end": 1457.575
      },
      "pred_interval": {
        "start": 1410.0,
        "end": 1415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.20900000000006,
        "end": 42.575000000000045,
        "average": 42.39200000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.6571999788284302,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the explanation follows the advice, but it omits the requested timing details (the specific timestamps and that the explanation immediately follows), making it incomplete for the 'when' question."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining to turn qualifications into questions, when does he give the example 'must be familiar with discourse analysis'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1492.512,
        "end": 1496.48
      },
      "pred_interval": {
        "start": 1415.0,
        "end": 1420.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.51199999999994,
        "end": 76.48000000000002,
        "average": 76.99599999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.5000000000000001,
        "text_similarity": 0.5939874649047852,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the example follows the explanation, but it omits the crucial timing information (the specific timestamps and that the example directly illustrates the previous point) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the idea of a 'bad response', when does he describe the specific bad response example?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1804.78,
        "end": 1808.35
      },
      "pred_interval": {
        "start": 185.0,
        "end": 186.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1619.78,
        "end": 1622.35,
        "average": 1621.065
      },
      "rationale_metrics": {
        "rouge_l": 0.2692307692307693,
        "text_similarity": 0.4774702191352844,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the specific example follows the introduction, but it incorrectly asserts it occurs 'immediately after' and omits the exact timing details (the example occurs about 21 seconds later), so it is incomplete and slightly misleading."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces his 'go-to response', when does he state that his weakness is being conflict-avoidant?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1887.2,
        "end": 1890.9
      },
      "pred_interval": {
        "start": 193.0,
        "end": 194.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1694.2,
        "end": 1696.9,
        "average": 1695.5500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.11111111111111113,
        "text_similarity": 0.20747493207454681,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that the weakness is stated immediately after the 'go-to response' (the target follows the anchor), but it omits the precise timing information and intervals given in the correct answer (1874.0\u20131885.5 and 1887.2\u20131890.9)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"So I just dug in. I said,\" when does he start listing possible uses for a brick?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2144.2,
        "end": 2157.5
      },
      "pred_interval": {
        "start": 2135.0,
        "end": 2140.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.199999999999818,
        "end": 17.5,
        "average": 13.349999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.5092289447784424,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation (listing begins when he says the line) but gives a wrong start time (2135.0s vs. the correct 2144.2s) and omits the listing end time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"Is everyone still with me? Are we good?\" when does the slide transition to show the 'S(T)AR' method?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2189.8,
        "end": 2191.0
      },
      "pred_interval": {
        "start": 2190.0,
        "end": 2195.0
      },
      "iou": 0.19230769230769904,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.1999999999998181,
        "end": 4.0,
        "average": 2.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7337225675582886,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states the relation ('after') and gives a timestamp (2190.0s) that falls within the reference interval (2189.8\u20132191.0s), accurately matching the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the action taken, when does he finish describing the result?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2376.449,
        "end": 2382.556
      },
      "pred_interval": {
        "start": 2395.0,
        "end": 2400.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.55099999999993,
        "end": 17.44399999999996,
        "average": 17.997499999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.563428521156311,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys the relative order (result occurs shortly after the action) but gives an incorrect/arthimetically different finish time (2395.0s vs 2376.05s) and omits the precise timestamps and details provided in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes talking about the program becoming institutionalized, when does he mention the 'tags' at the bottom of the slide?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2407.152,
        "end": 2412.282
      },
      "pred_interval": {
        "start": 2400.0,
        "end": 2405.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.152000000000044,
        "end": 7.282000000000153,
        "average": 7.217000000000098
      },
      "rationale_metrics": {
        "rouge_l": 0.2545454545454546,
        "text_similarity": 0.530559778213501,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (2400.0s) contradicts the correct timing (tags occur ~2407.152\u20132412.282s) and omits the start/end interval and relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions moving into the mock interview, when does he start explaining what seminal experiences are?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2572.085,
        "end": 2581.418
      },
      "pred_interval": {
        "start": 2536.0,
        "end": 2540.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.085000000000036,
        "end": 41.41800000000012,
        "average": 38.75150000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.5919455289840698,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only preserves the qualitative 'after/shortly after' relation but gives an incorrect mock-interview timestamp (2536.0s vs 2568.5s) and omits the precise start (2572.085s) and end (2581.418s) times, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the bullet point 'Create your talking points -> 3-4 SAR examples', when does he introduce the next bullet point about 'tagging' SAR examples?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2602.202,
        "end": 2611.574
      },
      "pred_interval": {
        "start": 2540.0,
        "end": 2544.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.202000000000226,
        "end": 67.57400000000007,
        "average": 64.88800000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.2769230769230769,
        "text_similarity": 0.3164135217666626,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (2540.0s) contradicts the reference (2602.202s) and omits the provided start/end times for both bullets; it is therefore incorrect and not aligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"So five minutes on the clock\", when does he tell the audience to look at the Muse article?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2689.809,
        "end": 2694.275
      },
      "pred_interval": {
        "start": 2670.0,
        "end": 2675.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.809000000000196,
        "end": 19.27500000000009,
        "average": 19.542000000000144
      },
      "rationale_metrics": {
        "rouge_l": 0.4375,
        "text_similarity": 0.8128807544708252,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives the wrong timestamp (2670.0s vs the correct 2686.3\u20132687.7s) and falsely claims the instruction occurs immediately, whereas the correct answer shows the Muse article is mentioned later at 2689.8\u20132694.3s."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states his two criteria for experience, when does he advise on what career stage experiences to focus on?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2808.94,
        "end": 2831.958
      },
      "pred_interval": {
        "start": 2685.0,
        "end": 2690.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.94000000000005,
        "end": 141.95800000000008,
        "average": 132.94900000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6118641495704651,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is factually incorrect about the timestamps (off by ~100s) and omits the key content that the advice targets grad school and earlier experiences; it only preserves the general ordering of events."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'm gonna speak the question for folks who are watching the recording here, so 'cause I think it's a really good question\", when does he read out the question from the chat?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2867.48,
        "end": 2878.688
      },
      "pred_interval": {
        "start": 2700.0,
        "end": 2705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 167.48000000000002,
        "end": 173.6880000000001,
        "average": 170.58400000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.42545437812805176,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially incorrect timestamps (2700/2705s vs the correct ~2862.5\u20132878.7s) and omits the two-event breakdown (setup then full read); it therefore contradicts the ground truth. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if it's useful to give a STAR story from a non-work context, when does he start discussing family examples as potentially too personal?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2888.2,
        "end": 2891.7
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 2930.0
      },
      "iou": 0.04375,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.19999999999982,
        "end": 38.30000000000018,
        "average": 38.25
      },
      "rationale_metrics": {
        "rouge_l": 0.13698630136986303,
        "text_similarity": 0.07813116908073425,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the order right (anchor before target) but the timestamps are substantially off\u2014anchor given as 2850.0s vs 2868.0\u20132871.0s and target as 2930.0s vs 2888.2\u20132891.7s\u2014so it is not accurately aligned with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will outline what they are going to do first, when does the screen transition to show the next set of instructions for the mock interview?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.0,
        "end": 2920.0
      },
      "pred_interval": {
        "start": 2960.0,
        "end": 3060.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.0,
        "end": 140.0,
        "average": 92.0
      },
      "rationale_metrics": {
        "rouge_l": 0.1142857142857143,
        "text_similarity": 0.3913162648677826,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the screen transition occurs after the speaker, but the reported timestamp (2960.0s) is far from the ground truth (transition begins ~2916.0s and is fully visible by 2920.0s) and it omits the start/end transition details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking 'Why are you interested in this position?', when does he mention 'Tell me about yourself' as an alternative?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3061.797,
        "end": 3062.728
      },
      "pred_interval": {
        "start": 3125.0,
        "end": 3130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.202999999999975,
        "end": 67.27199999999993,
        "average": 65.23749999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.07547169811320754,
        "text_similarity": -0.00394381582736969,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that 'Tell me about yourself' is presented after the prior question, but it omits the key factual details about the immediate succession and the specific timestamps provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will put the interview schedule into the chat box, when does he display the Muse article on behavioral interview questions?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.0,
        "end": 3125.6
      },
      "pred_interval": {
        "start": 3145.0,
        "end": 3160.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.0,
        "end": 34.40000000000009,
        "average": 29.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.2131865918636322,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the key temporal relation (the article is displayed after the statement) but omits the precise start/end timestamps and visibility/navigation details provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if there are any questions before opening breakout rooms, when does he describe the group sizes?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3206.19,
        "end": 3214.181
      },
      "pred_interval": {
        "start": 3185.0,
        "end": 3190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.190000000000055,
        "end": 24.18100000000004,
        "average": 22.685500000000047
      },
      "rationale_metrics": {
        "rouge_l": 0.1923076923076923,
        "text_similarity": 0.39231210947036743,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states that the speaker describes group sizes after asking for questions, but it omits the precise timestamp ranges and the detail that the clarification occurs immediately after the question, which are key elements in the reference answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'So, just plan your time accordingly', when does he say 'And you'll have about 25 minutes and we'll call you back at the end'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3215.09,
        "end": 3217.67
      },
      "pred_interval": {
        "start": 3215.0,
        "end": 3216.0
      },
      "iou": 0.3408239700373894,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.09000000000014552,
        "end": 1.6700000000000728,
        "average": 0.8800000000001091
      },
      "rationale_metrics": {
        "rouge_l": 0.17241379310344826,
        "text_similarity": 0.16704808175563812,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the target utterance occurs after the anchor, but it omits the crucial timestamp details (anchor and target start/end times) provided in the correct answer, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'If we're ready to go, we'll stop the recording and we'll send you off', when does the black screen with text 'For the remainder of the session...' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3231.62,
        "end": 3239.85
      },
      "pred_interval": {
        "start": 3249.0,
        "end": 3250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.38000000000011,
        "end": 10.150000000000091,
        "average": 13.7650000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.0923076923076923,
        "text_similarity": 0.0607108399271965,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the black screen occurs after the speaker (matching the anchor\u2192target order) but omits the key factual details\u2014the specific start/end timestamps and durations\u2014provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they will be doing a mock interview today, when does he start explaining what a mock interview is?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1623.186,
        "end": 1643.288
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.18599999999992,
        "end": 43.28800000000001,
        "average": 38.236999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.24000000000000005,
        "text_similarity": 0.3944358229637146,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states the explanation occurs after the remark but wrongly asserts it begins 'immediately' and omits the specific timestamps given in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the 'Tell me about yourself (TMAY)' question, when does he introduce 'Behavioral Questions'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1740.196,
        "end": 1747.784
      },
      "pred_interval": {
        "start": 1600.0,
        "end": 1610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 140.1959999999999,
        "end": 137.7840000000001,
        "average": 138.99
      },
      "rationale_metrics": {
        "rouge_l": 0.24999999999999994,
        "text_similarity": 0.4512398838996887,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the ground truth timings\u2014placing 'Behavioral Questions' at 1600\u20131610s rather than after TMAY (~1740\u20131748s)\u2014and thus is factually incorrect and not aligned with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains that \"put you on the spot questions\" are by definition questions you can't prepare for, when does he explicitly say he has an example of such a question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2004.224,
        "end": 2006.086
      },
      "pred_interval": {
        "start": 205.0,
        "end": 206.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1799.224,
        "end": 1800.086,
        "average": 1799.655
      },
      "rationale_metrics": {
        "rouge_l": 0.30508474576271183,
        "text_similarity": 0.6582808494567871,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single timestamp (205.0s) that is far from the correct interval (~2004.2\u20132006.1s) and omits the required 'after' relation to the prior explanation, so it is essentially incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"And here's the question\" to introduce the example, when does the slide titled \"THE BRICK QUESTION\" appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2048.659,
        "end": 2048.899
      },
      "pred_interval": {
        "start": 207.0,
        "end": 208.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1841.659,
        "end": 1840.899,
        "average": 1841.279
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.5992405414581299,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but gives a completely incorrect timestamp (207.0s) instead of the ground-truth ~2048.66\u20132048.90s and omits the speaker's actual interval, so it is largely factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks the audience \"What is the point of this question?\", when does he state that the entire purpose of the question is that you can't prepare for it?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2115.539,
        "end": 2118.102
      },
      "pred_interval": {
        "start": 210.0,
        "end": 211.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1905.5390000000002,
        "end": 1907.1019999999999,
        "average": 1906.3205
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.3659819960594177,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted time (210.0s) is far from the ground-truth interval (2115.539\u20132118.102s) and omits the relation and the initial question timestamp, so it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying they will stop the recording and send the attendees off, when does the black screen with information about practicing interviews appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3225.795,
        "end": 3228.795
      },
      "pred_interval": {
        "start": 3249.6,
        "end": 3250.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.804999999999836,
        "end": 21.804999999999836,
        "average": 22.804999999999836
      },
      "rationale_metrics": {
        "rouge_l": 0.12903225806451613,
        "text_similarity": 0.3972572386264801,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect and much later timestamps for the speaker finishing (3249.6\u20133250.6s) and fails to state when the black screen appears (should be ~3225.795s immediately after 3224.795s), so it contradicts the reference and omits the target event timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the screen displays text encouraging attendees to practice interviews, when is the next text displayed on the screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3236.0,
        "end": 3240.0
      },
      "pred_interval": {
        "start": 3250.6,
        "end": 3251.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.599999999999909,
        "end": 11.599999999999909,
        "average": 13.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.09836065573770492,
        "text_similarity": 0.408322811126709,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction does not provide the timing of the next text and misdescribes events (ties the practice text to the speaker's comment) rather than stating the next distinct text appears at about 3236s as in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the screen finishes displaying information about other LCL videos, when do the credits for 'Linguistics Career Launch 2021' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3241.0,
        "end": 3243.0
      },
      "pred_interval": {
        "start": 3251.6,
        "end": 3252.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.599999999999909,
        "end": 9.599999999999909,
        "average": 10.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.10909090909090907,
        "text_similarity": 0.3067705035209656,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction describes different on-screen text with incorrect timestamps that do not match the correct events (LCL videos text and subsequent credits), thus it is factually incorrect and unrelated to the reference answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first woman finishes describing the traits she looks for in people to hire, when does Bartolo introduce himself?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.812,
        "end": 9.402
      },
      "pred_interval": {
        "start": 185.0,
        "end": 190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 177.188,
        "end": 180.598,
        "average": 178.893
      },
      "rationale_metrics": {
        "rouge_l": 0.08888888888888889,
        "text_similarity": 0.27426624298095703,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the temporal relation that Bartolo introduces himself after the woman finishes, but it omits the key factual timing details (the specific timestamps 7.812s\u20139.402s) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'English with Gallery Teachers' title card, when does the background music play?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.0,
        "end": 25.6
      },
      "pred_interval": {
        "start": 19.0,
        "end": 20.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 5.600000000000001,
        "average": 3.8000000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869568,
        "text_similarity": 0.23504826426506042,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly states the relation that the background music plays during the title card, but it omits the key timing details (exact start/end times and that the music is audible throughout the period)."
      }
    },
    {
      "question_id": "003",
      "question": "After Rita mentions her company is not a pyramid system, when does she clarify she never uses the word 'employees'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 114.588,
        "end": 116.843
      },
      "pred_interval": {
        "start": 197.0,
        "end": 198.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.412,
        "end": 81.157,
        "average": 81.78450000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.08333333333333333,
        "text_similarity": -0.03964142128825188,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states that Rita clarifies she never uses 'employees' after denying a pyramid scheme, but it omits the precise timestamps, the brief pause, and the 'next' relation details provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman explains that certifications like CELTA or TEFL are not the sole hiring criteria, when does she state what essential qualities she looks for?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.0,
        "end": 343.3
      },
      "pred_interval": {
        "start": 365.0,
        "end": 372.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.0,
        "end": 28.69999999999999,
        "average": 27.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.11111111111111112,
        "text_similarity": 0.14697027206420898,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction fails to provide the requested timing (timestamps) and appears to reverse the sequence, implying certifications are mentioned after discussion of experience and passion; it only loosely matches content by naming qualities but contradicts the correct temporal ordering."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man mentions that he considers people with scout experience because it indicates passion and ability to manage people, when does the woman add that it shows likability?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 369.0,
        "end": 372.5
      },
      "pred_interval": {
        "start": 408.0,
        "end": 415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.0,
        "end": 42.5,
        "average": 40.75
      },
      "rationale_metrics": {
        "rouge_l": 0.14545454545454542,
        "text_similarity": 0.18293020129203796,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys that the woman speaks after the man and responds to his point, but it omits the key factual details (the exact timestamps and that her remark starts immediately at 369.0s) provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman mentions having 'trick questions' for interviews, such as 'what makes you happiest in teaching?', when does she describe what an ideal answer to that question sounds like?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 530.0,
        "end": 535.0
      },
      "pred_interval": {
        "start": 429.0,
        "end": 436.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.0,
        "end": 99.0,
        "average": 100.0
      },
      "rationale_metrics": {
        "rouge_l": 0.125,
        "text_similarity": 0.11855366826057434,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives the content of an ideal answer rather than the temporal information requested; it fails to state when (the specified time intervals) she describes the preferred answers."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks if they've not hired someone for crossing arms, when does the woman respond about not hiring people who didn't smile?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.6,
        "end": 521.2
      },
      "pred_interval": {
        "start": 540.0,
        "end": 542.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.399999999999977,
        "end": 20.799999999999955,
        "average": 22.599999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869562,
        "text_similarity": 0.23043692111968994,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states that the woman's reply is a direct/immediate response following the man's question (matches the relative timing), but it omits the exact timestamps provided in the reference answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says reading body language is very important, when does she give the example of loving children with a poker face?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.4,
        "end": 632.7
      },
      "pred_interval": {
        "start": 630.0,
        "end": 632.0
      },
      "iou": 0.46511627906976005,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6000000000000227,
        "end": 0.7000000000000455,
        "average": 1.150000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.07017543859649122,
        "text_similarity": 0.2796913981437683,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the relative ordering (the example comes shortly after her remark) but omits the key factual details and exact timestamps provided in the correct answer, making it incomplete and less precise."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about the opportunity to offer educational courses after switching to online learning, when does she mention people outside Chisinau who always wanted this alternative?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 707.0,
        "end": 715.5
      },
      "pred_interval": {
        "start": 725.0,
        "end": 730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 14.5,
        "average": 16.25
      },
      "rationale_metrics": {
        "rouge_l": 0.20895522388059704,
        "text_similarity": 0.19070348143577576,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states the relative temporal relation \u2014 the mention of people outside Chisinau occurs after the discussion of offering courses online \u2014 matching the reference description."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the pandemic simply forced them with the reality of online learning, when does she explain that they were partly stressed and partly happy about the transition?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 817.899,
        "end": 828.773
      },
      "pred_interval": {
        "start": 740.0,
        "end": 745.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.899,
        "end": 83.77300000000002,
        "average": 80.83600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.17721518987341772,
        "text_similarity": 0.2888907194137573,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction merely restates that the speaker expressed mixed feelings but gives no timing or timestamps and fails to indicate that this explanation immediately follows the pandemic statement, omitting key factual details from the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'It's not exactly like in class. I'm not gonna lie, it is not.', when does she give examples of what is missed in online classes?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 863.0,
        "end": 869.0
      },
      "pred_interval": {
        "start": 760.0,
        "end": 765.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 103.0,
        "end": 104.0,
        "average": 103.5
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.4236063063144684,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that examples occur after the quoted remark, but it omits the required timing details (the 863.0\u2013869.0s / 867.0\u2013869.0s timestamps) and thus fails to answer the 'when' precisely."
      }
    },
    {
      "question_id": "001",
      "question": "Once the female speaker finishes saying \"So why not see that?\", when does the male speaker begin to speak?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 992.475,
        "end": 994.716
      },
      "pred_interval": {
        "start": 925.0,
        "end": 930.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.47500000000002,
        "end": 64.71600000000001,
        "average": 66.09550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.1276595744680851,
        "text_similarity": 0.37094390392303467,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that the male starts speaking immediately after the female finishes, but it omits the precise timing information (start at 992.475s, end at 994.716s) given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female speaker finishes stating that Moldova and Romania have a huge number of emigrants, when does she list the countries where they live?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 903.0,
        "end": 908.8
      },
      "pred_interval": {
        "start": 940.0,
        "end": 945.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 36.200000000000045,
        "average": 36.60000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.0967741935483871,
        "text_similarity": 0.027035169303417206,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation that the listing occurs immediately after the preceding statement; it preserves the original meaning despite omitting exact timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male speaker finishes saying \"I agree with you completely\", when does he start talking about writing an article on the future of TEFL?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 996.979,
        "end": 1001.302
      },
      "pred_interval": {
        "start": 960.0,
        "end": 970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.97900000000004,
        "end": 31.30200000000002,
        "average": 34.14050000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.10909090909090909,
        "text_similarity": 0.2392575740814209,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures the relative relation (he begins talking right after finishing 'I agree with you completely'), but it omits the precise timing information (the provided timestamps and duration) given in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'So you have to be adaptable,' when does she say 'only the strongest survive'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1076.398,
        "end": 1078.041
      },
      "pred_interval": {
        "start": 1125.0,
        "end": 1130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.60200000000009,
        "end": 51.95900000000006,
        "average": 50.280500000000075
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.6485122442245483,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the phrases but the timestamps are far off (\u224849\u201353 seconds later) and it fails to reflect that 'only the strongest survive' immediately follows the prior line, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says, 'You cannot come at a lesson in a hoodie,' when does the man appear on screen wearing a red hoodie?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1112.044,
        "end": 1114.077
      },
      "pred_interval": {
        "start": 1145.0,
        "end": 1150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.955999999999904,
        "end": 35.923,
        "average": 34.43949999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.14084507042253522,
        "text_similarity": 0.47832608222961426,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the man appears after the woman's statement and mentions a red hoodie, but the timestamp (1145.0s) is significantly off from the correct 1112.044s and it omits the described smiling/pulling-hoodie gesture and end time, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes saying, 'It's been really nice to have you again on this channel,' when does the Facebook page overlay appear on screen?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1184.146,
        "end": 1186.146
      },
      "pred_interval": {
        "start": 1232.0,
        "end": 1237.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.85400000000004,
        "end": 50.85400000000004,
        "average": 49.35400000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322035,
        "text_similarity": 0.553324282169342,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the overlay appears after the man's remark, but the timestamp is substantially incorrect (1232.0s vs 1184.146s) and it omits the appearance duration and transition details from the reference."
      }
    }
  ]
}