{
  "topic_id": 10,
  "topic_name": "Restaurant Service Encounters",
  "num_evaluated": 23,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.1716475719887819,
      "rouge_l_std": 0.03716669220028244,
      "text_similarity_mean": 0.4643171591603238,
      "text_similarity_std": 0.16788896410807838,
      "llm_judge_score_mean": 2.260869565217391,
      "llm_judge_score_std": 1.62098712020063
    },
    "short": {
      "rouge_l_mean": 0.10374829411518402,
      "rouge_l_std": 0.08964500720723173,
      "text_similarity_mean": 0.35520354496396106,
      "text_similarity_std": 0.17526280129585242,
      "llm_judge_score_mean": 2.347826086956522,
      "llm_judge_score_std": 1.6045775728039946
    },
    "cider": {
      "cider_detailed": 0.00034752508359328605,
      "cider_short": 6.98870675768535e-07
    }
  },
  "per_entry_results": [
    {
      "video_id": "WQ_GdqOAyJM",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.15242494226327943,
        "text_similarity": 0.49914416670799255,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is a very generic cooking summary and omits almost all key specifics (private chef, ingredient sourcing, multiple distinct dishes like Pavlova, branzino, Ora King salmon, prime filet, guest catering and cleanup). It also includes a likely hallucination (pouring a bottle of wine over the dish) and lacks completeness and accuracy."
      },
      "short": {
        "rouge_l": 0.12598425196850394,
        "text_similarity": 0.49843353033065796,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the general theme of meal preparation in a professional setting and emphasis on fresh ingredients, but it omits nearly all key specifics from the reference (private chef context, shopping, named dishes and ingredients, multi-course dinner, dessert, and cleanup/exhaustion)."
      }
    },
    {
      "video_id": "k69HiX5I4as",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.26744186046511625,
        "text_similarity": 0.8799898624420166,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the main premise, size, and ingredient list of the CN Tower Burger Challenge, but it omits key details such as the burger's 14-pound weight, the eater's strategy and pauses, and the fact he successfully finished (becoming the first to do so), and it includes an unsupported specific timer detail."
      },
      "short": {
        "rouge_l": 0.4102564102564102,
        "text_similarity": 0.7880842685699463,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly names the challenge, its >5 ft size, main ingredients, and that it had never been completed, but it omits the crucial fact that a man successfully finished the ~14\u2011pound burger and misses cheese/pepper\u2011popper details; it also adds an unsupported timer detail."
      }
    },
    {
      "video_id": "GLDd5u1dizo",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.11386138613861387,
        "text_similarity": 0.528841495513916,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer describes a cooking demonstration and omits the video's actual content\u2014personal reflections on culinary school, career advice, financial costs, and the harsh realities and burnout of restaurant work\u2014so it does not match the reference."
      },
      "short": {
        "rouge_l": 0.078125,
        "text_similarity": 0.305841863155365,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer lists basic cooking actions unrelated to the video's content about culinary school vs real-world restaurant work, burnout, working conditions, and physical toll; it omits all key points and adds irrelevant details."
      }
    },
    {
      "video_id": "rPx6VIjkYco",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.2891566265060241,
        "text_similarity": 0.8865289688110352,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies all five featured food spots and their core items, but it omits several specific facts from the reference (e.g., Winkel 43 as the presenter\u2019s \"best ever\" pie, Ramen Kingdom\u2019s traditional style and cooking-show experience, Mannekenpis\u2019s 20+ sauces and long lines justification, and Van Stapele selling out in the afternoon), and slightly alters sequencing/spelling."
      },
      "short": {
        "rouge_l": 0.3308270676691729,
        "text_similarity": 0.8200558423995972,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction lists all five key places and their signature items (Winkel 43 apple pie, Ramen Kingdom, Mannekenpis fries, Van Stapele cookie, Rudi's stroopwafels), but it omits several specific descriptors (e.g., 'best ever' pie, ramen's traditional/in-front-of-you cooking, fries' 20+ sauces, and that the bakery has only one menu item)."
      }
    },
    {
      "video_id": "JJOTu9IkiUo",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.1925925925925926,
        "text_similarity": 0.7089450359344482,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only notes adding noodles, broth, and rice and someone eating, but it omits the central customer\u2013chef interaction, the Kaedama explanation, the chef insisting on finishing the broth with rice, the torching/\u2018ramen risotto\u2019 finish, and it adds unverified details (toppings), so it misses most key elements and some facts."
      },
      "short": {
        "rouge_l": 0.1414141414141414,
        "text_similarity": 0.5862225294113159,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures basic actions (preparing, adding noodles and rice, serving) but omits key elements like the Kaedama cultural context, the chef insisting on rice, the torch-used final 'Ramen Risotto' preparation, and the customer-chef interaction\u2014reducing completeness and fidelity."
      }
    },
    {
      "video_id": "4PyTLRh7k5w",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.16138328530259363,
        "text_similarity": 0.41476449370384216,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts and omits key facts from the correct answer (it hallucinates a handgun, ignores the restaurant owner, threats, dialogue, and setting) and only minimally matches that a purple-haired man and another person are present."
      },
      "short": {
        "rouge_l": 0.08988764044943821,
        "text_similarity": 0.20447590947151184,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction describes a gun-focused confrontation and person appearances that do not align with the reference: it omits the restaurant owner/mob claim, the demand to leave, the streamer's apology and swift exit in an outdoor dining area, and likely hallucinates a handgun, so it is mostly incorrect."
      }
    },
    {
      "video_id": "Yx4K6CuraOs",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.15441176470588236,
        "text_similarity": 0.40145736932754517,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only the high-level sequence (people eat and give feedback) but omits almost all key specifics (gold/silver-plated biryani and gulab jamun, restaurant name, prices, varied reactions and ratings) and even misrepresents participant composition, so it is largely incomplete and partly inaccurate."
      },
      "short": {
        "rouge_l": 0.08333333333333334,
        "text_similarity": 0.2988364100456238,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is overly generic and omits almost all key facts (gold-plated biryani, silver-plated Rabri Gulab Jamun, Karachi/Barahn, price, mixed vs overwhelmingly positive reviews). It only loosely matches the scene of people eating and giving feedback, so it is largely incomplete and insufficient."
      }
    },
    {
      "video_id": "WurBSP0mOmY",
      "video_number": "008",
      "detailed": {
        "rouge_l": 0.1641337386018237,
        "text_similarity": 0.520949125289917,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures generic shots (person speaking, people eating burgers) but omits nearly all key facts about the Whammy Burger Challenge, sponsorship, location, rules, times, participants' names, outcomes, and even includes an incorrect detail (Burger King sign); thus it is largely incomplete and partially inaccurate."
      },
      "short": {
        "rouge_l": 0.099009900990099,
        "text_similarity": 0.22605851292610168,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only notes basic visuals (man speaking, couple eating) and repeats them, but it omits nearly all key factual elements from the correct answer\u2014no mention of the Keeps promotion, the Whammy Burger Challenge details, outcomes (milkshake completions), or Raina's remaining food/time\u2014so it is largely incomplete."
      }
    },
    {
      "video_id": "B4BVNSrUJf8",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.182648401826484,
        "text_similarity": 0.39375489950180054,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer describes a casual couple dining scene with chips and salsa and omits all key elements of the Whammy Burger challenge (location, size, contest format, strategy, time limit, and outcome), so it fails to match the correct summary."
      },
      "short": {
        "rouge_l": 0.05714285714285715,
        "text_similarity": 0.2965553402900696,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction lists vague scene elements (menu, food presentation, enjoying meal) but fails to mention the Whammy Burger challenge, its location, size/ingredients, parallels to extreme-eating shows, participant strategy/progress, or the outcome, so it misses most key facts."
      }
    },
    {
      "video_id": "1iIOXO9k73E",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.14946619217081852,
        "text_similarity": 0.5339241027832031,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only conveys a generic dining/food-vlog vibe which loosely matches the video's theme, but it omits nearly all key facts from the correct answer\u2014no mention of the named participants, the spicy-food challenge, specific restaurants/dishes, or the Botanico Mexican Restaurant segment."
      },
      "short": {
        "rouge_l": 0.06837606837606838,
        "text_similarity": 0.5341933965682983,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only lists generic scenes (group photo, eating, opinions) and thus loosely matches the video's surface elements, but it omits almost all key facts: the no-water spicy Indian food challenge, participant/referee setup, the three named restaurants and escalating spice, cultural identity discussion, and the transition to Botanico Mexican Restaurant with its ambiance and cocktails."
      }
    },
    {
      "video_id": "1e0zfq8zksk",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.15204678362573099,
        "text_similarity": 0.3096747398376465,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only gives a very high-level sequence (outside to inside, dishes served, people interacting) but omits almost all key specifics from the correct answer\u2014restaurant name/location, friends' context, the named teas and appetizers, the detailed 'Taste of Mesob' platter, and the injera-eating demonstration."
      },
      "short": {
        "rouge_l": 0.07692307692307693,
        "text_similarity": 0.24257032573223114,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only generic scene progression (outside, interior, eating, leaving) but omits key facts about Ethiopian food, Mesob restaurant in Montclair, the 'Taste of Mesob' platter, and learning to eat with injera, so it fails to convey the video's main content."
      }
    },
    {
      "video_id": "S_QduJQCof0",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.17100371747211895,
        "text_similarity": 0.20207661390304565,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures very generic footage (a couple walking, eating, and filming) but omits virtually all key factual elements from the correct answer\u2014specific locations, foods, tapas/vermouth stops, croquette discussion, and the closing subscribe message\u2014so it is a poor semantic match."
      },
      "short": {
        "rouge_l": 0.07142857142857144,
        "text_similarity": 0.2845226526260376,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only generic actions (a couple walking and eating) but omits nearly all key specifics from the correct answer\u2014location (Alicante), the variety of local dishes and tastings, budget spots, discussion about croquettes, and the closing call-to-action\u2014so it lacks semantic completeness and detail."
      }
    },
    {
      "video_id": "eByLJB78i74",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.17117117117117117,
        "text_similarity": 0.3896733522415161,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only basic visuals (people eating large burgers and a timer) but omits almost all key facts from the reference\u2014challenge rules, names, location, burger details, sides, the topple, outcome and praise\u2014and even misidentifies participants and the video's conclusion."
      },
      "short": {
        "rouge_l": 0.03669724770642201,
        "text_similarity": 0.30754542350769043,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures a few basic visual elements (people eating large burgers and a timer) but omits almost all key facts from the reference\u2014participant names, restaurant/location, challenge details, outcome, and recommendation\u2014so it is largely incomplete."
      }
    },
    {
      "video_id": "W4adUBGfbmM",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.16494845360824742,
        "text_similarity": 0.362529993057251,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only the generic act of dining and brief satisfaction but omits nearly all key specifics from the reference (location, named dishes, ordering kiosks, characters, drink menu, and nuanced evaluations), so it is largely incomplete."
      },
      "short": {
        "rouge_l": 0.017391304347826087,
        "text_similarity": 0.3028472661972046,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only conveys a very generic dining experience and satisfaction, which loosely matches the video's theme, but it omits nearly all key specifics (location, dishes, kiosk ordering, ambiance, drink menu, lava egg, recommendation, and price/ambience discussion)."
      }
    },
    {
      "video_id": "qoKJoUb4o9g",
      "video_number": "015",
      "detailed": {
        "rouge_l": 0.1384083044982699,
        "text_similarity": 0.37092113494873047,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction conveys a generic busy restaurant/kitchen atmosphere but omits nearly all key specifics from the correct answer (Chef Sho, Torasho Ramen & Charcoal Bar, timeline, signature dishes, charcoal/butchery, meetings, and other detailed events), so it is largely incomplete though not contradictory."
      },
      "short": {
        "rouge_l": 0.049689440993788817,
        "text_similarity": 0.41939491033554077,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures very general scenes (busy kitchen, chef cooking, serving, restaurant atmosphere) but omits almost all key specifics from the correct answer\u2014no mention of Chef Sho, Torasho Ramen dishes, ingredient sourcing, tasting/assembly, staff meeting, times, or signature menu items."
      }
    },
    {
      "video_id": "St8rysYXm9w",
      "video_number": "016",
      "detailed": {
        "rouge_l": 0.171875,
        "text_similarity": 0.4751175045967102,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the basic sequence (walking trail, kayaking, beach break, restaurant, final photo) but omits most key specifics and events from the correct answer\u2014names/locations (Abu Dhabi, mangroves, Vanishing Island), unique plants and the life-jacket scare, specific meals and desserts, the meeting with a long-time fan and ride-back/hug, and the hotel night-view scene\u2014making it incomplete."
      },
      "short": {
        "rouge_l": 0.10344827586206896,
        "text_similarity": 0.32774120569229126,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures a few broad activities (kayaking, a meal) but is very terse and omits key specifics from the reference\u2014meeting the loyal viewer, the Majd Palace desserts (kanafa/date cake), the drive back and Jen's exhausted hotel/night view\u2014so it is incomplete."
      }
    },
    {
      "video_id": "iOm5Uj7EQUE",
      "video_number": "017",
      "detailed": {
        "rouge_l": 0.18518518518518515,
        "text_similarity": 0.22433312237262726,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer describes an entirely different sequence (monument, street walk, bench, receipt) and omits or contradicts all key elements of the reference (table-side Arabic dessert preparation, host's reactions, and coffee with dates), showing no semantic overlap."
      },
      "short": {
        "rouge_l": 0.017857142857142856,
        "text_similarity": 0.1287277340888977,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated and incorrect: it omits all key details about the dessert, the server presentation, the host's reactions, and the coffee and dates, and instead includes hallucinated actions (monument, receipt) not present in the correct summary."
      }
    },
    {
      "video_id": "zW9qFTtYpus",
      "video_number": "018",
      "detailed": {
        "rouge_l": 0.1626794258373206,
        "text_similarity": 0.351347416639328,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the general lively, interactive hibachi cooking and dishes being prepared, but it omits key specifics from the correct answer (Kringle and her brother, the birthday celebration, restaurant name, chef Ching Ching, drinks/salads, fried rice, the onion 'volcano', and the specific proteins) and adds unrelated details (title card and call to action)."
      },
      "short": {
        "rouge_l": 0.14285714285714282,
        "text_similarity": 0.3328702747821808,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction notes the chef cooking and patrons enjoying meals, but it omits key facts: the birthday celebration, the restaurant (Ohana Hibachi), the chef's theatrical flaming onion 'volcano', and the specific dishes prepared live; it also adds an unrelated call-to-action."
      }
    },
    {
      "video_id": "efBgaDGpM70",
      "video_number": "019",
      "detailed": {
        "rouge_l": 0.1836065573770492,
        "text_similarity": 0.5756738781929016,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only a vague gist (someone eating and enjoying a variety of dishes including tacos and a bowl of soup) but omits nearly all key specifics from the reference (host name, location, the 45-minute Texas-sized challenge, detailed item list like carnitas torta, masa quesadilla, chicharron gordita, pozole specifics) and includes minor inaccuracies (e.g., 'burger' and a woman holding the tray)."
      },
      "short": {
        "rouge_l": 0.055555555555555546,
        "text_similarity": 0.19346779584884644,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely matches that multiple dishes are consumed and the eater is satisfied, but it misidentifies participants (mentions a woman presenting), omits the Texas-sized challenge, Joel Hansen, location, and all specific dishes and sequence, so it fails to capture the correct answer's key facts."
      }
    },
    {
      "video_id": "7uJW0U7SGXc",
      "video_number": "020",
      "detailed": {
        "rouge_l": 0.14847161572052403,
        "text_similarity": 0.43469834327697754,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only gives a very generic description of people eating and dishes being shown and omits almost all key specifics from the correct answer (location, restaurant name, specific dishes like Xi'an spinach noodles, communal soups, enthusiastic praise and chef commendation), so it is largely incomplete despite not contradicting the reference."
      },
      "short": {
        "rouge_l": 0.08771929824561403,
        "text_similarity": 0.17163962125778198,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only notes people eating, dishes being served, and conversation, which is superficially related, but it omits nearly all key factual elements (location, cafeteria name, communal soup tradition, Xi'an spinach noodles, strong praise/five-star rating), so it is largely incomplete."
      }
    },
    {
      "video_id": "mfveFCjol8E",
      "video_number": "021",
      "detailed": {
        "rouge_l": 0.1529745042492918,
        "text_similarity": 0.38679391145706177,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only a vague, high-level flow (man eating, moving around, group conversation) but omits almost all key specifics from the correct answer\u2014restaurant name, staff interactions, specific dishes and presentations, decor details, interviews, and the rooftop view\u2014resulting in poor semantic alignment."
      },
      "short": {
        "rouge_l": 0.04615384615384615,
        "text_similarity": 0.28780943155288696,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures very generic actions (eating, walking, talking) and omits nearly all key specifics from the correct answer\u2014restaurant name/location, ambiance, allergy accommodations, signature milkshake, theatrical presentation, Amazon-themed decor, interviews with customers and the GM, and rooftop balcony\u2014so it is largely incomplete though not contradictory."
      }
    },
    {
      "video_id": "onpWsrNV8KY",
      "video_number": "022",
      "detailed": {
        "rouge_l": 0.17349397590361443,
        "text_similarity": 0.3991750478744507,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only generic visual elements (man speaking, restaurant, Christmas tree, food close-ups) but omits nearly all key factual details from the reference\u2014character dining with Snow White and villains, specific dishes and cocktails, interactions/parade, geyser visit, and dessert parade\u2014so it fails to match the correct summary."
      },
      "short": {
        "rouge_l": 0.0736842105263158,
        "text_similarity": 0.30216941237449646,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction lists generic visual scenes (man, trees, restaurants) but omits almost all key factual elements, names, events and menu details from the correct answer (Disney's Wilderness Lodge, Storybook Dining, geyser, Villains & Vice, specific dishes/cocktails/desserts), so it captures only minimal visual overlap."
      }
    },
    {
      "video_id": "EUaYNOGjjr4",
      "video_number": "023",
      "detailed": {
        "rouge_l": 0.14450867052023122,
        "text_similarity": 0.4289800822734833,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only minimal visual aspects (a man eating rice and meat on the floor and drinking tea) but omits almost all key factual details from the correct answer\u2014restaurant names, multiple dishes and their descriptions, the second restaurant visit, and the host's commentary\u2014so it is largely incomplete."
      },
      "short": {
        "rouge_l": 0.12244897959183675,
        "text_similarity": 0.30961787700653076,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is overly vague and omits almost all key details (two specific restaurants, Yemeni dishes, cardamom tea/coffee, atmosphere, recommendations), and it even adds an incorrect detail that the man 'prepares' the meal rather than arriving and tasting it."
      }
    }
  ]
}