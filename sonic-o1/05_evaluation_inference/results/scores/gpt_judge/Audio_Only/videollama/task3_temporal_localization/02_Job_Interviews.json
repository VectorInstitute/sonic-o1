{
  "topic_id": 2,
  "topic_name": "Job Interviews",
  "num_evaluated": 254,
  "aggregated_metrics": {
    "mean_iou": 0.04877468329280952,
    "std_iou": 0.08608867250413782,
    "median_iou": 0.020052883464943737,
    "R@0.3": {
      "recall": 0.015748031496062992,
      "count": 4,
      "total": 254
    },
    "R@0.5": {
      "recall": 0.007874015748031496,
      "count": 2,
      "total": 254
    },
    "R@0.7": {
      "recall": 0.003937007874015748,
      "count": 1,
      "total": 254
    },
    "mae": {
      "start_mean": 131.57825984251969,
      "end_mean": 157.3275196850394,
      "average_mean": 144.45288976377952
    },
    "rationale": {
      "rouge_l_mean": 0.24051144166187557,
      "rouge_l_std": 0.0980316132910363,
      "text_similarity_mean": 0.5439093754164815,
      "text_similarity_std": 0.1917527517575993,
      "llm_judge_score_mean": 2.484251968503937,
      "llm_judge_score_std": 2.0480805377884383
    },
    "rationale_cider": 0.21701199145696834
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the man asks the woman to sell him the pen, when does the woman start describing the pen as fantastic and smooth?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 3.47,
        "end": 8.757
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.11281319378369804,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.73,
        "end": 26.243000000000002,
        "average": 13.986500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.5598586797714233,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misidentifies both events and their timestamps (speaker intro and unrelated line instead of the man asking to buy the pen and the woman describing it); only the temporal relation 'after' matches the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes asking why the man needs the pen, when does the man reply, 'Ah, exactly. Now, this is the point...'?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 24.55,
        "end": 30.536
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.45,
        "end": 6.064,
        "average": 8.257
      },
      "rationale_metrics": {
        "rouge_l": 0.1639344262295082,
        "text_similarity": 0.5216042399406433,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different anchor/target timestamps and utterances (5.2s/35.0s vs correct 17.649s/24.55s) and thus fails to match the referenced exchange; the only partial match is the vague 'after' relation, not the correct 'immediately follows.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the man says 'You are not there just to carry the product to the customer', when does he list reasons like color or thick writing for wanting a pen?",
      "video_id": "xliS9BijbOs",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 60.0
      },
      "gt_interval": {
        "start": 39.244,
        "end": 50.436
      },
      "pred_interval": {
        "start": 36.6,
        "end": 47.4
      },
      "iou": 0.5894767273778548,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.6439999999999984,
        "end": 3.0360000000000014,
        "average": 2.84
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.47403261065483093,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction wrongly identifies both event times and contents (speaker intro and 'final year medical student' vs the anchor's statement at ~36.4s and the pen-reason list at ~39.2\u201350.4s). Only the coarse 'after' relation matches, so the answer fails on timing and semantic accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces herself as a former visa officer, when does she explain what American officials expect during an interview?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 34.481,
        "end": 40.61
      },
      "pred_interval": {
        "start": 5.2,
        "end": 30.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.281000000000002,
        "end": 10.61,
        "average": 19.945500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.0784313725490196,
        "text_similarity": 0.08964651823043823,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that she explains expectations after introducing herself, but it fails to provide the requested timing details (the specific timestamps and anchor/target intervals) given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman states that the visa officer is judging you, when does the text 'BE CONFIDENT!' appear on screen?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 106.12,
        "end": 111.935
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.12,
        "end": 75.33500000000001,
        "average": 73.2275
      },
      "rationale_metrics": {
        "rouge_l": 0.13043478260869565,
        "text_similarity": 0.10189831256866455,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly affirms that the text appears after the woman's line but fails to provide the requested timing details (the anchor and target timestamps and that the target occurs much later), so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes her advice about maintaining general eye contact, when does she begin talking about a slight smile?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 149.259,
        "end": 151.34
      },
      "pred_interval": {
        "start": 38.5,
        "end": 45.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 110.75899999999999,
        "end": 106.34,
        "average": 108.5495
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.1027664914727211,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the 'once_finished' relationship (that she begins talking about a slight smile immediately after), but it omits the key factual details\u2014specific start/end timestamps and duration\u2014provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman finishes stating that the visa officer will not have a favorable impression, when does she mention that this could influence a refusal?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 155.0,
        "end": 156.5
      },
      "pred_interval": {
        "start": 150.0,
        "end": 193.7
      },
      "iou": 0.03432494279176202,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 37.19999999999999,
        "average": 21.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.1,
        "text_similarity": 0.15520672500133514,
        "llm_judge_score": 7,
        "llm_judge_justification": "The predicted answer correctly captures the key relation that she mentions it immediately after finishing (matches 'once_finished'), but it omits the precise timestamps and formal temporal details provided in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman asks what to do to gain confidence, when does she state that practicing is first and foremost?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 159.766,
        "end": 161.729
      },
      "pred_interval": {
        "start": 150.0,
        "end": 193.7
      },
      "iou": 0.044919908466819744,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.765999999999991,
        "end": 31.970999999999975,
        "average": 20.868499999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.12244897959183673,
        "text_similarity": 0.1203789934515953,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys that she says practicing comes after the question, but it omits the required event labels, precise timestamps, and the note about the slight pause/\u2018after\u2019 relation, so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes explaining how Argo Visa can help gain confidence for visa issuance, when does the screen transition to 'Follow us:'?",
      "video_id": "2Ba98C_Zess",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 200.0
      },
      "gt_interval": {
        "start": 187.512,
        "end": 200.0
      },
      "pred_interval": {
        "start": 150.0,
        "end": 200.0
      },
      "iou": 0.24975999999999998,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.512,
        "end": 0.0,
        "average": 18.756
      },
      "rationale_metrics": {
        "rouge_l": 0.06349206349206349,
        "text_similarity": 0.3270114064216614,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the causal relation (the transition occurs once she finishes) but omits the key factual details in the reference \u2014 the precise timestamps for the anchor speech and the visual transition \u2014 making it incomplete. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the second house rule about turning on video, when does she state that without video the interview will not be conducted?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 29.731,
        "end": 32.777
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.09700636942675156,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.531000000000002,
        "end": 3.8230000000000004,
        "average": 14.177000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.7897340059280396,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction identifies the correct event and the 'after' relationship and roughly locates the anchor, but it omits the anchor end time and gives notably incorrect target timestamps (several seconds late), so the timing is not accurately matched."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how to use the chat icon, when does she start explaining how to use the raise hand icon?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 50.521,
        "end": 57.454
      },
      "pred_interval": {
        "start": 48.5,
        "end": 63.8
      },
      "iou": 0.4531372549019609,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.021000000000001,
        "end": 6.3459999999999965,
        "average": 4.183499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.6636500358581543,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps for both anchor and target differ substantially from the reference and the anchor is mischaracterized, though it correctly indicates the target comes after the anchor in a vague sense."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing all the continents where TTEC is located, when does she state that TTEC India is located in Ahmedabad?",
      "video_id": "IwGQoK9v5AA",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 97.0
      },
      "gt_interval": {
        "start": 84.04,
        "end": 88.665
      },
      "pred_interval": {
        "start": 64.9,
        "end": 77.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.14,
        "end": 11.265,
        "average": 15.2025
      },
      "rationale_metrics": {
        "rouge_l": 0.3835616438356164,
        "text_similarity": 0.8498077392578125,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the mention of Ahmedabad but the timestamps and anchor boundaries are substantially incorrect and inconsistent with the reference (predicted events occur much earlier and mislabel the anchor), so it fails to provide the correct temporal localization."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes describing the first reason for leaving a job, when does she start explaining the second reason?",
      "video_id": "2dgx53kiOBQ",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 46.0
      },
      "gt_interval": {
        "start": 10.023,
        "end": 15.169
      },
      "pred_interval": {
        "start": 35.0,
        "end": 41.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.977,
        "end": 26.630999999999997,
        "average": 25.804
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7531481981277466,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is largely incorrect: it gives completely different anchor/target timecodes and utterances and states a different temporal relation ('after' vs the correct immediate 'next'), so it contradicts the reference on key facts."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'you're on the hunt', when does he say '\u6211\u5728\u627e\u5de5\u4f5c'?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 3.0,
        "end": 4.9
      },
      "pred_interval": {
        "start": 5.2,
        "end": 10.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2,
        "end": 5.9,
        "average": 4.050000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.36666666666666664,
        "text_similarity": 0.7770813703536987,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies and swaps the anchor/target events and gives completely different timestamps (5.2s/10.8\u201316.2s vs. 1.633s/3.0\u20134.9s). Although both state the relation 'after', the event identities and timings are incorrect, so the answer is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying '\u5c65\u5386\u8868' (CV), when does he say '\u5e94\u5f81\u5de5\u4f5c' (To apply for jobs)?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 15.5,
        "end": 16.5
      },
      "pred_interval": {
        "start": 23.4,
        "end": 29.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.899999999999999,
        "end": 13.3,
        "average": 10.6
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.8305638432502747,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both events and their timings (correct anchor ends at 13.0s and target is 15.5\u201316.5s, whereas the prediction places them at ~23.4\u201336.6s and swaps labels). While both state a following relation, the temporal locations and event labels are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'when's the interview?', when does he ask about the salary for the position in Mandarin?",
      "video_id": "qKctM_6Ymbw",
      "video_number": "005",
      "segment": {
        "start": 0.0,
        "end": 42.0
      },
      "gt_interval": {
        "start": 33.0,
        "end": 36.4
      },
      "pred_interval": {
        "start": 35.0,
        "end": 40.6
      },
      "iou": 0.18421052631578924,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 4.200000000000003,
        "average": 3.1000000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.3703703703703704,
        "text_similarity": 0.7830612659454346,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer largely mismatches the reference: both anchor and target timestamps and utterance contents are incorrect (the anchor is misidentified as a speaker introduction and the target is a different phrase), though the temporal relation 'after' matches. These substantive errors justify a very low score."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the first tip about dressing to impress, when does she start giving the second tip?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 11.708,
        "end": 14.471
      },
      "pred_interval": {
        "start": 5.2,
        "end": 9.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.508,
        "end": 4.670999999999999,
        "average": 5.589499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.7489879131317139,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misidentifies both event timestamps and content (wrong start/end times and an irrelevant quoted phrase), and thus contradicts the ground truth; only the vague temporal relation ('after') is remotely similar to 'once_finished.'"
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the three quick tips for a job fair, when does she explain the second tip about doing research?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 14.811,
        "end": 20.96
      },
      "pred_interval": {
        "start": 10.6,
        "end": 14.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.211,
        "end": 6.66,
        "average": 5.4355
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.6850067377090454,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relative ordering right but misstates both event timestamps and durations by several seconds (anchor and target times disagree with the reference), omitting the correct timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Nothing else will make you stand out more than your personality', when does she say 'Be sure to use that'?",
      "video_id": "VV9MlsraXmA",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 32.0
      },
      "gt_interval": {
        "start": 28.591,
        "end": 29.734
      },
      "pred_interval": {
        "start": 15.1,
        "end": 17.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.491000000000001,
        "end": 12.134,
        "average": 12.8125
      },
      "rationale_metrics": {
        "rouge_l": 0.15625000000000003,
        "text_similarity": 0.726416826248169,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction mislabels and mis-times both events (timestamps are far off and the quoted lines are swapped), so it fails to locate when 'Be sure to use that' is said; only the vague 'after' relation is somewhat aligned."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking \"Tell me about yourself\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 10.0,
        "end": 16.993
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.2227070063694267,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.8,
        "end": 19.607000000000003,
        "average": 12.203500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.16438356164383558,
        "text_similarity": 0.6526464223861694,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference on key facts: it gives entirely different start/end times and content for both events and a different temporal relation, so it fails to match the correct timing and duration (only correctly implying an 'after' ordering in a very general sense)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes asking \"Why should we hire you?\", when does the green answer text appear on screen?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 29.937,
        "end": 39.249
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.17182130584192454,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.062999999999999,
        "end": 2.649000000000001,
        "average": 3.856
      },
      "rationale_metrics": {
        "rouge_l": 0.136986301369863,
        "text_similarity": 0.6670186519622803,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer largely mismatches the reference: the anchor (E1) timing is incorrect and the relation/description is imprecise, though the predicted E2 interval (35.0\u201336.6s) falls inside the true E2 span, so there is partial overlap but it fails to capture the correct start time and full display duration or the exact 'once_finished' relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes announcing \"Let's check your pronunciation. Listen and repeat.\", when does he start repeating the first smart answer?",
      "video_id": "9u2lc73bWzI",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 165.0
      },
      "gt_interval": {
        "start": 121.718,
        "end": 126.147
      },
      "pred_interval": {
        "start": 10.8,
        "end": 165.0
      },
      "iou": 0.028722438391699107,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 110.918,
        "end": 38.852999999999994,
        "average": 74.88550000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.19047619047619047,
        "text_similarity": 0.6678135395050049,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect\u2014timestamps and the quoted repeated utterance do not match the reference and the repeat timing is wrong; only the vague 'after' relation aligns, so it receives minimal credit."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of the virtual interview, when does she list the three things to set the right first impression?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 13.846,
        "end": 19.861
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.20184563758389262,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.646,
        "end": 15.139,
        "average": 11.8925
      },
      "rationale_metrics": {
        "rouge_l": 0.30508474576271183,
        "text_similarity": 0.7618652582168579,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relation 'after' is correct, the predicted timestamps are largely incorrect (especially E2, which is far later than the reference), so key factual elements are wrong and the answer is not aligned with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to use a virtual background or blur it, when does she start talking about sound and internet connection?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 40.176,
        "end": 42.981
      },
      "pred_interval": {
        "start": 35.0,
        "end": 70.4
      },
      "iou": 0.0792372881355932,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.176000000000002,
        "end": 27.419000000000004,
        "average": 16.297500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767127,
        "text_similarity": 0.7804234027862549,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely mismatches the reference: E1 is mis-identified (5.2s vs correct 39.594s), the relation label differs ('after' vs 'once_finished'), and the E2 timings are incorrect and inconsistently described, though the predicted E2 interval loosely covers the true E2 window."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker recommends connecting laptops to an ethernet cable, when is the next piece of advice she gives regarding avoiding distractions?",
      "video_id": "WOx3B-LSI3o",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 100.0
      },
      "gt_interval": {
        "start": 50.012,
        "end": 59.987
      },
      "pred_interval": {
        "start": 70.4,
        "end": 100.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.388000000000005,
        "end": 40.013,
        "average": 30.2005
      },
      "rationale_metrics": {
        "rouge_l": 0.1408450704225352,
        "text_similarity": 0.7019117474555969,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it misidentifies the anchor and target events and their timestamps, reverses the order of advice, and gives incorrect relation and fabricated timings, so it fails to match the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the video's topic about crushing a sales job interview, when does the animated logo for 'The Elliott Group' appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.378,
        "end": 13.048
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.19026845637583892,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.178,
        "end": 21.951999999999998,
        "average": 12.065
      },
      "rationale_metrics": {
        "rouge_l": 0.3793103448275862,
        "text_similarity": 0.7752580046653748,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the relationship 'after', but the timestamps are largely incorrect (E2 start given as 35.0s vs correct 7.378s) and the E1 end and E2 end times are missing/incorrect, so it fails to match the key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that many applicants are 'unprepared' for sales jobs, when does the text overlay 'COME PREPARED' appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 55.459,
        "end": 56.559
      },
      "pred_interval": {
        "start": 36.6,
        "end": 78.4
      },
      "iou": 0.02631578947368407,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.859,
        "end": 21.841000000000008,
        "average": 20.350000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3214285714285714,
        "text_similarity": 0.7095577716827393,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', its timestamps are substantially incorrect: the speaker event timing does not match the reference (ends at 48.408s) and the text overlay is mislocated at 78.4s instead of 55.459\u201356.559s, so it is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker describes looking 'unmanicured' for an interview, when does he make a specific hand gesture with both hands to emphasize his point?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 322.0,
        "end": 323.0
      },
      "pred_interval": {
        "start": 102.0,
        "end": 153.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 220.0,
        "end": 169.4,
        "average": 194.7
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.6962593793869019,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and temporal relation are incorrect: both event times do not match the reference and the relation is given as 'after' whereas the correct relation is 'during', so the prediction fails to capture the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that resumes are not needed, when does he ask what dealerships want instead?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 175.098,
        "end": 175.998
      },
      "pred_interval": {
        "start": 150.0,
        "end": 190.0
      },
      "iou": 0.02249999999999943,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.098000000000013,
        "end": 14.00200000000001,
        "average": 19.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.052631578947368425,
        "text_similarity": 0.018824666738510132,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the sequence\u2014that he asks what dealerships want instead after saying resumes aren't needed\u2014but it omits the precise timestamps and anchor/target annotations provided in the correct answer, making it incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's explanation of what 'the machine' is, when does a visual of a man and woman in a car showroom appear?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 307.098,
        "end": 311.098
      },
      "pred_interval": {
        "start": 190.0,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 117.09800000000001,
        "end": 101.09800000000001,
        "average": 109.09800000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2033898305084746,
        "text_similarity": 0.4281955361366272,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the showroom visual appears during the explanation but fails to provide the crucial timing details (the specific timestamps and containment within the anchor speech) given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining that customers and managers will see you as someone who 'works at the mall' if you are dressed poorly, when does he directly advise to 'dress nice'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 273.401,
        "end": 274.923
      },
      "pred_interval": {
        "start": 210.0,
        "end": 230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.40100000000001,
        "end": 44.923,
        "average": 54.162000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.29183557629585266,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that he immediately advises to 'dress nice' after that explanation, but it omits the key factual timing details (E1 ending at 272.338s and E2 spanning 273.401\u2013274.923s with a short pause) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if people are hiring someone average, when does he mention they are trying to hire a 'difference maker'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.877,
        "end": 375.04
      },
      "pred_interval": {
        "start": 330.0,
        "end": 480.0
      },
      "iou": 0.027753333333333408,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.87700000000001,
        "end": 104.95999999999998,
        "average": 72.9185
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.397711843252182,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the mention follows the question about hiring an average person, but it fails to provide the required timestamps and adds an unconfirmed remark about a text overlay, leaving out the precise timing given in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he is 'very coachable', when does the text overlay 'I'M VERY COACHABLE' appear on screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 414.192,
        "end": 418.53
      },
      "pred_interval": {
        "start": 450.0,
        "end": 510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.80799999999999,
        "end": 91.47000000000003,
        "average": 63.63900000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.2687308192253113,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps (450.0\u2013510.0s) are completely different from the reference (anchor ~413.93\u2013414.192s; target appears at 414.192s and ends at 4148.53s), contradicting the immediate appearance and the reported end time, so it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises to have 'eye contact', when does he directly look at the camera and say 'just like I'm looking at you in the camera'?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 533.923,
        "end": 537.649
      },
      "pred_interval": {
        "start": 510.0,
        "end": 540.0
      },
      "iou": 0.12419999999999996,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.923000000000002,
        "end": 2.350999999999999,
        "average": 13.137
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.39871957898139954,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted interval roughly contains the true event but is temporally inaccurate\u2014the predicted start (510.0s) is ~23.9s earlier and the end (540.0s) is ~2.35s later than the correct times\u2014and it fails to distinguish the anchor vs. target segments described in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"eye contact, write that down,\" when does he demonstrate eye contact with his hands?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 535.23,
        "end": 537.26
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 530.03,
        "end": 500.65999999999997,
        "average": 515.345
      },
      "rationale_metrics": {
        "rouge_l": 0.08955223880597014,
        "text_similarity": 0.5111771821975708,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely incorrect: it gives unrelated timestamps and quotes (5.2s/35.0s vs the correct ~532.21\u2013537.26s) and does not identify the speaker's hand demonstration described in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"How stupid would that be?\" when does he say, \"Eye contact, look.\"",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 549.39,
        "end": 551.41
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 514.39,
        "end": 514.81,
        "average": 514.5999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.14705882352941177,
        "text_similarity": 0.6183567047119141,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted events and timestamps do not match the correct anchor or target utterances (they reference unrelated lines and times), so the answer is essentially incorrect despite coincidentally stating 'after' for the relationship."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'll self-educate\" for the second time, when does the text overlay \"I'LL SELF EDUCATE\" appear on the screen?",
      "video_id": "N6M3gm6A4lw",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 710.0
      },
      "gt_interval": {
        "start": 637.11,
        "end": 642.12
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 631.91,
        "end": 605.52,
        "average": 618.7149999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.11940298507462688,
        "text_similarity": 0.5649176836013794,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely incorrect: it lists different timestamps and different utterances for both events and does not match the correct timing or content (the overlay timing and the quoted phrase are wrong)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"We help you land your dream job\", when does the text \"TRAGIC ENDINGS\" appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.968,
        "end": 13.737
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.056337579617834393,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.768,
        "end": 22.863,
        "average": 14.8155
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.6362273693084717,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer is largely incorrect: it gives wrong timestamps and identifies the wrong on-screen text/content for E2, and misreports E1 timing (start vs correct finish). Although both state the relation is 'after', the temporal boundaries and content do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the 'My Practice Interview' logo and text animation finishes, when does the speaker return on screen and say \"So, we're talking about casual interviews\"?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 50.841,
        "end": 52.768
      },
      "pred_interval": {
        "start": 84.5,
        "end": 154.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.659,
        "end": 102.03200000000001,
        "average": 67.8455
      },
      "rationale_metrics": {
        "rouge_l": 0.34375000000000006,
        "text_similarity": 0.7096764445304871,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: it gives wrong timestamps and misidentifies event content, and the relation 'after' contradicts the correct 'once_finished' timing relationship."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the client receiving an email stating 'Sorry, we're going with somebody else,' when does he say the client was devastated?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 176.5,
        "end": 177.7
      },
      "pred_interval": {
        "start": 150.0,
        "end": 208.7
      },
      "iou": 0.020442930153321787,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.5,
        "end": 31.0,
        "average": 28.75
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424243,
        "text_similarity": 0.6879234313964844,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relation as 'after', its timestamps are significantly incorrect (predicting E2 at 193.4s vs the reference 176.5\u2013177.7s), misreports E1 timing and adds unsupported visual-cue details, so it fails to match the key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is saying 'you come across as sounding actually inexperienced', when does the word 'INEXPERIENCED' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 225.1,
        "end": 228.2
      },
      "pred_interval": {
        "start": 193.4,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.69999999999999,
        "end": 18.19999999999999,
        "average": 24.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.6356876492500305,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's times for both the speaker utterance and the on-screen word are incorrect and the relation ('after') contradicts the correct 'during'; it also adds unsupported visual cues and misidentifies the anchor content."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing the three tips for casual interviews, when does 'KEY TIP #1 NEVER TREAT AN INTERVIEW AS A CASUAL AFFAIR' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 270.6,
        "end": 275.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 231.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.60000000000002,
        "end": 43.400000000000006,
        "average": 52.000000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.21176470588235297,
        "text_similarity": 0.5243785381317139,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly identifies E1 (gives a start time 210.0s rather than the anchor finishing at 270.7s), gives a wrong E2 time (225.0s vs 270.6s\u2013275.0s), and states an 'after' relation and visual cues that contradict the ground truth 'once_finished' alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'So, those are my three tips', when does the text 'YOU NEED TO PREPARE THE SAME WAY' appear on screen?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 379.3,
        "end": 382.2
      },
      "pred_interval": {
        "start": 330.0,
        "end": 369.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.30000000000001,
        "end": 13.0,
        "average": 31.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.5595250129699707,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies the events and timings (anchor given as 330.0s instead of 374.7s, and text timing 369.2\u2013483.0s instead of 379.3\u2013382.2s), includes unsupported visual details, and only the qualitative 'after' relation matches; therefore it is mostly incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining about the interview preparation videos, when does he start talking about his free ebook, 'My Practice Interview'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 401.4,
        "end": 409.8
      },
      "pred_interval": {
        "start": 369.2,
        "end": 474.5
      },
      "iou": 0.07977207977208009,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.19999999999999,
        "end": 64.69999999999999,
        "average": 48.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.6510757207870483,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps and event content do not match (ebook mention vs 'final year medical student'), the temporal relation is wrong (after vs immediately/once_finished), and it includes unrelated visual cues\u2014constituting major factual mismatches."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing his free ebook, when is the next time he mentions 'My Interview Accelerator Workshop'?",
      "video_id": "Yl4s9AIrijU",
      "video_number": "010",
      "segment": {
        "start": 330.0,
        "end": 483.0
      },
      "gt_interval": {
        "start": 418.2,
        "end": 421.9
      },
      "pred_interval": {
        "start": 474.5,
        "end": 587.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.30000000000001,
        "end": 165.80000000000007,
        "average": 111.05000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666669,
        "text_similarity": 0.6210615634918213,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it cites different anchor/target timestamps and a different resource name ('My Practice Interview' vs 'My Interview Accelerator Workshop'), and includes irrelevant visual cues, so it does not match the correct event."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces herself as a licensed hairdresser, when does she explain why she decided to take a break from the hair industry?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.03,
        "end": 28.03
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.1592356687898089,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.830000000000002,
        "end": 8.57,
        "average": 13.200000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.41025641025641024,
        "text_similarity": 0.7266281843185425,
        "llm_judge_score": 4,
        "llm_judge_justification": "The relation 'after' and the anchor timestamp are close to the reference, but the predicted target timing is substantially off (~12s later) and does not match the correct interval, so the answer is factually incorrect on the key timestamp."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states she needs to get ready, when does she announce that her hair and makeup are done?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.66,
        "end": 113.61
      },
      "pred_interval": {
        "start": 84.5,
        "end": 109.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.159999999999997,
        "end": 3.8100000000000023,
        "average": 14.985
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.765439510345459,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies both utterances and the 'after' relation, but it has significant timestamp errors (E1 is far off and E2's end time/duration is incorrect) and omits accurate timing details from the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says she will try on outfits, when does she begin to show her chosen interview outfit in the mirror?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 277.7,
        "end": 279.6
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.00904761904761921,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 127.69999999999999,
        "end": 80.39999999999998,
        "average": 104.04999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.4285375475883484,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the general sequence (she says she'll try outfits and then is shown in the mirror), but it omits the key timing details and the distinction between the initial mirrored appearance (277.7s) and when she fully shows the outfit (279.6s), so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman declares she has finally 'got the outfit down', when does she describe the specific clothing items she is wearing?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 257.6,
        "end": 272.0
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.06857142857142846,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 107.60000000000002,
        "end": 88.0,
        "average": 97.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3103448275862069,
        "text_similarity": 0.46061816811561584,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only vaguely says she shows off clothing afterward and even claims the video ends that way, but it omits the precise timing (starts at 257.6s, ends at 272.0s) and misrepresents/completely lacks the key temporal details from the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions her discount code, when does she start explaining Dossier's reward system?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 423.05,
        "end": 433.322
      },
      "pred_interval": {
        "start": 330.0,
        "end": 480.0
      },
      "iou": 0.06847999999999994,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 93.05000000000001,
        "end": 46.678,
        "average": 69.864
      },
      "rationale_metrics": {
        "rouge_l": 0.3018867924528302,
        "text_similarity": 0.393100380897522,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely incorrect timestamps (330.0s and 480.0s) and omits the actual code timing and precise start (423.05s) reported in the correct answer; although it retains an 'after' ordering, the factual timing information is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes spraying perfume on her neck/hair, when does she spray perfume on her wrist?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 365.341,
        "end": 366.421
      },
      "pred_interval": {
        "start": 480.0,
        "end": 600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 114.65899999999999,
        "end": 233.579,
        "average": 174.119
      },
      "rationale_metrics": {
        "rouge_l": 0.5,
        "text_similarity": 0.729634165763855,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives completely incorrect timestamps (480s and 600s vs. 364.9s and ~365.3\u2013366.4s) and implies a long delay rather than the immediate spraying indicated in the correct answer, so it contradicts key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker suggests bringing a resume, when does she explain why it's a good idea?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 440.04,
        "end": 452.824
      },
      "pred_interval": {
        "start": 600.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 159.95999999999998,
        "end": 267.176,
        "average": 213.56799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.3612426221370697,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction correctly identifies that explanation follows the suggestion but is factually wrong about the timing (huge, incorrect timestamps and delay), contradicting the precise times given in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman advises writing down a list of questions, when does she suggest asking about work hours?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 537.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 513.4,
        "end": 583.4
      },
      "iou": 0.03571428571428571,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.600000000000023,
        "end": 43.89999999999998,
        "average": 33.75
      },
      "rationale_metrics": {
        "rouge_l": 0.09677419354838708,
        "text_similarity": 0.3175927996635437,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the key temporal relation ('after') that she suggests asking about work hours, but it omits the precise timestamps and the exact phrasing of the question provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman suggests researching the salon's social media and website, when does she explain why this research is important?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 652.5,
        "end": 659.0
      },
      "pred_interval": {
        "start": 603.7,
        "end": 683.7
      },
      "iou": 0.08125,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.799999999999955,
        "end": 24.700000000000045,
        "average": 36.75
      },
      "rationale_metrics": {
        "rouge_l": 0.08108108108108109,
        "text_similarity": 0.21730126440525055,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') between the suggestion and the explanation, but it omits the precise timestamps and the quoted explanatory utterance given in the reference, so it's incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman recommends having a portfolio ready to show, when does she emphasize the importance of social media in the salon world?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 702.0
      },
      "pred_interval": {
        "start": 693.0,
        "end": 720.0
      },
      "iou": 0.2222222222222222,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0,
        "end": 18.0,
        "average": 10.5
      },
      "rationale_metrics": {
        "rouge_l": 0.11940298507462686,
        "text_similarity": 0.3357499837875366,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative relation ('after') but omits the requested timing details and specific segment timestamps/quote from the correct answer, so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman talks about social media being a big thing in the salon world, when does she explain that social media marketing can bring more clients into the salon?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 733.4,
        "end": 797.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 753.8
      },
      "iou": 0.1897674418604649,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.39999999999998,
        "end": 43.700000000000045,
        "average": 43.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.44178295135498047,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence (that the explanation comes after the initial comment) but fails to provide the requested timing details (the specific timestamps/intervals), so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman finishes saying that confidence and the way one presents themselves goes a long way, when does she talk about preferring a personable applicant?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 784.0,
        "end": 794.9
      },
      "pred_interval": {
        "start": 754.2,
        "end": 809.4
      },
      "iou": 0.19746376811594185,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.799999999999955,
        "end": 14.5,
        "average": 22.149999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.1851851851851852,
        "text_similarity": 0.5519074201583862,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (that she speaks about preferring a personable applicant after the confidence remark) but omits the key factual details\u2014specific timestamps and duration (E1=783.8s; E2=784.0\u2013794.9s)\u2014given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman talks about waiting for the car's air conditioning to cool down, when does she advise giving enough time to arrive early for the interview?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 854.5,
        "end": 861.7
      },
      "pred_interval": {
        "start": 810.0,
        "end": 870.0
      },
      "iou": 0.12000000000000076,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.5,
        "end": 8.299999999999955,
        "average": 26.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.34870094060897827,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys that the advice comes after the AC comment and matches the content, but it omits the precise timestamps and start/end timing details provided in the reference, so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'wish me luck', when does she say 'I'm back from the interview'?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1004.0
      },
      "gt_interval": {
        "start": 882.5,
        "end": 883.5
      },
      "pred_interval": {
        "start": 870.0,
        "end": 994.0
      },
      "iou": 0.008064516129032258,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.5,
        "end": 110.5,
        "average": 61.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25531914893617025,
        "text_similarity": 0.5751091241836548,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction preserves the correct 'after' relationship, the anchor and especially the target timestamps are substantially incorrect and the response adds unsupported visual/audio details, so it fails factual alignment with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says the interview was 'super easy', when does she mention there was 'none of that typical what are your strengths and weaknesses'?",
      "video_id": "FMWQ0jSOX8I",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 1004.0
      },
      "gt_interval": {
        "start": 927.9,
        "end": 932.1
      },
      "pred_interval": {
        "start": 870.0,
        "end": 994.0
      },
      "iou": 0.03387096774193585,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.89999999999998,
        "end": 61.89999999999998,
        "average": 59.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2545454545454546,
        "text_similarity": 0.5588867664337158,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target phrase and that it occurs after the anchor, but both anchor and target timestamps are substantially incorrect and it adds unsupported visual/audio details; key timing information is therefore wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the animated intro sequence concludes, when does the speaker greet the audience with \"Morning, everyone\"?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 51.533,
        "end": 52.234
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.333,
        "end": 15.634,
        "average": 30.9835
      },
      "rationale_metrics": {
        "rouge_l": 0.17857142857142858,
        "text_similarity": 0.6726263761520386,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both events and their timestamps (5.2s and 35.0\u201336.6s vs. 50.512s and 51.533\u201352.234s) and the quoted utterance differs; only the temporal relation ('after') matches, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"First, context\", when does the text \"Design exercise \u2260 white boarding\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 56.396,
        "end": 101.982
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.396,
        "end": 65.382,
        "average": 43.389
      },
      "rationale_metrics": {
        "rouge_l": 0.24489795918367344,
        "text_similarity": 0.7405396699905396,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the qualitative relation (it appears after 'First, context') but omits key factual details from the reference\u2014specific timestamps (appearing at 56.396, visible until 101.982) and the replacement event\u2014making it incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that the next thing to get right is the deliverables, when does the text introduce action item number two?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 195.3,
        "end": 198.0
      },
      "pred_interval": {
        "start": 150.0,
        "end": 210.0
      },
      "iou": 0.04499999999999981,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.30000000000001,
        "end": 12.0,
        "average": 28.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.20895522388059704,
        "text_similarity": 0.7016843557357788,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative 'after' relationship correct but the timestamps are largely wrong and contradict the reference (anchor ends at 192.6s; target appears 195.3\u2013198.0s), so it fails on the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he clearly missed the biggest thing, when does the text describe the final deliverable for a take-home exercise?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 256.5,
        "end": 261.7
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.024761904761904707,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 106.5,
        "end": 98.30000000000001,
        "average": 102.4
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290322,
        "text_similarity": 0.7217815518379211,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the temporal relation ('after') right but incorrectly labels and timestamps both events (swapping anchor/target and giving wrong start/end times), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"action item number three\", when does the text overlay \"Use standard patterns\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 348.0,
        "end": 352.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 360.0
      },
      "iou": 0.13333333333333333,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 8.0,
        "average": 13.0
      },
      "rationale_metrics": {
        "rouge_l": 0.25974025974025977,
        "text_similarity": 0.7283563613891602,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the text appears after the anchor and notes the visual/audio cues, but its timestamps are substantially off and it hallucinates much longer durations, so it fails on factual timing accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is talking about looking at other apps for design inspiration, when does the text overlay \"By looking at Google MD, Apple HIG\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 370.0,
        "end": 378.0
      },
      "pred_interval": {
        "start": 360.0,
        "end": 420.0
      },
      "iou": 0.13333333333333333,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 42.0,
        "average": 26.0
      },
      "rationale_metrics": {
        "rouge_l": 0.22429906542056074,
        "text_similarity": 0.7025449872016907,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the presence of the text overlay and its alignment with the speaker, but the timestamps are substantially incorrect (E2 is given as starting at 360.0s and ending at 420.0s vs. the reference 370.0\u2013378.0s), so it contradicts the key temporal facts."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"Action item number four\", when does the text overlay \"Pick the right prompt\" appear?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 382.7,
        "end": 386.0
      },
      "pred_interval": {
        "start": 420.0,
        "end": 480.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.30000000000001,
        "end": 94.0,
        "average": 65.65
      },
      "rationale_metrics": {
        "rouge_l": 0.2553191489361702,
        "text_similarity": 0.7602934837341309,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly names the text overlay and the audio cue, but the timestamps and durations are drastically incorrect (E1/E2 times differ by ~40s and E2's duration is wildly overstated) and it incorrectly claims E2 starts simultaneously with E1 rather than shortly after."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says \"Now is the fun one, fun time,\" when does the text \"Hot Take\" appear on screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 528.0,
        "end": 533.5
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 522.8,
        "end": 498.5,
        "average": 510.65
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384614,
        "text_similarity": 0.6395612955093384,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes that the target text appears after the anchor, but the reported timestamps are wildly incorrect (5.2s and 35.0s vs. 526.5\u2013527.9s and 528.0\u2013533.5s) and it omits the correct durations\u2014thus largely factually wrong despite the right ordering."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions his 'cover letter video', when does the YouTube video thumbnail related to cover letters appear on the screen?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 566.5,
        "end": 615.0
      },
      "pred_interval": {
        "start": 36.6,
        "end": 48.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 529.9,
        "end": 566.3,
        "average": 548.0999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7446024417877197,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the thumbnail appears after the anchor and notes the visual cue, but the provided timestamps are significantly different from the reference (and durations are omitted), so the temporal details are factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "While the speaker says \"smash the like button for the YouTube algorithm,\" when does he make a 'smashing' gesture with his arms?",
      "video_id": "jW__kov1qZs",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 639.0
      },
      "gt_interval": {
        "start": 607.0,
        "end": 609.0
      },
      "pred_interval": {
        "start": 60.3,
        "end": 72.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 546.7,
        "end": 536.1,
        "average": 541.4000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.1492537313432836,
        "text_similarity": 0.7402284741401672,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies both events but gives completely incorrect timestamps and the wrong temporal relation ('after' vs. overlapping/simultaneous). It correctly notes a gesture occurred but omits/end-times and contradicts the ground-truth timing, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the host states that Mr. Syed Hassan is a top LinkedIn voice with good experience in recruitment and HR, when does Syed greet the host?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.826,
        "end": 23.329
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.05043624161073826,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.626,
        "end": 11.671,
        "average": 14.1485
      },
      "rationale_metrics": {
        "rouge_l": 0.13559322033898305,
        "text_similarity": 0.5866653323173523,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relationship as 'after', it gives incorrect timestamps and incorrect event content for both the anchor and target (the target is not the greeting indicated in the reference), so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes asking what applicants should consider before applying for jobs, when does Syed begin his answer by saying 'Well, that's a very good question'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 74.466,
        "end": 81.582
      },
      "pred_interval": {
        "start": 37.4,
        "end": 108.4
      },
      "iou": 0.10022535211267605,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.065999999999995,
        "end": 26.818000000000012,
        "average": 31.942000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.14457831325301204,
        "text_similarity": 0.6051743030548096,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct target utterance but gives completely wrong timestamps for both events and an incorrect temporal relation ('after' rather than immediately following); therefore it fails to match the reference timing and relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After Syed states that many people apply for irrelevant jobs, when does he start explaining that most companies in Dubai use ATS systems?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 104.0,
        "end": 105.605
      },
      "pred_interval": {
        "start": 109.8,
        "end": 180.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.799999999999997,
        "end": 74.395,
        "average": 40.0975
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7371480464935303,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and the 'after' relationship, but both event timestamps are substantially incorrect compared to the ground truth, so the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first speaker finishes explaining that rejections in Dubai are automated, when does the second speaker give positive feedback?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 162.4,
        "end": 164.8
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.011428571428571456,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.400000000000006,
        "end": 195.2,
        "average": 103.8
      },
      "rationale_metrics": {
        "rouge_l": 0.2456140350877193,
        "text_similarity": 0.6019302606582642,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives a single time of 210.0s and repeats the question, whereas the correct answer specifies E1 ends at 161.8s and E2 runs from 162.4s\u2013164.8s (relation once_finished); key timing details are missing and wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the first speaker mentions that developers are mostly in demand in the UAE, when does he start listing specific types of developers?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.1,
        "end": 255.2
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.019523809523809495,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.1,
        "end": 104.80000000000001,
        "average": 102.95
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.2837175130844116,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (210.0s) contradicts the correct start time (251.1s) and thus is incorrect; it fails to match the referenced event timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the left (Hassan) starts explaining his screening process for resumes, when does he first mention checking the candidate's years of experience?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 364.35,
        "end": 366.36
      },
      "pred_interval": {
        "start": 330.0,
        "end": 429.5
      },
      "iou": 0.020201005025125538,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.35000000000002,
        "end": 63.139999999999986,
        "average": 48.745000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2077922077922078,
        "text_similarity": 0.574993371963501,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and that the mention occurs after the anchor, but the timestamps are substantially incorrect (E1 predicted at 330s vs 364.18s; E2 predicted at 406.5\u2013429.5s vs 364.35\u2013366.36s), so the timing information is unreliable."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the left (Hassan) mentions asking for a screening call, when does he advise to check for red flags during that call?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 429.39,
        "end": 432.42
      },
      "pred_interval": {
        "start": 430.0,
        "end": 513.5
      },
      "iou": 0.028771846391630194,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6100000000000136,
        "end": 81.07999999999998,
        "average": 40.845
      },
      "rationale_metrics": {
        "rouge_l": 0.21951219512195122,
        "text_similarity": 0.49234849214553833,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the high-level relation ('after') correct but fails on key facts: the anchor and target timestamps are substantially off (target predicted ~493\u2013513s vs correct ~429\u2013432s) and the quoted target phrase does not match the reference, indicating hallucinated/incorrect content."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the left (Hassan) states they shortlist a candidate, when does he mention calling them to assess them in person?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.58,
        "end": 443.3
      },
      "pred_interval": {
        "start": 514.0,
        "end": 664.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.42000000000002,
        "end": 221.2,
        "average": 146.81
      },
      "rationale_metrics": {
        "rouge_l": 0.2077922077922078,
        "text_similarity": 0.4996066689491272,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures a similar intent (assessing/interviewing the candidate) but the timestamps are substantially incorrect and the temporal relation ('after') does not match the correct 'once_finished' placement; overall the localization and relation are wrong despite partial semantic overlap."
      }
    },
    {
      "question_id": "001",
      "question": "After the man on the right finishes talking about the content being helpful for many working professionals, when does he mention sharing Mr. Hassan's profile?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 523.7,
        "end": 526.1
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 518.5,
        "end": 489.5,
        "average": 504.0
      },
      "rationale_metrics": {
        "rouge_l": 0.1282051282051282,
        "text_similarity": 0.5359294414520264,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's events and timestamps do not match the reference: both E1 and E2 times are completely different and the predicted E2 ('I am a final year medical student') does not correspond to mentioning sharing Mr. Hassan's profile, so it is incorrect despite the same temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "After the man on the right asks the audience if they have any questions, when does he tell them to write in the comments?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 542.0,
        "end": 543.5
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 507.0,
        "end": 506.9,
        "average": 506.95
      },
      "rationale_metrics": {
        "rouge_l": 0.10526315789473685,
        "text_similarity": 0.4408690929412842,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer fails to identify the correct events or timestamps\u2014both E1 and E2 contents and times are incorrect\u2014though it coincidentally matches the 'after' relation. Significant factual mismatches warrant a very low score."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man on the right finishes stating that he and Mr. Hassan will review and answer questions, when does the man on the left say 'Definitely, definitely'?",
      "video_id": "W6NUlYvx-C0",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 555.0
      },
      "gt_interval": {
        "start": 546.5,
        "end": 547.5
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 541.3,
        "end": 510.9,
        "average": 526.0999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.12121212121212122,
        "text_similarity": 0.5701541304588318,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is completely incorrect: it identifies different utterances, speakers, and timestamps (5.2s/35\u201336.6s) that do not match the reference events at ~546.5\u2013547.5s, and the stated relation ('after') contradicts the correct 'once_finished' immediate response."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says her first job interview was through LinkedIn, when does she mention the job tab on LinkedIn?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 112.525,
        "end": 116.189
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 107.325,
        "end": 79.589,
        "average": 93.457
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.7042555212974548,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the anchor and target timestamps and identifications are incorrect and do not match the ground truth, so it is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is demonstrating on her phone how to search for a job on LinkedIn, when does she verbally instruct to write 'architect or interior designer jobs'?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 146.179,
        "end": 148.622
      },
      "pred_interval": {
        "start": 35.0,
        "end": 49.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.179,
        "end": 99.22200000000001,
        "average": 105.2005
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.6792948246002197,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer gives completely different anchor and target times and event descriptions (\u224835\u201349s) that conflict with the reference timings (\u2248140.8\u2013148.6s) and thus fails to match the correct temporal alignment or events."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying 'keywords plays a very important role', when does her finger scroll down the list of job posts on the phone screen?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 170.3
      },
      "pred_interval": {
        "start": 41.5,
        "end": 50.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 128.5,
        "end": 119.4,
        "average": 123.95
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.666354775428772,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the ground truth: it misidentifies the anchor event timing and the target scrolling timing/duration (predicted ~43.8\u201350.9s vs correct ~170.0\u2013170.3s). Only the vague 'after' relation aligns, so the answer is mostly incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions finding multiple tabs when searching for architect jobs, when does she instruct the viewer to go to the 'posts' tab?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.4,
        "end": 158.9
      },
      "pred_interval": {
        "start": 153.9,
        "end": 204.6
      },
      "iou": 0.029585798816568053,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5,
        "end": 45.69999999999999,
        "average": 24.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.5218077898025513,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relation, its timestamps do not match the reference (anchor off by ~4s, target off by ~47s) and it introduces unverified visual cues; it therefore fails to capture the key temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises cross-checking company details before sending a CV, when does she suggest calling the company's mentioned number for verification?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 383.586,
        "end": 388.331
      },
      "pred_interval": {
        "start": 180.8,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 202.786,
        "end": 178.33100000000002,
        "average": 190.5585
      },
      "rationale_metrics": {
        "rouge_l": 0.23188405797101452,
        "text_similarity": 0.677898645401001,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the key factual elements\u2014the anchor/target timestamps\u2014are substantially incorrect and it adds unsupported visual-cue details, so it does not align with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says she went to the company's profile, when does she mention finding the number there?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 381.5,
        "end": 383.466
      },
      "pred_interval": {
        "start": 330.0,
        "end": 395.8
      },
      "iou": 0.029878419452887658,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.5,
        "end": 12.334000000000003,
        "average": 31.917
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6744304895401001,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misidentifies the event timestamps (both anchor and target) and gives a different temporal relation phrasing; only the general notion that one event follows another is retained, so it deserves minimal credit."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the company asked her to share her CV via email, when does she say she actually shared it?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 401.369,
        "end": 404.314
      },
      "pred_interval": {
        "start": 390.4,
        "end": 435.8
      },
      "iou": 0.06486784140969143,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.969000000000051,
        "end": 31.48599999999999,
        "average": 21.22750000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4044943820224719,
        "text_similarity": 0.7611473202705383,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the same utterances conceptually (company asked for CV and then she shared it) but the anchor/target timestamps are substantially wrong and the temporal relation ('after') contradicts the correct 'once_finished' overlap, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says she gave the company a call to check on hiring, when does she confirm that she did call and they were looking for a candidate?",
      "video_id": "VLxZYcza_Cg",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 435.803
      },
      "gt_interval": {
        "start": 388.572,
        "end": 395.942
      },
      "pred_interval": {
        "start": 408.5,
        "end": 435.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.927999999999997,
        "end": 39.858000000000004,
        "average": 29.893
      },
      "rationale_metrics": {
        "rouge_l": 0.2117647058823529,
        "text_similarity": 0.6419157981872559,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different timestamps, identifies wrong utterances, and labels the relation as 'after' rather than the correct immediate 'once_finished', so it fails to match the reference on key elements."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that not getting a job at a big company is not the case, when does she start introducing strategies for getting a job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 191.24,
        "end": 195.36
      },
      "pred_interval": {
        "start": 150.0,
        "end": 198.0
      },
      "iou": 0.08583333333333343,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.24000000000001,
        "end": 2.6399999999999864,
        "average": 21.939999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705885,
        "text_similarity": 0.28867313265800476,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that strategies begin after the anchor, but it omits the key factual details (the precise start/end timestamps and the quoted phrase) provided in the correct answer, so it is incomplete. "
      }
    },
    {
      "question_id": "002",
      "question": "After the 'BEFORE INTERVIEW' text appears on screen, when does the 'DURING INTERVIEW (ONSITE & OFFSITE)' text appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.12,
        "end": 209.32
      },
      "pred_interval": {
        "start": 204.0,
        "end": 360.0
      },
      "iou": 0.03327495621716283,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.8799999999999955,
        "end": 150.68,
        "average": 77.28
      },
      "rationale_metrics": {
        "rouge_l": 0.10344827586206896,
        "text_similarity": 0.40526461601257324,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference timing: the correct onset is 200.12s (lasting until 209.32s), whereas the prediction states 210.0s and omits duration, so it is factually incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that you should get ready technically, when does she start listing examples of how to get ready technically?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 338.44,
        "end": 347.64
      },
      "pred_interval": {
        "start": 330.0,
        "end": 480.0
      },
      "iou": 0.06133333333333326,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.439999999999998,
        "end": 132.36,
        "average": 70.4
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.21589790284633636,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction merely restates the question and provides no timestamps or details about when the examples start, omitting the key factual information in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says that the next topic is things to do during the interview, when does she state the first thing to do?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 405.02,
        "end": 415.34
      },
      "pred_interval": {
        "start": 480.0,
        "end": 690.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.98000000000002,
        "end": 274.66,
        "average": 174.82000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.11538461538461538,
        "text_similarity": 0.11005227267742157,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction merely restates that she announces the first item but gives no timing or the detailed timestamps provided in the correct answer, omitting the key temporal information and thus failing to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker tells the audience to ask interviewers questions about themselves during the chat, when does she start giving an example related to infrastructure as code?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 470.16,
        "end": 495.68
      },
      "pred_interval": {
        "start": 690.0,
        "end": 840.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 219.83999999999997,
        "end": 344.32,
        "average": 282.08
      },
      "rationale_metrics": {
        "rouge_l": 0.14084507042253522,
        "text_similarity": 0.1803518533706665,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction merely restates that she begins an example but provides no timing or the detailed timestamps given in the correct answer, omitting key factual elements required by the question."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises to 'get to know your interviewer, build a relationship with the person', when does she state that doing so leaves an impression?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.0,
        "end": 532.52
      },
      "pred_interval": {
        "start": 510.0,
        "end": 580.0
      },
      "iou": 0.050285714285714024,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.0,
        "end": 47.48000000000002,
        "average": 33.24000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.4816637933254242,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives a single timestamp of 580.0s, which is about 48 seconds later than the correct interval (529.0\u2013532.52s) when the speaker says it leaves an impression, so the timing is incorrect and not aligned with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the point 'Be yourself', when does she explain the consequences of trying to be fake?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 544.33,
        "end": 584.4
      },
      "pred_interval": {
        "start": 600.0,
        "end": 670.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.66999999999996,
        "end": 85.60000000000002,
        "average": 70.63499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.5855647325515747,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted time (670.0s) is far outside the correct explanation interval (544.33\u2013584.4s) and provides a single incorrect timestamp instead of the correct start/end range, so it fails to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that interviews are typically 'peer programming sessions', when does she give an example of reducing latency from 100 milliseconds to 8 milliseconds?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 668.88,
        "end": 677.08
      },
      "pred_interval": {
        "start": 680.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.120000000000005,
        "end": 42.91999999999996,
        "average": 27.019999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.3278688524590164,
        "text_similarity": 0.6005407571792603,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the event occurs after the peer-programming remark but gives a wrong timestamp (720.0s) that does not match the correct interval (668.88\u2013677.08s), so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes advising to 'add numbers as well if they are numbers to add', when does she say that 'there are sometimes that there won't be numbers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 704.38,
        "end": 708.06
      },
      "pred_interval": {
        "start": 690.0,
        "end": 723.5
      },
      "iou": 0.10985074626865522,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.379999999999995,
        "end": 15.440000000000055,
        "average": 14.910000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.22499999999999998,
        "text_similarity": 0.45985549688339233,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the order but the timestamps are noticeably off (E1 predicted 700.0s vs 703.38s; E2 predicted 710.0s falls after the correct 704.38\u2013708.06s interval) and it fails to reflect that the target immediately follows the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions an impact on '10 different teams', when does she talk about '10 million users or 10 million customers'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 723.39,
        "end": 725.25
      },
      "pred_interval": {
        "start": 730.0,
        "end": 748.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.610000000000014,
        "end": 23.149999999999977,
        "average": 14.879999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.35820895522388063,
        "text_similarity": 0.5522865653038025,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the correct ordering (target occurs after the anchor), but the timestamps are inaccurate\u2014especially the anchor (predicted 720.0s vs correct ~695s), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "When does the next numbered text overlay appear after the overlay '6. Mention past achievements you can comfortably talk about.'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 796.43,
        "end": 800.39
      },
      "pred_interval": {
        "start": 750.0,
        "end": 766.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.42999999999995,
        "end": 33.68999999999994,
        "average": 40.059999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.2028985507246377,
        "text_similarity": 0.737143874168396,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted time (760.0s) contradicts the ground-truth timings (overlay 6 appears ~795.23\u2013801.43s and overlay 7 ~796.43\u2013800.39s) and fails to identify the correct next overlay timing, so it is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions doing what you should be doing after the interview, when does the text overlay 'Sit back and focus on yourself.' appear?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 899.5,
        "end": 901.9
      },
      "pred_interval": {
        "start": 870.0,
        "end": 960.0
      },
      "iou": 0.026666666666666415,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.5,
        "end": 58.10000000000002,
        "average": 43.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.4948870539665222,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the overlay appears after the speaker's line) but omits the key numerical details (appearance/disappearance timestamps and exact timing of the utterance) required by the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that it's either you get an offer or a rejection, when does she say 'If you get an offer, congratulations to you'?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 917.6,
        "end": 919.6
      },
      "pred_interval": {
        "start": 930.0,
        "end": 990.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.399999999999977,
        "end": 70.39999999999998,
        "average": 41.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.417910447761194,
        "text_similarity": 0.5428594350814819,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'after' relation but gives a single timestamp of 930.0s, which contradicts the correct interval (starts at 917.6s and ends at 919.6s); the timing is substantially inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's invitation for help on social media, when do her social media handles appear on screen?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1020.0
      },
      "gt_interval": {
        "start": 983.0,
        "end": 987.0
      },
      "pred_interval": {
        "start": 960.0,
        "end": 1020.0
      },
      "iou": 0.06666666666666667,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.0,
        "end": 33.0,
        "average": 28.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.515078604221344,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the handles appear during the invitation but gives a single, incorrect timestamp (960.0s) that contradicts the ground-truth interval (983.0\u2013987.0s) and omits the correct onset/offset times; therefore it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the topic of '3 Things to know when INTERVIEWING', when does she start talking about people being chosen relative to other candidates?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 32.8,
        "end": 38.0
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.0670731707317074,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.599999999999998,
        "end": 3.0,
        "average": 15.299999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.8311057686805725,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' and the E2 span roughly overlaps the reference, but the anchor timing is significantly wrong (5.2s vs 20.0s) and E2 times are slightly off; these are substantial factual mismatches."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker talks about false positives and negatives in terms of accuracy, when does she explain that companies care about not hiring bad talents?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 103.0,
        "end": 106.0
      },
      "pred_interval": {
        "start": 37.5,
        "end": 70.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.5,
        "end": 35.599999999999994,
        "average": 50.55
      },
      "rationale_metrics": {
        "rouge_l": 0.45454545454545453,
        "text_similarity": 0.8180767297744751,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies both the anchor and target segments (wrong timestamps and content), only matching the relation 'after' coincidentally, so it fails to align with the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying what you should be doing after the interview, when does she mention that the video is about getting your dream job?",
      "video_id": "TLIoOLOjqwg",
      "video_number": "015",
      "segment": {
        "start": 870.0,
        "end": 1019.1500000000001
      },
      "gt_interval": {
        "start": 890.5,
        "end": 894.9
      },
      "pred_interval": {
        "start": 870.0,
        "end": 913.2
      },
      "iou": 0.10185185185185122,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.5,
        "end": 18.300000000000068,
        "average": 19.400000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315794,
        "text_similarity": 0.7150318622589111,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect \u2014 the event timestamps do not match the reference (E2 is placed much later), and it fails to capture that the target immediately follows the anchor; it also includes irrelevant visual details (hallucinations)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman mentions the STAR method, when does she explain what it stands for?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 159.92,
        "end": 164.1
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.01990476190476194,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.919999999999987,
        "end": 195.9,
        "average": 102.91
      },
      "rationale_metrics": {
        "rouge_l": 0.1923076923076923,
        "text_similarity": 0.6700683236122131,
        "llm_judge_score": 6,
        "llm_judge_justification": "The predicted answer correctly states the explanation occurs immediately after the mention, but it omits the precise start/end timestamps and duration given in the reference, so it's incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the man introduces the topic of what not to say, when does the woman advise against bad-mouthing former employers?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 185.4,
        "end": 191.0
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.02666666666666664,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.400000000000006,
        "end": 169.0,
        "average": 102.2
      },
      "rationale_metrics": {
        "rouge_l": 0.4067796610169491,
        "text_similarity": 0.5631341934204102,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly assigns the woman's advice to 174.5s (which is when the man introduces the topic) and omits the correct event interval 185.4\u2013191.0 and the quoted phrase 'Big red flag', so it fails to match the reference timing and key details."
      }
    },
    {
      "question_id": "003",
      "question": "After the man states that preparation ties back to building skills, when does he explain what deep research involves?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 247.12,
        "end": 252.48
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.025523809523809452,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.12,
        "end": 107.52000000000001,
        "average": 102.32000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.5335286855697632,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamp (208.0s) contradicts the reference, which locates the explanation at 247.120\u2013252.480s; the answer is therefore incorrect and omits the correct timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the man sips his coffee, when does he say 'it builds skills'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 342.0,
        "end": 343.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 366.0
      },
      "iou": 0.027777777777777776,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.0,
        "end": 23.0,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.32727272727272727,
        "text_similarity": 0.6120501756668091,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') and labels E2 as the utterance, but it mislocalizes both events (E1 time/content is wrong and E2 time and duration are incorrect), so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man finishes saying 'every single time', when does he start saying 'You show up differently'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 366.0
      },
      "gt_interval": {
        "start": 347.5,
        "end": 348.9
      },
      "pred_interval": {
        "start": 330.0,
        "end": 366.0
      },
      "iou": 0.03888888888888826,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.5,
        "end": 17.100000000000023,
        "average": 17.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.20338983050847456,
        "text_similarity": 0.6170534491539001,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the ground truth: event boundaries and timestamps are incorrect and events are misassigned (times 35.0s/330.0s vs correct 344.0\u2013348.9s), with only a vague 'after' relation loosely matching 'once_finished.'"
      }
    },
    {
      "question_id": "001",
      "question": "After the woman introduces the 'deep dive' into interview preparation, when does she mention covering 'surprising insights and steps'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 26.0,
        "end": 29.5
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.1174496644295302,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.8,
        "end": 5.5,
        "average": 13.15
      },
      "rationale_metrics": {
        "rouge_l": 0.15625,
        "text_similarity": 0.6582082509994507,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer only matches the temporal relation ('after') but the event timestamps and speaker/content identities are largely incorrect (E1/E2 times and speaker gender/content do not align with the reference), so it fails to capture the key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "During the man's explanation of how dressing formally applies to online interviews, when does he mention the concept of 'enclothed cognition'?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 77.0,
        "end": 80.0
      },
      "pred_interval": {
        "start": 108.4,
        "end": 158.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.400000000000006,
        "end": 78.4,
        "average": 54.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.1818181818181818,
        "text_similarity": 0.7000810503959656,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the mention of 'enclothed cognition' but gets both event timestamps wrong (much later than the reference) and misstates the relation ('after' vs. reference 'during'), so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the man finishes saying \"makes you stand out\", when does he say \"absolutely\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 335.4,
        "end": 336.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 365.6
      },
      "iou": 0.01685393258427029,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.399999999999977,
        "end": 29.600000000000023,
        "average": 17.5
      },
      "rationale_metrics": {
        "rouge_l": 0.18518518518518517,
        "text_similarity": 0.34790852665901184,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly indicates the word occurs shortly after the prior phrase (a brief pause) but omits the precise timing and the explicit 'once_finished'/immediately-next-utterance relation provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the man finishes sipping from his white cup, when does he say \"it builds skills\"?",
      "video_id": "btNqPT6-P0U",
      "video_number": "016",
      "segment": {
        "start": 330.0,
        "end": 365.616992
      },
      "gt_interval": {
        "start": 343.0,
        "end": 343.6
      },
      "pred_interval": {
        "start": 348.7,
        "end": 365.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.699999999999989,
        "end": 22.0,
        "average": 13.849999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.5063751935958862,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly indicates the utterance occurs after the sip, but it omits the provided timestamps and inaccurately characterizes the timing as \"immediately afterwards\" despite a ~2.1s gap, making it slightly imprecise."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions his parents advised him to always go on interviews, when does he explicitly state that interviews are practice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.064,
        "end": 43.554
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.864,
        "end": 6.954000000000001,
        "average": 20.409
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322032,
        "text_similarity": 0.6741511821746826,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted events do not match the correct timestamps or content (E1/E2 are incorrectly identified and describe different utterances); only the relation ('after') coincidentally matches, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising not to overstate qualifications, when does he tell viewers to know their worth for negotiation?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 105.944,
        "end": 117.861
      },
      "pred_interval": {
        "start": 74.8,
        "end": 118.4
      },
      "iou": 0.27332568807339447,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.144000000000005,
        "end": 0.5390000000000015,
        "average": 15.841500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2077922077922078,
        "text_similarity": 0.6325345039367676,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misidentifies E1 (wrong utterance and timestamp) and gives an inaccurate start time for E2 (111.8s vs 105.944s), though the end time and the notion of a subsequent relation are roughly similar; overall key factual elements and timestamps are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions strengths and weaknesses, when does he mention why you want this particular job?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 179.8,
        "end": 182.4
      },
      "pred_interval": {
        "start": 150.0,
        "end": 208.7
      },
      "iou": 0.044293015332197525,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.80000000000001,
        "end": 26.299999999999983,
        "average": 28.049999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.15686274509803924,
        "text_similarity": 0.062392719089984894,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly conveys the key relative relation that the 'why you want this job' segment occurs after the strengths/weaknesses segment, but it omits the explicit anchor/target labels and timestamp details provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's discussion about having coaches being important for self-improvement, when does he mention Roger Wakefield?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 215.9,
        "end": 218.0
      },
      "pred_interval": {
        "start": 210.4,
        "end": 239.9
      },
      "iou": 0.07118644067796591,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5,
        "end": 21.900000000000006,
        "average": 13.700000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.07547169811320756,
        "text_similarity": 0.07619251310825348,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that Roger Wakefield is mentioned in the discussion but provides no timestamps or the precise location specified in the reference (E1: 213.2\u2013232.0, E2: 215.9\u2013218.0) and is overly vague about timing, omitting key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes elaborating on researching a company and asking about their projects, when does he transition to discussing common questions about training and education?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.3,
        "end": 314.7
      },
      "pred_interval": {
        "start": 241.6,
        "end": 268.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.70000000000002,
        "end": 46.30000000000001,
        "average": 55.000000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.056338028169014086,
        "text_similarity": 0.10861551761627197,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states that the speaker transitions from asking about projects to questions on training/education, but it omits the precise timestamps and the explicit note that the target immediately follows the anchor, which are key factual elements in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what goals need to be achieved for more money, when does he say that the company will pay that money?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.5,
        "end": 340.9
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.006666666666666559,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.5,
        "end": 199.10000000000002,
        "average": 104.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3389830508474576,
        "text_similarity": 0.7053345441818237,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states that the payment is said to occur after E1 (the anchor), matching the reference's temporal relation; it omits the precise timestamps and the explicit E2 timing given in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions going for a union job, when does he describe understanding the process of safety and hazard assessments?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 381.5
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.03476190476190481,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.19999999999999,
        "end": 158.5,
        "average": 101.35
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.6465942859649658,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives completely incorrect timestamps and an unrelated utterance for E2, contradicting the ground-truth event boundaries; although it states 'after', the key factual elements (correct times and content) are wrong, amounting to a hallucination."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about expressing passion for the job, when does he advise being a student of construction?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 531.0,
        "end": 539.5
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 525.8,
        "end": 502.9,
        "average": 514.3499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.591478168964386,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('after'), it misidentifies and mistimes the target event (E2) and its content\u2014claiming a medical-student statement at ~35s instead of the speaker advising to be a student of construction around 21\u201329.5s\u2014so key factual elements are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks about the key responsibilities of a construction worker, when does he list these responsibilities?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 588.0,
        "end": 610.0
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 553.0,
        "end": 573.4,
        "average": 563.2
      },
      "rationale_metrics": {
        "rouge_l": 0.126984126984127,
        "text_similarity": 0.45618364214897156,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely incorrect: it cites the wrong events/content (mentions a medical student instead of the responsibilities question), wrong timestamps (35s vs correct 70\u2013100s), and an inaccurate temporal relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks about dealing with an unhappy supervisor, when does he advise owning up to your mistakes?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 705.0,
        "end": 711.0
      },
      "pred_interval": {
        "start": 36.6,
        "end": 48.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 668.4,
        "end": 663.0,
        "average": 665.7
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.5514302253723145,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both events and gives completely wrong timestamps (36.6\u201348.0s vs. correct 176.0\u2013201.5s) and even conflates E1 with the phrase 'own up to your mistakes'; although it labels the relationship 'after', the core events and timings are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining that the foreman will then explain to the general contractor, when does he start describing a similar dynamic between a journeyman and an apprentice?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 744.67,
        "end": 790.86
      },
      "pred_interval": {
        "start": 690.0,
        "end": 900.0
      },
      "iou": 0.2199523809523812,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.66999999999996,
        "end": 109.13999999999999,
        "average": 81.90499999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.5598814487457275,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction simply repeats the question and provides no timing information or relation; it omits all key factual details (start/end times and relation) present in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes reading the question about developing skill sets, when does he begin explaining his method of doing so?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 903.0
      },
      "pred_interval": {
        "start": 870.0,
        "end": 1080.0
      },
      "iou": 0.05238095238095238,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.0,
        "end": 177.0,
        "average": 99.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1509433962264151,
        "text_similarity": 0.19220787286758423,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly captures that the explanation begins immediately after the question (matching the 'once_finished' relation), but it omits the specific timestamps provided in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining due diligence regarding unsafe acts, when does he start discussing strengths and weaknesses for an interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 960.4,
        "end": 976.0
      },
      "pred_interval": {
        "start": 870.0,
        "end": 1080.0
      },
      "iou": 0.0742857142857144,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.39999999999998,
        "end": 104.0,
        "average": 97.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.19047619047619047,
        "text_similarity": 0.3782036602497101,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys the order\u2014that strengths/weaknesses discussion follows the due diligence explanation\u2014but it omits the precise timestamps and relation detail (once_finished) given in the correct answer, which are key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker highlights the 'Practice makes perfect' section, when does he advise being cool, collected, and confident for the upcoming interview?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1113.03,
        "end": 1118.08
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1260.0
      },
      "iou": 0.02404761904761883,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.02999999999997,
        "end": 141.92000000000007,
        "average": 102.47500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117646,
        "text_similarity": 0.5571707487106323,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the advice follows the 'Practice makes perfect' section, but the time window (1050\u20131260s) is overly broad and does not match the precise timestamps (1113.03\u20131118.08s) given in the correct answer, omitting the required exact timing and immediacy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker reads 'What is your main weakness?', when does he advise turning a weakness into a positive?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1213.307,
        "end": 1216.671
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1260.0
      },
      "iou": 0.016019047619047776,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 163.30700000000002,
        "end": 43.32899999999995,
        "average": 103.31799999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290325,
        "text_similarity": 0.6734403371810913,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the target happens after the question (matching the core relation), but the provided timestamp window (1050\u20131260s) does not match the precise times given in the reference and is therefore imprecise/partially incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'dress to impress', when does he read the sentence 'What you wear to an interview is very important'?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1258.4,
        "end": 1261.5
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1303.0
      },
      "iou": 0.04246575342465629,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.40000000000009,
        "end": 41.5,
        "average": 34.950000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.38202247191011235,
        "text_similarity": 0.5821276903152466,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies that the sentence occurs after 'dress to impress' and captures the sequence, but it omits the precise timestamps provided in the reference and adds an unsupported visual-cue detail not present in the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker reads 'dress professionally, not casually', when does he start reading the specific advice about what women should not wear?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1272.8,
        "end": 1277.3
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1303.0
      },
      "iou": 0.06164383561643835,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.799999999999955,
        "end": 25.700000000000045,
        "average": 34.25
      },
      "rationale_metrics": {
        "rouge_l": 0.34567901234567905,
        "text_similarity": 0.4699191451072693,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation ('after') between the two utterances, but it omits the precise timestamps given in the reference and introduces an unsupported visual cue (hand gesturing), which is a hallucinated detail."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes reading the dressing advice for women, when does he read the dressing advice for men?",
      "video_id": "JEBRZq1hbY0",
      "video_number": "017",
      "segment": {
        "start": 1230.0,
        "end": 1303.0
      },
      "gt_interval": {
        "start": 1277.7,
        "end": 1282.0
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1303.0
      },
      "iou": 0.05890410958904047,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.700000000000045,
        "end": 21.0,
        "average": 34.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.32941176470588235,
        "text_similarity": 0.4933308959007263,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the temporal order ('after') but omits the crucial detail that the men's advice begins immediately with precise timestamps and introduces an unsupported audio-cue claim (hallucination), so it is incomplete and partly inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker welcomes to the Job Search Preparation Resume Writing Workshop, when does she introduce herself as Cynthia White?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 9.61,
        "end": 15.95
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.2019108280254777,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.409999999999999,
        "end": 20.650000000000002,
        "average": 12.530000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818185,
        "text_similarity": 0.6083464622497559,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference by mis-timing the self-introduction (35\u201336.6s vs. 9.61\u201315.95s), mischaracterizing the welcome/intro events, and giving a different relation ('after' vs. 'once_finished'), so key facts are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that employers spend about 60 to 90 seconds reviewing a resume, when does she explain the purpose of a cover letter?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 93.56,
        "end": 100.47
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.56,
        "end": 63.87,
        "average": 61.215
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7026397585868835,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but misidentifies and mistimes the events (swapping the intro and the employer-review segment and giving incorrect timestamps), so the key factual elements do not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "During the initial 'You will learn' slide, when does the speaker mention how to format an electronic resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 170.0,
        "end": 172.9
      },
      "pred_interval": {
        "start": 150.0,
        "end": 208.7
      },
      "iou": 0.04940374787052822,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 35.79999999999998,
        "average": 27.89999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.321262925863266,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gives a vague relative ordering (between 'You will learn' and the resume design discussion) but omits the required timing details and does not explicitly state the event occurs within the 'You will learn' slide interval (170.0\u2013172.9s), so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying 'Design a resume that is simple and clean', when is the next checklist item, 'Limit the resume to two pages maximum', mentioned?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 233.0,
        "end": 235.8
      },
      "pred_interval": {
        "start": 209.3,
        "end": 264.9
      },
      "iou": 0.05035971223021606,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.69999999999999,
        "end": 29.099999999999966,
        "average": 26.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.25352112676056343,
        "text_similarity": 0.7231070399284363,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the 'Limit the resume to two pages maximum' item comes after 'Design a resume that is simple and clean', but it omits the key timing details (233.0\u2013235.8s) requested, so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing the tip 'Review the job announcement', when does she begin discussing 'Consider the employer's perspective'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 274.9,
        "end": 307.2
      },
      "pred_interval": {
        "start": 265.5,
        "end": 311.1
      },
      "iou": 0.7083333333333333,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 9.399999999999977,
        "end": 3.900000000000034,
        "average": 6.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7905608415603638,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly indicates the topic follows the prior one, but it omits the key factual details\u2014specifically the exact start time (274.9s), end time (307.2s), and the immediate/seamless transition\u2014so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that 'one size does not fit all', when does she finish explaining the need to edit a basic resume for each job?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.18,
        "end": 330.25
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.00033333333333330085,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.18000000000000682,
        "end": 209.75,
        "average": 104.965
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.49473196268081665,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the explanation follows the 'one size does not fit all' remark, but it omits the key timing details (E1 ends at 330.17s; E2 spans 330.18\u2013330.25) provided in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes describing the chronological resume, when does she start describing the skills-based resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.0,
        "end": 510.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.1761904761904762,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 143.0,
        "end": 30.0,
        "average": 86.5
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.6765755414962769,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that the skills-based resume is discussed after the chronological one but omits all required timing details (473.0s start, 510.0s end) and thus fails to answer the 'when' portion."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying that they need to be careful about concealing information, when does the 'Style: Combination Resume - Sample #1' title appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.5,
        "end": 519.3
      },
      "pred_interval": {
        "start": 510.0,
        "end": 580.0
      },
      "iou": 0.05428571428571364,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5,
        "end": 60.700000000000045,
        "average": 33.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.19354838709677416,
        "text_similarity": 0.3559154272079468,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction merely restates that the title appears after the speaker finishes but omits the crucial timestamps (514.3s, 515.5s, 519.3s) and the explicit 'once_finished' relation, so it is incomplete. "
      }
    },
    {
      "question_id": "002",
      "question": "After the 'Style: Combination Resume - Sample #2' title appears, when does the speaker start describing the benefits for job seekers like those changing industries or reentering the workforce?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 542.7,
        "end": 556.7
      },
      "pred_interval": {
        "start": 580.0,
        "end": 690.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.299999999999955,
        "end": 133.29999999999995,
        "average": 85.29999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.24250227212905884,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the description occurs after the title, but it omits the key timestamps (E1 at 539.8s; E2 from 542.7s\u2013556.7s) and the noted slight pause, so it is incomplete compared to the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes summarizing James' scenario, when does she state the recommended resume type for him?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 667.3,
        "end": 674.9
      },
      "pred_interval": {
        "start": 690.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.700000000000045,
        "end": 45.10000000000002,
        "average": 33.900000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.1923076923076923,
        "text_similarity": 0.5774930715560913,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the temporal relation that the recommendation follows the summary, but it omits the key factual details (the specific start/end timestamps and the explicit 'once_finished' relation/judging conversion) required by the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes discussing how to choose the appropriate resume style, when does she start talking about the actual resume and its contents?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 877.86,
        "end": 884.43
      },
      "pred_interval": {
        "start": 870.0,
        "end": 1080.0
      },
      "iou": 0.03128571428571398,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.860000000000014,
        "end": 195.57000000000005,
        "average": 101.71500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.12244897959183673,
        "text_similarity": 0.21097193658351898,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly conveys that she begins the next topic immediately after finishing the resume-style discussion, but it omits the key factual timestamps (start at 877.86s and end at 884.43s) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'name and contact information' as a basic category for a resume, when does she list the 'skills and accomplishments' category?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.09,
        "end": 922.44
      },
      "pred_interval": {
        "start": 870.0,
        "end": 1080.0
      },
      "iou": 0.0111904761904763,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.09000000000003,
        "end": 157.55999999999995,
        "average": 103.82499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.10344827586206896,
        "text_similarity": 0.0999140739440918,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer directly contradicts the correct answer by claiming no other categories were listed, while the reference states that 'skills and accomplishments' immediately follows 'name and contact information' (920.09\u2013922.44s)."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions that many employers are beginning to use emails for communication with job candidates, when does she advise opening a new email address?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1011.0,
        "end": 1024.0
      },
      "pred_interval": {
        "start": 870.0,
        "end": 1080.0
      },
      "iou": 0.06190476190476191,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 141.0,
        "end": 56.0,
        "average": 98.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1971830985915493,
        "text_similarity": 0.3966132402420044,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference by denying that she advises opening a new email address; it omits the key factual element that she explicitly recommends sometimes opening a new email address."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Skills & Accomplishments' section, when does she suggest using mynextmove.org?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1121.7,
        "end": 1126.15
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1160.0
      },
      "iou": 0.04045454545454587,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 71.70000000000005,
        "end": 33.84999999999991,
        "average": 52.77499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.21276595744680854,
        "text_similarity": 0.2801659107208252,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the suggestion to use mynextmove.org occurs after the speaker introduces the 'Skills & Accomplishments' section, matching the reference's key temporal relation; no factual contradictions or hallucinations."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes mentioning the onetonline.org website, when does the 'New Graduate' text appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1199.0,
        "end": 1199.5
      },
      "pred_interval": {
        "start": 1160.0,
        "end": 1370.0
      },
      "iou": 0.002380952380952381,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.0,
        "end": 170.5,
        "average": 104.75
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.692895770072937,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the relative order (the 'New Graduate' text appears after the speaker mentions onetonline.org) but omits the key factual timestamps (anchor at 1172.0s, target start at 1199.0s and fully displayed by 1199.5s), so it is incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "When is the next time a new category of individual appears on screen after the 'New Graduate' text is displayed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1202.0,
        "end": 1202.5
      },
      "pred_interval": {
        "start": 1370.0,
        "end": 1580.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 168.0,
        "end": 377.5,
        "average": 272.75
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.6445866823196411,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect and misaligned: it names a different category ('Recent Graduates' vs the correct 'Formerly Incarcerated') and gives completely different timestamps (1370\u20131580s vs ~1202s), so it hallucinates and contradicts the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about 'fragments in your work history', when does she introduce 'Summary Statements'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1278.3,
        "end": 1283.6
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1440.0
      },
      "iou": 0.02523809523809502,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.299999999999955,
        "end": 156.4000000000001,
        "average": 102.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.08888888888888889,
        "text_similarity": 0.3041536211967468,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the right event but gives a substantially incorrect timestamp (1380.0s vs the correct start at 1278.3s and finish at 1283.6s), contradicting the provided answer; therefore it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states the importance of including summary statements, when does she explain how they clearly communicate qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1341.0,
        "end": 1351.0
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1440.0
      },
      "iou": 0.047619047619047616,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 111.0,
        "end": 89.0,
        "average": 100.0
      },
      "rationale_metrics": {
        "rouge_l": 0.09523809523809525,
        "text_similarity": 0.44361913204193115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamp (1380.0s) does not match the correct interval where the explanation begins (1341.0\u20131351.0s) and thus fails to identify the actual start; it also omits the cited anchor/target timestamps and quoted content."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining how to list relevant skills using bullets, when does the 'Skills/Summary of Skills' section appear on screen?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1430.0,
        "end": 1431.0
      },
      "pred_interval": {
        "start": 1410.0,
        "end": 1620.0
      },
      "iou": 0.004761904761904762,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.0,
        "end": 189.0,
        "average": 104.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.6196298599243164,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the section appears after the explanation, but wrongly asserts it appears 'immediately' and omits the provided timestamps (starts at 1430.0s and fully visible by 1431.0s), so it lacks the key temporal detail and misrepresents timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'This is an accomplishment statement', when does the 'Example 4: Accomplishment Statement' text box appear?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1466.0,
        "end": 1466.5
      },
      "pred_interval": {
        "start": 1410.0,
        "end": 1620.0
      },
      "iou": 0.002380952380952381,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.0,
        "end": 153.5,
        "average": 104.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.5794110298156738,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is incorrect and contradictory: the correct answer specifies the box appears around 1466.0\u20131466.5s shortly after the speaker, whereas the prediction claims it appears at the end of the video, omitting the given timestamps."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing job duties and contributions, when does she start to explain how to list the most recent job first?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1599.24,
        "end": 1604.0
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1798.4
      },
      "iou": 0.0228406909788867,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.240000000000009,
        "end": 194.4000000000001,
        "average": 101.82000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.5266123414039612,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps significantly diverge from the ground truth: the finish time is ~8s off (1590.0s vs 1597.95s) and the predicted start time is wildly incorrect (~1798.4s vs 1599.24s), contradicting the relation that the explanation begins immediately after finishing."
      }
    },
    {
      "question_id": "002",
      "question": "After the yellow hexagonal graphics for 'Education/Training' fully appear on the screen, when does the speaker begin to list the types of earned qualifications?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1622.7,
        "end": 1628.27
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1800.0
      },
      "iou": 0.02652380952380922,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.700000000000045,
        "end": 171.73000000000002,
        "average": 102.21500000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.42124611139297485,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted times (1590.0s and 1800.0s) contradict the reference (1620.9s and 1622.7s) and thus fail to match the key factual timing information."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Body' section of the cover letter, when does she provide an example of an introduction?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1798.91,
        "end": 1805.84
      },
      "pred_interval": {
        "start": 1770.0,
        "end": 1980.0
      },
      "iou": 0.03299999999999922,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.910000000000082,
        "end": 174.16000000000008,
        "average": 101.53500000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.2641509433962264,
        "text_similarity": 0.5105504989624023,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly conveys that the example occurs after the 'Body' introduction, but it omits the required timing details (the specific timestamps and speech endpoints), making it incomplete for the asked 'when' question."
      }
    },
    {
      "question_id": "002",
      "question": "After the slide changes to display 'Sample Cover Letter', when does the speaker begin describing the included elements?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1893.78,
        "end": 1906.58
      },
      "pred_interval": {
        "start": 1770.0,
        "end": 1980.0
      },
      "iou": 0.06095238095238074,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.77999999999997,
        "end": 73.42000000000007,
        "average": 98.60000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.34615384615384615,
        "text_similarity": 0.6233811378479004,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only restates that the speaker begins describing the elements after the slide change, which matches the relation but omits the specific timings (start 1893.78s, end 1906.58s) required by the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes the cover letter tip about checking for errors, when does the slide transition to 'Electronic Resume'?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1944.0,
        "end": 1944.99
      },
      "pred_interval": {
        "start": 1770.0,
        "end": 1980.0
      },
      "iou": 0.004714285714285758,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 174.0,
        "end": 35.00999999999999,
        "average": 104.505
      },
      "rationale_metrics": {
        "rouge_l": 0.36,
        "text_similarity": 0.637092113494873,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the causal relation (the slide transitions once the speaker finishes) but omits the key factual timing details (start at 1944.0s and end at 1944.99s) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions applications and resumes being submitted online, when does she explain that an electronic resume will contain the same information as a standard resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1969.8,
        "end": 1974.8
      },
      "pred_interval": {
        "start": 5.2,
        "end": 10.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1964.6,
        "end": 1964.0,
        "average": 1964.3
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.5223597288131714,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the relation 'after' right but the anchor/target timestamps and described utterances are incorrect and do not match the ground-truth segments, so it fails to identify the correct events."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that electronic information needs to be in plain text, when does she mention that bolded or underlined text needs to be removed?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1980.1,
        "end": 1986.8
      },
      "pred_interval": {
        "start": 14.6,
        "end": 20.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1965.5,
        "end": 1966.6,
        "average": 1966.05
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.5993331670761108,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction vaguely matches that the removal is mentioned after the plain-text statement, but it misidentifies the anchor event, gives incorrect timestamps, and uses a non-specific 'after' relation instead of the immediate 'once_finished' relation\u2014major factual mismatches."
      }
    },
    {
      "question_id": "003",
      "question": "After the 'Electronic Resume Tips' slide appears, when does the speaker advise to limit each line to 65 characters?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2027.3,
        "end": 2029.4
      },
      "pred_interval": {
        "start": 22.6,
        "end": 25.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2004.7,
        "end": 2004.2,
        "average": 2004.45
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.7366737127304077,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the utterance and the 'after' relation, but the provided timestamps are substantially incorrect compared to the ground truth, making it factually wrong for the asked timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating their website address, when does she state her contact information is again right there?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2148.0,
        "end": 2152.0
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2161.0
      },
      "iou": 0.12903225806451613,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 9.0,
        "average": 13.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3492063492063492,
        "text_similarity": 0.7860469222068787,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the coarse relation ('after') but gives entirely incorrect timestamps (2161.0 vs 2148.0\u20132152.0), misses that the target occurs immediately after the anchor, and adds an unfounded visual cue, so it fails to align with key facts."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes thanking the viewer for taking a look at the workshop, when does the screen transition to the Extension logo?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2161.0
      },
      "gt_interval": {
        "start": 2160.1,
        "end": 2161.0
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2161.0
      },
      "iou": 0.029032258064519062,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.09999999999991,
        "end": 0.0,
        "average": 15.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317073,
        "text_similarity": 0.7654669880867004,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the segment end, but it misstates key timing details (E1 finish and E2 start), ties the transition to the wrong utterance, and adds an unsupported visual cue, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 'Britney' scenario, when does she explain that Britney should highlight skills from volunteering, sports, clubs, and other activities?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 729.63,
        "end": 736.05
      },
      "pred_interval": {
        "start": 690.0,
        "end": 723.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.629999999999995,
        "end": 12.549999999999955,
        "average": 26.089999999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.5396825396825397,
        "text_similarity": 0.568456768989563,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor time (690.0s) and that the explanation occurs after it, but incorrectly asserts it happens \"immediately after\" and omits the precise start/end timestamps (729.63s\u2013736.05s), so it is partially but not fully accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes Martha's current job as a teacher's assistant with three years of experience, when does the speaker state that Martha should use a skills-based resume?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 788.07,
        "end": 792.83
      },
      "pred_interval": {
        "start": 748.5,
        "end": 819.0
      },
      "iou": 0.06751773049645377,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.57000000000005,
        "end": 26.16999999999996,
        "average": 32.870000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.4210526315789474,
        "text_similarity": 0.6968731880187988,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly indicates the target occurs after the anchor but gives a wrong timestamp (748.5s vs the annotated 788.07\u2013792.83s), so it is factually inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning contacting them for interactive sessions on various topics, when does she start talking about the website?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2140.17,
        "end": 2150.24
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2160.8
      },
      "iou": 0.32694805194804055,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.170000000000073,
        "end": 10.5600000000004,
        "average": 10.365000000000236
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.6755868792533875,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it mislabels E1 timing (gives a start at 2130.0s instead of the correct finish at 2139.17s), places E2 at 2160.8s ('Thank you') rather than the website segment starting at 2140.17s and ending at 2150.24s, and therefore the relation ('after') does not match the correct 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states her name, Cynthia White, when does she thank the viewers for watching the workshop?",
      "video_id": "JQlpVKkKC_M",
      "video_number": "018",
      "segment": {
        "start": 2130.0,
        "end": 2160.75
      },
      "gt_interval": {
        "start": 2151.26,
        "end": 2155.3
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2160.8
      },
      "iou": 0.13116883116882921,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.26000000000022,
        "end": 5.5,
        "average": 13.38000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2142857142857143,
        "text_similarity": 0.6273075342178345,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the thanking phrase but has significant timestamp errors (anchor ~20s early, target ~9.5s late and zero duration) and uses a vague 'after' relation instead of the immediate 'once_finished', so it is only partially aligned."
      }
    },
    {
      "question_id": "001",
      "question": "After Michael Emery introduces himself as the Director of Human Resources at IOM, when does he explain that competency-based interviews are also referred to as behavioral or situational interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 17.414,
        "end": 23.021
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.17856687898089169,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.214000000000002,
        "end": 13.579,
        "average": 12.896500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.09876543209876544,
        "text_similarity": 0.5259994268417358,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly identifies the anchor start time but completely misidentifies and mis-timestamps the target event (wrong phrase and much later times); only the temporal relation 'after' matches, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'why do we use competency-based interviews?', when does he mention that a competency-based interview has a 0.51 predictor of success?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 91.2,
        "end": 97.969
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.2,
        "end": 61.36899999999999,
        "average": 58.784499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.11428571428571428,
        "text_similarity": 0.5742937326431274,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's anchor and target events (timestamps and contents) do not match the ground truth at all; only the vague temporal relation ('after') aligns, which is insufficient given the incorrect events and times."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes listing examples of what to prepare for, when does he start giving the second piece of advice about saving the vacancy notice?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.8,
        "end": 158.0
      },
      "pred_interval": {
        "start": 150.0,
        "end": 198.7
      },
      "iou": 0.10677618069815174,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8000000000000114,
        "end": 40.69999999999999,
        "average": 21.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.7025861740112305,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although the predicted answer matches the coarse relation ('after'), it is largely incorrect: the timestamps and event boundaries do not match the reference (predicted E2 at ~198.7s vs reference ~152.8\u2013158.0s) and it introduces a hallucinatory quote and wrong anchor timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker finishes explaining why it's important to save the vacancy notice by saying 'it's gone off the web', when does he start talking about the structure of an interview?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 207.8,
        "end": 209.8
      },
      "pred_interval": {
        "start": 204.3,
        "end": 229.9
      },
      "iou": 0.07812500000000001,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5,
        "end": 20.099999999999994,
        "average": 11.799999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7354863882064819,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer contradicts the reference on timestamps, quoted phrases, and relation (says 'when' and identical start times), introducing hallucinated details; it does not match the correct anchor/target timing or relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the fourth letter for the CAR principles, when does he explain what panels often ask about this fourth letter?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 330.34,
        "end": 330.41
      },
      "pred_interval": {
        "start": 330.0,
        "end": 360.0
      },
      "iou": 0.0023333333333350006,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.339999999999975,
        "end": 29.589999999999975,
        "average": 14.964999999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.25539374351501465,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly conveys the sequence (introduction then explanation) but omits the key factual details and precise timing in the reference (anchor at ~300.28\u2013300.36s and target at 330.34\u2013330.41s, immediately following the anchor), so it is too vague to match the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks how long a response should be, when does he warn that candidates can be marked down?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 419.29,
        "end": 427.37
      },
      "pred_interval": {
        "start": 450.0,
        "end": 470.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.70999999999998,
        "end": 42.629999999999995,
        "average": 36.66999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.46727967262268066,
        "llm_judge_score": 10,
        "llm_judge_justification": "The prediction correctly states that the warning comes after the question about response length and accurately captures the substance (be concise/not waffle), so it preserves the meaning of the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises having a pre-prepared statement for questions to the panel, when does he mention good 'bog standard questions'?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 494.0,
        "end": 501.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 530.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 29.0,
        "average": 22.5
      },
      "rationale_metrics": {
        "rouge_l": 0.21052631578947367,
        "text_similarity": 0.42314547300338745,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly states that the 'bog standard questions' are mentioned after the pre-prepared statement advice and preserves the meaning, but it omits the specific quoted example and timestamps provided in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the panel assessing your answers, when does he first equate this to diving in the Olympics?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.94,
        "end": 530.52
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 520.74,
        "end": 493.91999999999996,
        "average": 507.33
      },
      "rationale_metrics": {
        "rouge_l": 0.31168831168831174,
        "text_similarity": 0.7393852472305298,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer identifies completely different anchor and target segments with incorrect timestamps and utterances (speaker intro and 'I am a final year medical student') that do not match the ground-truth events about the panel assessing answers and 'diving in the Olympics'; only the relation 'after' coincides, so the prediction is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises choosing examples commensurate with the position level, when does he give the specific example of a candidate accused of cheating in high school?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 612.01,
        "end": 619.36
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 577.01,
        "end": 582.76,
        "average": 579.885
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.7776944637298584,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction completely misidentifies the anchor and target segments and their timings and content (different events), only coincidentally matching the 'after' relation; overall it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing the three different formats for interviews, when does the '3 types of interviews' graphic appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.1,
        "end": 710.8
      },
      "pred_interval": {
        "start": 690.0,
        "end": 894.5
      },
      "iou": 0.052322738386307736,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.100000000000023,
        "end": 183.70000000000005,
        "average": 96.90000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.5000000000000001,
        "text_similarity": 0.6587703824043274,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the graphic appears when the speaker finishes, but it omits the key factual details (the exact timestamps: appears at 700.1s and remains until 710.8s) required by the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'There's telephone interviews', when does the advice '1. Stand up' appear on screen for telephone interviews?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 717.2,
        "end": 807.3
      },
      "pred_interval": {
        "start": 804.0,
        "end": 894.5
      },
      "iou": 0.018612521150591966,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.79999999999995,
        "end": 87.20000000000005,
        "average": 87.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2807017543859649,
        "text_similarity": 0.6510167121887207,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the advice appears after the speaker's line, but it omits the crucial timing details (start at 717.2s, end at 807.3s) and the note that other content occurs in between, so it lacks the key factual elements of the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises on making sure the background is tidy for video interviews, when does the visual text '3. Appropriate clothing' appear?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 815.0
      },
      "pred_interval": {
        "start": 894.5,
        "end": 900.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 94.5,
        "end": 85.0,
        "average": 89.75
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.47145047783851624,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states the 'after' relationship (that the visual text appears after the background advice) but omits key factual details from the reference, namely the exact timestamps (800.0s\u2013815.0s) and the speaker's end time (798.7s)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the importance of eye contact in a face-to-face interview, when does he explain how to involve the entire panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.8,
        "end": 897.0
      },
      "pred_interval": {
        "start": 870.0,
        "end": 960.0
      },
      "iou": 0.13555555555555607,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.799999999999955,
        "end": 63.0,
        "average": 38.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3125,
        "text_similarity": 0.445607990026474,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states that the speaker explains how to involve the panel after mentioning eye contact, but it omits the key factual details (the specific timestamps and intervals) provided in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the young man's inappropriate attire during an interview, when does he advise to wear appropriate clothing?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 927.1,
        "end": 929.2
      },
      "pred_interval": {
        "start": 960.0,
        "end": 1080.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.89999999999998,
        "end": 150.79999999999995,
        "average": 91.84999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615383,
        "text_similarity": 0.38737669587135315,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that he advises wearing appropriate clothing but fails to provide the key temporal details from the reference (927.1\u2013929.2s and that it immediately follows the anecdote), so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says connecting with panel members on social media is a 'no-no', when does he say that sending a thank you is appropriate?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1087.0,
        "end": 1088.5
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1160.0
      },
      "iou": 0.013636363636363636,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 71.5,
        "average": 54.25
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.4129709005355835,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the sequence (the 'no-no' comment followed by when a thank-you is appropriate) but omits the required timestamps and adds an unsupported detail about a dysfunctional team example, so it is only partially aligned."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker gives the example question about a dysfunctional team, when does he ask about the three most important values?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1154.0,
        "end": 1158.0
      },
      "pred_interval": {
        "start": 1170.0,
        "end": 1380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 222.0,
        "average": 119.0
      },
      "rationale_metrics": {
        "rouge_l": 0.07547169811320756,
        "text_similarity": 0.38736671209335327,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the values question occurs after the dysfunctional team example) but omits the precise timestamps and timing details provided in the correct answer, so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes saying the phrase \"contrary evidence questions\", when does the specific text for \"5. Contrary-evidence questions\" appear on the blue panel?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1237.0,
        "end": 1257.7
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1302.0
      },
      "iou": 0.28750000000000064,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.0,
        "end": 44.299999999999955,
        "average": 25.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.41379310344827586,
        "text_similarity": 0.5980039834976196,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the order (the text appears after the phrase) but omits all specific timing details and duration provided in the correct answer, which are key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining contrary-evidence questions, when does the \"Closing words\" slide appear on screen?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1257.7,
        "end": 1259.0
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1302.0
      },
      "iou": 0.018055555555554922,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.700000000000045,
        "end": 43.0,
        "average": 35.35000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.14634146341463417,
        "text_similarity": 0.408736914396286,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the slide appears once the speaker finishes), but it omits the key factual details\u2014explicit timestamps and the slide's visibility window (1257.7s\u20131259.0s)."
      }
    },
    {
      "question_id": "003",
      "question": "After the man finishes stating that he hopes the tutorial was useful, when does he recommend visiting the IOM website for jobs?",
      "video_id": "ZgVeEGCJ9s4",
      "video_number": "019",
      "segment": {
        "start": 1230.0,
        "end": 1302.0
      },
      "gt_interval": {
        "start": 1275.9,
        "end": 1284.3
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1302.0
      },
      "iou": 0.11666666666666477,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.90000000000009,
        "end": 17.700000000000045,
        "average": 31.800000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.21818181818181817,
        "text_similarity": 0.5343115329742432,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') and the recommendation content, but it omits the key timing details (the specific timestamps and end time) provided in the correct answer, so it's incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker introduces the session as 'interviewing prep 101', when does he state that the session will build on other career presentations?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 27.466,
        "end": 37.226
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.28520577031162186,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.266000000000002,
        "end": 0.6259999999999977,
        "average": 11.446
      },
      "rationale_metrics": {
        "rouge_l": 0.2909090909090909,
        "text_similarity": 0.6927222013473511,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer contradicts the reference timing and boundaries (target should start ~27.466s and directly follow the anchor ending at 26.684s) and supplies incorrect target timestamps and a hallucinatory quote; only the vague 'after' relationship partially aligns."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker introduces himself as Daniel Moglen, when does he state where he works?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 66.316,
        "end": 68.83
      },
      "pred_interval": {
        "start": 37.4,
        "end": 58.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.916000000000004,
        "end": 10.030000000000001,
        "average": 19.473000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3272727272727273,
        "text_similarity": 0.7290521860122681,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's timestamps and event boundaries conflict completely with the reference (very different start/end times) and it gives the wrong relationship; it therefore fails to match the correct timing and sequencing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states that the workshops are just the beginning, when does he encourage everyone to stay in touch?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 169.9,
        "end": 175.8
      },
      "pred_interval": {
        "start": 153.9,
        "end": 204.6
      },
      "iou": 0.11637080867850112,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 28.799999999999983,
        "average": 22.39999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.25263157894736843,
        "text_similarity": 0.6636192202568054,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the encouragement to 'stay in touch' but gives incorrect timestamps and temporal relation (target much later and labeled 'after' vs. immediate 'once_finished'), and adds unsupported visual/audio details\u2014so it is largely temporally incorrect. "
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, cool', when does he welcome everyone?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 202.8,
        "end": 203.6
      },
      "pred_interval": {
        "start": 180.4,
        "end": 210.0
      },
      "iou": 0.027027027027026456,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.400000000000006,
        "end": 6.400000000000006,
        "average": 14.400000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.6613665819168091,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely disagrees with the ground truth: timings for both anchor and welcome are incorrect by many seconds, the welcome phrasing/timestamps don't match, the temporal relation ('after' vs once_finished) is wrong, and it adds unsupported visual/audio details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker shares his screen, when does he ask the audience to reflect on job interviews?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 298.6,
        "end": 303.3
      },
      "pred_interval": {
        "start": 180.4,
        "end": 210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 118.20000000000002,
        "end": 93.30000000000001,
        "average": 105.75000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2376237623762376,
        "text_similarity": 0.7766214609146118,
        "llm_judge_score": 1,
        "llm_judge_justification": "Although the predicted answer matches the 'after' relation, it is factually incorrect about both anchor and target timestamps (off by ~110s) and misses the correct screen-share/'Warm up' cue, while adding unsupported visual/audio details\u2014major factual errors. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks which interview questions the audience is most concerned about, when does he instruct them to type those questions in the chatbox?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.283,
        "end": 336.694
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.011480952380952387,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.2830000000000155,
        "end": 203.30599999999998,
        "average": 103.7945
      },
      "rationale_metrics": {
        "rouge_l": 0.11538461538461538,
        "text_similarity": 0.1542988121509552,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly restates that the speaker asks the audience to type questions in the chatbox but omits the key timing details (the E1/E2 timestamps and the relative timing), so it fails to answer when the instruction occurs."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker encourages the audience to keep all questions in mind, when does he start describing how he measures a successful interview?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 510.0,
        "end": 564.0
      },
      "pred_interval": {
        "start": 330.0,
        "end": 540.0
      },
      "iou": 0.1282051282051282,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 180.0,
        "end": 24.0,
        "average": 102.0
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.4515250325202942,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction merely restates the question and provides no timing or content details; it fails to specify the start time (around 510.0s) or the quoted phrasing from the correct answer, so it is largely incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if there's anything he would have wanted to do differently in an interview, when does he ask if he said everything he wanted to say?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 520.97,
        "end": 525.55
      },
      "pred_interval": {
        "start": 510.0,
        "end": 720.0
      },
      "iou": 0.021809523809523463,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.970000000000027,
        "end": 194.45000000000005,
        "average": 102.71000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.20843949913978577,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the order (the question follows the prior one) but omits the key temporal details and timestamps and does not indicate that the target event immediately follows the anchor, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'All right, so let's dive in here', when does the text 'DO NOT JUDGE YOUR PERFORMANCE - OR SELF WORTH - ON A JOB OFFER' appear on screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 570.09,
        "end": 574.39
      },
      "pred_interval": {
        "start": 510.0,
        "end": 720.0
      },
      "iou": 0.02047619047619026,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.09000000000003,
        "end": 145.61,
        "average": 102.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20588235294117646,
        "text_similarity": 0.40866947174072266,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the text appears after the speaker's line but omits all key timing details (start and fully displayed timestamps) and completeness provided in the correct answer, so it is overly vague."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker discusses applying for jobs that ask for too much experience, when does he state that getting interviews indicates a good resume and cover letter?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 606.28,
        "end": 616.41
      },
      "pred_interval": {
        "start": 510.0,
        "end": 720.0
      },
      "iou": 0.04823809523809522,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.27999999999997,
        "end": 103.59000000000003,
        "average": 99.935
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.43670445680618286,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly locates the statement as occurring after a transition to strategies, but it fails to provide the requested timing and omits the precise timestamps (606.28\u2013616.41s) given in the correct answer, making it incomplete for the 'when' question."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes describing his initial application for a graduate writing specialist position at UCSB, when does he mention being a finalist but not getting the job?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 714.0,
        "end": 718.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 738.4
      },
      "iou": 0.09297520661157029,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 19.899999999999977,
        "average": 21.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.15384615384615385,
        "text_similarity": 0.5024417638778687,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content (mentioning being a finalist) but gives incorrect timestamps and relation: it misplaces E2 (738.4s vs correct 714.0\u2013718.5s), lists a different anchor timing, and uses 'after' instead of the correct immediate 'once_finished' relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining that he and his family were not ready to move to apply for the second position, when does he reiterate that not getting a job interview is not necessarily unsuccessful?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.248,
        "end": 773.62
      },
      "pred_interval": {
        "start": 738.4,
        "end": 808.0
      },
      "iou": 0.07718390804597637,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.84800000000007,
        "end": 34.379999999999995,
        "average": 32.11400000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.15841584158415842,
        "text_similarity": 0.45312952995300293,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only loosely captures the temporal ordering ('after') but gives incorrect and inconsistent timestamps (wrong E1/E2 times, omits E1 finish and E2 finish) and thus fails to match the precise anchor/target timing and relation in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks what the main thing hiring managers are looking for when interviewing people, when does he state their response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 874.4,
        "end": 883.6
      },
      "pred_interval": {
        "start": 808.0,
        "end": 847.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.39999999999998,
        "end": 36.0,
        "average": 51.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.21782178217821782,
        "text_similarity": 0.5696825385093689,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') but misreports event timings (wrong start/finish times), omits the noted short pause and finish times, and its timestamps are inconsistent with the stated relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks for thoughts on 'likability' and mentions its subjective nature, when does he read the comment 'Doesn't sound fair' from the chat box?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 897.4,
        "end": 898.3
      },
      "pred_interval": {
        "start": 870.0,
        "end": 906.5
      },
      "iou": 0.02465753424657472,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.399999999999977,
        "end": 8.200000000000045,
        "average": 17.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2950819672131148,
        "text_similarity": 0.4731416702270508,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly conveys the sequence (the speaker asks about likability and then reads 'Doesn't sound fair'), but it omits the key factual details\u2014the exact timestamps and event labels\u2014provided in the reference answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes reading the chat comment 'I'm disheartened by that', when does he describe his own reaction to the concept of 'likability'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 937.485,
        "end": 939.668
      },
      "pred_interval": {
        "start": 906.5,
        "end": 943.0
      },
      "iou": 0.05980821917808199,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.985000000000014,
        "end": 3.3319999999999936,
        "average": 17.158500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.3739544153213501,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction merely restates that he describes his reaction and gives no timing or relative relation information; it omits the key timestamps and the fact that the reaction (jaw agape) occurs immediately after the chat comment. "
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating that 'Interviewing is an inexact science', when does he elaborate on this by posing a rhetorical question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 974.6,
        "end": 985.7
      },
      "pred_interval": {
        "start": 943.0,
        "end": 969.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.600000000000023,
        "end": 16.200000000000045,
        "average": 23.900000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.24137931034482757,
        "text_similarity": 0.4114972651004791,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the speaker elaborates via a rhetorical question, but it omits all required timing details (the specific start/end times and that E2 follows immediately after E1), so it is incomplete relative to the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks if something makes sense, when does he start talking about thinking about your audience?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1085.885,
        "end": 1093.694
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1180.0
      },
      "iou": 0.060069230769230535,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.88499999999999,
        "end": 86.30600000000004,
        "average": 61.095500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.0909090909090909,
        "text_similarity": 0.14999321103096008,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly conveys that he begins talking about the audience immediately after asking if it makes sense; it omits the precise timestamps and the note about a brief pause between the anchor and target."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces HR interview/phone screens, when does he refer to it as a 'gatekeeper'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1125.176,
        "end": 1128.0
      },
      "pred_interval": {
        "start": 1180.0,
        "end": 1360.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.82400000000007,
        "end": 232.0,
        "average": 143.41200000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.1904761904761905,
        "text_similarity": 0.41667965054512024,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (1205.0s) is substantially different from the correct target span (1125.176\u20131128.0s) and does not match the provided anchor/target timing, so it is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's discussion of site visits, when does he describe the current form of these visits?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1176.087,
        "end": 1183.755
      },
      "pred_interval": {
        "start": 1360.0,
        "end": 1560.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 183.913,
        "end": 376.2449999999999,
        "average": 280.07899999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.15999999999999998,
        "text_similarity": 0.31177783012390137,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives a single timestamp (1400.0s) that contradicts the correct intervals (~1173\u20131183s) and omits the anchor/target distinction and elaboration; it is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that panel interviews maintain fairness, when does he mention there's no feedback or response?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1247.396,
        "end": 1252.49
      },
      "pred_interval": {
        "start": 1230.0,
        "end": 1380.0
      },
      "iou": 0.03396000000000034,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.395999999999958,
        "end": 127.50999999999999,
        "average": 72.45299999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.6688907146453857,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the quoted content and that it occurs 'after', but the timestamps are substantially incorrect (off by ~100s) and it adds unsupported visual details, so it fails on factual timing and includes hallucinations."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker recommends being on a hiring committee, when does he share his personal experience as a grad student?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1287.554,
        "end": 1295.994
      },
      "pred_interval": {
        "start": 1380.0,
        "end": 1440.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 92.44599999999991,
        "end": 144.00600000000009,
        "average": 118.226
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.6623105406761169,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps and quoted utterance do not match the reference (different start/end times and speaker wording), and it adds extraneous visual details; only the vague 'after' relation is consistent. This contains hallucinated/incorrect facts, so it fails to align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions that the community is invited to sit on interviews for higher positions, when does he advise attending them to learn?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.58,
        "end": 1299.06
      },
      "pred_interval": {
        "start": 1440.0,
        "end": 1470.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 148.42000000000007,
        "end": 170.94000000000005,
        "average": 159.68000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.22916666666666666,
        "text_similarity": 0.6567668914794922,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misaligns the anchor/target timecodes and segments and omits the key advice content (attending to learn the types of questions), while adding unsupported visual details; only the vague 'after' relation matches the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising to always save the job description as a PDF, when does he explain that the job posting is often removed after expiry?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1452.209,
        "end": 1457.575
      },
      "pred_interval": {
        "start": 1410.0,
        "end": 1619.5
      },
      "iou": 0.025613365155131195,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.20900000000006,
        "end": 161.92499999999995,
        "average": 102.06700000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.34375,
        "text_similarity": 0.6609045267105103,
        "llm_judge_score": 7,
        "llm_judge_justification": "The predicted answer correctly conveys that the explanation occurs immediately after the advice (matching the reference's relative timing), but it omits the specific timestamps and event boundary details given in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes explaining to turn qualifications into questions, when does he give the example 'must be familiar with discourse analysis'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1492.512,
        "end": 1496.48
      },
      "pred_interval": {
        "start": 1410.0,
        "end": 1619.5
      },
      "iou": 0.018940334128878637,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.51199999999994,
        "end": 123.01999999999998,
        "average": 102.76599999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.46875,
        "text_similarity": 0.590952455997467,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the example is given immediately after the explanation, but it omits the required precise timing (the timestamps 1492.512\u20131496.480 and the prior segment 1492.13\u20131492.22), so it is incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the idea of a 'bad response', when does he describe the specific bad response example?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1804.78,
        "end": 1808.35
      },
      "pred_interval": {
        "start": 5.2,
        "end": 35.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1799.58,
        "end": 1773.35,
        "average": 1786.465
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.5556668639183044,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after'), but the timestamps are completely different from the reference and the predicted target content is incorrect/hallucinated, so it fails to match the key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces his 'go-to response', when does he state that his weakness is being conflict-avoidant?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1887.2,
        "end": 1890.9
      },
      "pred_interval": {
        "start": 1770.0,
        "end": 1980.0
      },
      "iou": 0.017619047619047836,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 117.20000000000005,
        "end": 89.09999999999991,
        "average": 103.14999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.6651145815849304,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and content do not match the ground truth: the predicted 'target' is about being a final-year student, not stating a conflict-avoidant weakness, and the anchor/target times are completely different, so the relationship is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says, \"So I just dug in. I said,\" when does he start listing possible uses for a brick?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2144.2,
        "end": 2157.5
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2340.0
      },
      "iou": 0.0633333333333342,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.199999999999818,
        "end": 182.5,
        "average": 98.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.49122807017543857,
        "text_similarity": 0.403681218624115,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates that the listing begins immediately after the quoted phrase but fails to provide the required timing details (start time 2144.2s and end time 2157.5s), thus omitting key factual elements."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks, \"Is everyone still with me? Are we good?\" when does the slide transition to show the 'S(T)AR' method?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2189.8,
        "end": 2191.0
      },
      "pred_interval": {
        "start": 2130.0,
        "end": 2340.0
      },
      "iou": 0.005714285714284848,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.80000000000018,
        "end": 149.0,
        "average": 104.40000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.6349206349206349,
        "text_similarity": 0.7372781038284302,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (the slide transitions after the speaker's question) but omits the key timing details (the specific timestamps and duration) provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the action taken, when does he finish describing the result?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2376.449,
        "end": 2382.556
      },
      "pred_interval": {
        "start": 2310.0,
        "end": 2520.0
      },
      "iou": 0.029080952380952242,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.44900000000007,
        "end": 137.44399999999996,
        "average": 101.94650000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2033898305084746,
        "text_similarity": 0.4677357077598572,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the action is followed by a result description (order), but it omits all precise timing details and incorrectly claims the result continues until the end of the video, contradicting the provided timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes talking about the program becoming institutionalized, when does he mention the 'tags' at the bottom of the slide?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2407.152,
        "end": 2412.282
      },
      "pred_interval": {
        "start": 2310.0,
        "end": 2520.0
      },
      "iou": 0.02442857142857195,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 97.15200000000004,
        "end": 107.71799999999985,
        "average": 102.43499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.5419111847877502,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the sequence (that he mentions the tags after finishing the institutionalization discussion) but omits the precise timestamps and the explicit relation/interval details provided in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions moving into the mock interview, when does he start explaining what seminal experiences are?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2572.085,
        "end": 2581.418
      },
      "pred_interval": {
        "start": 2490.0,
        "end": 2690.0
      },
      "iou": 0.04666500000000042,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.08500000000004,
        "end": 108.58199999999988,
        "average": 95.33349999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.6860790252685547,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but largely misstates and swaps the event timings and labels (E1/E2), with timestamps significantly off and an incorrect/overextended end time, so it fails to match the reference details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the bullet point 'Create your talking points -> 3-4 SAR examples', when does he introduce the next bullet point about 'tagging' SAR examples?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2602.202,
        "end": 2611.574
      },
      "pred_interval": {
        "start": 2500.0,
        "end": 2700.0
      },
      "iou": 0.046859999999999215,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 102.20200000000023,
        "end": 88.42599999999993,
        "average": 95.31400000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.16901408450704225,
        "text_similarity": 0.6137643456459045,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer has largely incorrect timestamps (2500/2510/2700 vs. 2568/2580/2602/2611) and an incorrect relationship labeling; it does mention ordering but misplaces key temporal details, so it is almost entirely mismatched."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying \"So five minutes on the clock\", when does he tell the audience to look at the Muse article?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2689.809,
        "end": 2694.275
      },
      "pred_interval": {
        "start": 2670.0,
        "end": 2735.0
      },
      "iou": 0.06870769230769068,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.809000000000196,
        "end": 40.72499999999991,
        "average": 30.267000000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.25974025974025977,
        "text_similarity": 0.7152923941612244,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but mislabels the events and provides significantly incorrect timestamps (and an extra visual cue), contradicting the reference's event boundaries and order."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states his two criteria for experience, when does he advise on what career stage experiences to focus on?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2808.94,
        "end": 2831.958
      },
      "pred_interval": {
        "start": 2735.0,
        "end": 2814.6
      },
      "iou": 0.05837579157985777,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.94000000000005,
        "end": 17.358000000000175,
        "average": 45.649000000000115
      },
      "rationale_metrics": {
        "rouge_l": 0.1395348837209302,
        "text_similarity": 0.6305586099624634,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the high-level relationship ('after') right but the key temporal annotations are largely incorrect (E1 and E2 start/end times do not match the reference, with only a small overlap), and it adds an unsupported visual cue; thus it fails to accurately identify the correct segments."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying \"I'm gonna speak the question for folks who are watching the recording here, so 'cause I think it's a really good question\", when does he read out the question from the chat?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2867.48,
        "end": 2878.688
      },
      "pred_interval": {
        "start": 2814.6,
        "end": 2849.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.88000000000011,
        "end": 29.088000000000193,
        "average": 40.98400000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.13636363636363638,
        "text_similarity": 0.5701184272766113,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the speaker says the quoted phrase but the timestamps are substantially earlier and the reported span/duration contradict the ground truth; it also adds an unverified visual cue, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if it's useful to give a STAR story from a non-work context, when does he start discussing family examples as potentially too personal?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2888.2,
        "end": 2891.7
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.016666666666666666,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.19999999999982,
        "end": 168.30000000000018,
        "average": 103.25
      },
      "rationale_metrics": {
        "rouge_l": 0.060606060606060615,
        "text_similarity": -0.07643137872219086,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer restates that he discusses family examples after the question but omits the key timing information (exact timestamps and relative timing) provided in the ground truth, making it incomplete for the asked 'when'."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will outline what they are going to do first, when does the screen transition to show the next set of instructions for the mock interview?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.0,
        "end": 2920.0
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 3060.0
      },
      "iou": 0.01904761904761905,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.0,
        "end": 140.0,
        "average": 103.0
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.3220785856246948,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the basic sequence (screen transitions after the speaker's remark) but omits all precise timing details and transition timing provided in the correct answer, so it is incomplete. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking 'Why are you interested in this position?', when does he mention 'Tell me about yourself' as an alternative?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3061.797,
        "end": 3062.728
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3240.0
      },
      "iou": 0.004433333333333524,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.797000000000025,
        "end": 177.27199999999993,
        "average": 104.53449999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.07272727272727271,
        "text_similarity": 0.06628861278295517,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly states that the speaker offers 'Tell me about yourself' as an alternative, but it omits the precise timestamps and the detail that the target immediately follows the anchor, so it is incomplete relative to the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says he will put the interview schedule into the chat box, when does he display the Muse article on behavioral interview questions?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.0,
        "end": 3125.6
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3240.0
      },
      "iou": 0.026666666666666235,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.0,
        "end": 114.40000000000009,
        "average": 102.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.1764705882352941,
        "text_similarity": 0.23362010717391968,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly states that the article is displayed after the speaker's remark, matching the core relation, but it omits all specific timing details (the anchor/target start and end timestamps and the absolute\u2192relative judge information) present in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if there are any questions before opening breakout rooms, when does he describe the group sizes?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3206.19,
        "end": 3214.181
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3240.0
      },
      "iou": 0.038052380952380886,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 176.19000000000005,
        "end": 25.81899999999996,
        "average": 101.00450000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.11320754716981132,
        "text_similarity": 0.41017216444015503,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that he describes group sizes after asking for questions, but it omits the key factual details in the reference\u2014specifically the exact timestamps and the note that clarification occurs immediately after asking\u2014so it is overly vague."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says 'So, just plan your time accordingly', when does he say 'And you'll have about 25 minutes and we'll call you back at the end'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3215.09,
        "end": 3217.67
      },
      "pred_interval": {
        "start": 3210.0,
        "end": 3258.0
      },
      "iou": 0.053749999999998486,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0900000000001455,
        "end": 40.32999999999993,
        "average": 22.710000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.6474807858467102,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction only matches the coarse 'after' relation but is otherwise incorrect: segment times and boundaries are substantially wrong, the anchor/target are misassigned, and it adds unsupported visual/audio details (hallucinations)."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says 'If we're ready to go, we'll stop the recording and we'll send you off', when does the black screen with text 'For the remainder of the session...' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3258.0
      },
      "gt_interval": {
        "start": 3231.62,
        "end": 3239.85
      },
      "pred_interval": {
        "start": 3258.0,
        "end": 3258.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.38000000000011,
        "end": 18.15000000000009,
        "average": 22.2650000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24175824175824176,
        "text_similarity": 0.6180156469345093,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction largely misstates the timestamps and durations (e.g., anchor given at 3258.0s vs correct 3221.2\u20133224.79s; target listed as instantaneous at 3258.0s instead of 3231.62\u20133239.85s) and incorrectly ties the visual target to the spoken phrase and audio cues; it only correctly identifies that the target occurs after the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they will be doing a mock interview today, when does he start explaining what a mock interview is?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1623.186,
        "end": 1643.288
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1700.0
      },
      "iou": 0.18274545454545535,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.18599999999992,
        "end": 56.71199999999999,
        "average": 44.948999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.6328620910644531,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but misplaces the anchor and target times (E1 at 1590 vs ~1617, E2 start 1630 vs 1623.186) and adds an incorrect end time and phrasing assignment, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker introduces the 'Tell me about yourself (TMAY)' question, when does he introduce 'Behavioral Questions'?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1740.196,
        "end": 1747.784
      },
      "pred_interval": {
        "start": 1630.0,
        "end": 1800.0
      },
      "iou": 0.044635294117648196,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 110.19599999999991,
        "end": 52.215999999999894,
        "average": 81.2059999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1016949152542373,
        "text_similarity": 0.5911509990692139,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted time spans and event boundaries are largely incorrect and inconsistent with the reference (E1/E2 times do not match and E2 is wildly overbroad/hallucinated); the relation 'after' is a weak mismatch to the specific 'next' relation and overall the prediction fails to preserve key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains that \"put you on the spot questions\" are by definition questions you can't prepare for, when does he explicitly say he has an example of such a question?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2004.224,
        "end": 2006.086
      },
      "pred_interval": {
        "start": 5.2,
        "end": 10.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1999.024,
        "end": 1995.286,
        "average": 1997.155
      },
      "rationale_metrics": {
        "rouge_l": 0.1951219512195122,
        "text_similarity": 0.4533140957355499,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets only the temporal relation ('after') right but misidentifies both event segments and their timings/content (E1 and E2 do not match the referenced utterances) and adds an unverified visual cue, so it largely fails to align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker says \"And here's the question\" to introduce the example, when does the slide titled \"THE BRICK QUESTION\" appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2048.659,
        "end": 2048.899
      },
      "pred_interval": {
        "start": 35.0,
        "end": 40.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2013.659,
        "end": 2008.299,
        "average": 2010.979
      },
      "rationale_metrics": {
        "rouge_l": 0.368421052631579,
        "text_similarity": 0.705021858215332,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates the timestamps and temporal relation: it claims the slide is simultaneous with the utterance ('equals') while the ground truth shows the slide appears much later ('after'). It also adds an unsupported visual cue and omits the correct absolute timing, so it largely contradicts the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks the audience \"What is the point of this question?\", when does he state that the entire purpose of the question is that you can't prepare for it?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2115.539,
        "end": 2118.102
      },
      "pred_interval": {
        "start": 41.2,
        "end": 48.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2074.3390000000004,
        "end": 2069.3019999999997,
        "average": 2071.8205
      },
      "rationale_metrics": {
        "rouge_l": 0.3614457831325302,
        "text_similarity": 0.6360360383987427,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: both event timestamps and the temporal relation do not match the ground truth (events occur at ~134s and ~2116s and relation is 'after'), and it adds an irrelevant visual cue\u2014thus it fails to capture the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying they will stop the recording and send the attendees off, when does the black screen with information about practicing interviews appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3225.795,
        "end": 3228.795
      },
      "pred_interval": {
        "start": 3210.0,
        "end": 3257.1
      },
      "iou": 0.06369426751592369,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.795000000000073,
        "end": 28.304999999999836,
        "average": 22.049999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.4031660556793213,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly states that the black screen follows the speaker, but it omits the precise E1/E2 timestamps and end times and does not explicitly convey the immediate timing noted in the reference, so it is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "After the screen displays text encouraging attendees to practice interviews, when is the next text displayed on the screen?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3236.0,
        "end": 3240.0
      },
      "pred_interval": {
        "start": 3257.1,
        "end": 3294.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.09999999999991,
        "end": 54.19999999999982,
        "average": 37.649999999999864
      },
      "rationale_metrics": {
        "rouge_l": 0.24561403508771928,
        "text_similarity": 0.6538951396942139,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamp (3257.1s) contradicts the reference, which indicates the next distinct text appears at 3236s (ending 3240s); the prediction is incorrect and omits the correct timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the screen finishes displaying information about other LCL videos, when do the credits for 'Linguistics Career Launch 2021' appear?",
      "video_id": "k7uk0udlw2U",
      "video_number": "020",
      "segment": {
        "start": 3210.0,
        "end": 3257.0699999999997
      },
      "gt_interval": {
        "start": 3241.0,
        "end": 3243.0
      },
      "pred_interval": {
        "start": 3294.2,
        "end": 3331.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.19999999999982,
        "end": 88.30000000000018,
        "average": 70.75
      },
      "rationale_metrics": {
        "rouge_l": 0.16393442622950818,
        "text_similarity": 0.5780006647109985,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives a timestamp (3294.2s) that contradicts the correct start time for the credits (3241s) and is far off from the referenced interval; it therefore fails to match the factual timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the first woman finishes describing the traits she looks for in people to hire, when does Bartolo introduce himself?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 7.812,
        "end": 9.402
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0506369426751592,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.612,
        "end": 27.198,
        "average": 14.905000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.36065573770491804,
        "text_similarity": 0.8047608137130737,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction contradicts the reference on all key points: anchor/target timestamps and content differ substantially, and the temporal relation ('after') does not match the correct 'once_finished' immediate succession; it includes incorrect/hallucinated timings and statements."
      }
    },
    {
      "question_id": "002",
      "question": "During the display of the 'English with Gallery Teachers' title card, when does the background music play?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 21.0,
        "end": 25.6
      },
      "pred_interval": {
        "start": 108.4,
        "end": 117.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.4,
        "end": 92.19999999999999,
        "average": 89.8
      },
      "rationale_metrics": {
        "rouge_l": 0.3571428571428571,
        "text_similarity": 0.9122681021690369,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although both answers state the same 'during' relation, the predicted timestamps are drastically incorrect and introduce unrelated details (speaker introduction and much later times), so it fails on key factual elements."
      }
    },
    {
      "question_id": "003",
      "question": "After Rita mentions her company is not a pyramid system, when does she clarify she never uses the word 'employees'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 114.588,
        "end": 116.843
      },
      "pred_interval": {
        "start": 138.4,
        "end": 153.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.812000000000012,
        "end": 36.157,
        "average": 29.984500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.7442491054534912,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer's timestamps and event boundaries are incorrect (they do not match the provided anchor/end times), and it mislabels the relationship; overall it contradicts the correct timing information."
      }
    },
    {
      "question_id": "002",
      "question": "After the female speaker states she never reads CVs, when does she mention that the HR director reads them?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 190.4,
        "end": 199.43
      },
      "pred_interval": {
        "start": 150.0,
        "end": 360.0
      },
      "iou": 0.043000000000000003,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.400000000000006,
        "end": 160.57,
        "average": 100.485
      },
      "rationale_metrics": {
        "rouge_l": 0.18750000000000003,
        "text_similarity": 0.6627686023712158,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies and mistimes both events (wrong timestamps and swapped/incorrect event descriptions), and adds irrelevant cues; only the relation 'after' coincides with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the woman explains that certifications like CELTA or TEFL are not the sole hiring criteria, when does she state what essential qualities she looks for?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 339.0,
        "end": 343.3
      },
      "pred_interval": {
        "start": 330.0,
        "end": 495.0
      },
      "iou": 0.026060606060606128,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.0,
        "end": 151.7,
        "average": 80.35
      },
      "rationale_metrics": {
        "rouge_l": 0.12307692307692307,
        "text_similarity": 0.1428762972354889,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the content (she discusses experience and passion) but fails to answer the asked 'when' by omitting the specified timestamps (E1: 330.0\u2013339.0s; E2: 339.0\u2013343.3s), so it misses a key factual element."
      }
    },
    {
      "question_id": "002",
      "question": "Once the man mentions that he considers people with scout experience because it indicates passion and ability to manage people, when does the woman add that it shows likability?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 369.0,
        "end": 372.5
      },
      "pred_interval": {
        "start": 495.0,
        "end": 600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 126.0,
        "end": 227.5,
        "average": 176.75
      },
      "rationale_metrics": {
        "rouge_l": 0.11764705882352941,
        "text_similarity": 0.18344783782958984,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the woman's response content but gives a wrong timestamp (around 500s vs the correct ~369\u2013372.5s) and omits the immediate follow-up timing, so it is factually inaccurate on the key temporal detail."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman mentions having 'trick questions' for interviews, such as 'what makes you happiest in teaching?', when does she describe what an ideal answer to that question sounds like?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 530.0,
        "end": 535.0
      },
      "pred_interval": {
        "start": 600.0,
        "end": 710.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 70.0,
        "end": 175.0,
        "average": 122.5
      },
      "rationale_metrics": {
        "rouge_l": 0.11594202898550723,
        "text_similarity": 0.16158264875411987,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: the correct times are ~484.5\u2013489.0 and ~530.0\u2013535.0 (with a gap), whereas the prediction mislocates the event at ~600s and omits the two-event/gap structure."
      }
    },
    {
      "question_id": "001",
      "question": "After the man asks if they've not hired someone for crossing arms, when does the woman respond about not hiring people who didn't smile?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 515.6,
        "end": 521.2
      },
      "pred_interval": {
        "start": 5.2,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 510.40000000000003,
        "end": 484.6,
        "average": 497.5
      },
      "rationale_metrics": {
        "rouge_l": 0.1518987341772152,
        "text_similarity": 0.6495628356933594,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but is largely incorrect: the event timestamps and durations do not match the ground truth (5.2s/35.0\u201336.6s vs 511.3\u2013514.9s/515.6\u2013521.2s), and it misidentifies anchor/target content and adds unsupported visual/audio details."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says reading body language is very important, when does she give the example of loving children with a poker face?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.4,
        "end": 632.7
      },
      "pred_interval": {
        "start": 35.0,
        "end": 36.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 593.4,
        "end": 596.1,
        "average": 594.75
      },
      "rationale_metrics": {
        "rouge_l": 0.17283950617283952,
        "text_similarity": 0.7019944787025452,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely incorrect: timestamps, speakers, and quoted content do not match the ground-truth events (different times, different utterances), and it hallucinates unrelated details instead of the referenced example."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker talks about the opportunity to offer educational courses after switching to online learning, when does she mention people outside Chisinau who always wanted this alternative?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 707.0,
        "end": 715.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 735.0
      },
      "iou": 0.18888888888888888,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 19.5,
        "average": 18.25
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526315,
        "text_similarity": 0.8170504570007324,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the content and that the target occurs after the anchor, but it omits the anchor end time and gives notably different start/end timestamps for the target, so the temporal details are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the pandemic simply forced them with the reality of online learning, when does she explain that they were partly stressed and partly happy about the transition?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 817.899,
        "end": 828.773
      },
      "pred_interval": {
        "start": 735.0,
        "end": 760.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.899,
        "end": 68.77300000000002,
        "average": 75.83600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2891566265060241,
        "text_similarity": 0.7894794940948486,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the target content and that it occurs 'after' the anchor, but the timestamps are substantially inaccurate and mislocate both E1 and E2 (and even mislabels the speaker), so it fails to align with the ground-truth intervals."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'It's not exactly like in class. I'm not gonna lie, it is not.', when does she give examples of what is missed in online classes?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 863.0,
        "end": 869.0
      },
      "pred_interval": {
        "start": 760.0,
        "end": 790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 103.0,
        "end": 79.0,
        "average": 91.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.7901262044906616,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer is largely incorrect: its anchor/target time ranges are far from the ground truth and segments are misaligned, though it correctly indicates examples occur after the statement. Significant timing and labeling errors warrant a very low score."
      }
    },
    {
      "question_id": "001",
      "question": "Once the female speaker finishes saying \"So why not see that?\", when does the male speaker begin to speak?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 992.475,
        "end": 994.716
      },
      "pred_interval": {
        "start": 870.0,
        "end": 903.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 122.47500000000002,
        "end": 91.31600000000003,
        "average": 106.89550000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.6953709721565247,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely incorrect: it gives entirely different timestamps, misidentifies the anchor/target timing, and includes hallucinated wording and a different relation, none of which match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female speaker finishes stating that Moldova and Romania have a huge number of emigrants, when does she list the countries where they live?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 903.0,
        "end": 908.8
      },
      "pred_interval": {
        "start": 903.4,
        "end": 958.8
      },
      "iou": 0.09677419354838677,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.39999999999997726,
        "end": 50.0,
        "average": 25.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.6720401048660278,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference on all key timestamps and event boundaries, misidentifies the quoted text, and gives a generic 'after' relation instead of the immediate 'once_finished' relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male speaker finishes saying \"I agree with you completely\", when does he start talking about writing an article on the future of TEFL?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 996.979,
        "end": 1001.302
      },
      "pred_interval": {
        "start": 958.8,
        "end": 1080.0
      },
      "iou": 0.035668316831682985,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.17900000000009,
        "end": 78.69799999999998,
        "average": 58.43850000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.29508196721311475,
        "text_similarity": 0.652033269405365,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the ground truth: timestamps are incorrect (E2 is given at 1080.0s vs 877.0\u20131001.302s) and the relationship/timing are wrong, with fabricated start/end times and inconsistent anchor labeling."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'So you have to be adaptable,' when does she say 'only the strongest survive'?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1076.398,
        "end": 1078.041
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1137.0
      },
      "iou": 0.0188850574712647,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.39799999999991,
        "end": 58.95900000000006,
        "average": 42.678499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.6309837102890015,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') but the timestamps are substantially incorrect (E2 off by ~61s and zero-length), and it adds unrelated visual cues; key timing details and continuity stated in the ground truth are missing or wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the woman says, 'You cannot come at a lesson in a hoodie,' when does the man appear on screen wearing a red hoodie?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1112.044,
        "end": 1114.077
      },
      "pred_interval": {
        "start": 1137.0,
        "end": 1237.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.955999999999904,
        "end": 122.923,
        "average": 73.93949999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.20833333333333331,
        "text_similarity": 0.6444733738899231,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and the man in a red hoodie, but the anchor/target timestamps and durations are grossly incorrect (off by over 2 minutes) and the predicted end time equals start time; it also adds unfounded visual details, so it fails to match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the man finishes saying, 'It's been really nice to have you again on this channel,' when does the Facebook page overlay appear on screen?",
      "video_id": "Dwj3JwVroJU",
      "video_number": "021",
      "segment": {
        "start": 1050.0,
        "end": 1237.0
      },
      "gt_interval": {
        "start": 1184.146,
        "end": 1186.146
      },
      "pred_interval": {
        "start": 1237.0,
        "end": 1424.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.85400000000004,
        "end": 237.85400000000004,
        "average": 145.35400000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.561273455619812,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the coarse 'after' relation but the timestamps are wildly incorrect (both E1 and E2), E2's duration is wrong (reported as instantaneous), and it omits the key detail that the Facebook overlay appears immediately after the statement while adding unrelated visual cues."
      }
    }
  ]
}