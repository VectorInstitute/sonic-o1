{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 13,
  "aggregated_metrics": {
    "detailed": {
      "rouge_l_mean": 0.09415201898965053,
      "rouge_l_std": 0.018771788275063934,
      "text_similarity_mean": 0.1672190920664714,
      "text_similarity_std": 0.08422925320951537,
      "llm_judge_score_mean": 0.0,
      "llm_judge_score_std": 0.0
    },
    "short": {
      "rouge_l_mean": 0.055232488754754956,
      "rouge_l_std": 0.036730748225814305,
      "text_similarity_mean": 0.10689640818880154,
      "text_similarity_std": 0.09909286511084338,
      "llm_judge_score_mean": 0.0,
      "llm_judge_score_std": 0.0
    },
    "cider": {
      "cider_detailed": 8.35470508879143e-08,
      "cider_short": 8.92990530563159e-06
    }
  },
  "per_entry_results": [
    {
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "detailed": {
        "rouge_l": 0.0967741935483871,
        "text_similarity": 0.12703624367713928,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the reference: it describes an environmental speech about recycling, whereas the correct answer details a courtroom sentencing, disorderly conduct dispute, and a promotion of uncensored platforms; there is no semantic overlap and key facts are omitted/contradicted."
      },
      "short": {
        "rouge_l": 0.07246376811594202,
        "text_similarity": 0.16551533341407776,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated and misses all key elements \u2014 it describes a speech about recycling with applause, whereas the correct answer details courtroom proceedings, a defendant's protests about First Amendment rights, and YouTube censorship; it omits and contradicts the original content."
      }
    },
    {
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "detailed": {
        "rouge_l": 0.09795918367346938,
        "text_similarity": 0.09446130692958832,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated and contradictory: it describes an environmental speech about recycling, omitting all key facts about the courtroom proceedings, victims, defendant, and crimes, and thus fails to match the correct answer."
      },
      "short": {
        "rouge_l": 0.056338028169014086,
        "text_similarity": 0.05086338147521019,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated\u2014discussing recycling and plastic waste\u2014while the correct answer summarizes a criminal case, victim impact statements, the defendant's lack of remorse, and the judge's remarks; it omits all key facts and context."
      }
    },
    {
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "detailed": {
        "rouge_l": 0.09146341463414634,
        "text_similarity": 0.10435269773006439,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, omitting all factual details about the Chandler Halderson trial and instead hallucinating an unrelated war monologue; it fails to match on content, facts, or scope."
      },
      "short": {
        "rouge_l": 0.06629834254143646,
        "text_similarity": 0.04888210445642471,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the reference: it describes a POW monologue while the correct answer summarizes a murder trial verdict and evidence for Chandler Halderson, thus omitting all key facts and introducing hallucinatory content."
      }
    },
    {
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "detailed": {
        "rouge_l": 0.08777429467084638,
        "text_similarity": 0.27038100361824036,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, omitting all key facts about the AI-generated courtroom avatar, judges' reaction, and implications for legal use of AI, instead introducing unrelated content about a fictitious war and empathy."
      },
      "short": {
        "rouge_l": 0.125,
        "text_similarity": 0.18124093115329742,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, missing all key facts about the courtroom AI-avatar incident and instead discussing war, empathy, and education, constituting a total mismatch."
      }
    },
    {
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "detailed": {
        "rouge_l": 0.1038961038961039,
        "text_similarity": 0.21014748513698578,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary: it describes an emotional scene with a woman and man, while the correct answer centers on Lyle Menendez's testimony about sexual abuse by his father; no key elements match."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": 0.15151900053024292,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer describes an unrelated interpersonal scene and omits the core factual elements about Lyle Menendez's testimony and the Menendez Brothers case, making it incorrect and misleading."
      }
    },
    {
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "detailed": {
        "rouge_l": 0.06962025316455696,
        "text_similarity": 0.21547025442123413,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary, describing a fictitious war speech rather than the court proceedings and legal arguments in Hothi v. Musk; it omits all key facts and introduces hallucinated content."
      },
      "short": {
        "rouge_l": 0.053763440860215055,
        "text_similarity": 0.21910881996154785,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary\u2014it describes a wartime personal account and omits all key factual elements about the Hothi v. Musk oral argument, making it incorrect."
      }
    },
    {
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "detailed": {
        "rouge_l": 0.07457627118644068,
        "text_similarity": 0.20772689580917358,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary: it describes an environmental speech about recycling, while the ground truth details a Supreme Court confirmation exchange about legal standing and self-identification. It omits all key facts and introduces hallucinatory content."
      },
      "short": {
        "rouge_l": 0.09790209790209792,
        "text_similarity": 0.18730777502059937,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary\u2014it describes a speech about recycling while the correct answer summarizes a Senate exchange between Senator Cruz and Judge Ketanji Brown Jackson about legal standing; no key facts match."
      }
    },
    {
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "detailed": {
        "rouge_l": 0.1,
        "text_similarity": 0.06213369965553284,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct answer: it describes an environmental speech about recycling, omitting all courtroom details, key actors, accusations, and outcomes, and thus contradicts the ground truth."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": -0.050465818494558334,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is unrelated to the video summary and omits all key facts (court cross-examination, Pettis's admission, Lankford's identification and outburst); it introduces irrelevant topics instead."
      }
    },
    {
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "detailed": {
        "rouge_l": 0.06642066420664207,
        "text_similarity": 0.28193557262420654,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct summary\u2014it describes a POW interview about a fictitious war, omitting all key points about civil litigation, legal practice advice, and courtroom training presented in the correct answer."
      },
      "short": {
        "rouge_l": 0.038314176245210725,
        "text_similarity": 0.22804610431194305,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is entirely unrelated to the correct answer: it describes a POW wartime account while the reference summarizes civil litigation practice and legal-advice content, omitting all key legal elements and introducing incompatible content."
      }
    },
    {
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "detailed": {
        "rouge_l": 0.1081081081081081,
        "text_similarity": 0.0853884220123291,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the reference: it describes a veteran's war account rather than the prosecutor's opening about a drug distribution incident, fleeing, and battery, omitting all key factual details and introducing unrelated content."
      },
      "short": {
        "rouge_l": 0.07792207792207792,
        "text_similarity": -0.012901164591312408,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, describing a war experience instead of the prosecution's case about cocaine distribution, an assault, and vehicle evidence; it omits all key facts and contradicts the video's subject."
      }
    },
    {
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "detailed": {
        "rouge_l": 0.07954545454545454,
        "text_similarity": 0.0977153480052948,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is a generic description of an emotional conversation and bears no resemblance to the detailed incident in the correct answer (car break-in, theft, identification of Walter Merchant, assault on a deputy, items found, and arrest), omitting all key facts and introducing unrelated content."
      },
      "short": {
        "rouge_l": 0.0,
        "text_similarity": 0.014072448015213013,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer contains only vague emotional descriptors and does not reflect any factual details from the correct summary (vandalism, theft, 911 call, Merchant's arrest and items found), so it is completely incorrect."
      }
    },
    {
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "detailed": {
        "rouge_l": 0.10894941634241247,
        "text_similarity": 0.32205796241760254,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, describing a prisoner-of-war narrative instead of a detailed legal discussion on appellate practice, cases, and advocacy; it omits all key factual elements and introduces irrelevant content."
      },
      "short": {
        "rouge_l": 0.05309734513274336,
        "text_similarity": 0.2230236679315567,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct answer\u2014describing a POW wartime story instead of a session on appellate advocacy\u2014so it omits all key elements and introduces hallucinated content."
      }
    },
    {
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "detailed": {
        "rouge_l": 0.1388888888888889,
        "text_similarity": 0.09504130482673645,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is wholly unrelated to the correct summary, mischaracterizing the video's topic (recycling/environment vs witness preparation) and omitting all key details about courtroom familiarisation, cross-examination, and the described training methodology."
      },
      "short": {
        "rouge_l": 0.07692307692307693,
        "text_similarity": -0.01655927672982216,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is completely unrelated to the correct summary, discussing recycling and life lessons instead of witness preparation, courtroom challenges, cross-examination risks, and familiarisation training; it omits all key factual elements."
      }
    }
  ]
}