{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 343,
  "aggregated_metrics": {
    "mean_iou": 0.03916831373527118,
    "std_iou": 0.13271676780638644,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.052478134110787174,
      "count": 18,
      "total": 343
    },
    "R@0.5": {
      "recall": 0.037900874635568516,
      "count": 13,
      "total": 343
    },
    "R@0.7": {
      "recall": 0.008746355685131196,
      "count": 3,
      "total": 343
    },
    "mae": {
      "start_mean": 63.82279591836734,
      "end_mean": 65.33440524781341,
      "average_mean": 64.57860058309039
    },
    "rationale": {
      "rouge_l_mean": 0.26797711828084325,
      "rouge_l_std": 0.07403987534711451,
      "text_similarity_mean": 0.709794442394732,
      "text_similarity_std": 0.10826664980030203,
      "llm_judge_score_mean": 2.690962099125364,
      "llm_judge_score_std": 1.4700528366650116
    },
    "rationale_cider": 0.0868652040960196
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the attorney states they will only proceed with the sentencing hearing for disorderly conduct and not the breach of bail, when does Frank ask if the breach of bail charge is being dropped?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.292,
        "end": 41.433
      },
      "pred_interval": {
        "start": 38.0,
        "end": 40.3
      },
      "iou": 0.002330323332361074,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.2920000000000016,
        "end": 1.1330000000000027,
        "average": 1.7125000000000021
      },
      "rationale_metrics": {
        "rouge_l": 0.21176470588235294,
        "text_similarity": 0.6322969198226929,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same two events and the 'after' relation, but the timestamps for E2 are misaligned (it places Frank's question ~2.3s earlier than the reference and even overlaps the attorney's statement), and the claim that the question 'immediately follows' is inconsistent with the provided reference timing."
      }
    },
    {
      "question_id": "002",
      "question": "After Frank states he is being persecuted for exercising his First Amendment rights, when does he say he was found guilty for being loud due to a disability?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 133.165,
        "end": 141.734
      },
      "pred_interval": {
        "start": 85.2,
        "end": 91.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.96499999999999,
        "end": 50.134000000000015,
        "average": 49.0495
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.7255606055259705,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the semantic content and correct relation ('after'), but both event timestamps are substantially incorrect compared to the ground truth, which is a major factual error."
      }
    },
    {
      "question_id": "003",
      "question": "After the attorney asks Frank why he is getting angry, when does Frank respond that he is angry because he is there for nothing?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 119.983,
        "end": 123.427
      },
      "pred_interval": {
        "start": 78.1,
        "end": 82.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.88300000000001,
        "end": 40.92700000000001,
        "average": 41.40500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168677,
        "text_similarity": 0.7298503518104553,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the predicted text matches the dialogue and relation, its temporal grounding is grossly incorrect (predicting ~77\u201382s vs reference 117\u2013123s), so the events are not correctly localized."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'Frank, you're in a courthouse,' when does the man respond by telling her to arrest him?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 173.35,
        "end": 176.25
      },
      "pred_interval": {
        "start": 185.0,
        "end": 186.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.650000000000006,
        "end": 10.449999999999989,
        "average": 11.049999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2153846153846154,
        "text_similarity": 0.6334599256515503,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the dialogue content and the 'after' relation right, but the reported timestamps are substantially off (E1 predicted at ~185s vs actual 168\u2013169.3s; E2 predicted at 186.7s vs actual 173.35\u2013176.25s), so it fails as an accurate temporal localization."
      }
    },
    {
      "question_id": "003",
      "question": "Once the text 'Libraries already protected more than one video that YouTube took down' finishes being described by the narrator, when does the text describing YouTube's strike appear?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 169.23,
        "end": 175.0
      },
      "pred_interval": {
        "start": 177.0,
        "end": 178.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.77000000000001,
        "end": 3.4000000000000057,
        "average": 5.585000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.16470588235294117,
        "text_similarity": 0.6030091047286987,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general sequence (anchor then target) but is substantially incorrect: both timestamps are off by ~11+ seconds, the target text is misidentified ('Now in 2021' vs YouTube strike text), and key temporal details from the ground truth are missing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the defendant was convicted of three counts of homicide by intoxicated use of a motor vehicle, when does he mention a count of injury by intoxicated use of a motor vehicle?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 16.341,
        "end": 19.565
      },
      "pred_interval": {
        "start": 15.13,
        "end": 15.65
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.2110000000000003,
        "end": 3.915000000000001,
        "average": 2.5630000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324323,
        "text_similarity": 0.5961397886276245,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly preserves the event ordering (E2 occurs after E1) but the timestamps are noticeably off (E1 end and E2 start differ by several tenths of a second) and the relation label ('after') is less precise than the reference's 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the defendant was serving a significant prison sentence for those convictions, when does he specify the total sentence length?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.5,
        "end": 46.7
      },
      "pred_interval": {
        "start": 38.42,
        "end": 39.17
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.0799999999999983,
        "end": 7.530000000000001,
        "average": 5.305
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424243,
        "text_similarity": 0.7131668329238892,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events and relation, but timing is inaccurate and incomplete: E1 is given as a single timestamp (though within the true interval), E2 start (39.17s) is far earlier than the reference (41.5s) and no end times are provided, so key temporal details are incorrect or missing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker laments that Tim had a long life ahead of him, when does he state that rehabilitation is one of the reasons people are sent to prison?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.786,
        "end": 207.069
      },
      "pred_interval": {
        "start": 115.07,
        "end": 115.53
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 88.71600000000001,
        "end": 91.53899999999999,
        "average": 90.1275
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.6054915189743042,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target events and the 'after' relation, but the timestamps are substantially wrong (predicted ~115s vs correct 145.3\u2013207.1s) and it misrepresents the temporal gap between the events, so key factual details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male attorney finishes his address to the court, when does the judge ask if there are representatives of the victim?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 304.214,
        "end": 307.942
      },
      "pred_interval": {
        "start": 227.0,
        "end": 229.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 77.214,
        "end": 78.94200000000001,
        "average": 78.078
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.7523095011711121,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the order (judge speaks after the attorney) but has significantly incorrect timestamps (227\u2013228s vs. 300\u2013304s) and omits the E2 end time; the relation label is also less precise than the reference, so key factual details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge states that the person has to use the microphone, when does the man in the white t-shirt (victim) begin moving to the microphone?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 356.0
      },
      "pred_interval": {
        "start": 234.0,
        "end": 237.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 118.0,
        "end": 119.0,
        "average": 118.5
      },
      "rationale_metrics": {
        "rouge_l": 0.5853658536585366,
        "text_similarity": 0.8140394687652588,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the order (judge then man) but places both events at vastly different timestamps than the ground truth and mischaracterizes timing; therefore it is largely incorrect despite the correct relative order."
      }
    },
    {
      "question_id": "003",
      "question": "During the man's speech to the court, when does he say, 'You killed somebody that meant a lot'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 401.276,
        "end": 403.024
      },
      "pred_interval": {
        "start": 272.0,
        "end": 275.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.276,
        "end": 128.024,
        "average": 128.65
      },
      "rationale_metrics": {
        "rouge_l": 0.3283582089552239,
        "text_similarity": 0.6877832412719727,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relationship ('during') right but the provided timestamps are drastically incorrect compared to the reference (anchor and target times differ by ~97\u2013129s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge warns the man in the white shirt about interrupting, when does the judge get up and leave the bench?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.13,
        "end": 331.15
      },
      "pred_interval": {
        "start": 437.2,
        "end": 441.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 106.07,
        "end": 110.35000000000002,
        "average": 108.21000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2380952380952381,
        "text_similarity": 0.8141213655471802,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly identifies the temporal relation as 'after,' it gives wholly incorrect timestamps and inconsistent event wording (hallucinated dialogue) compared to the ground truth, so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes asking the man if he wishes to address him, when does the man respond?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.38,
        "end": 331.39
      },
      "pred_interval": {
        "start": 454.8,
        "end": 455.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.42000000000002,
        "end": 123.91000000000003,
        "average": 123.66500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.22471910112359553,
        "text_similarity": 0.7759281992912292,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('immediately after') but gives entirely different timestamps and misidentifies the man's reply (saying 'June 3rd, 1997' vs. the reference 'Yes, sir'), thus contradicting key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the man in the white shirt states his child's birth date, when does he describe being happy and proud about becoming a parent?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.55,
        "end": 331.58
      },
      "pred_interval": {
        "start": 504.9,
        "end": 508.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 173.34999999999997,
        "end": 177.02000000000004,
        "average": 175.185
      },
      "rationale_metrics": {
        "rouge_l": 0.17142857142857143,
        "text_similarity": 0.6574918031692505,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct, but it misidentifies the anchor event (quotes a blessing/parenthood line rather than the child's birth date) and gives substantially different timestamps, so it fails on key factual alignment and timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the man finishes his emotional statement, when does the woman with long dark hair start walking towards the speaker's table?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.096,
        "end": 512.12
      },
      "pred_interval": {
        "start": 651.0,
        "end": 653.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 138.904,
        "end": 140.88,
        "average": 139.892
      },
      "rationale_metrics": {
        "rouge_l": 0.19148936170212766,
        "text_similarity": 0.6160074472427368,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the actors, but the timestamps are substantially incorrect (off by ~140s) and it introduces an unmentioned causal visual cue (man's movement as trigger), so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman with long dark hair finishes sitting down at the table, when does she say 'Good afternoon'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.245,
        "end": 512.259
      },
      "pred_interval": {
        "start": 654.0,
        "end": 655.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 141.755,
        "end": 142.74099999999999,
        "average": 142.248
      },
      "rationale_metrics": {
        "rouge_l": 0.32098765432098764,
        "text_similarity": 0.6955915689468384,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'immediate after' relation, but the timestamps and duration for both events contradict the ground truth (654.0s vs ~512.24s) and it adds an unsupported audio-cue detail; key factual timing elements are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman says 'my son did not deserve this', when does she list his family relationships?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 513.109,
        "end": 513.197
      },
      "pred_interval": {
        "start": 683.0,
        "end": 691.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 169.89099999999996,
        "end": 177.803,
        "average": 173.84699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.1702127659574468,
        "text_similarity": 0.5079038143157959,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the listing happens after and quotes the lines, but it gives substantially incorrect timestamps and wrongly claims a direct continuation, contradicting the ground-truth observation of a short pause/crying between the statements."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states she misses the things that irritated her about her son, when does she request life without the possibility of parole?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 779.1,
        "end": 786.0
      },
      "pred_interval": {
        "start": 77.2,
        "end": 78.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 701.9,
        "end": 707.4,
        "average": 704.65
      },
      "rationale_metrics": {
        "rouge_l": 0.2153846153846154,
        "text_similarity": 0.7083532810211182,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation and event content roughly right but the timestamps are significantly incorrect (off by about an order of magnitude for both E1 and E2), so it fails to match the key factual timing details in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first woman finishes her statement saying 'That's it', when does attorney Koenig begin describing the defendant's difficult upbringing?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 829.7,
        "end": 831.0
      },
      "pred_interval": {
        "start": 128.0,
        "end": 130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 701.7,
        "end": 701.0,
        "average": 701.35
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.7222427129745483,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation 'after' correct but the event timestamps are far off from the reference (predicted 128.0s/130.0s vs correct ~791.2s and ~829.7s), so it fails on critical factual accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After attorney Koenig explains that the defendant did not pursue an NGI defense, when does she state that the NGI defense actually came from her and attorney Zawada?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 900.0
      },
      "pred_interval": {
        "start": 148.0,
        "end": 150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 744.0,
        "end": 750.0,
        "average": 747.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.5794805288314819,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer identifies the correct utterances and relation ('after') but the timestamps are drastically incorrect compared to the ground truth, so the answer is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"mental illness\", when does she state that Joshua Skolman needs treatment in the institution?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.573,
        "end": 922.798
      },
      "pred_interval": {
        "start": 885.0,
        "end": 887.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.57299999999998,
        "end": 35.798,
        "average": 35.68549999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17721518987341772,
        "text_similarity": 0.6632646918296814,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') but the timestamps are incorrect and inconsistent with the reference (it misplaces both anchor and target and even claims they are simultaneous/immediate), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks Joshua Skolman if he has anything to tell him, when does Skolman explicitly deny having mental illness?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1001.283,
        "end": 1002.784
      },
      "pred_interval": {
        "start": 900.0,
        "end": 902.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.28300000000002,
        "end": 100.78399999999999,
        "average": 101.0335
      },
      "rationale_metrics": {
        "rouge_l": 0.26315789473684215,
        "text_similarity": 0.7504364252090454,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the denial utterance and that it occurs after the judge's question, but the provided timestamps are substantially incorrect (\u2248900s vs the correct \u22481000\u20131002s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After Joshua Skolman states \"I don't have mental illness\", when does he next explicitly claim that \"you guys\" have the mental illness or defect?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1006.129,
        "end": 1009.331
      },
      "pred_interval": {
        "start": 910.0,
        "end": 913.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.12900000000002,
        "end": 96.33100000000002,
        "average": 96.23000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575347,
        "text_similarity": 0.8425213098526001,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the anchor and target timestamps are substantially wrong (off by ~100s) and the event boundaries differ, so it fails to match the correct annotations."
      }
    },
    {
      "question_id": "001",
      "question": "After the Judge says, 'Thank you, Mr. Scolman', when does he state that only God creates life?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1149.0,
        "end": 1151.0
      },
      "pred_interval": {
        "start": 1080.28,
        "end": 1085.52
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.72000000000003,
        "end": 65.48000000000002,
        "average": 67.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324326,
        "text_similarity": 0.5694663524627686,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the reported timestamps are far off (anchor ~36s early, target ~69s early) and the audio-cue claim is incorrect, so it fails to match the key factual timing information."
      }
    },
    {
      "question_id": "002",
      "question": "Once the clerk finishes commanding silence, when does the deputy say 'Be seated'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1109.8,
        "end": 1110.5
      },
      "pred_interval": {
        "start": 1054.8,
        "end": 1055.08
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.0,
        "end": 55.42000000000007,
        "average": 55.210000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.5559135675430298,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives substantially wrong timestamps and an inconsistent description of immediacy (claims both immediate and a ~1.8s gap), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the Judge states 'Every life has value', when does he begin speaking about humanity's 'terrible, terrible dark side'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1164.5,
        "end": 1169.5
      },
      "pred_interval": {
        "start": 1095.68,
        "end": 1097.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.81999999999994,
        "end": 71.70000000000005,
        "average": 70.25999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488377,
        "text_similarity": 0.5394142866134644,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation right ('after') and identifies the anchor quote, but both anchor and target timestamps are substantially incorrect compared to the ground truth (off by ~64s), it omits end times, and it adds an unsupported audio-cue detail."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states he doesn't think it's a mental illness, when does he define a thoughtful conviction?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.453,
        "end": 1238.14
      },
      "pred_interval": {
        "start": 1249.35,
        "end": 1255.44
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.896999999999935,
        "end": 17.299999999999955,
        "average": 16.598499999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.5287424325942993,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps and temporal relation contradict the ground truth: it places both events much later and as simultaneous/during, whereas the reference has distinct earlier anchor (1230.677\u20131232.032) and a later target (1233.453\u20131238.14); the added audio-cue claim is unsupported."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if he needs to recite historical crimes, when does he state that it causes one to pause and lose their breath?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1259.682,
        "end": 1264.588
      },
      "pred_interval": {
        "start": 1279.74,
        "end": 1284.69
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.057999999999993,
        "end": 20.10200000000009,
        "average": 20.08000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.26262626262626265,
        "text_similarity": 0.6999059319496155,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the quoted dialogue and correctly identifies the semantic relation, but the timestamps are substantially incorrect (anchor/target shifted and even given the same start), so it fails on the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge declares that civilized society values life, when does he state that taking a life is the highest crime?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1362.484,
        "end": 1366.753
      },
      "pred_interval": {
        "start": 1301.43,
        "end": 1304.52
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.05399999999986,
        "end": 62.23299999999995,
        "average": 61.643499999999904
      },
      "rationale_metrics": {
        "rouge_l": 0.25862068965517243,
        "text_similarity": 0.5470538139343262,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction quotes the correct lines but the timestamps are far off and overlap (placing both around 1301s), contradicting the reference temporal ordering and omitting the camera-zoom detail; thus it is largely incorrect despite matching phrasing."
      }
    },
    {
      "question_id": "001",
      "question": "After the inmate first looks down at the paper handed to him, when is he handed paper again by an officer?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1603.0,
        "end": 1603.4
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 1.599999999999909,
        "average": 1.7999999999999545
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.8203281164169312,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it gives completely different timestamps (1605.0s) and claims the events are simultaneous, whereas the ground truth has E1 at ~1590.8\u20131590.9s and E2 at ~1603.0\u20131603.4s with the target occurring after the anchor. This contradicts the correct temporal ordering and times."
      }
    },
    {
      "question_id": "002",
      "question": "After the inmate turns his head to the side, when does he start walking away, escorted by the officers?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1626.0,
        "end": 1627.0
      },
      "pred_interval": {
        "start": 1611.0,
        "end": 1611.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 16.0,
        "average": 15.5
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.8166261911392212,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same events (head turn and escorted walking) but gives incorrect timestamps (both at 1611.0s) and wrongly claims they occur immediately after one another, contradicting the ground truth where walking starts ~25.8s later at 1626.0s."
      }
    },
    {
      "question_id": "003",
      "question": "After the distinct sound of a door or gate opening, when does the inmate walk through the door?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1637.0
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1620.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 17.0,
        "average": 16.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.8321744203567505,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives incorrect timestamps for both events and falsely states they occur simultaneously, whereas the correct answer shows the inmate walks through the door ~33s after the door-opening sound."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states, \"You will never be released from the state prison system,\" when does he mention a \"compass evaluation\" as part of the extended supervision conditions?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1433.0,
        "end": 1436.0
      },
      "pred_interval": {
        "start": 24.3,
        "end": 29.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1408.7,
        "end": 1406.6,
        "average": 1407.65
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.674429178237915,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely misidentifies and swaps the anchor and target events, gives incorrect timestamps, and asserts the opposite temporal relationship; it therefore contradicts the correct answer and is almost entirely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge mentions that the 25-year sentence for Count 2 will be consecutive to any sentence the defendant is now serving, when does the camera cut to a shot of the back of the defendant's head?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.8,
        "end": 1440.5
      },
      "pred_interval": {
        "start": 38.8,
        "end": 40.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1401.0,
        "end": 1400.2,
        "average": 1400.6
      },
      "rationale_metrics": {
        "rouge_l": 0.24528301886792453,
        "text_similarity": 0.679036557674408,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events (judge speaking and camera cutting) but gives timestamps that are far off from the ground truth and incorrectly describes the cut as 'immediately after' when the actual cut occurs ~10 seconds later; thus it is largely temporally incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge orders the $5,000 paid by the Compensation Panel to be part of the restitution order, when does the defendant stand up in the courtroom?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1539.0,
        "end": 1542.0
      },
      "pred_interval": {
        "start": 72.1,
        "end": 74.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1466.9,
        "end": 1467.4,
        "average": 1467.15
      },
      "rationale_metrics": {
        "rouge_l": 0.20454545454545456,
        "text_similarity": 0.5983006954193115,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same events but the timestamps are completely wrong (72.1s/74.6s vs. 1465.0s/1539.0s) and it claims the defendant stood 'immediately after' the judge, whereas in the reference the defendant stands ~74 seconds later; key temporal facts are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the man looks up and turns his head to his left, when do the man and deputies open the door and exit the room?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1642.992
      },
      "gt_interval": {
        "start": 1631.0,
        "end": 1634.0
      },
      "pred_interval": {
        "start": 1590.0,
        "end": 1643.0
      },
      "iou": 0.05660377358490566,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 9.0,
        "average": 25.0
      },
      "rationale_metrics": {
        "rouge_l": 0.33663366336633666,
        "text_similarity": 0.7638663053512573,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative 'after' relation but is factually incorrect: both timestamps are substantially off (E1 predicted 1615.8s vs correct 1625\u20131627s; E2 predicted 1618.6s vs correct 1631\u20131634s) and it omits the E2 finish time and misstates the timing gap."
      }
    },
    {
      "question_id": "001",
      "question": "While the male anchor announces a jury has reached a verdict in the Chandler Halderson case, when does the on-screen text 'JURY REACHES VERDICT' first appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 4.6,
        "end": 22.8
      },
      "pred_interval": {
        "start": 5.18,
        "end": 5.65
      },
      "iou": 0.025824175824175854,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5800000000000001,
        "end": 17.15,
        "average": 8.864999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.9096275568008423,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly places the anchor speech within the correct interval (5.18s is inside 0.03\u20136.6s) and notes the text appears during the announcement, but it gives the wrong first appearance time for the on-screen text (5.18s vs. correct 4.6s) and incorrectly asserts they start exactly simultaneously."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female anchor states Halderson faces eight charges, when does the on-screen graphic detailing the charges appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.7,
        "end": 35.8
      },
      "pred_interval": {
        "start": 16.19,
        "end": 16.73
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.509999999999998,
        "end": 19.069999999999997,
        "average": 13.289999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.7294847369194031,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and claims the graphic appears simultaneously at 16.19s, contradicting the ground truth that E1 ends at 23.6s and E2 appears at 23.7s; it also misstates the anchor's wording, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male anchor says 'Let's actually go to the courtroom now live', when does the judge begin speaking?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.7,
        "end": 204.9
      },
      "pred_interval": {
        "start": 144.11,
        "end": 144.58
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.589999999999975,
        "end": 60.31999999999999,
        "average": 59.954999999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.8127479553222656,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction gives entirely different timestamps and claims the judge begins simultaneously with the anchor, which contradicts the correct answer that the judge starts later (203.7s vs anchor ending 200.9s); key temporal details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking if there is anything from the state, when does the state reply?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.02,
        "end": 151.03
      },
      "pred_interval": {
        "start": 208.2,
        "end": 208.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.17999999999998,
        "end": 57.66999999999999,
        "average": 57.42499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.7163605093955994,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the judge's question, the state's one-word reply, and an 'immediately after'/'once_finished' relation, but the timestamps are substantially different from the ground truth, so the answer is largely temporally incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the female reporter states the jury deliberated for just over two hours, when does the male reporter comment on the quick verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.45,
        "end": 152.5
      },
      "pred_interval": {
        "start": 227.9,
        "end": 237.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.45000000000002,
        "end": 84.5,
        "average": 79.97500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.1839080459770115,
        "text_similarity": 0.6200632452964783,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly captures the dialogue content and the 'after' relationship, but both event timestamps are substantially incorrect (off by ~75s), so the temporal localization is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes asking if the jury reached a verdict on all counts, when does the jury foreman respond?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.2,
        "end": 153.2
      },
      "pred_interval": {
        "start": 251.3,
        "end": 252.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 98.10000000000002,
        "end": 99.60000000000002,
        "average": 98.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.7009645700454712,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the same utterances and correct ordering (response immediately after the judge), but the timestamps are substantially wrong (~98 seconds off) compared to the ground truth, a significant factual mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After the foreperson states that the jury reached a verdict, when does the court staff receive the verdict folder?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.2,
        "end": 357.9
      },
      "pred_interval": {
        "start": 351.6,
        "end": 353.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.599999999999966,
        "end": 4.599999999999966,
        "average": 5.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.2637362637362637,
        "text_similarity": 0.7341921329498291,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies E1 (it labels the judge's request rather than the foreperson's verdict confirmation) and gives a different timing/causal framing ('immediately after') than the reference ('after' with a later staff receipt). It also adds unsupported sensory detail about the sound of paper being passed."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes reading the verdict for Count 1, when does he begin reading the verdict for Count 2?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.7,
        "end": 445.2
      },
      "pred_interval": {
        "start": 63.7,
        "end": 65.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 378.0,
        "end": 380.0,
        "average": 379.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3684210526315789,
        "text_similarity": 0.7637333869934082,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps (63.7s/65.2s vs. 441.7s/441.7\u2013445.2s) and asserts a delay ('after') rather than the immediate 'once_finished' transition in the ground truth, so it contradicts key facts."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge finishes reading the verdict for Count 8, when does he state that 'not guilty' verdict forms were also returned?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 630.9,
        "end": 641.0
      },
      "pred_interval": {
        "start": 170.1,
        "end": 171.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 460.79999999999995,
        "end": 469.4,
        "average": 465.09999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3703703703703703,
        "text_similarity": 0.7485554814338684,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (E2 occurs after E1) but gives substantially incorrect timestamps, omits E2's end time, and incorrectly characterizes the relation as 'immediately after' versus the ground-truth timing, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge finishes reading the final guilty verdict for count 8, when does he begin to inquire if the jury members agreed with the verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 528.9,
        "end": 619.0
      },
      "pred_interval": {
        "start": 510.0,
        "end": 510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.899999999999977,
        "end": 109.0,
        "average": 63.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.39534883720930236,
        "text_similarity": 0.6921002864837646,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates both timestamps (E1 off by ~3s and E2 off by ~19s), incorrectly claims E2 begins immediately/concurrently with E1, and introduces quoted phrasing not supported by the reference, so it does not align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the last juror confirms their agreement to the verdicts, when does the judge begin his speech thanking the jury for their service?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 621.0,
        "end": 665.0
      },
      "pred_interval": {
        "start": 584.0,
        "end": 585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.0,
        "end": 80.0,
        "average": 58.5
      },
      "rationale_metrics": {
        "rouge_l": 0.32432432432432434,
        "text_similarity": 0.6743232011795044,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces both event timestamps by a large margin (~33\u201336 seconds), gives an incorrect start time for the judge's speech and omits the correct end time, and adds unsupported details (name/quote); while it preserves the general sequence, the factual timing is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes instructing the defendant's table to be seated, when does Attorney Brown make his motion?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 741.0
      },
      "pred_interval": {
        "start": 719.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.0,
        "end": 21.0,
        "average": 19.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111116,
        "text_similarity": 0.7740910053253174,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer identifies the correct events but gives substantially incorrect timestamps (judge at 719s vs 732s; motion start at 720s vs 737s), omits the motion end time (741s), and incorrectly claims the motion occurred 'immediately after' the judge's instruction."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking about any pre-sentence request, when does Attorney Brown state that a pre-sentence investigation should be performed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "pred_interval": {
        "start": 690.0,
        "end": 700.0
      },
      "iou": 0.25,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 2.5,
        "average": 3.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.7096582651138306,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that Attorney Brown says a pre-sentence investigation should be performed immediately after the judge's question and the E2 timing is nearly correct, but the E1 (judge question) timestamp is substantially off and the relation label is less precise than the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge orders a pre-sentence investigation, when does he specify that there should not be any recommendations made within the report?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.6,
        "end": 754.5
      },
      "pred_interval": {
        "start": 740.0,
        "end": 750.0
      },
      "iou": 0.027586206896550156,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.600000000000023,
        "end": 4.5,
        "average": 7.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424246,
        "text_similarity": 0.7423067092895508,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures that the judge specified no recommendations and that this occurs after the order, but the temporal boundaries are substantially incorrect (both E1 and E2 times are earlier than the reference) and the relation label is less precise than 'once_finished.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the news anchor finishes stating the sentencing hearing dates, when does the District Attorney begin his statement to the media?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 935.0,
        "end": 938.5
      },
      "pred_interval": {
        "start": 830.0,
        "end": 840.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.0,
        "end": 98.5,
        "average": 101.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666667,
        "text_similarity": 0.8273141980171204,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets only the temporal relation ('after') correct but the timestamps for both E1 and E2 are substantially wrong (off by ~70\u2013100s), omits the DA's end time, and introduces an unverified quoted remark\u2014so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the system does not work without jurors, when does he mention that the jury of 18 sacrificed a month of their life?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 901.4,
        "end": 908.9
      },
      "pred_interval": {
        "start": 879.7,
        "end": 890.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.699999999999932,
        "end": 18.799999999999955,
        "average": 20.249999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.16,
        "text_similarity": 0.4419809579849243,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly quotes the relevant phrase but gives an incorrect anchor timestamp (879.7s vs. 894.7\u2013899.8s), fails to provide the correct target timing (901.4\u2013908.9s) and misaligns the speech boundaries, so it is largely temporally inaccurate despite matching content."
      }
    },
    {
      "question_id": "002",
      "question": "Once the District Attorney states that the process had to take a whole week off because of the virus, when does he commend people for sacrificing their time and potentially their safety?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 971.4,
        "end": 982.1
      },
      "pred_interval": {
        "start": 893.4,
        "end": 902.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.0,
        "end": 79.89999999999998,
        "average": 78.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.16666666666666666,
        "text_similarity": 0.6127473711967468,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the following phrase and that it comes after the week-off remark, but it gives incorrect timestamps (893.4s vs ~971s in the reference) and omits the completion/context about the community's backbone, so key factual elements are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer asks for a message to Barton Krista's family, when does the District Attorney state he will speak to them after?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1027.2,
        "end": 1028.7
      },
      "pred_interval": {
        "start": 921.8,
        "end": 923.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 105.40000000000009,
        "end": 104.90000000000009,
        "average": 105.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753085,
        "text_similarity": 0.5209729075431824,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the DA's immediate reply and wording ('I will speak to them after'), but it gives incorrect timestamps and misplaces the interviewer event (921.8s vs the reference 1026.6s/1027.2s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the Sheriff's Department could get the case to trial in short order, when does he talk about the professionalism and integrity of law enforcement?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1088.6,
        "end": 1095.4
      },
      "pred_interval": {
        "start": 1160.79,
        "end": 1172.02
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 72.19000000000005,
        "end": 76.61999999999989,
        "average": 74.40499999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.13953488372093023,
        "text_similarity": 0.22841724753379822,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly conveys that the comment about professionalism and integrity comes after the trial remark, but it gives substantially incorrect timestamps (1160.79s vs. ~1086\u20131095s) and misstates timing/details, so it is largely factually inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer asks about the historic significance and challenges of the trial, when does the District Attorney confirm the trial was unprecedented?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.2,
        "end": 1202.0
      },
      "pred_interval": {
        "start": 1209.56,
        "end": 1216.24
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.3599999999999,
        "end": 14.240000000000009,
        "average": 11.799999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.14893617021276595,
        "text_similarity": 0.24747321009635925,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the ground truth timestamps and inserts fabricated quoted phrases; while both claim the DA agrees, the predicted answer misplaces the response by ~10s and hallucinates dialogue, so it fails to match the correct annotation."
      }
    },
    {
      "question_id": "003",
      "question": "After the District Attorney says they may never know 'the why' behind the crime, when does the news anchor cut in to summarize this point?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1358.6,
        "end": 1367.8
      },
      "pred_interval": {
        "start": 1229.02,
        "end": 1232.02
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.57999999999993,
        "end": 135.77999999999997,
        "average": 132.67999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1956521739130435,
        "text_similarity": 0.3709532618522644,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference timestamps and sequence: it claims an immediate anchor summary at 1229.02s, whereas the correct answer places the anchor summary at 1358.6\u20131367.8s after the DA speaks; this major timing/sequence error makes the response essentially incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the news anchor finishes stating that the system worked and the verdict graphic is fully shown, when does the narrator begin listing the specific guilty verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1265.0,
        "end": 1275.0
      },
      "pred_interval": {
        "start": 1243.6,
        "end": 1244.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.40000000000009,
        "end": 30.90000000000009,
        "average": 26.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.24742268041237106,
        "text_similarity": 0.6947277188301086,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general relation that the narrator follows the anchor, but it gives substantially wrong timestamps (1244s vs 1257s for E1 and 1244s vs 1265s for E2), slightly misquotes the name, and even implies overlap\u2014so it contradicts key factual timing details in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After reporter Jaymes Langrehr finishes saying that the sheer volume of evidence proved to be overwhelming for the jurors, when does he explain that DNA analysts proved bloodstains belonged to the parents?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1350.0,
        "end": 1364.0
      },
      "pred_interval": {
        "start": 1280.3,
        "end": 1285.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.70000000000005,
        "end": 78.20000000000005,
        "average": 73.95000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.24347826086956523,
        "text_similarity": 0.6466989517211914,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction references the same reporter quotes but gives timestamps that conflict substantially with the ground truth (off by ~55\u201375s) and incorrectly labels the relation as 'during' rather than 'after', so it is factually misaligned despite partial content overlap."
      }
    },
    {
      "question_id": "003",
      "question": "Once reporter Jaymes Langrehr finishes saying there was no one specific piece of evidence that 'really stuck out', when is the next time he mentions bringing in the DNA analysts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1348.0,
        "end": 1352.0
      },
      "pred_interval": {
        "start": 1288.5,
        "end": 1290.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.5,
        "end": 61.299999999999955,
        "average": 60.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.1680672268907563,
        "text_similarity": 0.7576419115066528,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the DNA analyst is mentioned immediately after the 'no one piece ... really stuck out' remark, but the timestamps are significantly wrong and even overlap (1286\u20131289s vs. the ground-truth 1341s and 1348\u20131352s), so the answer is not temporally accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After MS. NULAND finishes asking \"What's your response to that?\", when does the Sheriff begin responding about his faith in the team?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.47,
        "end": 1430.395
      },
      "pred_interval": {
        "start": 1418.81,
        "end": 1419.52
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.660000000000082,
        "end": 10.875,
        "average": 9.267500000000041
      },
      "rationale_metrics": {
        "rouge_l": 0.2531645569620253,
        "text_similarity": 0.7254369258880615,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the sheriff's reply follows the question and even quotes the line, but the timestamps are significantly off (predicted ~1419s vs ground truth ~1426s) and it labels the relation 'immediately after' contrary to the annotated timing and relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Sheriff finishes explaining what he will remember most about the case, when does a reporter ask about the case informing future work?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1491.452,
        "end": 1494.597
      },
      "pred_interval": {
        "start": 1484.73,
        "end": 1485.29
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.72199999999998,
        "end": 9.307000000000016,
        "average": 8.014499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.16091954022988506,
        "text_similarity": 0.7278813719749451,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the general ordering (question follows the Sheriff's remark) but gives incorrect timestamps (both events ~6s earlier than ground truth) and omits the reporter's end time; the relation label is a near-synonym but key factual timing details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the reporter finishes their commentary thanking Sheriff Calvin Baer and his deputies for their hard work, when does the reporter next mention that \"A lot of resources involved for a very long time\"?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1528.402,
        "end": 1530.927
      },
      "pred_interval": {
        "start": 1503.57,
        "end": 1504.48
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.832000000000107,
        "end": 26.44699999999989,
        "average": 25.639499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753085,
        "text_similarity": 0.696783185005188,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the subsequent utterance and its immediate-after relation, but the reported timestamps are substantially incorrect (off by ~25s), so it fails to match the ground-truth timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the main anchor introduces Tahlil Maudeen, when does Tahlil begin reporting on the emotion in the gallery?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1701.902,
        "end": 1708.327
      },
      "pred_interval": {
        "start": 1641.4,
        "end": 1643.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.50199999999995,
        "end": 64.92699999999991,
        "average": 62.71449999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.2337662337662338,
        "text_similarity": 0.7886860370635986,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the anchor precedes Tahlil, but it gives completely different timestamps and falsely claims the handoff is immediate; these factual discrepancies about timing and relation contradict the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the main anchor asks Tahlil if she talked to Chandler Halderson's defense attorneys, when does the anchor next ask about whom Tahlil interviewed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1765.828,
        "end": 1767.07
      },
      "pred_interval": {
        "start": 1675.0,
        "end": 1677.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 90.82799999999997,
        "end": 90.06999999999994,
        "average": 90.44899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2653061224489796,
        "text_similarity": 0.8412243127822876,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two questions and their sequential relation, but the timestamps are substantially wrong (off by ~80+ seconds) and it inaccurately asserts the prosecutor question directly follows the defense-attorney question, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After Tahlil Maudeen reports that the defense attorneys were unavailable for interviews, when does she report that the DA seemed pleased with the verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1769.555,
        "end": 1783.597
      },
      "pred_interval": {
        "start": 1703.4,
        "end": 1705.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 66.15499999999997,
        "end": 78.19699999999989,
        "average": 72.17599999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.28235294117647064,
        "text_similarity": 0.8152437210083008,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the DA's pleased reaction text but mislocates and misidentifies the anchor (wrong utterance and timestamps) and incorrectly asserts an 'immediately after' relation, so it largely disagrees with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the reporter asks if people were surprised by the quick jury return, when does she explain why they were expecting it?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1789.692,
        "end": 1798.408
      },
      "pred_interval": {
        "start": 1778.92,
        "end": 1787.32
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.771999999999935,
        "end": 11.087999999999965,
        "average": 10.92999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.1935483870967742,
        "text_similarity": 0.6984773874282837,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the reporter explains why they expected a quick verdict, but the timestamps are substantially incorrect and it incorrectly labels the target as occurring 'during' the anchor rather than after it, contradicting the ground truth temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host says, 'Thank you all,' when does she begin to introduce the website for more information on the case?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1809.891,
        "end": 1815.742
      },
      "pred_interval": {
        "start": 1800.14,
        "end": 1814.12
      },
      "iou": 0.27105499294961233,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.750999999999976,
        "end": 1.622000000000071,
        "average": 5.686500000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.2826086956521739,
        "text_similarity": 0.8195086717605591,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the general relation ('immediately after') but is factually incorrect on timings: it places both events ~9 seconds earlier and even starts E2 at the same time as E1 (implying overlap), contradicting the correct sequential timestamps; minor end-time difference aside, these are major temporal errors."
      }
    },
    {
      "question_id": "003",
      "question": "Once the host says, 'Thanks for joining us,' when does she announce the return to regular programming?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1830.005,
        "end": 1831.628
      },
      "pred_interval": {
        "start": 1831.42,
        "end": 1837.02
      },
      "iou": 0.02965074839627371,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4149999999999636,
        "end": 5.392000000000053,
        "average": 3.403500000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.32608695652173914,
        "text_similarity": 0.7897385358810425,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps for both events are off by over a second, the target's duration is much longer than the reference, and it wrongly states the events occur 'during' each other instead of the target immediately following the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "After the narrator explains that the judges stopped the video, when does the judge in the embedded video actually stop the video and question the man?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 217.92,
        "end": 221.605
      },
      "pred_interval": {
        "start": 25.2,
        "end": 26.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 192.72,
        "end": 195.405,
        "average": 194.0625
      },
      "rationale_metrics": {
        "rouge_l": 0.1818181818181818,
        "text_similarity": 0.7147241830825806,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction is incorrect: it places E2 at ~25s (should be ~218s), reports a different judge utterance, and labels the relation as simultaneous instead of after, thus contradicting the reference and adding hallucinatory details."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if the video is counsel for the case, when does the man reply, 'I generated that'?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 224.77,
        "end": 225.951
      },
      "pred_interval": {
        "start": 48.0,
        "end": 49.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 176.77,
        "end": 176.451,
        "average": 176.6105
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.7608414888381958,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the dialogue and relation ('after') correct but the timestamps are drastically wrong (\u224847\u201349s vs correct \u2248218\u2013226s), so it fails on the key factual element of when the reply occurs."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes telling the man he is not going to use the courtroom as a business launch, when does she instruct him to stand up for oral argument time?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 323.425,
        "end": 328.018
      },
      "pred_interval": {
        "start": 138.5,
        "end": 140.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 184.925,
        "end": 187.61799999999997,
        "average": 186.2715
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.7942467927932739,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (the instruction follows the judge's statement) and the dialogue content, but the timestamps are substantially wrong (off by ~174 seconds) and it understates the pause length, making it factually inaccurate for this video."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says \"That is not a real person\", when does the judge say it would have been nice to know that information?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 154.321,
        "end": 158.401
      },
      "pred_interval": {
        "start": 153.81,
        "end": 157.81
      },
      "iou": 0.7599651492049658,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5109999999999957,
        "end": 0.5910000000000082,
        "average": 0.5510000000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.6716266870498657,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the event order correct and the target start is close to the reference, but the anchor timestamp is notably inaccurate (150.0s vs 152.291s) and end times are omitted, so it misses key timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes describing the man's past verbal conversations with her staff, when does she say she doesn't appreciate being misled?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 177.031,
        "end": 181.351
      },
      "pred_interval": {
        "start": 167.75,
        "end": 169.64
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.281000000000006,
        "end": 11.711000000000013,
        "average": 10.49600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7078462839126587,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target utterance ('I don't appreciate being misled') and that it follows the anchor, but the reported timestamps are substantially earlier than the ground truth and the temporal relation lacks the specified immediacy, so the localization is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge states that the man is not going to use the courtroom as a launch for his business, when does she command him to \"shut that off\"?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 200.411,
        "end": 201.981
      },
      "pred_interval": {
        "start": 175.76,
        "end": 176.76
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.65100000000001,
        "end": 25.221000000000004,
        "average": 24.936000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2898550724637681,
        "text_similarity": 0.8107229471206665,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'shut that off' utterance but places both anchor and target timestamps far earlier than the ground truth and mislabels the anchor phrase, so it fails to match the correct temporal locations and context."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interrogator asks what kind of objects were used, when does the witness name a toothbrush and shaving utensil?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 150.14,
        "end": 150.22
      },
      "pred_interval": {
        "start": 151.8,
        "end": 156.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.660000000000025,
        "end": 6.3799999999999955,
        "average": 4.02000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428564,
        "text_similarity": 0.8237010836601257,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly names the items and states the witness responds after the question, but the timestamps and durations are significantly off (predicted ~1\u20136s later and a much longer answer window) versus the reference's immediate 150.14\u2013150.22s, so it does not match the temporal grounding."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interrogator asks if the abuser decided to use something besides a toothbrush, when does the witness answer 'Yes'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.11,
        "end": 151.12
      },
      "pred_interval": {
        "start": 185.4,
        "end": 186.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.28999999999999,
        "end": 35.28,
        "average": 34.785
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317073,
        "text_similarity": 0.8176085948944092,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the interrogator question, the witness's one-word 'Yes', and the 'immediately after' relationship, but the timestamps are substantially incorrect (185.4/186.4s vs. 151.01/151.11s), so the events are misaligned."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interrogator asks what his mom said, when does the witness recount his mother's full response?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.06,
        "end": 153.23
      },
      "pred_interval": {
        "start": 253.0,
        "end": 265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.94,
        "end": 111.77000000000001,
        "average": 105.855
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424243,
        "text_similarity": 0.8269750475883484,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the semantic content (the witness gives a full answer and the relation is 'after') but the timestamps are substantially incorrect and do not reflect the immediate adjacency specified in the ground truth, so it is largely factually misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What did your mom say?\", when does the man respond about his mom telling him to stop and that he was exaggerating?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 340.0
      },
      "pred_interval": {
        "start": 349.0,
        "end": 361.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 21.5,
        "average": 18.25
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.850193440914154,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the same utterance and the 'after' relation, but the temporal boundaries are substantially off (anchor and target are shifted ~14\u201321s later than the reference), so the localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he was afraid to tell anyone because his dad didn't want him to, when does he reveal his dad said it was 'our secret'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 387.0,
        "end": 389.0
      },
      "pred_interval": {
        "start": 382.7,
        "end": 392.8
      },
      "iou": 0.19801980198019759,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.300000000000011,
        "end": 3.8000000000000114,
        "average": 4.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2666666666666667,
        "text_similarity": 0.8162723779678345,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and the 'after' relation, and preserves the semantic content; only minor discrepancies in the reported timestamps justify a slight deduction."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes asking, \"What did you do to your brother?\", when does the man describe taking him to the woods and playing with a toothbrush in the same way?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 427.0,
        "end": 438.0
      },
      "pred_interval": {
        "start": 416.9,
        "end": 431.0
      },
      "iou": 0.1895734597156396,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.100000000000023,
        "end": 7.0,
        "average": 8.550000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.2619047619047619,
        "text_similarity": 0.6753597259521484,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction captures the same events (man describing taking him to the woods and playing with him) but the anchor/target timestamps differ substantially from the reference and the relation is labeled 'after' rather than the specified 'once_finished', so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once Lyle Menendez removes his hand from his mouth/face, when does he put his fingers back into his mouth while crying?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 515.8,
        "end": 517.7
      },
      "pred_interval": {
        "start": 558.2,
        "end": 558.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.40000000000009,
        "end": 41.0,
        "average": 41.700000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962022,
        "text_similarity": 0.17623446881771088,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the sequence (hand away then back in) but gives timestamps that conflict with the ground truth by ~42 seconds, so it is factually incorrect despite matching the temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "While the female voice asks, 'It stopped for you with your dad when you were eight,' when is Erik Menendez first shown on screen?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 536.0,
        "end": 579.0
      },
      "pred_interval": {
        "start": 527.7,
        "end": 528.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.299999999999955,
        "end": 50.799999999999955,
        "average": 29.549999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6694442629814148,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timings contradict the ground truth: the correct anchor speech is 533.5\u2013536.8s and Erik first appears at 536.0s, whereas the prediction places both at ~527.7s. This is a clear factual mismatch in key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After Erik Menendez says 'Yes' in response to doing something about it, when does the female voice ask 'What did you do?'",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 560.0,
        "end": 560.8
      },
      "pred_interval": {
        "start": 541.9,
        "end": 542.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.100000000000023,
        "end": 18.399999999999977,
        "average": 18.25
      },
      "rationale_metrics": {
        "rouge_l": 0.18823529411764706,
        "text_similarity": 0.39304399490356445,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct order ('after') but the timestamps are substantially wrong (predicted ~541.9\u2013542.4s vs correct 557.2\u2013560.8s) and the timing/interval details disagree, so it fails on key factual elements."
      }
    },
    {
      "question_id": "001",
      "question": "After the camera shows Lyle Menendez crying and sniffling, when does the female voice ask about the abuse stopping when he was eight?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 533.5,
        "end": 536.5
      },
      "pred_interval": {
        "start": 510.0,
        "end": 510.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.5,
        "end": 26.399999999999977,
        "average": 24.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.6811720132827759,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies E1's start and the relation label, but it erroneously places E2 at 510.1s instead of 533.5s (and omits end times), which contradicts the correct timing and undermines the asserted 'after' relation."
      }
    },
    {
      "question_id": "002",
      "question": "During the female voice asking if he thought it might be happening to someone else, when is Erik Menendez shown with a distressed facial expression?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 539.0,
        "end": 545.8
      },
      "pred_interval": {
        "start": 525.6,
        "end": 527.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.399999999999977,
        "end": 17.899999999999977,
        "average": 15.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.2368421052631579,
        "text_similarity": 0.6516968011856079,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events (female question and Erik's distressed expression) but gives substantially different start times (~11\u201313s earlier) and an incorrect temporal relation ('after' vs. correct 'during' where E2 overlaps entire E1), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female voice finishes asking 'And who did you think it was happening to?', when does Erik Menendez answer 'Eric'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 551.0,
        "end": 551.5
      },
      "pred_interval": {
        "start": 531.4,
        "end": 532.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.600000000000023,
        "end": 19.200000000000045,
        "average": 19.400000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.6797275543212891,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the order and 'immediately after' relation right, but the event timestamps are substantially different from the ground truth (predicted 531.4/532.3s vs correct 548.8/551.0s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the Presiding Justice asks for the appearance of counsel, when does the appellant's counsel introduce himself?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.401,
        "end": 18.83
      },
      "pred_interval": {
        "start": 10.21,
        "end": 17.13
      },
      "iou": 0.66461716937355,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.190999999999999,
        "end": 1.6999999999999993,
        "average": 1.4454999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.683336615562439,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction recognizes that Liffrak speaks immediately after the justice, but it misreports key timestamps (E1 actually starts at 6.275s and ends at 11.381s; E2 starts at 11.401s) and incorrectly suggests they occur simultaneously rather than clearly after, so the temporal alignment is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the Presiding Justice's question about public importance, when does Mr. Lifrak remain silent and attentive?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.5,
        "end": 103.0
      },
      "pred_interval": {
        "start": 35.0,
        "end": 60.91
      },
      "iou": 0.31485294117647056,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.5,
        "end": 42.09,
        "average": 23.295
      },
      "rationale_metrics": {
        "rouge_l": 0.3058823529411765,
        "text_similarity": 0.7651785016059875,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relationship and that Mr. Lifrak is silent and listening, but the timestamps are inconsistent with the reference (predicted E1 starts at 35.0 vs 39.5 and omits the 103.0s end) and predicted E2 ends at 60.91s, so it does not cover the full period the reference specifies."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Lifrak finishes asking to reserve time for rebuttal, when does the Presiding Justice grant permission?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 109.412,
        "end": 110.2
      },
      "pred_interval": {
        "start": 68.19,
        "end": 70.84
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.22200000000001,
        "end": 39.36,
        "average": 40.291000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.25287356321839083,
        "text_similarity": 0.7362837791442871,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the sequence (the justice grants permission after Liffrak's request) but it gives substantially incorrect timestamps/durations and reports a different relation label, so it contains major factual inaccuracies."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (bottom left) mentions Mr. Hothi injected himself into controversies, when does he state that Mr. Hothi said very publicly on Twitter that Mr. Musk and Tesla were lying?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 196.5,
        "end": 201.5
      },
      "pred_interval": {
        "start": 42.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 154.5,
        "end": 154.5,
        "average": 154.5
      },
      "rationale_metrics": {
        "rouge_l": 0.14814814814814814,
        "text_similarity": 0.7253312468528748,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the quoted phrase and the 'after' relation, but the timestamps are completely incorrect (42s vs correct 188.6\u2013191.6s and 196.5\u2013201.5s), so it fails on the key factual timing elements."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker (bottom left) is detailing Mr. Hothi's actions, when does he mention Mr. Hothi hitting an employee with a car?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 283.6,
        "end": 285.5
      },
      "pred_interval": {
        "start": 94.0,
        "end": 96.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 189.60000000000002,
        "end": 189.5,
        "average": 189.55
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.6686409115791321,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event relationship right (\"during\") and identifies the speaker listing actions, but the timestamps are vastly incorrect compared to the reference (94s vs. ~278\u2013285s), so it fails on the key factual element of timing."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge (top left) finishes expressing her doubt about the relevance of Mr. Hothi's actions, when does the speaker (bottom left) begin his response?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 339.9,
        "end": 350.0
      },
      "pred_interval": {
        "start": 149.0,
        "end": 150.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 190.89999999999998,
        "end": 200.0,
        "average": 195.45
      },
      "rationale_metrics": {
        "rouge_l": 0.4722222222222222,
        "text_similarity": 0.8152576088905334,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the general ordering (anchor then response) but the timestamps and quoted content differ substantially from the reference, so the answer is largely incorrect despite a similar relation notion."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge (top right) asks what if Mr. Musk made an ad hominem attack on Mr. Hothi, when does the lawyer (bottom left) respond by giving examples of unrelated attacks?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 380.5
      },
      "pred_interval": {
        "start": 330.0,
        "end": 340.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.19999999999999,
        "end": 40.5,
        "average": 42.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.2708333333333333,
        "text_similarity": 0.7568349838256836,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target roles and that the target occurs after the anchor, but it significantly misstates both timestamps and even adds specific example accusations not present in the reference, so the timing and some content are inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge (top center) states that the lawsuit is about Mr. Hothi almost killing Tesla employees, when does the lawyer (bottom left) clarify Mr. Musk's statements regarding Mr. Hothi harassing employees and almost killing one?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 553.0,
        "end": 561.0
      },
      "pred_interval": {
        "start": 370.0,
        "end": 380.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 183.0,
        "end": 181.0,
        "average": 182.0
      },
      "rationale_metrics": {
        "rouge_l": 0.25242718446601947,
        "text_similarity": 0.7541531920433044,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the semantic content (the lawyer clarifies Musk's statement about Hothi harassing employees and almost killing one) and that it follows the judge, but the timestamps and timing relationship are significantly incorrect (predicted times \u2260 reference and not immediately following), so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the lawyer (bottom left) finishes explaining why 'almost killed' is not a false statement of fact using a sidewalk analogy, when does the judge (top center) ask if that just means the lawyer might win the lawsuit?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 586.8
      },
      "pred_interval": {
        "start": 445.0,
        "end": 450.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 139.0,
        "end": 136.79999999999995,
        "average": 137.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3191489361702128,
        "text_similarity": 0.8420112133026123,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the semantic relationship (judge's question immediately follows the lawyer's point and quotes the question), but it gives incorrect timestamps and a mislocated/shortened anchor segment, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (bottom left) states the problem is that two people would be debating under different rules because Mr. Hothi is protected by anti-SLAPP, when does he state that the target of Mr. Hothi's speech would not have the same protection?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.405,
        "end": 511.559
      },
      "pred_interval": {
        "start": 105.0,
        "end": 111.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 406.405,
        "end": 400.559,
        "average": 403.48199999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.20224719101123595,
        "text_similarity": 0.6888496279716492,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the gist of the anchor/target content and the 'after' relationship, but the timestamps are substantially incorrect (105/111s vs ~511s) and the event boundaries do not match the ground truth, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom right) begins his turn by saying 'I mean, I think that the difficulty, Mr. Lefebvre, that I have is', when does he explain that the argument is based on Mr. Hothi entering the public sphere?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.597,
        "end": 512.074
      },
      "pred_interval": {
        "start": 117.0,
        "end": 120.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 394.597,
        "end": 392.07399999999996,
        "average": 393.33549999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.22222222222222224,
        "text_similarity": 0.7492146492004395,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct verbal content and that E2 follows E1, but the timestamps are drastically off and do not match the reference temporal boundaries, so it fails to correctly localize the events."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker (bottom right) finishes giving the example of Mr. Musk commenting on Mr. Houthi's methodology and data, when does he question the relevance of 'almost killing someone in the parking lot'?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.302,
        "end": 512.387
      },
      "pred_interval": {
        "start": 141.0,
        "end": 144.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 371.302,
        "end": 368.38699999999994,
        "average": 369.8445
      },
      "rationale_metrics": {
        "rouge_l": 0.1836734693877551,
        "text_similarity": 0.7712125778198242,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer uses entirely different timestamps and mislocates both the anchor and target events, so it does not match the ground-truth timing; it only loosely matches the discourse relation ('after') and some wording, but fails on the key factual element (correct timestamps/immediate succession)."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks why this is less attenuated than in the Wilson case, when does he begin explaining the issue?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 703.5
      },
      "pred_interval": {
        "start": 697.92,
        "end": 701.96
      },
      "iou": 0.538666666666677,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.919999999999959,
        "end": 1.5399999999999636,
        "average": 1.7299999999999613
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.7075008153915405,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mislocates both events (E1 at 697.92s vs correct 693.0\u2013695.3s, and E2 at 699.92s vs correct start 696.0s), so while the temporal direction ('after'/'once_finished') is roughly captured, the key timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that their analysis is relevant to 'what he did', when does he give the example of trespassing?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 764.0,
        "end": 768.7
      },
      "pred_interval": {
        "start": 724.08,
        "end": 728.48
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.91999999999996,
        "end": 40.22000000000003,
        "average": 40.06999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2051282051282051,
        "text_similarity": 0.5864206552505493,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' but the event timestamps are substantially wrong (predicted E1/E2 are ~35\u201340 seconds earlier than the ground truth), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the presiding justice turns the floor over to the opponent for replies, when does the opponent begin speaking?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 802.5
      },
      "pred_interval": {
        "start": 765.84,
        "end": 770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.15999999999997,
        "end": 32.5,
        "average": 33.329999999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.2580645161290323,
        "text_similarity": 0.726063072681427,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the predicted timestamps (765.84s/770.0s) substantially contradict the ground-truth times (791.0s/800.0s), so it contains major factual errors and omits the correct timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mr. Greenspan finishes saying that the purpose was not to monitor or harass employees, when does he begin explaining that the person was counting cars off the production line?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.67,
        "end": 1058.61
      },
      "pred_interval": {
        "start": 1060.8,
        "end": 1061.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.129999999999882,
        "end": 2.3900000000001,
        "average": 4.759999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.16326530612244897,
        "text_similarity": 0.6922045946121216,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation/order correct (immediately after) but the provided timestamps are substantially off from the ground truth (about 10 seconds later) and misstate the anchor/target boundaries; it also adds unverified visual/audio details. "
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Greenspan finishes asking if there are any specific questions, when does the Presiding Justice respond by asking the panel if they have questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.969,
        "end": 1135.213
      },
      "pred_interval": {
        "start": 1102.8,
        "end": 1103.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.169000000000096,
        "end": 32.212999999999965,
        "average": 30.69100000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6557305455207825,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the Presiding Justice speaks immediately after Mr. Greenspan, but the timestamps are substantially incorrect (off by ~18\u201329 seconds) and it fails to match the anchor/target intervals given in the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Liffrec finishes explaining that the Nidal case was about a university building something in a public park and that was the public debate, when does he state that the university then made statements about protesters inciting violence?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1159.099,
        "end": 1164.925
      },
      "pred_interval": {
        "start": 1151.0,
        "end": 1151.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.098999999999933,
        "end": 13.724999999999909,
        "average": 10.91199999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.17142857142857146,
        "text_similarity": 0.6313519477844238,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the sequence and relation (immediately after/once finished) and the content, but the timestamps are substantially different from the reference (about 8 seconds earlier) and it adds unverified audio/visual cues, so it is factually misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's discussion about harassment, when does he mention \"extensive evidence of harassment\"?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1240.5,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 1251.0,
        "end": 1254.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.5,
        "end": 12.0,
        "average": 11.25
      },
      "rationale_metrics": {
        "rouge_l": 0.19753086419753085,
        "text_similarity": 0.608832597732544,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies E1 and E2 and their temporal relationship (E2 occurs within E1) and even quotes the phrase; it lacks the precise timestamps and slightly overstates timing as 'immediately after' rather than simply 'during', so minor detail is missing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom left) concludes his argument, when does the Presiding Justice (center top) begin asking if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1301.0,
        "end": 1302.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.2159999999998945,
        "end": 2.770999999999958,
        "average": 3.9934999999999263
      },
      "rationale_metrics": {
        "rouge_l": 0.15789473684210525,
        "text_similarity": 0.6269662380218506,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the speaker's conclusion and that the Presiding Justice speaks next, but it omits the provided timestamps and incorrectly claims an immediate, seamless transition\u2014whereas the reference shows a ~1.6s gap (E1 ends 1294.2s, E2 starts 1295.784s), so it contradicts the timing detail."
      }
    },
    {
      "question_id": "003",
      "question": "After the Justice (top right) states that the Nadel case came out in 1994, when does he ask if the court would have considered things the same way with the Filmon Supreme Court's decision?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1310.105,
        "end": 1318.758
      },
      "pred_interval": {
        "start": 1324.0,
        "end": 1326.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.894999999999982,
        "end": 7.241999999999962,
        "average": 10.568499999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.2045454545454545,
        "text_similarity": 0.6373711824417114,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies E1 and E2 and that E2 follows E1, but it omits the required timestamps and wrongly asserts E2 starts immediately after E1, contradicting the reference that E2 begins ~8 seconds later."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'And with that, I'd submit', when does the Presiding Justice ask if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1329.112,
        "end": 1330.232
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.327999999999975,
        "end": 31.00299999999993,
        "average": 32.16549999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.32941176470588235,
        "text_similarity": 0.6740633249282837,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that the Justice speaks immediately after the speaker and asks for other questions, but the absolute timestamps are substantially incorrect (off by ~34s), so key factual timing details do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the Presiding Justice asks if there are any other questions, when does Justice Sanchez start speaking about the Nadel case?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1300.609,
        "end": 1302.492
      },
      "pred_interval": {
        "start": 1330.528,
        "end": 1332.138
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.919000000000096,
        "end": 29.645999999999958,
        "average": 29.782500000000027
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.6338403224945068,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative order (Sanchez speaks after the question) but the absolute timestamps are substantially off (~31s) and it omits the intervening 'No' by Justice Marquardt; therefore it is largely incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "After Senator Cruz asks if the same principle applies to other protected characteristics, when does he ask if he could decide he was an Asian man?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 14.506,
        "end": 16.388
      },
      "pred_interval": {
        "start": 15.694,
        "end": 17.394
      },
      "iou": 0.2403047091412747,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1880000000000006,
        "end": 1.0059999999999967,
        "average": 1.0969999999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7027050852775574,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but misidentifies and mistimes the events: E1's span and content do not match the reference (wrong timing and quote), and E2's timing is offset from the ground truth. These key factual errors warrant a low score despite the correct relation."
      }
    },
    {
      "question_id": "002",
      "question": "Once Judge Jackson finishes explaining that she is not able to answer hypotheticals, when does Senator Cruz interrupt to re-ask his question about assessing standing?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 30.6,
        "end": 32.439
      },
      "pred_interval": {
        "start": 30.654,
        "end": 31.814
      },
      "iou": 0.6307775965198483,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.053999999999998494,
        "end": 0.625,
        "average": 0.33949999999999925
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.7584432363510132,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the events and correct ordering (interruption follows her statement) and uses a similar temporal relation, but the timestamps are off by about 1\u20131.2s and it omits the E2 end time provided in the reference, so it is only a partially accurate match."
      }
    },
    {
      "question_id": "003",
      "question": "While Judge Jackson is explaining how she would assess standing, when does she state that she would 'consider the relevant precedents'?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 45.0,
        "end": 47.8
      },
      "pred_interval": {
        "start": 45.444,
        "end": 47.134
      },
      "iou": 0.6035714285714284,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.4440000000000026,
        "end": 0.6659999999999968,
        "average": 0.5549999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2682926829268293,
        "text_similarity": 0.7089641094207764,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly locates both phrases roughly in the same time window (the anchor and the 'relevant precedents' phrase fall within the correct intervals), but it misrepresents E1 as a single time rather than the full span and incorrectly labels the temporal relation as 'after' when the correct relation is 'during.'"
      }
    },
    {
      "question_id": "001",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain why that detective asked her to do it?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 37.335,
        "end": 44.121
      },
      "pred_interval": {
        "start": 35.68,
        "end": 38.19
      },
      "iou": 0.10129131619476325,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.6550000000000011,
        "end": 5.9310000000000045,
        "average": 3.793000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317074,
        "text_similarity": 0.6776458621025085,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the relation ('after') and roles of E1/E2, but the time stamps are substantially inaccurate (E1 is placed much later than the ground truth and E2 is marked earlier than its true start), and the claim of immediate continuation is inconsistent with the reference intervals."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mickey Haller finishes stating that the witness pointed to Detective Lee Langford, when does Langford begin his angry outburst?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 66.887,
        "end": 72.795
      },
      "pred_interval": {
        "start": 62.92,
        "end": 66.33
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9669999999999987,
        "end": 6.465000000000003,
        "average": 5.216000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.638088583946228,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'immediately after' but the timestamps are significantly incorrect (predicted E1/E2 around 62.92s vs correct E1 end 66.867s and E2 start 66.887s) and it omits the E2 end time, so key factual timing is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Detective Lee Langford's angry outburst about the witness, when does the judge declare the court in recess?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 82.826,
        "end": 85.59
      },
      "pred_interval": {
        "start": 78.74,
        "end": 81.77
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0859999999999985,
        "end": 3.8200000000000074,
        "average": 3.953000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.7052100300788879,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and relation ('after') but gives substantially incorrect timestamps (E1 end and E2 start differ by several seconds), omits E2's end time, and incorrectly asserts the judge speaks immediately after the outburst, contradicting the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mickey Haller finishes asking Detective Pettis if she was in the Bonaventure Hotel on October 20th, when does Detective Pettis answer?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 16.239,
        "end": 16.76
      },
      "pred_interval": {
        "start": 14.8,
        "end": 15.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.439,
        "end": 1.2600000000000016,
        "average": 1.3495000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.30434782608695654,
        "text_similarity": 0.7967826724052429,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relationship ('immediately after') right but the timestamps are significantly off (predicted anchor 14.8s vs truth 16.219s; predicted target 15.0\u201315.5s vs truth 16.239\u201316.76s), so it is factually incorrect on the key temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain her personal motivation for complying?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 46.707,
        "end": 55.417
      },
      "pred_interval": {
        "start": 25.0,
        "end": 27.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.707,
        "end": 28.217000000000002,
        "average": 24.962000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.23762376237623764,
        "text_similarity": 0.7708964347839355,
        "llm_judge_score": 0,
        "llm_judge_justification": "The prediction misidentifies both anchor and target content and timestamps (places them at ~25\u201327s) whereas the correct anchor is at 32.008s and the target is 46.707\u201355.417s; the predicted temporal relationship ('same time as') directly contradicts the correct 'target happens after anchor.'"
      }
    },
    {
      "question_id": "003",
      "question": "After Mickey Haller asks if the man Detective Pettis made a deal with is in the courtroom, when does she point him out?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 61.92,
        "end": 62.701
      },
      "pred_interval": {
        "start": 62.3,
        "end": 63.4
      },
      "iou": 0.2709459459459488,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.37999999999999545,
        "end": 0.6989999999999981,
        "average": 0.5394999999999968
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.7874852418899536,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives incorrect timestamps and temporal relation (saying they are simultaneous when the target actually occurs after the anchor) and also misattributes dialogue; therefore it is largely incorrect despite matching actors."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chaturved introduces Mr. Uday Hula, when does he state that Mr. Uday Hula has become a popular name pan India and beyond?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.33,
        "end": 43.1
      },
      "pred_interval": {
        "start": 42.76,
        "end": 47.44
      },
      "iou": 0.055646481178396635,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4299999999999997,
        "end": 4.339999999999996,
        "average": 2.884999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.20253164556962022,
        "text_similarity": 0.5675234794616699,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the anchor, the quoted sentence, and the 'after' relation, but the provided timestamps differ from the reference (E2 starts later and ends much later) and the anchor timestamp is omitted, so timing accuracy is incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Vikas Chaturved says 'Over to you Mr. Trikram', when does Mr. Trikram start speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 153.433,
        "end": 154.776
      },
      "pred_interval": {
        "start": 59.2,
        "end": 59.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 94.23299999999999,
        "end": 94.97600000000001,
        "average": 94.6045
      },
      "rationale_metrics": {
        "rouge_l": 0.38554216867469876,
        "text_similarity": 0.8042216300964355,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speakers and that Trikram speaks after Vikas, but the timestamps are drastically incorrect (59.2/59.8s vs 151.953/153.433s) and the timing/relation details are therefore factually wrong; this major discrepancy warrants a very low score."
      }
    },
    {
      "question_id": "003",
      "question": "After Mr. Trikram finishes his welcome, when does Mr. Uday Holla begin his speech?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "pred_interval": {
        "start": 72.4,
        "end": 73.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 96.6,
        "end": 98.8,
        "average": 97.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2784810126582279,
        "text_similarity": 0.7332669496536255,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer matches only the temporal relation ('after') but contradicts key factual elements: the timestamps for both events are substantially different from the ground truth (72.4s/73.2s vs 147.207s/169\u2013172s) and it misreports the speaker start time and duration, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker emphasizes that facts are important, when does he state that a lawyer must have the patience of a crane?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 354.8
      },
      "pred_interval": {
        "start": 24.84,
        "end": 26.24
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 327.16,
        "end": 328.56,
        "average": 327.86
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.6621951460838318,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their temporal relation ('after'), but the provided timestamps are far off from the ground-truth intervals, so the answer is largely incorrect for a video-timestamping task."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that appearing for government becomes more difficult, when does he recount his personal experience as Advocate General?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.1,
        "end": 412.9
      },
      "pred_interval": {
        "start": 122.7,
        "end": 123.78
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 281.40000000000003,
        "end": 289.12,
        "average": 285.26
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.6047073602676392,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right and partially identifies the Advocate General anecdote, but both event timestamps are far off and the anchor (E1) content/timing does not match the reference, so key factual alignment fails."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a lawyer must know the law to draft better, when does he offer an illustration?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 504.9,
        "end": 506.3
      },
      "pred_interval": {
        "start": 153.52,
        "end": 154.74
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 351.38,
        "end": 351.56,
        "average": 351.47
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.6127340793609619,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterances and that the illustration comes after the statement, but the timestamps are completely wrong (off by several minutes) and thus key factual timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that there has been an amendment, when does he state that everment is taken out?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.3,
        "end": 532.0
      },
      "pred_interval": {
        "start": 549.828,
        "end": 555.168
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.52800000000002,
        "end": 23.168000000000006,
        "average": 21.848000000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.7525303363800049,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the sequence and wording (anchor then immediate follow-up) but the timestamps are substantially incorrect and the target duration is much longer than the reference, so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that readiness and willingness was mandatory prior to 2018, when does he explicitly state that the suit was liable to be dismissed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.568,
        "end": 583.193
      },
      "pred_interval": {
        "start": 579.068,
        "end": 585.268
      },
      "iou": 0.5846774193548344,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 2.0750000000000455,
        "average": 1.2875000000000227
      },
      "rationale_metrics": {
        "rouge_l": 0.3300970873786408,
        "text_similarity": 0.7473641633987427,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misplaces the anchor event (should be 533.4\u2013553.9s but predicted at 579.068s) and shifts the target timing later than the reference (predicted 583.488\u2013585.268s vs. 579.568\u2013583.193s), and it incorrectly characterizes the temporal proximity (predicted 'immediately following' vs. the reference's much later occurrence)."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the balance of the purchase amount, when does he explain the necessity for the plaintiff to prove they had the necessary funds?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 634.565,
        "end": 644.056
      },
      "pred_interval": {
        "start": 597.168,
        "end": 606.628
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.39700000000005,
        "end": 37.428,
        "average": 37.41250000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2352941176470588,
        "text_similarity": 0.6771876811981201,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target utterances and their ordering, but the provided timestamps are substantially different from the ground-truth intervals (off by ~30s), so the temporal alignment is incorrect and the answer is therefore largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a well-drafted plaint helps portray personality to the judge, when does he begin talking about the second benefit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.9,
        "end": 708.7
      },
      "pred_interval": {
        "start": 690.0,
        "end": 712.0
      },
      "iou": 0.35454545454545766,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.899999999999977,
        "end": 3.2999999999999545,
        "average": 7.099999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.2716049382716049,
        "text_similarity": 0.7246096134185791,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the sequence (second benefit follows the first) but gives incorrect and conflicting timestamps (E1 end and E2 start differ substantially from the reference) and adds an unfounded later start, so it fails to match the ground-truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a civil case lingers for a number of years, when does he explain that one becomes a senior by the time the case comes up for evidence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 719.5,
        "end": 724.9
      },
      "pred_interval": {
        "start": 720.0,
        "end": 735.0
      },
      "iou": 0.31612903225806305,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 10.100000000000023,
        "average": 5.300000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272724,
        "text_similarity": 0.7296158075332642,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct semantic relation and content (the consequence after cases linger) but the anchor/target time intervals are significantly misaligned with the reference, so the key factual timestamps are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing client satisfaction with plaint length and associated fees, when does he introduce the strategy of putting facts at the beginning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 795.264,
        "end": 805.511
      },
      "pred_interval": {
        "start": 800.0,
        "end": 806.0
      },
      "iou": 0.5133196721311449,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.73599999999999,
        "end": 0.48900000000003274,
        "average": 2.6125000000000114
      },
      "rationale_metrics": {
        "rouge_l": 0.24390243902439027,
        "text_similarity": 0.7246935963630676,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the ordering (E2 occurs after E1) and roughly the target interval, but the timestamps are several seconds off (predicts E1 at ~798s vs 794.0s and E2 start at 800.0s vs 795.264s), so it is not fully accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Supreme Court, when does he state the specific paragraph number of the judgment?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 925.404,
        "end": 950.0
      },
      "pred_interval": {
        "start": 890.0,
        "end": 893.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.403999999999996,
        "end": 57.0,
        "average": 46.202
      },
      "rationale_metrics": {
        "rouge_l": 0.22727272727272727,
        "text_similarity": 0.8132805228233337,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction incorrectly identifies both anchor and target time spans and mislabels the anchor phrase (case citation vs 'the Supreme Court'); while it preserves the 'after' relation, the key factual elements (correct timestamps and quoted phrases) are wrong, so it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing Justice Sanjay Kishan Kaul's judgment, when does he mention the 'Ren and Martin principle of pressie writing'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.581,
        "end": 990.021
      },
      "pred_interval": {
        "start": 935.0,
        "end": 940.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.58100000000002,
        "end": 50.02099999999996,
        "average": 50.80099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.46938775510204084,
        "text_similarity": 0.9296160936355591,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the quoted phrases and the temporal relation ('after'), but the provided timestamps are substantially incorrect (off by ~46\u201351 seconds) compared to the ground truth, so it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using 'short paragraphs, short sentences, direct sentences' in a plaint, when does he warn that a judge will lose patience with lengthy paragraphs?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1005.669,
        "end": 1012.012
      },
      "pred_interval": {
        "start": 965.0,
        "end": 970.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.66899999999998,
        "end": 42.011999999999944,
        "average": 41.34049999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.7243578433990479,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the quoted phrases and the temporal relation ('after'), but the provided timestamps are substantially incorrect compared to the reference (off by ~40s), so it is factually inaccurate on the required timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a civil dispute is always before a civil court, when does he emphasize that the lawyer must be thorough with the civil procedure code?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1077.5,
        "end": 1083.7
      },
      "pred_interval": {
        "start": 1123.89,
        "end": 1136.72
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.3900000000001,
        "end": 53.01999999999998,
        "average": 49.70500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.3111111111111111,
        "text_similarity": 0.6789774894714355,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect timestamps (\u22481124s vs correct 1071\u20131083s) and the wrong relation: the reference states the target occurs after the anchor, whereas the prediction claims overlap/during; this contradicts the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that settlement will not benefit in the short term, when does he explain how it will benefit in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1250.671,
        "end": 1254.734
      },
      "pred_interval": {
        "start": 1223.08,
        "end": 1255.06
      },
      "iou": 0.1270481550969316,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.591000000000122,
        "end": 0.32600000000002183,
        "average": 13.958500000000072
      },
      "rationale_metrics": {
        "rouge_l": 0.391304347826087,
        "text_similarity": 0.7277052402496338,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction partially identifies the same utterance about short-term vs long-term benefits, but it gives incorrect timestamps (anchor and target starts differ substantially from the reference) and wrongly labels the relation as 'during' instead of the target occurring after the anchor."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that the entire procedure for civil disputes is in the civil procedure code, when does he mention that states also have civil rules of practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1098.4,
        "end": 1101.7
      },
      "pred_interval": {
        "start": 1162.8,
        "end": 1167.25
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.39999999999986,
        "end": 65.54999999999995,
        "average": 64.97499999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.6063097715377808,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect time spans (\u22481162\u20131167s vs reference 1085\u20131101s) and mislabels the relation as 'during' with overlapping spans, whereas the reference has distinct anchor and target times with the target occurring after the anchor; thus it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how settlement will benefit you, when does he state that it will definitely help in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1238.9,
        "end": 1241.9
      },
      "pred_interval": {
        "start": 1258.89,
        "end": 1260.42
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.99000000000001,
        "end": 18.519999999999982,
        "average": 19.254999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.6941457986831665,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct utterances semantically, but the timestamps are substantially off (by ~20\u201326s) and the relationship is labeled only as 'after' rather than the correct 'immediately follows', so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker first mentions there's a saying in Kannada, when does he explain the saying about winners and losers in court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1273.6,
        "end": 1278.4
      },
      "pred_interval": {
        "start": 1278.95,
        "end": 1287.98
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.350000000000136,
        "end": 9.579999999999927,
        "average": 7.465000000000032
      },
      "rationale_metrics": {
        "rouge_l": 0.2388059701492537,
        "text_similarity": 0.7310446500778198,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction references the same saying and the 'after' relation, but the anchor and target time intervals are substantially shifted (and effectively swapped) compared to the ground truth, and it even misstates 'Kannada' as 'Canada'; therefore it is largely incorrect. "
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker begins discussing the drafting of the plaint, when does he advise to draft it in a professional manner?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1319.385,
        "end": 1344.757
      },
      "pred_interval": {
        "start": 1328.35,
        "end": 1331.58
      },
      "iou": 0.12730569131325908,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.964999999999918,
        "end": 13.177000000000135,
        "average": 11.071000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.8719474077224731,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the phrase about drafting the plaint but mislocates the anchor (1328.35 vs correct 1315.8\u20131319.5) and severely shortens the target span (1328.35\u20131331.58 vs correct 1319.385\u20131344.757); the relationship labeling is therefore also incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states 'You have order two, rule two, which specifies that', when does he elaborate on what it specifies?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.147,
        "end": 1451.484
      },
      "pred_interval": {
        "start": 1489.5,
        "end": 1501.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.353000000000065,
        "end": 49.81600000000003,
        "average": 50.08450000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818185,
        "text_similarity": 0.7466709613800049,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the ordering (the elaboration happens after the anchor) and marks both events, but its timestamps are substantially different from the reference (off by roughly 50s) and it includes phrasing/details not present in the ground truth, so it is not temporally accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'life of the lawyer becomes easier', when does he list Order Six, Seven, and Eight?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1460.096,
        "end": 1410.578
      },
      "pred_interval": {
        "start": 1510.3,
        "end": 1517.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.20399999999995,
        "end": 107.32200000000012,
        "average": 78.76300000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.379746835443038,
        "text_similarity": 0.774325966835022,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but the timestamps are off by about 50 seconds (major factual timing errors); while it captures the sequence ('after'), the event boundaries do not align with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Then comes the stage of evidence', when does he explain that a lawyer must sit with clients to understand what kind of evidence to lead?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1553.662,
        "end": 1566.557
      },
      "pred_interval": {
        "start": 1551.0,
        "end": 1565.3
      },
      "iou": 0.7480876775727909,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 2.6620000000000346,
        "end": 1.2570000000000618,
        "average": 1.9595000000000482
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7682653665542603,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target explanation timing (E2 start closely matches and end is slightly early) and the 'after' relationship, but the anchor (E1) timestamp is substantially off (~30s later) compared to the reference, so the pairing is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the written statement, when does he begin talking about Order 8?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.811,
        "end": 1624.02
      },
      "pred_interval": {
        "start": 1631.96,
        "end": 1634.88
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.149000000000115,
        "end": 10.860000000000127,
        "average": 13.504500000000121
      },
      "rationale_metrics": {
        "rouge_l": 0.3050847457627119,
        "text_similarity": 0.7649365067481995,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the 'after' relation but the crucial timestamps are significantly off (reference E1 ends ~1606s vs predicted 1631.96s; reference E2 starts ~1615.81s vs predicted 1634.88s), and it introduces a quoted phrase not present in the ground truth, so it is factually incorrect on key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a general denial is not sufficient, when does he mention the mistake lawyers normally commit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1651.02,
        "end": 1671.11
      },
      "pred_interval": {
        "start": 1672.68,
        "end": 1678.08
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.660000000000082,
        "end": 6.970000000000027,
        "average": 14.315000000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.42666666666666664,
        "text_similarity": 0.8189256191253662,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same utterance content and the 'after' relation, but it gives substantially incorrect timestamps for both E1 and E2 (both are shifted ~22\u201328s later than the ground truth), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that a lawyer must know the law, when does he explain the specific areas a lawyer should be thorough with?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.607,
        "end": 1763.816
      },
      "pred_interval": {
        "start": 1731.92,
        "end": 1746.56
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.686999999999898,
        "end": 17.256000000000085,
        "average": 21.971499999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.4935064935064935,
        "text_similarity": 0.6582324504852295,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relationship, but the provided timestamps for both E1 and E2 substantially disagree with the reference, so the key factual timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "When is the next time the speaker mentions an 'Order six, Rule' after mentioning 'Order six, Rule four'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.2,
        "end": 1830.9
      },
      "pred_interval": {
        "start": 1800.0,
        "end": 1820.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.200000000000045,
        "end": 10.900000000000091,
        "average": 19.050000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.3023255813953488,
        "text_similarity": 0.7671277523040771,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the next rule mentioned ('Order six, Rule eight') and the 'after' relationship, but the timestamps conflict with the ground truth and it adds an unfounded claim about the speaker correcting himself, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker explains that fraud and undue influence require specific pleas, when does he state that a general plea is not sufficient?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1802.1,
        "end": 1806.5
      },
      "pred_interval": {
        "start": 1836.0,
        "end": 1845.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.90000000000009,
        "end": 38.5,
        "average": 36.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.25287356321839083,
        "text_similarity": 0.727928876876831,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the content and sequence (specific-plea rule then 'general plea is not sufficient') but both E1 and E2 timestamps are substantially off from the reference (predicted ~1836\u20131843s vs. ground truth ~1789.8\u20131806.5s), so temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using short sentences and small paragraphs in a written statement, when does he transition to discussing 'evidence'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1908.4,
        "end": 1914.4
      },
      "pred_interval": {
        "start": 1954.0,
        "end": 1965.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.59999999999991,
        "end": 50.59999999999991,
        "average": 48.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951805,
        "text_similarity": 0.5719922780990601,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the topics (advice then evidence) but gives substantially incorrect timestamps and wrongly claims an immediate transition/explicit 'Then' cue; it therefore fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that leading questions are to be eschewed, when does he advise on how to prepare for examination?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1964.967,
        "end": 1965.937
      },
      "pred_interval": {
        "start": 2027.1,
        "end": 2032.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.13299999999981,
        "end": 66.0630000000001,
        "average": 64.09799999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.6722083687782288,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies the anchor (it labels the \"prepare well\" line as the leading-question statement), gives incorrect timestamps, and asserts the events are simultaneous, contradicting the reference which places the \"prepare well\" advice after the eschewing of leading questions; therefore it largely fails to match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes how some lawyers approach cross-examination without preparation, when does he explain what a good lawyer always does?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2010.4,
        "end": 2018.651
      },
      "pred_interval": {
        "start": 2032.0,
        "end": 2042.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.59999999999991,
        "end": 24.148999999999887,
        "average": 22.874499999999898
      },
      "rationale_metrics": {
        "rouge_l": 0.22448979591836735,
        "text_similarity": 0.5359668135643005,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the phrase that introduces what a good lawyer does, but it mislabels the segments, gives incorrect timestamps, and incorrectly states the temporal relation (saying 'same moment' instead of 'once finished'), so it largely disagrees with the reference."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's explanation of pitfalls when unprepared for cross-examination, when does he mention forgetting to ask relevant questions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2044.393,
        "end": 2049.878
      },
      "pred_interval": {
        "start": 2055.7,
        "end": 2058.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.306999999999789,
        "end": 8.422000000000025,
        "average": 9.864499999999907
      },
      "rationale_metrics": {
        "rouge_l": 0.19148936170212766,
        "text_similarity": 0.6788600087165833,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the content (mentioning forgetting relevant questions) but the timestamps are substantially off and the relation ('the same moment') contradicts the reference ('during'); thus it fails on precise temporal alignment and relation. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that cross-examination is an art by itself, when does he state that watching a good cross-examiner will help enormously?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2187.557,
        "end": 2201.817
      },
      "pred_interval": {
        "start": 2139.6,
        "end": 2144.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.95699999999988,
        "end": 57.016999999999825,
        "average": 52.48699999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.4155844155844156,
        "text_similarity": 0.8312750458717346,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the anchor phrase and correct 'after' relationship and a similar target phrase, but both anchor and target timestamps are substantially incorrect and the predicted target omits the key word 'enormously', so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker describes law as a noble profession, when does he mention lawyers dedicating themselves to clients to ensure justice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2222.5,
        "end": 2233.2
      },
      "pred_interval": {
        "start": 2226.7,
        "end": 2232.8
      },
      "iou": 0.5700934579439689,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.199999999999818,
        "end": 0.3999999999996362,
        "average": 2.299999999999727
      },
      "rationale_metrics": {
        "rouge_l": 0.2337662337662338,
        "text_similarity": 0.6070079803466797,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events, the quoted phrase, and the 'within' relationship; its start times fall inside the reference intervals and convey the same meaning, but it omits end times and has small timestamp offsets."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that delays are endemic, when does he provide the reason as a call for settlement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2338.838,
        "end": 2346.208
      },
      "pred_interval": {
        "start": 2338.2,
        "end": 2340.0
      },
      "iou": 0.1451048951048662,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6380000000003747,
        "end": 6.208000000000084,
        "average": 3.423000000000229
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.8059048652648926,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies an immediate follow-up relationship, but it misstates the anchor timing (2338.2s vs correct 2330.838s), gives an incorrect and much shorter target span (ends 2340.0s vs 2346.208s), and thus omits key timing/content details from the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that delays are endemic, when does he ask to ensure settlements occur?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2346.0
      },
      "pred_interval": {
        "start": 2364.0,
        "end": 2372.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 26.199999999999818,
        "average": 25.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.819891631603241,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the request occurs after the 'delays are endemic' remark, but both event timestamps (and an end time for E1) are significantly different from the reference, so the answer is factually inaccurate on key details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker notes it's already 40 minutes, when does he state that he will give some time for questioning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.8,
        "end": 2371.0
      },
      "pred_interval": {
        "start": 2384.0,
        "end": 2392.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.199999999999818,
        "end": 21.0,
        "average": 18.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.4418604651162791,
        "text_similarity": 0.7454453706741333,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the speaker offers time for questions but gives incorrect timestamps (off by ~18s) and wrongly states the target is simultaneous with the anchor rather than immediately following it, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions postponing because he was busy, when does he thank someone for 'pestering' him to educate the lawyer community?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2391.454,
        "end": 2399.123
      },
      "pred_interval": {
        "start": 2425.0,
        "end": 2432.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.54599999999982,
        "end": 32.87699999999995,
        "average": 33.21149999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2588235294117647,
        "text_similarity": 0.6096869707107544,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the semantic order right (speaker cites being busy then thanks/pestering), but the timestamps are substantially off and it fails to note that the target immediately follows the anchor, so it is only a partial match."
      }
    },
    {
      "question_id": "001",
      "question": "How long does the speaker talk about the importance of reading entire judgments, not just highlighted portions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2578.041,
        "end": 2584.494
      },
      "pred_interval": {
        "start": 2525.0,
        "end": 2536.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.04100000000017,
        "end": 48.49400000000014,
        "average": 50.767500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.2962962962962963,
        "text_similarity": 0.45810866355895996,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relationship as a duration, but the start/end timestamps (2525.0\u20132536.0s) conflict with the correct times (2568.041\u20132578.041s) and thus provide incorrect temporal boundaries and duration."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes saying he will take questions, when does Mr. Vikas begin speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2614.902,
        "end": 2617.184
      },
      "pred_interval": {
        "start": 2657.0,
        "end": 2659.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.097999999999956,
        "end": 41.8159999999998,
        "average": 41.95699999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.2772277227722772,
        "text_similarity": 0.7886686325073242,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the order right but substantially misstates key facts: both E1 and E2 timestamps differ by ~40\u201360s and the anchor quote is wrong; it also omits the target end time and introduces a pause contrary to the asserted direct transition."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that online law lexicons are not sufficient, when does he directly say 'You must read. A lawyer must read.'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2522.2,
        "end": 2525.3
      },
      "pred_interval": {
        "start": 2509.0,
        "end": 2510.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.199999999999818,
        "end": 15.300000000000182,
        "average": 14.25
      },
      "rationale_metrics": {
        "rouge_l": 0.425531914893617,
        "text_similarity": 0.7902711629867554,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the sequence and that the phrase is an immediate follow-up, but it gives incorrect/large timestamps that conflict with the reference's precise timing, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he would fall asleep reading the Civil Procedure Code as it is, when does he explain that he would be enthusiastic if he had a case on hand?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2688.6,
        "end": 2699.0
      },
      "pred_interval": {
        "start": 2704.48,
        "end": 2715.16
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.88000000000011,
        "end": 16.159999999999854,
        "average": 16.019999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.7745346426963806,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the 'after' relationship, it misidentifies and swaps the anchor/target utterances and gives timestamps that are ~22s later than the ground truth, so the segment alignment is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'Specific relief act, what are the sections?', when does he advise to go to the AR manual?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2720.5,
        "end": 2722.3
      },
      "pred_interval": {
        "start": 2750.12,
        "end": 2755.12
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.61999999999989,
        "end": 32.81999999999971,
        "average": 31.2199999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.43478260869565216,
        "text_similarity": 0.7961608171463013,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer preserves the anchor/target utterances and the 'after' relationship, but the timestamps are substantially off from the reference and end times are omitted, so temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions he suffered from COVID, when does he start describing a doctor's experience during the peak of COVID?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.119,
        "end": 2850.7
      },
      "pred_interval": {
        "start": 2788.68,
        "end": 2793.12
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.439000000000306,
        "end": 57.57999999999993,
        "average": 38.009500000000116
      },
      "rationale_metrics": {
        "rouge_l": 0.27848101265822783,
        "text_similarity": 0.6616694331169128,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct utterances and the 'after' relationship, but the reported timestamps conflict substantially with the ground truth (roughly 12+ seconds earlier) and the prediction omits end times, so it is not accurately aligned."
      }
    },
    {
      "question_id": "001",
      "question": "Once Vikas Chatrath finishes asking how to structure arguments, when does Udaya Holla begin to explain what one must always remember?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.46,
        "end": 2963.1
      },
      "pred_interval": {
        "start": 2963.94,
        "end": 2965.06
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.48000000000002,
        "end": 1.9600000000000364,
        "average": 24.720000000000027
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.6699810028076172,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that Udaya speaks after Vikas and even quotes the target phrase, but the timestamps are substantially off (anchor/target times mismatch by ~50s) and the temporal relation is labeled 'after' rather than the correct immediate 'once_finished', so key factual timing and relation details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Udaya Holla suggests preparing a list of dates and synopsis, when does he mention that the High Court has adopted this practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2941.0,
        "end": 2942.8
      },
      "pred_interval": {
        "start": 2995.04,
        "end": 2996.32
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.039999999999964,
        "end": 53.51999999999998,
        "average": 53.77999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7576267719268799,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but the provided timestamps are substantially incorrect compared to the reference (predicted ~2995\u20132996s vs correct anchor 2929.5s and target 2941.0\u20132942.8s), so the factual timing is wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes asking how to specifically plead in a summary suit, when does Udaya Holla ask for clarification 'In a summary suit, is it?'",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2999.596,
        "end": 3000.717
      },
      "pred_interval": {
        "start": 3036.56,
        "end": 3037.62
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.96399999999994,
        "end": 36.90299999999979,
        "average": 36.93349999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.6236355304718018,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the order (Udaya asks after Vikas) but the timestamps are substantially incorrect (~38s later than the reference) and it fails to capture that Udaya's question is an immediate response; key factual timing details are therefore wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes lawyers arguing endlessly for hours on end, when does he state that the judge then sleeps?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3046.2,
        "end": 3047.7
      },
      "pred_interval": {
        "start": 3125.56,
        "end": 3129.36
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.36000000000013,
        "end": 81.66000000000031,
        "average": 80.51000000000022
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.8261419534683228,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the correct temporal relation ('after') but the event timestamps are substantially incorrect and do not match the ground truth anchoring, so the prediction fails to correctly locate the events despite getting the relation right."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker confirms the order number as '37, correct. Sorry.', when does the other speaker (Vikas Chaprath) begin to introduce a common question?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3157.242,
        "end": 3163.028
      },
      "pred_interval": {
        "start": 3188.04,
        "end": 3189.16
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.797999999999774,
        "end": 26.132000000000062,
        "average": 28.464999999999918
      },
      "rationale_metrics": {
        "rouge_l": 0.1978021978021978,
        "text_similarity": 0.778797447681427,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the verbal content and the 'after' relationship (the second speaker begins right after the first), but the timestamps are substantially incorrect (off by ~38 seconds) and it uses single timepoints rather than the referenced intervals, so it fails to match the ground truth timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that preliminary objections concerning maintainability must appear in the beginning of a written statement, when does he begin describing how lawyers often present their defense?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3301.7,
        "end": 3309.9
      },
      "pred_interval": {
        "start": 3232.92,
        "end": 3238.24
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 68.77999999999975,
        "end": 71.66000000000031,
        "average": 70.22000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.1894736842105263,
        "text_similarity": 0.7391071319580078,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the temporal relation ('after') and the content (rule then example), but both event timestamps are substantially wrong (E1 off by ~48s, E2 off by ~63s) and the claim that the example immediately follows contradicts the reference timing (the target actually begins ~15.5s after the anchor)."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker recommends setting out your own facts in the written statement, when does he mention preliminary objections?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3223.3,
        "end": 3224.548
      },
      "pred_interval": {
        "start": 3246.5,
        "end": 3250.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.199999999999818,
        "end": 25.452000000000226,
        "average": 24.326000000000022
      },
      "rationale_metrics": {
        "rouge_l": 0.1846153846153846,
        "text_similarity": 0.5239571332931519,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct but the event timestamps and quoted content are substantially wrong (off by ~27\u201330s and inconsistent wording), so it fails to match the key factual elements of the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions misjoinder or non-joinder of parties, when does he list territorial lack of jurisdiction as an objection?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3254.547,
        "end": 3258.914
      },
      "pred_interval": {
        "start": 3277.8,
        "end": 3285.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.253000000000156,
        "end": 26.385999999999967,
        "average": 24.819500000000062
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7085064649581909,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the semantic order (misjoinder then territorial jurisdiction) but the timecodes differ substantially from the reference (off by ~28s), and the relation/anchoring is imprecise, so it fails on crucial factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After Vikas finishes asking his question about the Advocate General's experience, when does Udaya Holla begin to respond about the State Council's disabilities?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3417.766,
        "end": 3429.231
      },
      "pred_interval": {
        "start": 3337.5,
        "end": 3340.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.26600000000008,
        "end": 89.23100000000022,
        "average": 84.74850000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.757337749004364,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relation, its timestamps are substantially incorrect (off by ~78 seconds), misplacing both when Vikas finishes and when Udaya Holla begins, so it does not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying the colloquial Kannada phrase, when does he translate it to English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.2,
        "end": 3417.7
      },
      "pred_interval": {
        "start": 3434.0,
        "end": 3442.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.800000000000182,
        "end": 24.300000000000182,
        "average": 22.550000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.1797752808988764,
        "text_similarity": 0.743324339389801,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the English translation immediately follows the Kannada phrase, but it gives timestamps that differ substantially from the ground truth (off by ~23s) and adds a quoted English line not present in the reference, so the timing and some factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes talking about the difficulties faced by state councils in tough situations, when does the second speaker say 'Vikram'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.82,
        "end": 3472.161
      },
      "pred_interval": {
        "start": 3477.0,
        "end": 3480.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.179999999999836,
        "end": 7.838999999999942,
        "average": 6.509499999999889
      },
      "rationale_metrics": {
        "rouge_l": 0.2469135802469136,
        "text_similarity": 0.6740567088127136,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the second speaker says 'Vikram' after the first speaker, but the provided time boundaries are substantially off (predicted ~3477\u20133478s vs. reference 3469.8\u20133471.82s) and it adds an unsupported end-point for the first speaker; thus the temporal alignment is incorrect despite the right ordering."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises Kannada medium students to acquire more knowledge of English, when does he suggest joining websites for better English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3527.318,
        "end": 3535.0
      },
      "pred_interval": {
        "start": 3493.0,
        "end": 3495.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.31800000000021,
        "end": 40.0,
        "average": 37.159000000000106
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.5979725122451782,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same utterances but gives substantially incorrect timestamps and the wrong temporal relation (claims immediate continuation at ~3494s vs the reference which places E2 at 3527.318s, ~28s later), so it contradicts key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker talks about drafting in Kannada, when does he emphasize having mastery over the Kannada language?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3590.3,
        "end": 3592.0
      },
      "pred_interval": {
        "start": 3621.0,
        "end": 3628.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.699999999999818,
        "end": 36.0,
        "average": 33.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.9201469421386719,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the phrase about mastery of Kannada but misstates both event timestamps and which clip is the anchor vs target, and incorrectly labels their temporal relationship as simultaneous instead of occurring immediately after; major alignment errors remain."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks about scheduling a lawyer's day, when does the first speaker refer to the question as a 'multi-million dollar question'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3696.0,
        "end": 3697.2
      },
      "pred_interval": {
        "start": 3713.0,
        "end": 3720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.0,
        "end": 22.800000000000182,
        "average": 19.90000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767116,
        "text_similarity": 0.7894441485404968,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the timestamps and speaker roles and labels the events as simultaneous, whereas the reference shows the target speech occurs clearly after the anchor; thus the essential temporal relationship and timings are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises taking books on management, when does he mention keeping one's wife happy at home?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3701.248,
        "end": 3706.6
      },
      "pred_interval": {
        "start": 3651.0,
        "end": 3658.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.24800000000005,
        "end": 48.59999999999991,
        "average": 49.42399999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.5161290322580646,
        "text_similarity": 0.7821016311645508,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contains the quoted line but assigns entirely different timestamps, swaps/mislabels the anchor/target events, and incorrectly states a 'simultaneous' relationship instead of the reference 'after' relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises lawyers on how to handle client pleas, when does he suggest setting out facts in the client's presence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.2,
        "end": 3750.22
      },
      "pred_interval": {
        "start": 3785.41,
        "end": 3787.85
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.210000000000036,
        "end": 37.63000000000011,
        "average": 36.42000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2708333333333333,
        "text_similarity": 0.5881036520004272,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the advice content ('set out the facts in his presence') but mislocates both anchor and target timestamps by ~35 seconds and gives an ambiguous relationship, so it does not accurately match the reference temporal localization."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that a judge would have three drafts of his judgment, when does he elaborate on what the first draft is for?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.51,
        "end": 3750.56
      },
      "pred_interval": {
        "start": 3836.49,
        "end": 3844.99
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.97999999999956,
        "end": 94.42999999999984,
        "average": 90.2049999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7457008361816406,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures that the speaker elaborates immediately after and gives the correct explanation content for the first draft, but the provided time stamps do not match the ground-truth intervals, so the answer is factually incorrect on the key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Palkiwala's initial lack of work, when does he refer to Justice Chawla's memoir?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3903.539,
        "end": 3917.722
      },
      "pred_interval": {
        "start": 3857.87,
        "end": 3862.72
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.669000000000324,
        "end": 55.00200000000041,
        "average": 50.335500000000366
      },
      "rationale_metrics": {
        "rouge_l": 0.25490196078431376,
        "text_similarity": 0.7601221203804016,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic link that Chawla's memoir is used to relate to Palkiwala's story, but it gives substantially incorrect timestamps and a much shorter target interval than the ground truth, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Make it a habit', when does he mention biting the nail?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3936.731,
        "end": 3942.304
      },
      "pred_interval": {
        "start": 3945.58,
        "end": 3948.58
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.848999999999705,
        "end": 6.27599999999984,
        "average": 7.562499999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.8074806928634644,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the verbal content and the immediate-after relationship right, but the timestamps are significantly off (anchor ~10s late and target ~12s late) so it does not match the correct temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker finishes requesting the first speaker to talk about staying fit, when does the first speaker repeat 'How to stay fit, physically fit, sir'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3986.204,
        "end": 3987.967
      },
      "pred_interval": {
        "start": 4050.43,
        "end": 4053.95
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.22599999999966,
        "end": 65.98299999999972,
        "average": 65.10449999999969
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951805,
        "text_similarity": 0.825891375541687,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both timestamps and the quoted target utterance (it gives a different line), so it hallucinates key factual details despite correctly labeling the temporal relationship as 'immediately after.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that 'fitness is absolutely essential in any profession', when does he explain that 'our work transcends the entire waking hours of our time'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4056.898,
        "end": 4064.789
      },
      "pred_interval": {
        "start": 4105.72,
        "end": 4113.59
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.822000000000116,
        "end": 48.80099999999993,
        "average": 48.811500000000024
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.8175874948501587,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same anchor and target utterances and the correct 'after' relationship, but the timestamps are significantly different from the reference (shifted by ~66s) and it omits the precise 'directly after' adjacency, so the answer is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the judge smiling for the first time, when does he explain why the judge smiled?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4157.778,
        "end": 4164.121
      },
      "pred_interval": {
        "start": 4150.0,
        "end": 4153.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.778000000000247,
        "end": 11.121000000000095,
        "average": 9.449500000000171
      },
      "rationale_metrics": {
        "rouge_l": 0.26829268292682923,
        "text_similarity": 0.7277014255523682,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation as 'after' but gives substantially incorrect timestamps for E2 (and an incorrect end time), omits the correct completion time, and adds/assumes wording not present in the reference, so only the coarse relation is preserved."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker uses a cricket analogy involving Kumble, when does he advise the audience to 'Go and observe'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4289.867,
        "end": 4291.509
      },
      "pred_interval": {
        "start": 4200.0,
        "end": 4203.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.86700000000019,
        "end": 88.50900000000001,
        "average": 89.1880000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.847155749797821,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the 'Go and observe' event occurs after the analogy, but the timestamps are substantially wrong (off by ~70s) and it incorrectly describes the advice as the conclusion of the analogy rather than a later event, so it largely mismatches the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker recommends reading Dale Carnegie's book, when does he mention other books on management techniques?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4206.04,
        "end": 4211.851
      },
      "pred_interval": {
        "start": 4212.0,
        "end": 4216.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.960000000000036,
        "end": 4.149000000000342,
        "average": 5.054500000000189
      },
      "rationale_metrics": {
        "rouge_l": 0.4,
        "text_similarity": 0.852394700050354,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor/target content and that the mention of other books comes after the Dale Carnegie recommendation, but the reported start/end timestamps and durations are several seconds off from the ground-truth intervals, so the timing is not accurately matched."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises lawyers not to sit in the canteen, when does he instruct them to sit in a court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4301.616,
        "end": 4305.419
      },
      "pred_interval": {
        "start": 4298.2,
        "end": 4300.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.4160000000001673,
        "end": 4.618999999999687,
        "average": 4.017499999999927
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.6913348436355591,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct semantic relation (the speaker immediately gives 'sit in court' as the alternative) but the temporal boundaries are incorrect and incomplete (timestamps differ substantially from the reference and the predicted answer omits the E2 end time), so it fails to match the precise span information."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nitika finishes asking her question on YouTube, when does the speaker ask for the question to be repeated?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4378.889,
        "end": 4380.233
      },
      "pred_interval": {
        "start": 4434.7,
        "end": 4436.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.810999999999694,
        "end": 56.06700000000001,
        "average": 55.93899999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.7092254161834717,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the speaker asks for a repeat immediately after Nitika and even gives the phrasing, but the reported timestamps are substantially wrong (off by ~57 seconds) compared to the reference, so it is not a correct match."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'Then which are the sections?' related to the Contract Act, when does he begin describing the illustration about a property purchase agreement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4437.734,
        "end": 4450.995
      },
      "pred_interval": {
        "start": 4454.9,
        "end": 4456.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.165999999999258,
        "end": 5.404999999999745,
        "average": 11.285499999999502
      },
      "rationale_metrics": {
        "rouge_l": 0.22500000000000003,
        "text_similarity": 0.6649623513221741,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but the timestamps are substantially incorrect and contradictory: it places E1/E2 ~50s later and claims 'immediately after', whereas the reference has E1 ending at 4402.161s and E2 starting at 4437.734s (not immediate), so the answer is factually wrong on timing."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he closes his case, when does he explain why?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4475.036,
        "end": 4480.501
      },
      "pred_interval": {
        "start": 4470.0,
        "end": 4485.0
      },
      "iou": 0.36433333333334306,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.036000000000058,
        "end": 4.498999999999796,
        "average": 4.767499999999927
      },
      "rationale_metrics": {
        "rouge_l": 0.326530612244898,
        "text_similarity": 0.6466048359870911,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly preserves the 'after' relation and captures the reason (bank/balance statement) but has notably different timestamps for both events and an inaccurate claim of an 'immediate' response; these timestamp and timing inaccuracies reduce correctness."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises lawyers to practice in trial court, when does he explain the main skill acquired there?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4559.082,
        "end": 4593.185
      },
      "pred_interval": {
        "start": 4502.0,
        "end": 4518.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.082000000000335,
        "end": 75.1850000000004,
        "average": 66.13350000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.1702127659574468,
        "text_similarity": 0.5737340450286865,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the overall relation ('after') and roughly that the speaker advises trial-court practice, but both event timestamps are far off and the second event is misidentified (it should explain the importance of cross-examination), so key factual content and timing are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that 'habit would be derived of five words', when does he proceed to list and explain those words?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4629.048,
        "end": 4640.287
      },
      "pred_interval": {
        "start": 4578.0,
        "end": 4590.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.047999999999774,
        "end": 50.28700000000026,
        "average": 50.66750000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3913043478260869,
        "text_similarity": 0.7269955277442932,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives substantially different timestamps (off by ~20\u201330+ seconds) and wrongly states the explanation occurs immediately after the anchor, whereas the correct answer places the listing much later; thus the temporal relation and times are largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking about working parallelly in High Court and Trial Court, when does another speaker give an affirmative answer?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4668.869,
        "end": 4673.473
      },
      "pred_interval": {
        "start": 4684.193,
        "end": 4685.193
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.324000000000524,
        "end": 11.720000000000255,
        "average": 13.52200000000039
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.7209790945053101,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies an immediate affirmative response and matches the relation, but the provided timestamps conflict significantly with the ground truth (large time offsets and inconsistent ordering), so the answer is largely incorrect on factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains how a youngster can gain practical experience in a smaller trial court office, when does he state this would not be possible in a larger office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4722.411,
        "end": 4727.419
      },
      "pred_interval": {
        "start": 4708.337,
        "end": 4710.107
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.073999999999614,
        "end": 17.311999999999898,
        "average": 15.692999999999756
      },
      "rationale_metrics": {
        "rouge_l": 0.24999999999999994,
        "text_similarity": 0.6169300079345703,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation and wording roughly right but the timestamps are significantly incorrect and inconsistent with the correct spans (anchor/end and target/start do not match the reference), so it fails to align precisely with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes sharing the Japanese quote about a frog in a well, when does another speaker interject with a related Sanskrit saying?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4760.624,
        "end": 4763.847
      },
      "pred_interval": {
        "start": 4744.012,
        "end": 4745.422
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.61200000000008,
        "end": 18.425000000000182,
        "average": 17.51850000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555556,
        "text_similarity": 0.6938875913619995,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the interjection occurs after the frog quote, but the timestamps are substantially incorrect (off by ~12\u201316s), it misstates that the anchor fully ends before the interjection (the original continues slightly), and it introduces a specific Sanskrit phrase not present in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks how a new associate creates space in a big office with many existing associates, when does the host add that the guest also has a very big office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4866.42,
        "end": 4872.823
      },
      "pred_interval": {
        "start": 4924.25,
        "end": 4928.65
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.82999999999993,
        "end": 55.826999999999316,
        "average": 56.82849999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.24175824175824173,
        "text_similarity": 0.6020095348358154,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction largely contradicts the reference: it misidentifies which utterance is the anchor vs. follow-up and gives timestamps ~58s off, only matching the generic 'after' relation but not the correct segments or timings."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (guest) finishes explaining that a junior lawyer catches the senior's eye by preparing research and synopses, when does the speaker pose a rhetorical question about how a junior lawyer could get attention by arriving late?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4940.902,
        "end": 4951.577
      },
      "pred_interval": {
        "start": 4987.2,
        "end": 4989.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.297999999999774,
        "end": 38.02300000000014,
        "average": 42.160499999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.17094017094017092,
        "text_similarity": 0.7645397782325745,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same rhetorical question and that it follows the prior remark, but the timestamps are substantially incorrect and the predicted 'immediately after' timing contradicts the ground-truth boundaries and durations, indicating factual mismatch."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (guest) emphasizes that hard work is the most important thing in the legal profession, when does the speaker begin to describe a scenario where a client observes their lawyer arguing a case diligently?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4985.39,
        "end": 4996.139
      },
      "pred_interval": {
        "start": 5013.3,
        "end": 5016.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.909999999999854,
        "end": 19.860999999999876,
        "average": 23.885499999999865
      },
      "rationale_metrics": {
        "rouge_l": 0.25000000000000006,
        "text_similarity": 0.8213934898376465,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both event timestamps and swaps the anchor/target: the anchor (hard work emphasis) and target (client-observation scenario) times do not match the reference, and the relation is incorrectly labeled as simultaneous rather than 'after'."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker emphasizes that youngsters must do hard work, when does the second speaker agree with the point about hard work?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5024.62,
        "end": 5033.21
      },
      "pred_interval": {
        "start": 5010.0,
        "end": 5020.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.61999999999989,
        "end": 13.210000000000036,
        "average": 13.914999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.18749999999999997,
        "text_similarity": 0.6567398309707642,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and that the second speaker agrees, but both event timestamps are substantially wrong (E1 ~14s early, E2 ~4.6s early) and the predicted answer adds a hallucinated quote/detail, so it is factually misaligned with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks how to master lower court procedures without much practice, when does the first speaker suggest sitting in courts to learn?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5044.49,
        "end": 5051.81
      },
      "pred_interval": {
        "start": 5032.0,
        "end": 5041.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.489999999999782,
        "end": 10.8100000000004,
        "average": 11.650000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.1839080459770115,
        "text_similarity": 0.6639713048934937,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the speakers' QA content and the 'after' relationship, but the provided timestamps are inaccurate (off by ~2\u20133.5s) and omit the end times given in the ground truth, so it is only a partial match."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker asks if one should have High Court experience before practicing in the Supreme Court, when does the first speaker suggest that one is better off going to the trial court first?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5126.422,
        "end": 5141.99
      },
      "pred_interval": {
        "start": 5052.0,
        "end": 5061.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 74.42199999999957,
        "end": 80.98999999999978,
        "average": 77.70599999999968
      },
      "rationale_metrics": {
        "rouge_l": 0.15841584158415845,
        "text_similarity": 0.5457116365432739,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the conversational content and the 'after' relation, but the provided timestamps are substantially incorrect (off by ~39\u201365 seconds) and do not match the reference intervals, so the temporal alignment is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the difference between 'learn' and 'earn', when does he say, 'So thank you everyone'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5198.8,
        "end": 5199.7
      },
      "pred_interval": {
        "start": 5197.182,
        "end": 5198.192
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.618000000000393,
        "end": 1.5079999999998108,
        "average": 1.5630000000001019
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.6851980686187744,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and the immediate-after relationship, but the timestamps are noticeably off (E1 end ~0.7s early and E2 start ~1.6s earlier than the reference), so it is not temporally precise."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker thanks 'Mr. Hola' and mentions it's always a pleasure connecting, when does the second speaker say 'Thank you very much'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5219.7,
        "end": 5221.2
      },
      "pred_interval": {
        "start": 5213.252,
        "end": 5213.852
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.447999999999411,
        "end": 7.347999999999956,
        "average": 6.8979999999996835
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.7299032211303711,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the second speaker's phrase and that it follows the first speaker, but it gives substantially incorrect timestamps (5213.252s vs. reference 5219.7s) and incorrectly claims simultaneous timing, contradicting the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker says 'You are the educator of lawyers', when is the next time the first speaker says 'Thank you'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5224.9,
        "end": 5226.9
      },
      "pred_interval": {
        "start": 5219.392,
        "end": 5219.692
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.507999999999811,
        "end": 7.207999999999629,
        "average": 6.35799999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.32653061224489793,
        "text_similarity": 0.7452253103256226,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference on speaker identities and timing (predicts 5219.392s vs correct 5224.9s) and mislabels who said 'You are the educator of lawyers'; it only matches the general idea of a following 'Thank you' but is otherwise incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Trivikram thanks Mr. Vikas Chaturvedi, when does he extend a warm welcome to Mr. Udaya Holla?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.288,
        "end": 166.858
      },
      "pred_interval": {
        "start": 203.8,
        "end": 207.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.512,
        "end": 40.141999999999996,
        "average": 40.327
      },
      "rationale_metrics": {
        "rouge_l": 0.18918918918918917,
        "text_similarity": 0.6314258575439453,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies and mis-times the events (welcome is given as the anchor at ~203.8s instead of thanks at ~153\u2013159s and the target welcome timing is incorrect), though it correctly states the 'after' relationship; major factual and timestamp errors warrant a low score."
      }
    },
    {
      "question_id": "002",
      "question": "During his discussion of strategies for a civil dispute, when does Mr. Udaya Holla state that preparation is what counts the most?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.8,
        "end": 254.67
      },
      "pred_interval": {
        "start": 294.0,
        "end": 296.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.19999999999999,
        "end": 41.72999999999999,
        "average": 41.96499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.557288408279419,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives incorrect timestamps and misidentifies the discussion anchor (should be E1 at 219.424s and E2 at 251.8\u2013254.67s); it also wrongly labels the relationship as 'same as' rather than E2 occurring within the broader discussion, so it fails temporal and relational alignment despite mentioning the quoted phrase."
      }
    },
    {
      "question_id": "001",
      "question": "After Vikas Chatrath finishes explaining the difference between 'learn' and 'earn', when does he thank everyone and wish them to stay safe and blessed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5198.086,
        "end": 5203.11
      },
      "pred_interval": {
        "start": 5202.987,
        "end": 5206.867
      },
      "iou": 0.0140075162281737,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.90099999999984,
        "end": 3.7570000000005166,
        "average": 4.329000000000178
      },
      "rationale_metrics": {
        "rouge_l": 0.275,
        "text_similarity": 0.7136759161949158,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the thanking occurs after the explanation, but the timestamps are substantially incorrect (anchor actually ends at 5196.08s vs predicted 5202.987s; target starts at 5198.086s vs predicted 5202.987s) and it wrongly asserts immediate adjacency, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "During the announcement of the next session, when does Vikas Chatrath mention Mr. Shingar Murali?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5207.213,
        "end": 5209.213
      },
      "pred_interval": {
        "start": 5210.478,
        "end": 5212.638
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.2650000000003274,
        "end": 3.425000000000182,
        "average": 3.3450000000002547
      },
      "rationale_metrics": {
        "rouge_l": 0.3880597014925374,
        "text_similarity": 0.7253555059432983,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the same anchor, target, and the 'during' relationship, but the provided timestamps are offset by about 3\u20134 seconds and the target timing/duration does not exactly match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes telling everyone to stay safe and blessed, when does he say it's always a pleasure connecting with Mr. Hola?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5201.609,
        "end": 5204.971
      },
      "pred_interval": {
        "start": 5217.187,
        "end": 5219.567
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.57799999999952,
        "end": 14.596000000000458,
        "average": 15.086999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.3421052631578947,
        "text_similarity": 0.5974122285842896,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the relative ordering ('immediately after') but the timestamps are substantially different from the reference (off by ~16s) and it omits the additional phrase ('and Thrikram and associates'), so key factual details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the prosecutor finishes introducing his opening statement, when does he explain why he gets to go first?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.329,
        "end": 50.118
      },
      "pred_interval": {
        "start": 37.5,
        "end": 42.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.829000000000001,
        "end": 7.2180000000000035,
        "average": 6.523500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.2,
        "text_similarity": 0.7553610801696777,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only partially overlaps the reference: it misidentifies the anchor timing (37.5s vs 41.646s), significantly misplaces and shortens the burden-of-proof segment (41.3\u201342.9s vs 43.329\u201350.118s), and states an incorrect relationship, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the prosecutor mentions the bartender, John Thomas, and the server, Roberta Jones, when does John observe Mr. Miller pass a small glass bottle?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 150.298,
        "end": 158.469
      },
      "pred_interval": {
        "start": 104.2,
        "end": 106.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.098,
        "end": 51.76899999999999,
        "average": 48.933499999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.7332062721252441,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps and event boundaries are far from the ground truth (104\u2013106s vs 134.772s and 150.298\u2013158.469s) and it incorrectly asserts an 'immediately after' relationship, so it contradicts key factual elements."
      }
    },
    {
      "question_id": "003",
      "question": "Once John observes the unknown man pull a gun on the defendant, when does the gunman pull out his shield and shout 'Police! You are under arrest!'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 176.477,
        "end": 185.645
      },
      "pred_interval": {
        "start": 132.6,
        "end": 138.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.87700000000001,
        "end": 47.14500000000001,
        "average": 45.51100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3269230769230769,
        "text_similarity": 0.8085993528366089,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('immediately after') right but the temporal annotations are largely incorrect and inconsistent with the reference (predicted times are ~40s earlier and different endpoints, and the anchor/target boundaries do not match the ground truth)."
      }
    },
    {
      "question_id": "001",
      "question": "After John observes Mr. Miller pass a small glass bottle with white powder, when does John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.6,
        "end": 166.8
      },
      "pred_interval": {
        "start": 150.0,
        "end": 165.0
      },
      "iou": 0.08333333333333362,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.599999999999994,
        "end": 1.8000000000000114,
        "average": 7.700000000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.4722222222222222,
        "text_similarity": 0.8408753871917725,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies both events and their temporal relation (after), and its E2 timestamp (165.0s) falls within the ground-truth interval; it omits the explicit end times and slightly differs from the precise E2 start (163.6s), so minor timing detail is missing."
      }
    },
    {
      "question_id": "002",
      "question": "After the officer gives chase and the defendant pushes him, when does the officer trip and hit his head, opening a big gash on his forehead?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 216.0,
        "end": 226.824
      },
      "pred_interval": {
        "start": 245.0,
        "end": 255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.0,
        "end": 28.175999999999988,
        "average": 28.587999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.819091796875,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the correct 'after' relationship, but the timestamps are substantially wrong (off by ~30s), so it fails to match the key factual timing details in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes sees the defendant exiting the saloon like a bat out of hell, when does she decide she should get back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 343.5
      },
      "pred_interval": {
        "start": 305.0,
        "end": 315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.0,
        "end": 28.5,
        "average": 28.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3505154639175258,
        "text_similarity": 0.8374006748199463,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right and captures that she decides to return, but both event timestamps are substantially incorrect and the quoted wording/timing of the target event does not match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Reyes finishes thinking 'something seems wrong', when does she think 'I should get back to that bar'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.3,
        "end": 340.5
      },
      "pred_interval": {
        "start": 330.0,
        "end": 334.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.300000000000011,
        "end": 6.5,
        "average": 6.900000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.4096385542168675,
        "text_similarity": 0.8123518824577332,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the correct temporal order (a subsequent/after relation) but the event timings are notably incorrect (E1 given at 330.0s vs 334.1\u2013336.0s, E2 at 334.0s vs 337.3\u2013340.5s) and the relation label is less precise than 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the car was seized, when does he mention a forensic technician finding cocaine residue?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.1,
        "end": 428.2
      },
      "pred_interval": {
        "start": 404.0,
        "end": 408.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.100000000000023,
        "end": 20.19999999999999,
        "average": 18.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.4081632653061224,
        "text_similarity": 0.9094828367233276,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the core events and the 'after' relation, but the timestamps are substantially off and it omits key details (E1 missing 'taken to the crime lab'; E2 missing residue locations), so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker specifies the time and date of the incident, when does he mention the car being seized?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 415.9,
        "end": 419.1
      },
      "pred_interval": {
        "start": 404.0,
        "end": 410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.899999999999977,
        "end": 9.100000000000023,
        "average": 10.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3820224719101123,
        "text_similarity": 0.8880995512008667,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same anchor/target events and correct temporal relation, but the provided timestamps are significantly different from the reference intervals and it omits the exact quoted spans, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the prosecution states that the defendant Carl Miller was meeting with his business clients, when does the evidence show that the defendant was in the saloon at 3:55 p.m.?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 510.41,
        "end": 510.45
      },
      "pred_interval": {
        "start": 510.0,
        "end": 530.0
      },
      "iou": 0.001999999999998181,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.410000000000025,
        "end": 19.55000000000001,
        "average": 9.980000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.47457627118644075,
        "text_similarity": 0.8353221416473389,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misstates the timestamps (both at 510.0s) and labels the events as simultaneous, while the reference shows E1 at 510.31\u2013510.38 and E2 at 510.41\u2013510.45; the timing and relationship are therefore incorrect and incomplete."
      }
    },
    {
      "question_id": "002",
      "question": "Once John observed Mr. Miller pass a small glass bottle with a white powder in it to his buddy, when did John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 631.0,
        "end": 638.07
      },
      "pred_interval": {
        "start": 580.0,
        "end": 590.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.0,
        "end": 48.07000000000005,
        "average": 49.535000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.3440860215053763,
        "text_similarity": 0.8417021036148071,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both events are incorrect and inconsistent with the reference (E1 at ~616.5s and E2 at 631\u2013638s), and it incorrectly asserts an 'immediately after' relation and specific audio cue that contradict the ground truth. This contains significant factual errors and timing misalignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the gunman shouted, 'Police! You are under arrest!', when did the defendant immediately take off running towards the exit?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 673.544,
        "end": 683.655
      },
      "pred_interval": {
        "start": 640.0,
        "end": 650.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.54399999999998,
        "end": 33.65499999999997,
        "average": 33.59949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.4042553191489362,
        "text_similarity": 0.791113018989563,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates both event time spans (they do not match the reference intervals) and incorrectly claims the defendant ran 'immediately after' the shout, even inventing an 'immediately' audio cue; these factual and temporal errors make the answer essentially incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Dr. Reyes sees the defendant exiting the saloon, when does she wonder if something is wrong?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.4,
        "end": 751.6
      },
      "pred_interval": {
        "start": 690.0,
        "end": 705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.39999999999998,
        "end": 46.60000000000002,
        "average": 51.5
      },
      "rationale_metrics": {
        "rouge_l": 0.27368421052631575,
        "text_similarity": 0.8996431827545166,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer uses entirely different timestamps and describes different actions for both anchor and target than the ground truth, so it fails to match the key events and timing; only the generic 'after' relation aligns. Significant factual mismatches in content and timing warrant a very low score."
      }
    },
    {
      "question_id": "002",
      "question": "After the defendant jumped into a convertible, when does he look again at Dr. Reyes while starting the car?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.3,
        "end": 773.5
      },
      "pred_interval": {
        "start": 730.0,
        "end": 735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.299999999999955,
        "end": 38.5,
        "average": 38.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.35789473684210527,
        "text_similarity": 0.8555136919021606,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but gives substantially incorrect timestamps and flips the temporal relation to 'while/during' instead of the correct 'after', so it fails on key factual elements (timing and relation)."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes writes down the Mustang's plate number, when does she decide to go back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 797.9,
        "end": 808.8
      },
      "pred_interval": {
        "start": 760.0,
        "end": 770.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.89999999999998,
        "end": 38.799999999999955,
        "average": 38.349999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8863804340362549,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct events and the 'after' relation, but the timestamp boundaries are substantially incorrect (off by ~40s and with wrong durations), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"You will not hear one single person,\" when does the speaker mention the date \"September 8th, 2020\"?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 882.8,
        "end": 885.6
      },
      "pred_interval": {
        "start": 937.8,
        "end": 945.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.0,
        "end": 59.60000000000002,
        "average": 57.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32098765432098764,
        "text_similarity": 0.7778521776199341,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the target occurs after the anchor, both timestamps are inaccurate (especially E2, off by ~55s) and it adds an unsupported audio-cue claim; key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that the car was seized and taken to the crime lab, when does the speaker begin describing the finding of cocaine residue by a forensic technician?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 891.0,
        "end": 904.4
      },
      "pred_interval": {
        "start": 952.2,
        "end": 956.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.200000000000045,
        "end": 52.30000000000007,
        "average": 56.75000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3218390804597701,
        "text_similarity": 0.7694347500801086,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states the target begins immediately after the anchor, but the provided timestamps are substantially incorrect (off by ~54\u201361 seconds) and thus factually wrong about when each segment occurs."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's statement about finding the defendant Carl Miller guilty of felonies, when is \"fleeing and eluding\" specifically mentioned?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 930.8,
        "end": 947.9
      },
      "pred_interval": {
        "start": 968.1,
        "end": 971.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.30000000000007,
        "end": 23.5,
        "average": 30.400000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.2653061224489796,
        "text_similarity": 0.8186807036399841,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relationship (target embedded in the anchor) but the timestamps are significantly incorrect compared to the ground truth, misplacing both the anchor and the 'fleeing and eluding' segment by tens of seconds, so it fails factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks the witness to state her name and spell her last name, when does the witness spell out her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.561,
        "end": 36.805
      },
      "pred_interval": {
        "start": 26.815,
        "end": 30.025
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.745999999999999,
        "end": 6.780000000000001,
        "average": 6.763
      },
      "rationale_metrics": {
        "rouge_l": 0.18604651162790697,
        "text_similarity": 0.6845411658287048,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the content of the witness's response but gives incorrect timings and relationship: E1 timing is slightly off and E2 is placed immediately after E1 (29.455\u201330.025s) whereas the correct E2 occurs much later (33.561\u201336.805s). These timing and relation errors materially contradict the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks the witness if she was working as a plumber at a construction site on the evening of October 27th, 2020, when does the witness describe finding the broken window of her vehicle?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 69.075,
        "end": 74.643
      },
      "pred_interval": {
        "start": 64.915,
        "end": 74.145
      },
      "iou": 0.5211759868421049,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.159999999999997,
        "end": 0.49800000000000466,
        "average": 2.3290000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.17204301075268816,
        "text_similarity": 0.6573861241340637,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (male's question and the witness describing the broken window) but gives substantially incorrect timestamps and an incorrect temporal relation (overlap/direct answer vs. correct 'after'), so the alignment is largely wrong despite partial content match."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker asks the witness to describe the area where she parked her car and if she was worried about its safety, when does the witness explain that she was not worried because she always parked near a bar?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.14,
        "end": 128.77
      },
      "pred_interval": {
        "start": 94.135,
        "end": 110.955
      },
      "iou": 0.023531110148693447,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.004999999999995,
        "end": 17.815000000000012,
        "average": 16.910000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.18348623853211007,
        "text_similarity": 0.6444717645645142,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the semantic relation (E2 answers E1) but the temporal boundaries are substantially incorrect and mostly do not match the reference (predicted times are ~10\u201313s earlier with only minimal overlap), so it fails to align with the correct timestamps."
      }
    },
    {
      "question_id": "001",
      "question": "After Ms. Mendoza reports that her tablet and ice skates were stolen, when does the lawyer ask her if she made any attempt to look for the perpetrators or immediately contacted the police?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 164.191,
        "end": 172.955
      },
      "pred_interval": {
        "start": 150.0,
        "end": 153.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.191000000000003,
        "end": 19.155,
        "average": 16.673000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.23684210526315788,
        "text_similarity": 0.6945740580558777,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly identifies that the lawyer asks about her actions and the relation 'after', it significantly misstates the event timestamps (claims E1 ends at 150.0s and E2 starts at 150.0s\u2013153.8s) which contradicts the ground truth timings (E1 ends at 162.133s, E2 starts at 164.191s) and wrongly asserts immediacy. These large timing errors and omission of the correct event durations make the answer largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza explains that she noticed a suspicious man around her car while on the phone with the operator, when does the lawyer ask her what the officer did?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 261.353,
        "end": 262.466
      },
      "pred_interval": {
        "start": 246.2,
        "end": 247.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.15300000000002,
        "end": 14.665999999999997,
        "average": 14.909500000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.28235294117647053,
        "text_similarity": 0.7215033769607544,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after') but the event timestamps are substantially incorrect (predicted E1/E2 around 246s\u2013247.8s vs correct 229.555\u2013239.949s and 261.353\u2013262.466s), so it fails to match the factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After Ms. Mendoza states that the thief got out of the car and started running down the street, when does she describe the thief punching the arresting officer?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 321.13,
        "end": 331.68
      },
      "pred_interval": {
        "start": 274.8,
        "end": 278.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.329999999999984,
        "end": 53.48000000000002,
        "average": 49.905
      },
      "rationale_metrics": {
        "rouge_l": 0.21621621621621623,
        "text_similarity": 0.6473193168640137,
        "llm_judge_score": 3,
        "llm_judge_justification": "While both answers label the relation as 'after', the predicted timestamps are substantially incorrect and assert the assault was described immediately after the run, contradicting the ground truth timing (E2 occurs much later after apprehension/resistance). Key factual timing and context are therefore missing/incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza to point out the man who assaulted the officer, when does Ms. Mendoza describe him?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 349.986,
        "end": 355.253
      },
      "pred_interval": {
        "start": 360.4,
        "end": 361.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.413999999999987,
        "end": 6.5470000000000255,
        "average": 8.480500000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.21568627450980393,
        "text_similarity": 0.6587983965873718,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures Ms. Mendoza's verbal description ('skinny... gray hair'), but it misidentifies the lawyer's question (anchor) and gives an incorrect timestamp for when she begins describing the man; the relation/timing claim ('immediately after') is more specific than supported. These substantive mismatches justify a low\u2013moderate score."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking if the man settled down and cooperated after being put in the patrol car, when does Ms. Mendoza reply that he did not?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 457.548,
        "end": 460.233
      },
      "pred_interval": {
        "start": 462.8,
        "end": 466.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.2520000000000095,
        "end": 6.166999999999973,
        "average": 5.709499999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.20408163265306126,
        "text_similarity": 0.6764898896217346,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the lawyer's question and Ms. Mendoza's direct negative reply and the immediate temporal relation, but it gives an incorrect E2 timestamp (~462.8s vs 457.548s) and omits the precise E1 timing, so it is semantically correct but temporally inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the new lawyer says 'Good morning', when does he ask Ms. Mendoza to state and spell her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 502.241,
        "end": 504.904
      },
      "pred_interval": {
        "start": 522.5,
        "end": 524.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.259000000000015,
        "end": 19.99599999999998,
        "average": 20.127499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.708570122718811,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor and target events and their order, but gives an incorrect timestamp for E2 (522.5s vs. 502.241s in the reference) and asserts 'immediately after' rather than simply 'after', so it misstates key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza if she prefers to testify with an interpreter, when does Ms. Mendoza confirm her preference?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.733,
        "end": 528.876
      },
      "pred_interval": {
        "start": 512.644,
        "end": 515.344
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.088999999999942,
        "end": 13.531999999999925,
        "average": 13.310499999999934
      },
      "rationale_metrics": {
        "rouge_l": 0.22499999999999995,
        "text_similarity": 0.6674851179122925,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', its timestamps are substantially different from the ground truth and it adds an unsupported detail (reply in Spanish) and an inaccurate characterization of timing, so it is largely incorrect. "
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza states that she saw someone inside her car through the broken window, when does the lawyer acknowledge her statement?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 559.358,
        "end": 561.878
      },
      "pred_interval": {
        "start": 540.484,
        "end": 541.384
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.87399999999991,
        "end": 20.494000000000028,
        "average": 19.68399999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705885,
        "text_similarity": 0.783218502998352,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and their 'after' relation, but the timestamps are significantly incorrect and inconsistent with the reference, so the answer is largely factually wrong despite capturing the basic interaction."
      }
    },
    {
      "question_id": "003",
      "question": "After the lawyer asks Ms. Mendoza if anything was removed from her vehicle, when does Ms. Mendoza list the stolen items?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.801,
        "end": 634.921
      },
      "pred_interval": {
        "start": 566.175,
        "end": 568.775
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.62600000000009,
        "end": 66.14600000000007,
        "average": 61.38600000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.16494845360824742,
        "text_similarity": 0.7679885029792786,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the high-level relation ('after') right but misstates almost all temporal details\u2014timestamps and durations are substantially different and it omits/contradicts the correct note that the listing begins after an initial 'Por supuesto' rather than immediately. These factual timing errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the witness finishes describing the man looking through the car window, when does the lawyer state that a deputy was arriving?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 711.914,
        "end": 714.633
      },
      "pred_interval": {
        "start": 750.0,
        "end": 751.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.08600000000001,
        "end": 36.86699999999996,
        "average": 37.47649999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.32352941176470584,
        "text_similarity": 0.7531249523162842,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but the timestamps are substantially off from the ground truth (predicted ~750\u2013751.5s vs correct ~711.6\u2013711.9s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking what the officer did, when does the witness describe the officer speaking into his radio and telling her to stay put?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.233,
        "end": 766.142
      },
      "pred_interval": {
        "start": 807.0,
        "end": 807.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 64.76700000000005,
        "end": 41.35799999999995,
        "average": 53.0625
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.6369342803955078,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the lawyer and the witness's radio remark, but it gives substantially wrong timestamps and a different temporal relation, so it does not match the ground-truth alignment."
      }
    },
    {
      "question_id": "003",
      "question": "After the witness finishes stating she is absolutely sure about identifying the defendant, when does the lawyer ask about the suspect's behavior after being cuffed?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 851.101,
        "end": 861.493
      },
      "pred_interval": {
        "start": 853.0,
        "end": 853.5
      },
      "iou": 0.048113933795226854,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.899000000000001,
        "end": 7.993000000000052,
        "average": 4.946000000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.7596306800842285,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the timestamps are inaccurate (E1 end differs by ~4.6s, E2 start differs and its end time is omitted), so it omits key temporal details from the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the male lawyer asks what happened when they got to the patrol car, when does Ms. Mendoza describe the officers searching the suspect?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 885.63,
        "end": 894.463
      },
      "pred_interval": {
        "start": 870.0,
        "end": 875.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.629999999999995,
        "end": 19.462999999999965,
        "average": 17.54649999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.21333333333333337,
        "text_similarity": 0.6692623496055603,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same question and response content and preserves the 'after' relation, but both event timestamps/spans are substantially earlier than the reference (E1 ~12s early, E2 ~10s early) and do not match the precise grounded intervals."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ms. Mendoza finishes explaining she got closer to see what was happening, when does she state that's why she remembers it well?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 919.062,
        "end": 923.688
      },
      "pred_interval": {
        "start": 903.0,
        "end": 905.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.062000000000012,
        "end": 18.687999999999988,
        "average": 17.375
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.7014840245246887,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the utterance and the temporal relation (immediately after/once_finished), but the provided timestamps are significantly incorrect (off by ~10\u201314 seconds) and thus factually misaligned with the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male lawyer finishes asking if the suspect settled down and cooperated in the patrol car, when does Ms. Mendoza state he did not cooperate?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 937.892,
        "end": 940.207
      },
      "pred_interval": {
        "start": 919.0,
        "end": 920.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.892000000000053,
        "end": 20.206999999999994,
        "average": 19.549500000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.6784647107124329,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the reply content and a sequential relation, but the timestamps are substantially incorrect (off by ~13\u201318 seconds) and the response adds a filler phrase; because the question asks when she states it, the wrong timing makes the answer largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Good evening, friends,' when does he mention that their journey started in 2020 during the COVID times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 5.453,
        "end": 8.514
      },
      "pred_interval": {
        "start": 3.68,
        "end": 8.31
      },
      "iou": 0.5910219280099297,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.7730000000000001,
        "end": 0.20399999999999885,
        "average": 0.9884999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.38235294117647056,
        "text_similarity": 0.5950326919555664,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and roughly the end time, but the anchor and target start timestamps are substantially off (anchor 2.8s vs 3.592s, target start 3.68s vs 5.453s) and it adds an unsupported claim ('within the same sentence'), so key factual timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker (Vikas Chatrath) says 'I think there's some issue with the net,' when does the screen go black with his name displayed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.726,
        "end": 75.546
      },
      "pred_interval": {
        "start": 70.12,
        "end": 70.12
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.3940000000000055,
        "end": 5.426000000000002,
        "average": 3.9100000000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.6401066184043884,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the target event and the 'immediately after' relation, but it gives a wrong anchor time (69.3s vs 63.456s), misstates the target start (70.12s vs 67.726s) and omits the target end time, so key timing details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath says 'Over to you, sir,' when does Mr. R.S. Cheema begin speaking?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.165,
        "end": 175.752
      },
      "pred_interval": {
        "start": 153.23,
        "end": 153.23
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.935000000000002,
        "end": 22.52200000000002,
        "average": 19.22850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.6601136922836304,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the 'immediately after' relationship, but the timestamps are significantly incorrect (predicted ~152\u2013153s vs. ground truth ~167\u2013169s), a major factual mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chatrath finishes his speech and invites Mr. Cheema, when does Mr. Cheema describe the topic as very generic and vast?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.048,
        "end": 204.229
      },
      "pred_interval": {
        "start": 238.22,
        "end": 245.28
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.172,
        "end": 41.05099999999999,
        "average": 39.61149999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.6537539958953857,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterance and the 'after' relation, but the provided timestamps (\u2248234\u2013238s) are wildly inconsistent with the correct times (\u224815\u201354s), a major factual error."
      }
    },
    {
      "question_id": "002",
      "question": "After Mr. Cheema says he agreed with the choice of the broad topic, when does he explain the reason for his agreement?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 229.669,
        "end": 242.242
      },
      "pred_interval": {
        "start": 252.42,
        "end": 256.74
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.750999999999976,
        "end": 14.498000000000019,
        "average": 18.624499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.738984227180481,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the relation ('after') right, it mislocates both E1 and E2 by large margins and contradicts the ground-truth timestamps, so the timing information is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During Mr. Cheema's description of going into division bench courts to hear idols and icons, when does he mention that the judges are not in a hurry?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.317,
        "end": 313.619
      },
      "pred_interval": {
        "start": 278.96,
        "end": 280.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.357000000000028,
        "end": 32.71900000000005,
        "average": 29.53800000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.4318181818181818,
        "text_similarity": 0.7836213111877441,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives different timestamps for both anchor and target that do not overlap, so the claimed 'during' relation is incorrect; it therefore fails to match the correct timings despite matching the textual phrase."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions he has been appearing in three High Courts, when does he state that criminal appeals have not been argued for a long time in the Punjab and Haryana High Court?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 379.0,
        "end": 385.0
      },
      "pred_interval": {
        "start": 404.4,
        "end": 406.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.399999999999977,
        "end": 21.899999999999977,
        "average": 23.649999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.6486937999725342,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the same statements and the 'after' relation, but the event timestamps are incorrect (offset by ~40s from the ground-truth intervals), so it misses a key factual element."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions there is very little hearing in appeal, when does he state that the most 'scaring part' is the flood of outstanding litigation causing the hearing to get a beating?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 408.915,
        "end": 418.887
      },
      "pred_interval": {
        "start": 425.2,
        "end": 426.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.284999999999968,
        "end": 7.512999999999977,
        "average": 11.898999999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.16129032258064516,
        "text_similarity": 0.6854649782180786,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relation ('after') and mentions the same phrases, but it misidentifies and swaps the events and gives incorrect timestamps (425\u2013426s vs. the correct ~405.5\u2013418.9s), so the temporal anchors are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says, 'Let's first also define what the appeals are', when does he begin to define criminal appeals as generally understood?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.605,
        "end": 487.757
      },
      "pred_interval": {
        "start": 458.2,
        "end": 460.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.40500000000003,
        "end": 27.456999999999994,
        "average": 21.43100000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424246,
        "text_similarity": 0.7641679048538208,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the phrasing of the definition but the timestamps are significantly incorrect (both E1 and E2 are ~12\u201315s earlier than ground truth) and the temporal relation is mischaracterized, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes stating that the real purpose of this exercise is to scuttle the path of the litigant, when does he discuss today's purpose?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 562.2,
        "end": 567.6
      },
      "pred_interval": {
        "start": 588.52,
        "end": 588.72
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.319999999999936,
        "end": 21.120000000000005,
        "average": 23.71999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.24,
        "text_similarity": 0.783808708190918,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their order (target follows anchor) but gives substantially incorrect timestamps (off by ~26s), omits interval end times, and fails to note that the target immediately follows the anchor as in the reference."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is listing the second category of appeals, when does he mention the 'Essential Commodities Act'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 595.2,
        "end": 601.662
      },
      "pred_interval": {
        "start": 607.32,
        "end": 608.52
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.120000000000005,
        "end": 6.857999999999947,
        "average": 9.488999999999976
      },
      "rationale_metrics": {
        "rouge_l": 0.303030303030303,
        "text_similarity": 0.8059173226356506,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relationship ('during') but the timestamps are substantially off: both E1 and E2 times do not match the ground truth (predicted events occur ~6\u201312s later and do not fall within the reference intervals), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating he will confine today's discussion to the Indian Penal Code, when does he explain that he doesn't want to just touch and go?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.624,
        "end": 635.538
      },
      "pred_interval": {
        "start": 630.76,
        "end": 631.76
      },
      "iou": 0.144634075788256,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.1359999999999673,
        "end": 3.77800000000002,
        "average": 2.9569999999999936
      },
      "rationale_metrics": {
        "rouge_l": 0.2926829268292683,
        "text_similarity": 0.6706603765487671,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the anchor and target events but gives incorrect timestamps (placing the anchor inside the true target interval) and mischaracterizes the relation as merely 'after' rather than the correct 'immediately follows', so it is largely misaligned with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that their judgments are not very clear, when does he begin to explain that, unlike IPC, other acts *do* have provisions for vicarious liability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.063,
        "end": 753.951
      },
      "pred_interval": {
        "start": 690.0,
        "end": 692.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.06299999999999,
        "end": 61.95100000000002,
        "average": 59.007000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820515,
        "text_similarity": 0.7303025126457214,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterances and the 'after' relationship, but the timestamps are substantially wrong (about 50+ seconds early) and the predicted answer omits the E2 end time, so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the first clause of the provisions, when does he explain that an individual is a 'deemed accused' by virtue of their office?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 772.927,
        "end": 777.037
      },
      "pred_interval": {
        "start": 737.0,
        "end": 747.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.92700000000002,
        "end": 30.037000000000035,
        "average": 32.98200000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.1590909090909091,
        "text_similarity": 0.7139326930046082,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction is largely incorrect: both E1 and E2 timestamps are about 35\u201340s earlier than the ground truth and the relation contradicts the correct 'once_finished' label by claiming they are the same/part of one event; only the term 'deemed accused' matches."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that the 'second part is most important', when does he begin describing this second part of the provisions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 786.039,
        "end": 799.875
      },
      "pred_interval": {
        "start": 779.0,
        "end": 782.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.038999999999987,
        "end": 17.875,
        "average": 12.456999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.7635565996170044,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the ordering (E2 follows E1) but the timestamps are notably off (E1 and E2 start times differ by several seconds from the reference), the predicted answer omits E2's end time, and thus does not match the reference timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking if people are vicariously liable, when does he state that the answer is 'yes and no both'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.653,
        "end": 886.655
      },
      "pred_interval": {
        "start": 998.86,
        "end": 1000.95
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 114.207,
        "end": 114.29500000000007,
        "average": 114.25100000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.3902439024390244,
        "text_similarity": 0.7807207107543945,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the answer follows the question and quotes the phrases, but the timestamps are substantially different from the reference (and the prediction omits the question end time), so it does not align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the Food Safety and Security Act allows a company to appoint a competent person as a nominee, when does he mention the Environment Act?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 954.035,
        "end": 958.702
      },
      "pred_interval": {
        "start": 1027.43,
        "end": 1030.13
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 73.3950000000001,
        "end": 71.42800000000011,
        "average": 72.4115000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2954545454545454,
        "text_similarity": 0.7153598070144653,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and that the Environment Act is introduced next, but the provided timestamps (and E1/E2 boundaries) are substantially different from the reference and thus factually incorrect; it also shifts from finish to start times, so key temporal details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes that the rest of the appeals would always be technical, when does he introduce the topic of drafting an appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1052.092,
        "end": 1055.115
      },
      "pred_interval": {
        "start": 1063.56,
        "end": 1066.11
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.467999999999847,
        "end": 10.99499999999989,
        "average": 11.231499999999869
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.5772963762283325,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but misidentifies both event timestamps and the second event's content (mentions the Arms Act rather than drafting an appeal), contradicting the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions looking into the nuances of drafting, when does he state what he found regarding drafting and appeals?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1054.8,
        "end": 1058.3
      },
      "pred_interval": {
        "start": 1050.0,
        "end": 1065.0
      },
      "iou": 0.23333333333333334,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.7999999999999545,
        "end": 6.7000000000000455,
        "average": 5.75
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.880355954170227,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies E1 and the temporal relation ('after'), but mislocates E2 by about 10 seconds (predicts 1065.0s vs reference 1054.8\u20131058.3s), so the key timing of the reported finding is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that criminal proceedings are not adversarial, when does he discuss how formal errors and oversights can be overcome?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1120.624,
        "end": 1125.731
      },
      "pred_interval": {
        "start": 1110.0,
        "end": 1130.0
      },
      "iou": 0.2553499999999985,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.624000000000024,
        "end": 4.2690000000000055,
        "average": 7.4465000000000146
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.8320696949958801,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase and the 'after' relationship, but both timestamps are inaccurate (E1 is ~4s early and E2 is ~9s late compared to the ground truth), so the temporal alignment is unreliable."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that the 313 was not correctly recorded in a Ropar case, when does he recall the accused saying he was made to sign a blank paper?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1194.217,
        "end": 1254.6
      },
      "pred_interval": {
        "start": 1165.0,
        "end": 1185.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.2170000000001,
        "end": 69.59999999999991,
        "average": 49.408500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923075,
        "text_similarity": 0.8245542645454407,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same anchor and target utterances and the 'after' relationship, but the provided timestamps are substantially earlier than the reference times, so the timing information is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what they should do, when does he ask if an application needs to be filed for additional evidence in the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1251.721,
        "end": 1266.539
      },
      "pred_interval": {
        "start": 1295.58,
        "end": 1305.58
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.858999999999924,
        "end": 39.04099999999994,
        "average": 41.44999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.3777777777777777,
        "text_similarity": 0.7868794798851013,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and their 'after' relation and matches the quoted content, but the timestamps are substantially different from the reference, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the court made a mistake by not formally proving a cassette recording, when does he explain that they had to make an application for evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.0,
        "end": 1294.14
      },
      "pred_interval": {
        "start": 1309.5,
        "end": 1320.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.5,
        "end": 26.3599999999999,
        "average": 22.42999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.14285714285714285,
        "text_similarity": 0.5251259207725525,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misidentifies and mis-timestamps the anchor event (does not capture the speaker stating the court's mistake) and gives a substantially different span for E2; while the temporal relation is similar in spirit, the key content and timing are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that first reactions may never come again if not noted down, when does he state that he made a practice of noting his reactions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1394.269,
        "end": 1401.682
      },
      "pred_interval": {
        "start": 1360.2,
        "end": 1366.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.06899999999996,
        "end": 35.182000000000016,
        "average": 34.62549999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.17391304347826086,
        "text_similarity": 0.701661229133606,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the content and the 'after' relation, but it gives incorrect and inconsistent timecodes for both E1 and E2 compared to the ground truth, omitting the correct timestamp ranges."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says one should not put the detail in an appeal, when does he suggest that subtle points may be hinted at when drafting grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1444.582,
        "end": 1452.109
      },
      "pred_interval": {
        "start": 1444.0,
        "end": 1451.0
      },
      "iou": 0.7914662720434028,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.5820000000001073,
        "end": 1.1089999999999236,
        "average": 0.8455000000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.22580645161290325,
        "text_similarity": 0.7309563755989075,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies a target suggesting subtle points may be hinted and the 'after' relationship, but it mislocates and misrepresents the anchor (wrong time and different utterance), so key anchor timing/content is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'What is the key question?', when does he compare a trial or appeal to writing a novel?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1536.909,
        "end": 1545.661
      },
      "pred_interval": {
        "start": 1478.0,
        "end": 1485.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.909000000000106,
        "end": 60.66100000000006,
        "average": 59.78500000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384614,
        "text_similarity": 0.6542501449584961,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the comparison follows the question and names the quoted line, but the timestamps are substantially incorrect (~58\u201359s earlier) and the relation is weakened to 'after' instead of the correct immediate/direct follow, so the answer is largely factually wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions keeping the core issue in mind, when does he advise that the first reading of an appeal should be relaxed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1594.74,
        "end": 1607.579
      },
      "pred_interval": {
        "start": 1558.0,
        "end": 1565.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.74000000000001,
        "end": 42.57899999999995,
        "average": 39.65949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.8259842991828918,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the temporal relation ('after') and the advice content, but the anchor/target timestamps and anchor wording are substantially incorrect compared to the reference (off by ~35\u201340s and different utterances), so it is largely misaligned."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says that the first reading of an appeal should be relaxed, when does he compare it to watching a soap opera, reading a novel, or listening to music?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1608.151,
        "end": 1615.0
      },
      "pred_interval": {
        "start": 1595.4,
        "end": 1600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.750999999999976,
        "end": 15.0,
        "average": 13.875499999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.23076923076923078,
        "text_similarity": 0.6522220969200134,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the quoted content and a similar temporal relation, but the time spans are off by about 10\u201312 seconds (incorrect E1/E2 boundaries) and it adds an unsupported audio-cue claim; key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises being as neutral as possible, when does he explain that this enables objectivity and unbiased case knowledge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1638.375,
        "end": 1647.567
      },
      "pred_interval": {
        "start": 1602.8,
        "end": 1610.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.575000000000045,
        "end": 37.4670000000001,
        "average": 36.52100000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.1320754716981132,
        "text_similarity": 0.5480692386627197,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both event timestamps and quoted content, reverses the anchor/target roles, and incorrectly labels the temporal relation as 'simultaneous' rather than 'after', so it does not match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if he has a good case, when does he follow up by asking if he has a bad case or one in between?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1674.0,
        "end": 1681.0
      },
      "pred_interval": {
        "start": 1617.7,
        "end": 1622.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.299999999999955,
        "end": 59.0,
        "average": 57.64999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.7654929757118225,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the follow-up question and relation (immediately after/next) but the timestamps are substantially wrong (off by ~52 seconds from the reference) and it adds an unsupported audio-cue detail, so it fails key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that three to four things would matter when preparing an appeal, what is the first thing he suggests?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1824.432,
        "end": 1828.85
      },
      "pred_interval": {
        "start": 1813.84,
        "end": 1815.56
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.592000000000098,
        "end": 13.289999999999964,
        "average": 11.941000000000031
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.7811503410339355,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies an anchor and a subsequent first item, but the timestamps disagree substantially with the reference and the relation is labeled only as 'after' rather than the immediate follow described; key temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker emphasizes that stages in an appeal should be clear in one's mind, when does he start talking about the 'finest lawyers'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1890.082,
        "end": 1904.09
      },
      "pred_interval": {
        "start": 1899.92,
        "end": 1900.48
      },
      "iou": 0.0399771559109047,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.837999999999965,
        "end": 3.6099999999999,
        "average": 6.723999999999933
      },
      "rationale_metrics": {
        "rouge_l": 0.225,
        "text_similarity": 0.7318164110183716,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer matches the quoted phrases but the timestamps are substantially off (anchor ~13 s late, target start ~10 s late) and it mischaracterizes the relationship as 'immediately after' despite the reference noting a slight pause; therefore the semantic alignment is weak despite correct wording."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes his point about preparation, when does he announce moving to the next phase of the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1921.825,
        "end": 1924.428
      },
      "pred_interval": {
        "start": 1943.8,
        "end": 1945.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.97499999999991,
        "end": 20.57199999999989,
        "average": 21.2734999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428572,
        "text_similarity": 0.768319308757782,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the same verbal cue and the 'immediately after' relationship, but the timestamps are substantially incorrect (off by ~25 seconds), so it fails to match the key factual timing information in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a judge will eventually respond to an appeal, when does he mention being placed before a newly transferred judge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1984.778,
        "end": 1991.706
      },
      "pred_interval": {
        "start": 2070.4,
        "end": 2085.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.62200000000007,
        "end": 94.09400000000028,
        "average": 89.85800000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.27999999999999997,
        "text_similarity": 0.7957257032394409,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('after') correct but misidentifies both event spans, timestamps, and quoted text (anchors and timings differ substantially from the reference), so it fails to match the correct answer's key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes how he handles a client's ticklish case by listening to their story, when does he next explain that judges would analyze in a similar way and evolve their style?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2000.528,
        "end": 2006.954
      },
      "pred_interval": {
        "start": 2086.8,
        "end": 2101.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.27200000000016,
        "end": 94.44600000000014,
        "average": 90.35900000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.2857142857142857,
        "text_similarity": 0.7795042395591736,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the judge comment follows the ticklish-case remark and even quotes the judge line, but it gives entirely different timestamps and mislocates the anchor/target boundaries compared to the reference, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a diligent lawyer would form and correct their opinion about the bench every day, when does he next say that this practice saves the court's time?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2071.617,
        "end": 2074.641
      },
      "pred_interval": {
        "start": 2127.4,
        "end": 2136.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.7829999999999,
        "end": 61.759000000000015,
        "average": 58.77099999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2891566265060241,
        "text_similarity": 0.7648380994796753,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the target quote about saving the court's time but misidentifies the anchor utterance and gives different timestamps and a looser 'after' relation instead of the correct 'next' adjacency, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what makes a good lawyer, when does he begin to answer by describing a man who has prepared his brief fully?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.444,
        "end": 2200.017
      },
      "pred_interval": {
        "start": 2143.49,
        "end": 2150.82
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.95400000000018,
        "end": 49.19699999999966,
        "average": 49.07549999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.20512820512820512,
        "text_similarity": 0.7076756954193115,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives entirely different timestamps (~39s earlier) and a different target utterance than the reference (it cites 'I think I have already answered' instead of the description of the man who prepared his brief), so although it correctly notes the 'after' relationship, it fails on timing and content."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a fresh judge saying 'no reading of evidence', when does he express his opinion that it's illegal and unconstitutional?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2237.752,
        "end": 2243.727
      },
      "pred_interval": {
        "start": 2197.46,
        "end": 2200.16
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.291999999999916,
        "end": 43.56700000000001,
        "average": 41.92949999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2058823529411765,
        "text_similarity": 0.5768190026283264,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction captures the speaker's opinion phrase but gives entirely incorrect timestamps and misidentifies the anchor/target timing and sequence versus the ground truth, so it fails to match the correct temporal boundaries."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to make the judge read Section 54 of the Evidence Act, when does he explain its content regarding the previous character of an accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2302.58,
        "end": 2310.354
      },
      "pred_interval": {
        "start": 2215.88,
        "end": 2220.57
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.69999999999982,
        "end": 89.78399999999965,
        "average": 88.24199999999973
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.7246702909469604,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and their immediate-following relationship and paraphrases the explanation, but the provided timestamps are substantially incorrect (off by over a minute), so the temporal alignment is not accurate."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of the second question as a heinous crime, when does he describe specific examples like 'chopped off the hand'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2354.498,
        "end": 2357.601
      },
      "pred_interval": {
        "start": 2323.92,
        "end": 2333.38
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.577999999999975,
        "end": 24.221000000000004,
        "average": 27.39949999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.8547413945198059,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially incorrect (about 23\u201328 seconds earlier than the reference) and the temporal relation is wrong: the correct relation is 'during' while the prediction says 'after', so it does not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states 'this is the second kind of roadblock', when does he introduce the 'Third kind of roadblock'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2414.579,
        "end": 2418.664
      },
      "pred_interval": {
        "start": 2346.88,
        "end": 2350.52
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 67.69900000000007,
        "end": 68.14400000000023,
        "average": 67.92150000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.2588235294117647,
        "text_similarity": 0.776250958442688,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the sequence (E2 follows E1) and captures the general phrasing, but the timestamps are substantially incorrect (off by ~63 seconds) and the E2 quote is incomplete, so it fails on factual timing and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes explaining 'these are the questions which are put to you', when does he transition to discussing myths about 'a dying declaration'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2462.739,
        "end": 2468.126
      },
      "pred_interval": {
        "start": 2375.22,
        "end": 2377.94
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.51900000000023,
        "end": 90.18600000000015,
        "average": 88.85250000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.22784810126582278,
        "text_similarity": 0.7995837926864624,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation (E2 follows E1) but the anchor/target timestamps are completely different from the ground truth (off by ~80s), so the answer is factually incorrect despite a similar relative ordering."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker extensively discusses the 1925 Lahore Bakshish Singh judgment, when does he bring up the Lakshmi versus Om Prakash case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2549.448,
        "end": 2555.294
      },
      "pred_interval": {
        "start": 2618.74,
        "end": 2622.92
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.29199999999992,
        "end": 67.6260000000002,
        "average": 68.45900000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.2745098039215686,
        "text_similarity": 0.7392637729644775,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps and asserts an 'immediately after' transition, contradicting the reference which shows E2 begins ~24.5s after E1 ends; it also adds unsupported audio/visual cues, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying, 'These are the cases when you have opened the door of a hearing', when does he state, 'These are situations which we must know'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2605.712,
        "end": 2610.678
      },
      "pred_interval": {
        "start": 2639.06,
        "end": 2642.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.347999999999956,
        "end": 32.021999999999935,
        "average": 32.684999999999945
      },
      "rationale_metrics": {
        "rouge_l": 0.3272727272727273,
        "text_similarity": 0.6038913726806641,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the quick succession/continuation relationship, but it gives incorrect and inconsistent timestamps (both events at 2639.06s vs ground-truth ~2605.17\u20132610.68s) and thus fails to match the key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises, 'But when you go to this, don't offend the judge', when does he describe making notes in 'three phases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2647.594,
        "end": 2653.382
      },
      "pred_interval": {
        "start": 2671.62,
        "end": 2673.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.02599999999984,
        "end": 20.317999999999756,
        "average": 22.171999999999798
      },
      "rationale_metrics": {
        "rouge_l": 0.2830188679245283,
        "text_similarity": 0.5722058415412903,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates the second event follows the advice, but the timestamps are substantially incorrect (both set to 2671.62s versus 2646.614\u20132653.382s in reference), it missegments the events, and it introduces an unverified audio cue, so it fails to match the ground truth details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises a lawyer to stop if their best argument isn't selling, when does he suggest the lawyer might say, 'Your lordship may hear me on other issues'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2687.877,
        "end": 2690.499
      },
      "pred_interval": {
        "start": 2670.0,
        "end": 2700.0
      },
      "iou": 0.08739999999999479,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.876999999999953,
        "end": 9.501000000000204,
        "average": 13.689000000000078
      },
      "rationale_metrics": {
        "rouge_l": 0.2528735632183908,
        "text_similarity": 0.7348841428756714,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the content and that E2 occurs after E1, but the provided timestamps are notably misaligned with the reference (E1/E2 boundaries and E2 start differ by several seconds and exact end times are omitted)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how language helps with flexibility and playing cool in arguments, when does he introduce 'sense of humor' as an important quality?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2718.862,
        "end": 2725.929
      },
      "pred_interval": {
        "start": 2700.0,
        "end": 2730.0
      },
      "iou": 0.2355666666666669,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.86200000000008,
        "end": 4.070999999999913,
        "average": 11.466499999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2988505747126437,
        "text_similarity": 0.8617036938667297,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that 'sense of humor' is introduced after the language explanation, but the timestamps are inaccurate (predicting E2 at 2714.0s vs the true 2718.862s) and it omits the anchor's end time, leading to a potential contradiction with the true immediate-follow relationship."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker identifies 'trap cases' and 'DA cases' as the majority of Corruption Act cases, when does he introduce 'scam cases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2786.395,
        "end": 2789.04
      },
      "pred_interval": {
        "start": 2730.0,
        "end": 2760.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.39499999999998,
        "end": 29.039999999999964,
        "average": 42.71749999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.3684210526315789,
        "text_similarity": 0.8835652470588684,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly states that the scam cases are introduced after the two main categories, but it gives substantially incorrect timestamps (starts at 2730s/2750s vs 2781.409\u20132789.04s) and thus fails to match the precise temporal spans and immediacy in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if a busy or young lawyer needs a register of important judgments, when does he directly ask if it is a necessary tool?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2898.935,
        "end": 2905.299
      },
      "pred_interval": {
        "start": 2916.0,
        "end": 2925.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.065000000000055,
        "end": 19.701000000000022,
        "average": 18.383000000000038
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540986,
        "text_similarity": 0.5516166687011719,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives timestamps that are significantly different from the reference and misidentifies the anchor/target relationship; it fails to match the correct anchor (2867.7\u20132880.6), the target (2898.9\u20132905.3), and the noted pause between them."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes emphasizing the need to know, read, and re-read foundational judgments, when does he state he will recount a few of them?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.268,
        "end": 2935.553
      },
      "pred_interval": {
        "start": 2950.0,
        "end": 2954.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.73199999999997,
        "end": 18.447000000000116,
        "average": 19.089500000000044
      },
      "rationale_metrics": {
        "rouge_l": 0.09999999999999999,
        "text_similarity": 0.5939574241638184,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the speaker's intention to recount judgments but gives substantially incorrect timestamps for E1 and omits the referenced E2/follow-up, so it is largely mismatched to the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing the question about a false defense or wrong answer given by the accused, when does he explicitly state his suggestion and request for foundational judgments?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3029.268,
        "end": 3033.995
      },
      "pred_interval": {
        "start": 2988.0,
        "end": 2998.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.26800000000003,
        "end": 35.99499999999989,
        "average": 38.63149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.11267605633802817,
        "text_similarity": 0.39277660846710205,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misstates the timestamps and asserts the explicit suggestion occurs much earlier, contradicting the reference which places E1 at 3008.582s and the target at 3029.268\u20133033.995s; it therefore fails to match the correct temporal relation and details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'Virsar Singh's judgment, 1958', when does he mention 'Vivian Bose's judgment, a classic'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3045.106,
        "end": 3052.317
      },
      "pred_interval": {
        "start": 3159.31,
        "end": 3160.78
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 114.20399999999972,
        "end": 108.46300000000019,
        "average": 111.33349999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2758620689655173,
        "text_similarity": 0.6154252290725708,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but the timestamps are significantly off from the reference (predicted ~3159\u20133160s vs correct ~3045\u20133052s), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker refers to the case 'Prabhu versus the Emperor, 1941', when does he ask if the initial onus on the prosecution transfers itself?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.51,
        "end": 3129.351
      },
      "pred_interval": {
        "start": 3179.87,
        "end": 3180.92
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.35999999999967,
        "end": 51.56899999999996,
        "average": 55.464499999999816
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.661658525466919,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the utterances and the 'after' relation, but the timecodes are significantly off (~60s later) and the predicted answer omits the E2 end time, so it fails to match the key factual timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying '3.', when does he explain that the court has a 'broader picture' with the accused's evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3158.541,
        "end": 3164.576
      },
      "pred_interval": {
        "start": 3223.94,
        "end": 3226.32
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.39899999999989,
        "end": 61.74400000000014,
        "average": 63.571500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7167960405349731,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the qualitative idea that 'Third' introduces an explanation, but it gives substantially incorrect timestamps for both anchor and target (~65\u201368s off) and mislabels the relation as 'after' instead of the correct immediate 'once_finished', so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 1954 Supreme Court case, when does he detail the specific allegation and offense?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3228.663,
        "end": 3239.981
      },
      "pred_interval": {
        "start": 3242.0,
        "end": 3247.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.336999999999989,
        "end": 7.218999999999596,
        "average": 10.277999999999793
      },
      "rationale_metrics": {
        "rouge_l": 0.2727272727272727,
        "text_similarity": 0.818090558052063,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction is largely incorrect: timestamps differ substantially (predicted 3242.0s vs ground-truth E1 3218.301\u20133224.254 and E2 3228.663\u20133239.981), and it wrongly asserts overlap/immediate transition whereas the ground truth shows E2 begins after a gap; the added audio-cue claim is unsupported."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying that Tanu Bedi is present, when does he begin describing the case involving an Afghan Airlines pilot?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3267.18,
        "end": 3280.178
      },
      "pred_interval": {
        "start": 3296.7,
        "end": 3299.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.519999999999982,
        "end": 18.922000000000025,
        "average": 24.221000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.27956989247311825,
        "text_similarity": 0.8155601620674133,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the pilot case follows mention of Tanu Bedi but is largely incorrect: the timestamps are off by ~32 seconds, it claims the events overlap (where the reference shows E2 begins after E1 ends), and event boundaries are misaligned."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if they can continue for 10 minutes, when does someone respond with 'Yes, of course, of course'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3407.405,
        "end": 3409.588
      },
      "pred_interval": {
        "start": 3387.9,
        "end": 3389.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.50500000000011,
        "end": 19.888000000000375,
        "average": 19.696500000000242
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8552207946777344,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct utterances but gives substantially incorrect timestamps and an incorrect relation (overlap/immediate vs the true delayed 'once_finished' response), so it largely contradicts the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how long they can continue, when does he ask if they can go on for 10 minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3405.21,
        "end": 3408.57
      },
      "pred_interval": {
        "start": 3401.16,
        "end": 3402.26
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.050000000000182,
        "end": 6.309999999999945,
        "average": 5.180000000000064
      },
      "rationale_metrics": {
        "rouge_l": 0.19999999999999998,
        "text_similarity": 0.6262709498405457,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and that the target occurs after the anchor, but the provided timestamps are notably off (anchor is given after the ground-truth anchor interval and the target is ~3s earlier than the reference), so timing alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions the judge saying 'no, you are convicted', when does he recall playing basketball?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3490.94,
        "end": 3501.65
      },
      "pred_interval": {
        "start": 3463.74,
        "end": 3465.34
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.200000000000273,
        "end": 36.309999999999945,
        "average": 31.75500000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.23188405797101447,
        "text_similarity": 0.7420672178268433,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor phrase, the basketball memory, and their 'after' relation, but the provided timestamps are substantially misplaced compared to the ground truth (off by ~15\u201325s), so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises that 'Your case must bother you', when does he begin telling another interesting case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3552.29,
        "end": 3555.63
      },
      "pred_interval": {
        "start": 3537.94,
        "end": 3538.62
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.349999999999909,
        "end": 17.01000000000022,
        "average": 15.680000000000064
      },
      "rationale_metrics": {
        "rouge_l": 0.24615384615384617,
        "text_similarity": 0.7853153944015503,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct utterances and the temporal relation ('after'), but both timestamp estimates are substantially earlier than the reference intervals (anchor off by ~5\u20136s and target off by ~14s), so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the detailed injury on the forehead, when does he state that the benefit of doubt went to the accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3636.678,
        "end": 3640.261
      },
      "pred_interval": {
        "start": 3663.48,
        "end": 3669.22
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.802000000000135,
        "end": 28.958999999999833,
        "average": 27.880499999999984
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.7695424556732178,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and the event descriptions, but the provided timestamps are substantially incorrect and do not match the reference intervals, so key factual timing information is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's description of the trickster character from Maupassant's story and his famous acts, when does he specifically mention the trickster showing different tricks to people?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3732.928,
        "end": 3736.294
      },
      "pred_interval": {
        "start": 3710.72,
        "end": 3719.88
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.208000000000084,
        "end": 16.41399999999976,
        "average": 19.310999999999922
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.741287112236023,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction places both events at substantially different times (about 17s earlier) and gives E1 as a single timestamp rather than the correct span, so the temporal alignment is incorrect despite both claiming a 'during' relation."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing that there are 'two more cases', when does he name the location of the first of these new cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3665.138,
        "end": 3668.806
      },
      "pred_interval": {
        "start": 3727.8,
        "end": 3731.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.66200000000026,
        "end": 62.99400000000014,
        "average": 62.8280000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.18461538461538463,
        "text_similarity": 0.631391167640686,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the second case follows immediately after the first, but the timestamps are substantially off (about 66s later) and the location name is misspelled, so the key factual timing/detail information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the main speaker finishes stating that 'So that persuaded the court', when does he begin talking about other interesting cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.541,
        "end": 3838.467
      },
      "pred_interval": {
        "start": 3782.81,
        "end": 3786.79
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.73100000000022,
        "end": 51.677000000000135,
        "average": 51.20400000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.27499999999999997,
        "text_similarity": 0.7447797060012817,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the quoted phrases but the timestamps are substantially incorrect and it asserts an immediate transition ('after') unlike the reference which places the target speech ~23s later (relation 'once_finished'); therefore the temporal alignment is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the host says 'Thank you, sir', when does he describe how the session shifted to English?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3905.664,
        "end": 3912.762
      },
      "pred_interval": {
        "start": 3801.13,
        "end": 3806.55
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.5340000000001,
        "end": 106.21199999999999,
        "average": 105.37300000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413793,
        "text_similarity": 0.6999375224113464,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation as 'after' but the anchor and target timestamps differ substantially from the reference and it wrongly asserts the explanation 'immediately' follows; therefore it fails on factual timing accuracy and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the host announces that there are questions received on the chat, when does he ask the first question about supplementary grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3949.503,
        "end": 3955.411
      },
      "pred_interval": {
        "start": 3845.49,
        "end": 3848.43
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 104.01300000000037,
        "end": 106.98100000000022,
        "average": 105.4970000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.20408163265306126,
        "text_similarity": 0.6749825477600098,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the announcement and the immediate follow-up question and matches the relation, but the timestamps are substantially different from the reference (and the anchor timing is incorrect), so the answer is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they can combine the two questions, when does he state that all questions of fact can be raised?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3972.976,
        "end": 3975.039
      },
      "pred_interval": {
        "start": 4006.0,
        "end": 4010.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.02399999999989,
        "end": 35.76099999999997,
        "average": 34.39249999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.6060314178466797,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the ordering right (target after anchor) and identifies relevant utterances, but the timestamps are substantially incorrect compared to the ground truth and it adds an unsupported claim that the target 'immediately follows.' These are key factual mismatches."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that supplementary grounds are not needed in a normal case, when does he explain that all questions of fact and law are open in a normal case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4030.938,
        "end": 4036.242
      },
      "pred_interval": {
        "start": 4045.8,
        "end": 4055.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.86200000000008,
        "end": 19.35799999999972,
        "average": 17.1099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.1923076923076923,
        "text_similarity": 0.5087600946426392,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the semantic relationship (the target follows immediately after the anchor), but the timestamps are substantially incorrect (off by ~15s and different end time), so the temporal alignment is not accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining how a long note was required in the Sajan Kumar case, when does he describe the judge quoting his written note 80 times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4130.358,
        "end": 4139.64
      },
      "pred_interval": {
        "start": 4125.8,
        "end": 4139.4
      },
      "iou": 0.6533236994219196,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.557999999999993,
        "end": 0.24000000000069122,
        "average": 2.399000000000342
      },
      "rationale_metrics": {
        "rouge_l": 0.25225225225225223,
        "text_similarity": 0.7459797859191895,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (the judge quoted the note after the anchor) and the target content, but it gives incorrect and incomplete timestamps (anchor timing differs substantially from the reference and lacks an end time), so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking about making an opening statement to impress the court, when does the main speaker begin explaining that a prepared lawyer can present their case in two minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4186.419,
        "end": 4209.102
      },
      "pred_interval": {
        "start": 4124.61,
        "end": 4125.73
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.8090000000002,
        "end": 83.3720000000003,
        "average": 72.59050000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.21951219512195122,
        "text_similarity": 0.6139928102493286,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct semantic line and the 'after' relation, but the timestamps for both E1 and E2 are substantially incorrect and do not match the ground-truth intervals, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "During the main speaker's narration of the Churchill story, when does he say that Churchill responded he needed '15 days before' to prepare for a two-minute speech?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4275.956,
        "end": 4281.618
      },
      "pred_interval": {
        "start": 4245.25,
        "end": 4246.41
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.70600000000013,
        "end": 35.20800000000054,
        "average": 32.957000000000335
      },
      "rationale_metrics": {
        "rouge_l": 0.20289855072463767,
        "text_similarity": 0.7027159929275513,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the phrase and the 'during' relation qualitatively, but the provided timestamps are ~30s earlier than the ground truth and do not overlap (predicted E2 falls after predicted E1), contradicting the correct timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the main speaker finishes explaining that the less one has to speak, the more deeply one has to prepare, when does he begin defining a summary as a test of a lawyer's ability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4306.932,
        "end": 4319.831
      },
      "pred_interval": {
        "start": 4299.97,
        "end": 4300.64
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.961999999999534,
        "end": 19.190999999999804,
        "average": 13.076499999999669
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818185,
        "text_similarity": 0.5596092343330383,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction captures the same relation and even quotes the defining line, but the temporal boundaries are significantly off (E1/E2 times differ by ~5\u20136s from the reference), so the alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks about the most challenging situations, when does the guest start explaining the amount of preparation needed for short vs long speeches?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4289.354,
        "end": 4303.141
      },
      "pred_interval": {
        "start": 4326.0,
        "end": 4328.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.64599999999973,
        "end": 24.85900000000038,
        "average": 30.752500000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.2318840579710145,
        "text_similarity": 0.832726240158081,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the event relationship (guest explanation occurs after the host question) and the content, but the reported timestamps are significantly off (~41\u201342 seconds later) compared to the ground truth, so the temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes rephrasing the question about the impact of oral advocacy versus drafting, when does the guest immediately state that oral advocacy is more impactful?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4347.622,
        "end": 4350.188
      },
      "pred_interval": {
        "start": 4344.0,
        "end": 4345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.6220000000002983,
        "end": 5.188000000000102,
        "average": 4.4050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.8517142534255981,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures that the guest immediately states oral advocacy is key and preserves the 'immediately after' relationship, but the reported timestamps differ noticeably from the reference (off by ~2\u20133 seconds), so the timing is not accurately matched."
      }
    },
    {
      "question_id": "003",
      "question": "After the guest confirms that the framing of questions of law is 'all important', when does he state that 'manifest injustice' is the category for most cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4407.975,
        "end": 4412.261
      },
      "pred_interval": {
        "start": 4370.0,
        "end": 4371.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.975000000000364,
        "end": 41.26100000000042,
        "average": 39.61800000000039
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.6488795280456543,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the ordering right (anchor before target) but misstates both event timestamps by ~28\u201337 seconds and incorrectly asserts they are 'immediately after' when the ground truth shows a separate start ~5.4s after the anchor ends; thus significant temporal errors. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the interviewer finishes asking how the speaker's different fields and readings affect their argument style, when does the speaker respond by saying 'Sir, you're absolutely right'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4537.667,
        "end": 4541.776
      },
      "pred_interval": {
        "start": 4545.6,
        "end": 4550.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.932999999999993,
        "end": 9.024000000000342,
        "average": 8.478500000000167
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.8340407609939575,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'immediately after' relationship, but the timestamp values are significantly off (predicted events are shifted by ~6 s and have different start/end times) so it does not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that 'Advocacy is a communicative skill', when does he elaborate that 'One part of a communicative skill is language'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4562.343,
        "end": 4567.786
      },
      "pred_interval": {
        "start": 4598.8,
        "end": 4603.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.457000000000335,
        "end": 36.11399999999958,
        "average": 36.285499999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.7997376918792725,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer misstates the timestamps by ~35 seconds and reverses the temporal relation: it claims E2 starts immediately after E1, whereas the ground truth shows E2 began earlier and overlapped/contained E1, directly elaborating it. These are major factual mismatches."
      }
    },
    {
      "question_id": "003",
      "question": "After the interviewer mentions watching Justice Muralidhar's YouTube, when does the interviewer give an example from him about being asked 'did you murder?'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4617.635,
        "end": 4624.683
      },
      "pred_interval": {
        "start": 4612.1,
        "end": 4618.5
      },
      "iou": 0.06874354287529263,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.5349999999998545,
        "end": 6.182999999999993,
        "average": 5.858999999999924
      },
      "rationale_metrics": {
        "rouge_l": 0.4210526315789474,
        "text_similarity": 0.8073858022689819,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the sequence and the example ('did you murder?') immediately following the mention, but the timestamps are substantially off (E1 and E2 start/end differ by about 5\u20137 seconds from the ground truth), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines the appeal as an 'extension of the trial', when does he explain that notes can 'awaken you to the subtleties of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4673.51,
        "end": 4680.899
      },
      "pred_interval": {
        "start": 4759.96,
        "end": 4772.11
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.44999999999982,
        "end": 91.21099999999933,
        "average": 88.83049999999957
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.7076074481010437,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('after') but gives substantially incorrect timestamps (both events are shifted ~95s later) and omits the anchor's end time, so it fails to match the ground-truth timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising how to objectively approach an appeal, when does he state that 'You have to absorb all the witnesses of the surrounding environment'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4746.348,
        "end": 4752.661
      },
      "pred_interval": {
        "start": 4782.71,
        "end": 4795.71
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.36200000000008,
        "end": 43.04899999999998,
        "average": 39.70550000000003
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.795343279838562,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly gives substantially different and inconsistent timestamps (anchor/target shifted ~40s later and overlapping start times) and wrong durations, though it preserves the qualitative 'immediately after' relation; overall the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'then I become a lawyer', when does he describe the 'story of the lawyer of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4805.697,
        "end": 4824.178
      },
      "pred_interval": {
        "start": 4812.11,
        "end": 4821.11
      },
      "iou": 0.48698663492235883,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.412999999999556,
        "end": 3.068000000000211,
        "average": 4.740499999999884
      },
      "rationale_metrics": {
        "rouge_l": 0.208955223880597,
        "text_similarity": 0.7197755575180054,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the target elaborates on the anchor, but the timestamps are significantly incorrect (anchor and target times are misplaced and overlapping compared to the reference) and the relationship labeling ('immediately after') is inconsistent with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the 'art of communication' as the 'foundation of law', when does he state that its effectiveness depends on the 'quality of your preparation'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4848.488,
        "end": 4862.369
      },
      "pred_interval": {
        "start": 4863.84,
        "end": 4870.12
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.351999999999862,
        "end": 7.751000000000204,
        "average": 11.551500000000033
      },
      "rationale_metrics": {
        "rouge_l": 0.28037383177570097,
        "text_similarity": 0.5131241679191589,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target statement about preparation and the 'after' relation, but it mislabels the anchor phrase (does not match the 'foundation of law' wording) and the timestamps for both events are notably inaccurate/shifted."
      }
    },
    {
      "question_id": "002",
      "question": "After the initial speaker says 'Thank you, sir', when does the second speaker begin apologizing for not taking all questions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4960.598,
        "end": 4970.611
      },
      "pred_interval": {
        "start": 4910.72,
        "end": 4916.56
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.8779999999997,
        "end": 54.050999999999476,
        "average": 51.96449999999959
      },
      "rationale_metrics": {
        "rouge_l": 0.21428571428571427,
        "text_similarity": 0.7648965120315552,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and that the apology follows the thank-you, but the anchor and target timestamps are substantially incorrect and the prediction adds/changes spoken content and timing (including an incorrect claim of immediacy), so it fails to match the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the host refers to 'Q and Q, quality and quantity insurance', when does he mention the three important judgments: Prabhu, Rishikesh, and Vijay?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4996.141,
        "end": 5009.076
      },
      "pred_interval": {
        "start": 4957.68,
        "end": 4962.88
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.46099999999933,
        "end": 46.19599999999991,
        "average": 42.32849999999962
      },
      "rationale_metrics": {
        "rouge_l": 0.2786885245901639,
        "text_similarity": 0.7708935737609863,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same three judgments and an 'after' relation, but the anchor and target timecodes are substantially incorrect (off by ~44s) and the temporal relation 'immediately after' overstates proximity; major factual timing errors warrant a low score."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes quoting 'abhi na jao chhod ke, dil abhi bhara nahi', when does he state 'That's what the common sense is'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5019.2,
        "end": 5022.6
      },
      "pred_interval": {
        "start": 5015.44,
        "end": 5017.01
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.7600000000002183,
        "end": 5.5900000000001455,
        "average": 4.675000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.449438202247191,
        "text_similarity": 0.785115122795105,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the sequence (E2 follows E1) but the timestamps and durations are substantially different from the ground truth (predicted events occur ~3\u20134s earlier and are much shorter), and the relation label ('after') is less precise than the ground truth ('once_finished')."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker expresses surprise about people increasing viewership on YouTube, when does he mention everybody was viewed in the Zoom chat?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5030.3,
        "end": 5032.8
      },
      "pred_interval": {
        "start": 5023.02,
        "end": 5025.39
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.279999999999745,
        "end": 7.4099999999998545,
        "average": 7.3449999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.8379002809524536,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer correctly captures the event types and the 'after' relation, but the temporal boundaries are substantially inaccurate (both events are placed ~4\u20135 seconds earlier and are much shorter than the reference), so it fails to match the ground-truth timings."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Thank you, sir' for the first time, when does he specifically thank Tanu Bedi?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5046.2,
        "end": 5049.1
      },
      "pred_interval": {
        "start": 5037.33,
        "end": 5038.43
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.86999999999989,
        "end": 10.670000000000073,
        "average": 9.769999999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.31884057971014496,
        "text_similarity": 0.7906882762908936,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the utterance content and the 'after' relation, but the event timestamps and durations are substantially incorrect (off by ~7\u20139 seconds), so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once Alex finishes asking about the key challenges facing individuals who are called to give evidence, when does Paul begin to talk about nervousness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.237,
        "end": 36.762
      },
      "pred_interval": {
        "start": 32.4,
        "end": 33.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.8370000000000033,
        "end": 3.7620000000000005,
        "average": 2.299500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.41995686292648315,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the ordering and that Paul speaks immediately after Alex, but the provided timestamps are noticeably off from the ground truth and it adds an unsupported quoted phrase, indicating minor factual inaccuracies and a small hallucination."
      }
    },
    {
      "question_id": "002",
      "question": "Once Alex finishes asking what makes the actual process of giving evidence so difficult, when does Paul begin to describe the cross-examination process?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.92,
        "end": 92.191
      },
      "pred_interval": {
        "start": 88.4,
        "end": 88.7
      },
      "iou": 0.041259799202310155,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.480000000000004,
        "end": 3.4909999999999997,
        "average": 3.485500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.25925925925925924,
        "text_similarity": 0.3673415780067444,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that Paul speaks after Alex, but the provided timestamps are several seconds off from the ground truth (Alex: 88.4s vs 83.718s; Paul start: 88.7s vs 84.92s) and thus misalign the event timing; minor phrase match doesn't compensate for the timing errors."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes describing the anecdote about the expert witness losing credibility, when does he start describing the other example of a fact witness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 172.683,
        "end": 180.236
      },
      "pred_interval": {
        "start": 148.1,
        "end": 148.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.583,
        "end": 31.73599999999999,
        "average": 28.159499999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.5727971792221069,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('immediately after') but gives substantially incorrect timestamps for both events (148.1s/148.5s vs. 171.923s/172.683s), so the key factual timing is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker describes the sarcastic comments made about expert witness answers, when does he recount the witness's emotional reaction?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.3,
        "end": 164.2
      },
      "pred_interval": {
        "start": 116.42,
        "end": 122.23
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.88000000000001,
        "end": 41.969999999999985,
        "average": 41.425
      },
      "rationale_metrics": {
        "rouge_l": 0.33663366336633666,
        "text_similarity": 0.8254166841506958,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes the reaction follows the sarcastic comments, but the timestamps and event boundaries are substantially wrong (predicted ~116.4s vs ground-truth 150.0\u2013157.3s), and the relation label ('immediately after') mischaracterizes the ground-truth timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker asks what witness familiarisation actually means, when does the first speaker begin to define the term?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 197.7,
        "end": 204.0
      },
      "pred_interval": {
        "start": 136.68,
        "end": 141.79
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.01999999999998,
        "end": 62.21000000000001,
        "average": 61.614999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.20833333333333331,
        "text_similarity": 0.48295480012893677,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the speakers and the temporal relation (definition begins immediately after the question) but gives substantially incorrect timestamps and durations (off by ~58s), so the answer is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker finishes describing the benefits of witness familiarisation for individuals, when does he next outline the benefits for instructing solicitors?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 300.4,
        "end": 308.0
      },
      "pred_interval": {
        "start": 179.52,
        "end": 188.48
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 120.87999999999997,
        "end": 119.52000000000001,
        "average": 120.19999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8254351615905762,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the subsequent relation and even the target phrase, but it gives substantially incorrect timestamps for both the anchor and target (off by ~120s), so the key factual timing information is wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker explains why they don't jump straight into cross-examination, when does he start talking about the need for theory?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 356.6,
        "end": 365.0
      },
      "pred_interval": {
        "start": 355.5,
        "end": 356.9
      },
      "iou": 0.031578947368416266,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1000000000000227,
        "end": 8.100000000000023,
        "average": 4.600000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.20895522388059704,
        "text_similarity": 0.7373260259628296,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the anchor, target, and the 'after' relation and matches E1's end time, but it incorrectly gives E2's start as 355.5s (should be 356.6s) and omits E2's end time (365.0s)."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the first speaker is discussing the differences in virtual hearings, when does he mention judges requiring witnesses to demonstrate they are alone?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 394.0,
        "end": 398.0
      },
      "pred_interval": {
        "start": 382.0,
        "end": 386.4
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.0,
        "end": 11.600000000000023,
        "average": 11.800000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.30985915492957744,
        "text_similarity": 0.7812016010284424,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies E1 and the 'during' relation, but it gives an incorrect timestamp for E2 (382.0s) that contradicts the reference interval (394.0\u2013398.0s), omitting the key factual timing detail."
      }
    },
    {
      "question_id": "003",
      "question": "Once the second speaker finishes thanking Paul, when does he mention sharing some of his insights?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 435.4,
        "end": 437.5
      },
      "pred_interval": {
        "start": 403.0,
        "end": 404.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.39999999999998,
        "end": 33.19999999999999,
        "average": 32.79999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.6493716835975647,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the mention follows the thank-you, but it gives substantially incorrect timestamps and misstates the timing (saying they are simultaneous/immediately after), so key factual timing details are wrong."
      }
    }
  ]
}