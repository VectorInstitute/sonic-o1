{
  "topic_id": 5,
  "topic_name": "Courtroom Proceedings",
  "num_evaluated": 343,
  "aggregated_metrics": {
    "mean_iou": 0.04206320440512064,
    "std_iou": 0.1426559138340296,
    "median_iou": 0.0,
    "R@0.3": {
      "recall": 0.061224489795918366,
      "count": 21,
      "total": 343
    },
    "R@0.5": {
      "recall": 0.026239067055393587,
      "count": 9,
      "total": 343
    },
    "R@0.7": {
      "recall": 0.014577259475218658,
      "count": 5,
      "total": 343
    },
    "mae": {
      "start_mean": 38.99898250728862,
      "end_mean": 39.99396793002916,
      "average_mean": 39.49647521865889
    },
    "rationale": {
      "rouge_l_mean": 0.3172284979588414,
      "rouge_l_std": 0.0769290483535118,
      "text_similarity_mean": 0.7540428952121179,
      "text_similarity_std": 0.07670577136653117,
      "llm_judge_score_mean": 2.676384839650146,
      "llm_judge_score_std": 1.5377454220688738
    },
    "rationale_cider": 0.23588486120695673
  },
  "per_question_results": [
    {
      "question_id": "001",
      "question": "After the attorney states they will only proceed with the sentencing hearing for disorderly conduct and not the breach of bail, when does Frank ask if the breach of bail charge is being dropped?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 40.292,
        "end": 41.433
      },
      "pred_interval": {
        "start": 15.0,
        "end": 18.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.292,
        "end": 23.433,
        "average": 24.3625
      },
      "rationale_metrics": {
        "rouge_l": 0.2222222222222222,
        "text_similarity": 0.5854954719543457,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives incorrect timestamps (E2 at 15\u201318s vs. correct 40.292\u201341.433s), omits the correct E1 timing, and its provided times contradict the stated 'after' relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Frank states he is being persecuted for exercising his First Amendment rights, when does he say he was found guilty for being loud due to a disability?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 133.165,
        "end": 141.734
      },
      "pred_interval": {
        "start": 55.0,
        "end": 58.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 78.16499999999999,
        "end": 83.73400000000001,
        "average": 80.9495
      },
      "rationale_metrics": {
        "rouge_l": 0.3170731707317073,
        "text_similarity": 0.7399955987930298,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives incorrect timestamps (E2 at 55\u201358s vs. correct 133.165\u2013141.734s) and omits E1 timestamps, so the temporal details are largely wrong despite correctly labeling the relation as 'after.'"
      }
    },
    {
      "question_id": "003",
      "question": "After the attorney asks Frank why he is getting angry, when does Frank respond that he is angry because he is there for nothing?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 119.983,
        "end": 123.427
      },
      "pred_interval": {
        "start": 120.0,
        "end": 123.0
      },
      "iou": 0.8710801393728217,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.016999999999995907,
        "end": 0.4270000000000067,
        "average": 0.2220000000000013
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238805,
        "text_similarity": 0.7609730958938599,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and gives E2 timing (120.0\u2013123.0s) and the 'after' relation, with E2 closely matching the reference; it omits an explicit end time for E1 and has a minor 0.427s end-time discrepancy for E2."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman says, 'Frank, you're in a courthouse,' when does the man respond by telling her to arrest him?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 173.35,
        "end": 176.25
      },
      "pred_interval": {
        "start": 170.5,
        "end": 172.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.8499999999999943,
        "end": 4.25,
        "average": 3.549999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168675,
        "text_similarity": 0.5672011375427246,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer gets the temporal relation right and the E1 start time, but it omits E1's end time and gives E2 timestamps that are several seconds earlier than the ground truth, so the timing is substantially incorrect/incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "Once the text 'Libraries already protected more than one video that YouTube took down' finishes being described by the narrator, when does the text describing YouTube's strike appear?",
      "video_id": "TVriGlkPexA",
      "video_number": "001",
      "segment": {
        "start": 150.0,
        "end": 251.0
      },
      "gt_interval": {
        "start": 169.23,
        "end": 175.0
      },
      "pred_interval": {
        "start": 210.0,
        "end": 212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.77000000000001,
        "end": 37.0,
        "average": 38.885000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.27999999999999997,
        "text_similarity": 0.5715879797935486,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('once' / narrator finishes then text appears) and mentions the visual cue, but the key factual elements\u2014start/end timestamps and durations\u2014are significantly incorrect (off by ~43s and wrong end times), so it fails on precise timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes mentioning the defendant was convicted of three counts of homicide by intoxicated use of a motor vehicle, when does he mention a count of injury by intoxicated use of a motor vehicle?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 16.341,
        "end": 19.565
      },
      "pred_interval": {
        "start": 45.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.659,
        "end": 27.435,
        "average": 28.046999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2816901408450704,
        "text_similarity": 0.6997913122177124,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction references the same events but gives substantially incorrect timestamps (44\u201347s vs correct 10.7\u201319.565s) and labels the relation as 'after' instead of the specified 'once_finished', so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the defendant was serving a significant prison sentence for those convictions, when does he specify the total sentence length?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.5,
        "end": 46.7
      },
      "pred_interval": {
        "start": 75.0,
        "end": 77.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.5,
        "end": 30.299999999999997,
        "average": 31.9
      },
      "rationale_metrics": {
        "rouge_l": 0.31250000000000006,
        "text_similarity": 0.7699028849601746,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relation but the event timestamps are substantially wrong (E1/E2 times differ dramatically from the ground truth), failing to match key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker laments that Tim had a long life ahead of him, when does he state that rehabilitation is one of the reasons people are sent to prison?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.786,
        "end": 207.069
      },
      "pred_interval": {
        "start": 120.0,
        "end": 122.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 83.786,
        "end": 85.06899999999999,
        "average": 84.4275
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.7123520374298096,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly labels the relation as 'after', it mislocates both anchor and target timestamps by a large margin (119\u2013122s vs the reference 145.3\u2013157.2s and 203.8\u2013207.1s) and removes the noted noticeable gap, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the male attorney finishes his address to the court, when does the judge ask if there are representatives of the victim?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 304.214,
        "end": 307.942
      },
      "pred_interval": {
        "start": 180.5,
        "end": 182.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 123.714,
        "end": 125.94200000000001,
        "average": 124.828
      },
      "rationale_metrics": {
        "rouge_l": 0.36111111111111116,
        "text_similarity": 0.718218207359314,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer contradicts the reference on both event timings (178.0s/180.5\u2013182.0s vs 300.0s/304.214\u2013307.942s) and the temporal relation ('after' vs 'once_finished'), so it does not match the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge states that the person has to use the microphone, when does the man in the white t-shirt (victim) begin moving to the microphone?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 356.0
      },
      "pred_interval": {
        "start": 190.0,
        "end": 192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 162.0,
        "end": 164.0,
        "average": 163.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5348837209302325,
        "text_similarity": 0.753317654132843,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly preserves the temporal relation ('after') and that the man moves shortly after the judge speaks, but it gives wrong timestamps and durations for both events (misplacing E1 and E2 by a large margin and using different interval lengths), so it is factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "During the man's speech to the court, when does he say, 'You killed somebody that meant a lot'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 401.276,
        "end": 403.024
      },
      "pred_interval": {
        "start": 210.5,
        "end": 212.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 190.776,
        "end": 190.524,
        "average": 190.65
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179104,
        "text_similarity": 0.7198477387428284,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps for both the speech start and the phrase occurrence are substantially incorrect compared to the reference (205.0 vs 368.0s and 210.5\u2013212.5s vs 401.276\u2013403.024s), though it correctly labels the relationship as 'during'."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge warns the man in the white shirt about interrupting, when does the judge get up and leave the bench?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.13,
        "end": 331.15
      },
      "pred_interval": {
        "start": 410.5,
        "end": 412.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.37,
        "end": 80.85000000000002,
        "average": 80.11000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.25316455696202533,
        "text_similarity": 0.8553589582443237,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and their temporal relation ('after') and the visual cue, but the reported timestamps for both the anchor and target are substantially different from the reference, so the temporal localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes asking the man if he wishes to address him, when does the man respond?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.38,
        "end": 331.39
      },
      "pred_interval": {
        "start": 475.2,
        "end": 476.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 143.82,
        "end": 144.61,
        "average": 144.215
      },
      "rationale_metrics": {
        "rouge_l": 0.21176470588235297,
        "text_similarity": 0.7607365250587463,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly labels the relationship as 'after', but its timestamps for both anchor and target are completely different from the reference and its claimed 'immediately after' audio cue contradicts the provided predicted timings, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the man in the white shirt states his child's birth date, when does he describe being happy and proud about becoming a parent?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 331.55,
        "end": 331.58
      },
      "pred_interval": {
        "start": 520.3,
        "end": 522.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 188.74999999999994,
        "end": 190.92000000000002,
        "average": 189.83499999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32098765432098764,
        "text_similarity": 0.8303531408309937,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the relationship as 'after' and the nature of the events, its timestamps are substantially different from the ground truth and thus factually incorrect; it also adds an audio-cue detail not present in the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the man finishes his emotional statement, when does the woman with long dark hair start walking towards the speaker's table?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.096,
        "end": 512.12
      },
      "pred_interval": {
        "start": 550.0,
        "end": 553.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.903999999999996,
        "end": 40.879999999999995,
        "average": 39.391999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2597402597402597,
        "text_similarity": 0.7069514989852905,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the relation ('after') and the visual cue (woman moves toward table) but gives significantly incorrect timestamps that contradict the ground truth, so it is largely factually wrong despite some semantic alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the woman with long dark hair finishes sitting down at the table, when does she say 'Good afternoon'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.245,
        "end": 512.259
      },
      "pred_interval": {
        "start": 560.0,
        "end": 561.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.754999999999995,
        "end": 49.240999999999985,
        "average": 48.49799999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7336313128471375,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events (woman says 'Good afternoon' after sitting) and notes the audio cue, but the timestamps are substantially incorrect and the relation 'after' does not capture the immediate 'once_finished' timing indicated in the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the woman says 'my son did not deserve this', when does she list his family relationships?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 513.109,
        "end": 513.197
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 86.89099999999996,
        "end": 91.803,
        "average": 89.34699999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.6667934060096741,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets only the qualitative order ('after') but the timestamps and durations are completely incorrect (predicted 595\u2013605s vs ground truth ~512.6\u2013513.2s) and it fails to match the short pause timing, so it does not align with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman states she misses the things that irritated her about her son, when does she request life without the possibility of parole?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 779.1,
        "end": 786.0
      },
      "pred_interval": {
        "start": 720.5,
        "end": 722.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.60000000000002,
        "end": 64.0,
        "average": 61.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6922603845596313,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the event timestamps are substantially incorrect compared to the ground truth, and it adds an unsupported audio-cue detail; key factual elements (exact times) are mismatched."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first woman finishes her statement saying 'That's it', when does attorney Koenig begin describing the defendant's difficult upbringing?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 829.7,
        "end": 831.0
      },
      "pred_interval": {
        "start": 750.0,
        "end": 755.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 79.70000000000005,
        "end": 76.0,
        "average": 77.85000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.40449438202247195,
        "text_similarity": 0.7595005631446838,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps for both events are substantially incorrect compared to the reference, and it includes an extra visual cue (shuffling papers) not supported by the ground truth. These factual/time errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After attorney Koenig explains that the defendant did not pursue an NGI defense, when does she state that the NGI defense actually came from her and attorney Zawada?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 892.0,
        "end": 900.0
      },
      "pred_interval": {
        "start": 810.0,
        "end": 812.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.0,
        "end": 88.0,
        "average": 85.0
      },
      "rationale_metrics": {
        "rouge_l": 0.30588235294117644,
        "text_similarity": 0.6647233366966248,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer's timestamps are significantly incorrect (much earlier and much shorter than the ground-truth intervals), so it fails to match the correct anchor/target times; only the 'after' relation is consistent."
      }
    },
    {
      "question_id": "001",
      "question": "After the female speaker mentions \"mental illness\", when does she state that Joshua Skolman needs treatment in the institution?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 920.573,
        "end": 922.798
      },
      "pred_interval": {
        "start": 890.5,
        "end": 892.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.07299999999998,
        "end": 30.798000000000002,
        "average": 30.43549999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.19672131147540983,
        "text_similarity": 0.652843713760376,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the correct temporal relation ('after') but the reported timestamps for both the anchor and target are substantially incorrect compared to the reference, so key factual timing information is wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks Joshua Skolman if he has anything to tell him, when does Skolman explicitly deny having mental illness?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1001.283,
        "end": 1002.784
      },
      "pred_interval": {
        "start": 945.0,
        "end": 946.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.283000000000015,
        "end": 56.28399999999999,
        "average": 56.283500000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.2153846153846154,
        "text_similarity": 0.7244501113891602,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that Skolman's denial occurs after the judge's question and identifies the event types, but both anchor and target timestamps are substantially incorrect compared to the ground truth, so the timing is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After Joshua Skolman states \"I don't have mental illness\", when does he next explicitly claim that \"you guys\" have the mental illness or defect?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1006.129,
        "end": 1009.331
      },
      "pred_interval": {
        "start": 955.2,
        "end": 957.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.928999999999974,
        "end": 52.33100000000002,
        "average": 51.629999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.8245127201080322,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but both the anchor and target timestamps (946.5s/955.2\u2013957.0s) substantially disagree with the ground-truth intervals (1001.283\u20131002.784s and 1006.129\u20131009.331s), so key factual elements are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the Judge says, 'Thank you, Mr. Scolman', when does he state that only God creates life?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1149.0,
        "end": 1151.0
      },
      "pred_interval": {
        "start": 1110.5,
        "end": 1113.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.5,
        "end": 37.799999999999955,
        "average": 38.14999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285715,
        "text_similarity": 0.6655243635177612,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') correct, but the timestamp locations for both anchor and target are substantially incorrect compared to the ground truth, and it adds an unsupported visual cue (Judge turns to the audience)."
      }
    },
    {
      "question_id": "002",
      "question": "Once the clerk finishes commanding silence, when does the deputy say 'Be seated'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1109.8,
        "end": 1110.5
      },
      "pred_interval": {
        "start": 1175.0,
        "end": 1176.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.20000000000005,
        "end": 65.70000000000005,
        "average": 65.45000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6801731586456299,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordinal relation (deputy speaks after the clerk) but the timestamps are substantially incorrect, durations do not match, and it adds an unverified audio cue; overall key factual details are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the Judge states 'Every life has value', when does he begin speaking about humanity's 'terrible, terrible dark side'?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1164.5,
        "end": 1169.5
      },
      "pred_interval": {
        "start": 1230.4,
        "end": 1234.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.90000000000009,
        "end": 64.5,
        "average": 65.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.6591605544090271,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the anchor and target timestamps are substantially different from the ground truth, and it adds an unverified visual cue; therefore it is largely incorrect despite getting the relation right."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states he doesn't think it's a mental illness, when does he define a thoughtful conviction?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1233.453,
        "end": 1238.14
      },
      "pred_interval": {
        "start": 1250.5,
        "end": 1260.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.047000000000025,
        "end": 21.8599999999999,
        "average": 19.453499999999963
      },
      "rationale_metrics": {
        "rouge_l": 0.3582089552238806,
        "text_similarity": 0.7268766164779663,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation correct ('after') but the provided timestamps differ substantially from the reference and the anchor end time is omitted, so key factual timing details are incorrect or missing."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if he needs to recite historical crimes, when does he state that it causes one to pause and lose their breath?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1259.682,
        "end": 1264.588
      },
      "pred_interval": {
        "start": 1305.0,
        "end": 1312.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.317999999999984,
        "end": 47.412000000000035,
        "average": 46.36500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.7674236297607422,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but the provided timestamps are substantially different from the ground truth, so the key factual elements (accurate timings) are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge declares that civilized society values life, when does he state that taking a life is the highest crime?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1362.484,
        "end": 1366.753
      },
      "pred_interval": {
        "start": 1385.0,
        "end": 1395.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.516000000000076,
        "end": 28.24700000000007,
        "average": 25.381500000000074
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.5716748237609863,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the event sequence correct (target after anchor) but the provided start/end timestamps are substantially different from the ground truth (off by ~20\u201330s) and the predicted target duration is much longer; it also omits the note about the camera zoom/audio continuity."
      }
    },
    {
      "question_id": "001",
      "question": "After the inmate first looks down at the paper handed to him, when is he handed paper again by an officer?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1603.0,
        "end": 1603.4
      },
      "pred_interval": {
        "start": 1602.5,
        "end": 1603.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 0.40000000000009095,
        "average": 0.4500000000000455
      },
      "rationale_metrics": {
        "rouge_l": 0.3373493975903615,
        "text_similarity": 0.8517004251480103,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the temporal relationship ('after') and roughly locates the second handing around 1603s, but the anchor timestamp is off by ~9s and the event boundaries for both events differ from the reference, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the inmate turns his head to the side, when does he start walking away, escorted by the officers?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1626.0,
        "end": 1627.0
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1622.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 5.0,
        "average": 5.5
      },
      "rationale_metrics": {
        "rouge_l": 0.31707317073170727,
        "text_similarity": 0.8970130681991577,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly states the temporal relation ('after') and describes the events, its timestamps for both E1 and E2 are substantially inconsistent with the ground truth (off by many seconds), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "003",
      "question": "After the distinct sound of a door or gate opening, when does the inmate walk through the door?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1644.0
      },
      "gt_interval": {
        "start": 1636.0,
        "end": 1637.0
      },
      "pred_interval": {
        "start": 1635.5,
        "end": 1636.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.5,
        "end": 1.0,
        "average": 0.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3529411764705882,
        "text_similarity": 0.9066085815429688,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and the audio-visual cue, but it misreports the event times (anchor is ~30s later than the ground truth and the target timing is shifted/shortened), so it mislocates the events."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge states, \"You will never be released from the state prison system,\" when does he mention a \"compass evaluation\" as part of the extended supervision conditions?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1433.0,
        "end": 1436.0
      },
      "pred_interval": {
        "start": 1452.3,
        "end": 1455.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.299999999999955,
        "end": 19.700000000000045,
        "average": 19.5
      },
      "rationale_metrics": {
        "rouge_l": 0.29545454545454547,
        "text_similarity": 0.7686444520950317,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies both utterances and the 'after' relationship, but the reported timestamps are substantially later than the ground truth (both E1 and E2 shifted by ~30s), so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge mentions that the 25-year sentence for Count 2 will be consecutive to any sentence the defendant is now serving, when does the camera cut to a shot of the back of the defendant's head?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.8,
        "end": 1440.5
      },
      "pred_interval": {
        "start": 1508.9,
        "end": 1510.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 69.10000000000014,
        "end": 70.0,
        "average": 69.55000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.30588235294117644,
        "text_similarity": 0.7883691787719727,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal relation ('after') but the event boundaries and timestamps are substantially incorrect and conflict with the ground truth, so it fails on factual accuracy and completeness."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge orders the $5,000 paid by the Compensation Panel to be part of the restitution order, when does the defendant stand up in the courtroom?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1539.0,
        "end": 1542.0
      },
      "pred_interval": {
        "start": 1585.6,
        "end": 1588.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.59999999999991,
        "end": 46.200000000000045,
        "average": 46.39999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2816901408450704,
        "text_similarity": 0.7702416777610779,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the causal 'after' relationship correct but the event timestamps are substantially wrong (E1 actually ends at 1465.0s vs predicted ~1580s; E2 actually starts at 1539.0s vs predicted 1585.6s), so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the man looks up and turns his head to his left, when do the man and deputies open the door and exit the room?",
      "video_id": "Z_6JBsE07b8",
      "video_number": "002",
      "segment": {
        "start": 1590.0,
        "end": 1642.992
      },
      "gt_interval": {
        "start": 1631.0,
        "end": 1634.0
      },
      "pred_interval": {
        "start": 1620.5,
        "end": 1623.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.5,
        "end": 11.0,
        "average": 10.75
      },
      "rationale_metrics": {
        "rouge_l": 0.411764705882353,
        "text_similarity": 0.7571507692337036,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relation (E2 occurs after E1) but the event timestamps are substantially earlier than the ground truth (off by ~7\u201311 seconds) and it adds audio cues not mentioned in the reference, so it fails on factual timing and includes extra, unsupported detail."
      }
    },
    {
      "question_id": "001",
      "question": "While the male anchor announces a jury has reached a verdict in the Chandler Halderson case, when does the on-screen text 'JURY REACHES VERDICT' first appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 4.6,
        "end": 22.8
      },
      "pred_interval": {
        "start": 12.5,
        "end": 17.0
      },
      "iou": 0.2472527472527472,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.9,
        "end": 5.800000000000001,
        "average": 6.8500000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.9051854610443115,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timings contradict the ground truth: the anchor actually speaks around 0.03\u20136.6s but the prediction places it at 10.0s, and the on-screen text appears at 4.6s (not 12.5s) and persists to 22.8s (not 17.0s). The predicted relationship ('shortly after') is incorrect since the text appears during the anchor's announcement."
      }
    },
    {
      "question_id": "002",
      "question": "Once the female anchor states Halderson faces eight charges, when does the on-screen graphic detailing the charges appear?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 23.7,
        "end": 35.8
      },
      "pred_interval": {
        "start": 45.0,
        "end": 50.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.3,
        "end": 14.200000000000003,
        "average": 17.75
      },
      "rationale_metrics": {
        "rouge_l": 0.23728813559322037,
        "text_similarity": 0.7563154697418213,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timing and relationship contradict the reference: the ground truth has E1 ending at 23.6s and E2 appearing immediately at 23.7s until 35.8s, whereas the prediction places the anchor at 40.0s and the graphic at 45.0\u201350.0s after a pause, which is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male anchor says 'Let's actually go to the courtroom now live', when does the judge begin speaking?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 203.7,
        "end": 204.9
      },
      "pred_interval": {
        "start": 105.0,
        "end": 110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 98.69999999999999,
        "end": 94.9,
        "average": 96.8
      },
      "rationale_metrics": {
        "rouge_l": 0.21917808219178084,
        "text_similarity": 0.8171780109405518,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps and durations conflict completely with the ground truth (GT anchor finishes at 200.9s and judge speaks 203.7\u2013204.9s), so the timing is incorrect despite preserving the correct order of events."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking if there is anything from the state, when does the state reply?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.02,
        "end": 151.03
      },
      "pred_interval": {
        "start": 172.5,
        "end": 173.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.47999999999999,
        "end": 22.77000000000001,
        "average": 22.125
      },
      "rationale_metrics": {
        "rouge_l": 0.20000000000000004,
        "text_similarity": 0.7418380975723267,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer\u2019s relation loosely matches ('immediately after' vs 'once_finished'), but the event times and durations are substantially different (predicted ~170\u2013173s vs ground truth ~150.6\u2013151.03s), so it fails to correctly locate the events and largely misaligns with the reference."
      }
    },
    {
      "question_id": "002",
      "question": "After the female reporter states the jury deliberated for just over two hours, when does the male reporter comment on the quick verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 152.45,
        "end": 152.5
      },
      "pred_interval": {
        "start": 245.0,
        "end": 246.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 92.55000000000001,
        "end": 93.69999999999999,
        "average": 93.125
      },
      "rationale_metrics": {
        "rouge_l": 0.21917808219178084,
        "text_similarity": 0.548317551612854,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering right (male comment follows the female remark) but the timestamps are substantially incorrect and it wrongly labels the interaction as 'immediately after' despite the reference noting intervening discussion; overall largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes asking if the jury reached a verdict on all counts, when does the jury foreman respond?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.2,
        "end": 153.2
      },
      "pred_interval": {
        "start": 310.5,
        "end": 312.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 157.3,
        "end": 158.8,
        "average": 158.05
      },
      "rationale_metrics": {
        "rouge_l": 0.17910447761194026,
        "text_similarity": 0.6987471580505371,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the order (judge then foreman) but the timestamps and durations are substantially incorrect and the response is not marked as immediate as in the ground truth; overall the key factual timing and relation are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the foreperson states that the jury reached a verdict, when does the court staff receive the verdict folder?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 357.2,
        "end": 357.9
      },
      "pred_interval": {
        "start": 345.0,
        "end": 346.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.199999999999989,
        "end": 11.899999999999977,
        "average": 12.049999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.3218390804597701,
        "text_similarity": 0.7200831770896912,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but the event timestamps are substantially off and it omits the judge's instruction; therefore it does not accurately match the reference timing or key causal detail."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes reading the verdict for Count 1, when does he begin reading the verdict for Count 2?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 441.7,
        "end": 445.2
      },
      "pred_interval": {
        "start": 360.0,
        "end": 362.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 81.69999999999999,
        "end": 83.19999999999999,
        "average": 82.44999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.32876712328767127,
        "text_similarity": 0.79366135597229,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction's timestamps and temporal relation contradict the ground truth: the correct answer states an immediate start at 441.7s (relation: once_finished), whereas the prediction gives much earlier, different times (358.0s\u2192360.0s\u2192362.0s) and labels the relation as 'after', so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge finishes reading the verdict for Count 8, when does he state that 'not guilty' verdict forms were also returned?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 630.9,
        "end": 641.0
      },
      "pred_interval": {
        "start": 410.0,
        "end": 412.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 220.89999999999998,
        "end": 229.0,
        "average": 224.95
      },
      "rationale_metrics": {
        "rouge_l": 0.41666666666666663,
        "text_similarity": 0.7315983176231384,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches, the predicted event timestamps (E1 at 408.0s, E2 410.0\u2013412.0s) conflict substantially with the reference times (E1 ~628.8s, E2 630.9\u2013641.0s), so the answer is largely factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge finishes reading the final guilty verdict for count 8, when does he begin to inquire if the jury members agreed with the verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 528.9,
        "end": 619.0
      },
      "pred_interval": {
        "start": 530.0,
        "end": 533.0
      },
      "iou": 0.03329633740288567,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1000000000000227,
        "end": 86.0,
        "average": 43.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.47500000000000003,
        "text_similarity": 0.7738210558891296,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly captures the order and roughly the start of E2, but it misstates E1 by a significant margin (525.0s vs 513.0s) and omits the E2 end time (619.0s), making it incomplete and partially inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the last juror confirms their agreement to the verdicts, when does the judge begin his speech thanking the jury for their service?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 621.0,
        "end": 665.0
      },
      "pred_interval": {
        "start": 580.0,
        "end": 585.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.0,
        "end": 80.0,
        "average": 60.5
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.7027941942214966,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the order (judge speaks after the last juror) but both timestamps are substantially wrong (575s vs 617s and 580s vs 621s) and it omits the E2 end time, thus failing to match key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes instructing the defendant's table to be seated, when does Attorney Brown make his motion?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 737.0,
        "end": 741.0
      },
      "pred_interval": {
        "start": 620.0,
        "end": 625.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 117.0,
        "end": 116.0,
        "average": 116.5
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7849922180175781,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted temporal relation ('after') matches, the predicted timestamps are substantially incorrect (off by ~117s) and the prediction omits the event end time, so it fails to preserve key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the judge finishes asking about any pre-sentence request, when does Attorney Brown state that a pre-sentence investigation should be performed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 695.0,
        "end": 697.5
      },
      "pred_interval": {
        "start": 702.5,
        "end": 706.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5,
        "end": 8.5,
        "average": 8.0
      },
      "rationale_metrics": {
        "rouge_l": 0.2769230769230769,
        "text_similarity": 0.7373310327529907,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the high-level ordering (Brown speaks after the judge) but the timestamps are substantially different and the relation 'after' fails to match the precise 'once_finished'/immediate timing in the reference, so key factual details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge orders a pre-sentence investigation, when does he specify that there should not be any recommendations made within the report?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 749.6,
        "end": 754.5
      },
      "pred_interval": {
        "start": 715.0,
        "end": 718.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.60000000000002,
        "end": 36.5,
        "average": 35.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068966,
        "text_similarity": 0.7410212755203247,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives substantially different timestamps for both events and a different temporal relation ('after' vs the correct immediate 'once_finished'), so it fails to match the key factual elements of the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the news anchor finishes stating the sentencing hearing dates, when does the District Attorney begin his statement to the media?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 935.0,
        "end": 938.5
      },
      "pred_interval": {
        "start": 850.0,
        "end": 853.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.0,
        "end": 85.5,
        "average": 85.25
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.8466464281082153,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct temporal relation ('after') but the provided timestamps are significantly and incorrectly shifted from the ground truth (anchor 903.8s vs 849.0s; DA 935.0s vs 850.0s), so key factual elements are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the system does not work without jurors, when does he mention that the jury of 18 sacrificed a month of their life?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 901.4,
        "end": 908.9
      },
      "pred_interval": {
        "start": 890.5,
        "end": 895.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.899999999999977,
        "end": 13.899999999999977,
        "average": 12.399999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.27027027027027023,
        "text_similarity": 0.674609899520874,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the correct semantic relation (E2 occurs after the anchor) and references the same utterances, but the provided timestamps are substantially offset (~10\u201314s) from the ground truth, the anchor end time is missing, and the intervals do not match the reference."
      }
    },
    {
      "question_id": "002",
      "question": "Once the District Attorney states that the process had to take a whole week off because of the virus, when does he commend people for sacrificing their time and potentially their safety?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 971.4,
        "end": 982.1
      },
      "pred_interval": {
        "start": 950.0,
        "end": 955.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.399999999999977,
        "end": 27.100000000000023,
        "average": 24.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2439024390243902,
        "text_similarity": 0.7013006210327148,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures that the commendation follows the DA's remark, but the provided timestamps are substantially incorrect and do not match the precise intervals given in the correct answer, so the response is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interviewer asks for a message to Barton Krista's family, when does the District Attorney state he will speak to them after?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1027.2,
        "end": 1028.7
      },
      "pred_interval": {
        "start": 1010.0,
        "end": 1015.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.200000000000045,
        "end": 13.700000000000045,
        "average": 15.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.6027705669403076,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer has substantially incorrect timestamps (around 1005\u20131015s vs. the correct 1026.6\u20131028.7s), so the temporal alignment is wrong; although it correctly indicates an 'after' relationship, the key timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the District Attorney states that the Sheriff's Department could get the case to trial in short order, when does he talk about the professionalism and integrity of law enforcement?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1088.6,
        "end": 1095.4
      },
      "pred_interval": {
        "start": 1085.0,
        "end": 1095.0
      },
      "iou": 0.6153846153846187,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.599999999999909,
        "end": 0.40000000000009095,
        "average": 2.0
      },
      "rationale_metrics": {
        "rouge_l": 0.38709677419354843,
        "text_similarity": 0.8000373840332031,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relation ('after') and roughly locates the target utterance, but the anchor timing is substantially off (predicted 1075.0s vs correct end 1086.1s) and the target start time is a few seconds early, so the temporal alignment is only approximate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interviewer asks about the historic significance and challenges of the trial, when does the District Attorney confirm the trial was unprecedented?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1200.2,
        "end": 1202.0
      },
      "pred_interval": {
        "start": 1150.0,
        "end": 1160.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.200000000000045,
        "end": 42.0,
        "average": 46.10000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3548387096774194,
        "text_similarity": 0.7563629150390625,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timings and relation are completely inconsistent with the ground truth\u2014the prediction places the DA's confirmation much earlier (1150\u20131160s) and labels the relation 'after', whereas the correct target occurs at 1200.2\u20131202.0s immediately following the anchor ('once_finished')."
      }
    },
    {
      "question_id": "003",
      "question": "After the District Attorney says they may never know 'the why' behind the crime, when does the news anchor cut in to summarize this point?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1358.6,
        "end": 1367.8
      },
      "pred_interval": {
        "start": 1220.0,
        "end": 1230.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 138.5999999999999,
        "end": 137.79999999999995,
        "average": 138.19999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.46376811594202894,
        "text_similarity": 0.8921222686767578,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different timestamps and places the anchor summary immediately after the DA, contradicting the reference which locates the summary much later (after the DA speaks about family/prosecution); the relation and key temporal details are therefore incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the news anchor finishes stating that the system worked and the verdict graphic is fully shown, when does the narrator begin listing the specific guilty verdicts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1265.0,
        "end": 1275.0
      },
      "pred_interval": {
        "start": 1250.0,
        "end": 1265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.0,
        "end": 10.0,
        "average": 12.5
      },
      "rationale_metrics": {
        "rouge_l": 0.4155844155844156,
        "text_similarity": 0.7683089971542358,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction contradicts the reference on both key timestamps and the anchor wording (1245.0s/'the system worked' vs 1257.0s/'bring the Haldersons back') and also misstates the narrator's start/end times (1250.0\u20131265.0s vs 1265.0\u20131275.0s), so it is largely incorrect despite a similar temporal relation."
      }
    },
    {
      "question_id": "002",
      "question": "After reporter Jaymes Langrehr finishes saying that the sheer volume of evidence proved to be overwhelming for the jurors, when does he explain that DNA analysts proved bloodstains belonged to the parents?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1350.0,
        "end": 1364.0
      },
      "pred_interval": {
        "start": 1320.0,
        "end": 1330.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.0,
        "end": 34.0,
        "average": 32.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.693185031414032,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the provided timestamps are substantially incorrect (off by ~20\u201334 seconds from the reference), so the timing information is not accurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once reporter Jaymes Langrehr finishes saying there was no one specific piece of evidence that 'really stuck out', when is the next time he mentions bringing in the DNA analysts?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1348.0,
        "end": 1352.0
      },
      "pred_interval": {
        "start": 1380.0,
        "end": 1395.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.0,
        "end": 43.0,
        "average": 37.5
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.7789293527603149,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the same events (Jaymes saying 'really stuck out' and then mentioning DNA analysts) but gives substantially incorrect timestamps and durations and uses 'after' instead of the immediate 'next' relation, so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "001",
      "question": "After MS. NULAND finishes asking \"What's your response to that?\", when does the Sheriff begin responding about his faith in the team?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1426.47,
        "end": 1430.395
      },
      "pred_interval": {
        "start": 1422.5,
        "end": 1430.0
      },
      "iou": 0.4471184293856847,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.9700000000000273,
        "end": 0.3949999999999818,
        "average": 2.1825000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615385,
        "text_similarity": 0.7431619763374329,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and approximate segment, but the timestamps are noticeably off (E1 end ~5.6s early and E2 start ~4s early versus the reference), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the Sheriff finishes explaining what he will remember most about the case, when does a reporter ask about the case informing future work?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1491.452,
        "end": 1494.597
      },
      "pred_interval": {
        "start": 1505.0,
        "end": 1512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.548000000000002,
        "end": 17.40300000000002,
        "average": 15.475500000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.24242424242424243,
        "text_similarity": 0.7220102548599243,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the coarse ordering (reporter question follows the sheriff) but gives substantially incorrect timestamps, mislabels the speaker, and uses a different relation ('after' vs. the more specific 'once_finished'), so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the reporter finishes their commentary thanking Sheriff Calvin Baer and his deputies for their hard work, when does the reporter next mention that \"A lot of resources involved for a very long time\"?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1528.402,
        "end": 1530.927
      },
      "pred_interval": {
        "start": 1580.0,
        "end": 1588.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.597999999999956,
        "end": 57.07300000000009,
        "average": 54.335500000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.2105263157894737,
        "text_similarity": 0.6965992450714111,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (reporter and phrase) but the timestamps are substantially wrong (~50s off) and the relation is labeled 'after' instead of the correct 'next', so it is largely incorrect despite partial recognition."
      }
    },
    {
      "question_id": "001",
      "question": "After the main anchor introduces Tahlil Maudeen, when does Tahlil begin reporting on the emotion in the gallery?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1701.902,
        "end": 1708.327
      },
      "pred_interval": {
        "start": 1595.0,
        "end": 1600.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 106.90200000000004,
        "end": 108.327,
        "average": 107.61450000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4745762711864407,
        "text_similarity": 0.8506830930709839,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target report occurs 'after' the anchor and describes the same event, but the provided timestamps are substantially incorrect and inconsistent with the ground truth (anchor/end times differ by ~100s), so it fails to answer the when accurately."
      }
    },
    {
      "question_id": "002",
      "question": "After the main anchor asks Tahlil if she talked to Chandler Halderson's defense attorneys, when does the anchor next ask about whom Tahlil interviewed?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1765.828,
        "end": 1767.07
      },
      "pred_interval": {
        "start": 1620.0,
        "end": 1625.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 145.82799999999997,
        "end": 142.06999999999994,
        "average": 143.94899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.368421052631579,
        "text_similarity": 0.7825734615325928,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction names the same interview-related events but gives substantially incorrect timestamps and mislabels the relation ('after' vs. the correct 'next'), so it fails to match the precise temporal anchors and relation in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After Tahlil Maudeen reports that the defense attorneys were unavailable for interviews, when does she report that the DA seemed pleased with the verdict?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1769.555,
        "end": 1783.597
      },
      "pred_interval": {
        "start": 1640.0,
        "end": 1645.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 129.55500000000006,
        "end": 138.59699999999998,
        "average": 134.07600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.8585532903671265,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the event timestamps are substantially incorrect (off by ~130s) and do not match the ground truth anchor/target spans, so it fails on key factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the reporter asks if people were surprised by the quick jury return, when does she explain why they were expecting it?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1789.692,
        "end": 1798.408
      },
      "pred_interval": {
        "start": 1780.5,
        "end": 1785.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.192000000000007,
        "end": 12.707999999999856,
        "average": 10.949999999999932
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444442,
        "text_similarity": 0.8224412202835083,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation ('after'), the reported event timestamps and spans differ substantially from the ground truth (both E1 and E2 are shifted by ~8\u201313s and the intervals do not match), so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host says, 'Thank you all,' when does she begin to introduce the website for more information on the case?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1809.891,
        "end": 1815.742
      },
      "pred_interval": {
        "start": 1802.2,
        "end": 1805.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.691000000000031,
        "end": 9.84199999999987,
        "average": 8.76649999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7924337387084961,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (host saying 'Thank you all' and introducing the website) but the timestamps are off by several seconds and the temporal relation is wrong\u2014the correct target immediately follows the anchor, whereas the prediction places it earlier and merely as 'after.'"
      }
    },
    {
      "question_id": "003",
      "question": "Once the host says, 'Thanks for joining us,' when does she announce the return to regular programming?",
      "video_id": "u0IZ4j1g25s",
      "video_number": "003",
      "segment": {
        "start": 1770.0,
        "end": 1851.0
      },
      "gt_interval": {
        "start": 1830.005,
        "end": 1831.628
      },
      "pred_interval": {
        "start": 1830.4,
        "end": 1833.8
      },
      "iou": 0.323583662714068,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.3949999999999818,
        "end": 2.1720000000000255,
        "average": 1.2835000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.3714285714285714,
        "text_similarity": 0.8339327573776245,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction roughly locates the same events but has multiple factual errors: E1 start time is off and its end time is omitted, E2 end time is significantly later than the reference, and the relationship 'after' contradicts the reference's 'immediately follows.' These inaccuracies reduce completeness and correctness."
      }
    },
    {
      "question_id": "001",
      "question": "After the narrator explains that the judges stopped the video, when does the judge in the embedded video actually stop the video and question the man?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 217.92,
        "end": 221.605
      },
      "pred_interval": {
        "start": 45.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 172.92,
        "end": 174.605,
        "average": 173.7625
      },
      "rationale_metrics": {
        "rouge_l": 0.38888888888888884,
        "text_similarity": 0.6996060609817505,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but both event timestamps are grossly incorrect compared to the ground truth (E1 ~22\u201326s vs predicted 40s; E2 ~218\u2013222s vs predicted 45\u201347s), indicating hallucinated/incorrect event localization."
      }
    },
    {
      "question_id": "002",
      "question": "After the judge asks if the video is counsel for the case, when does the man reply, 'I generated that'?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 224.77,
        "end": 225.951
      },
      "pred_interval": {
        "start": 55.0,
        "end": 56.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 169.77,
        "end": 169.451,
        "average": 169.6105
      },
      "rationale_metrics": {
        "rouge_l": 0.39344262295081966,
        "text_similarity": 0.6580427885055542,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') right but the event timestamps are completely incorrect (53\u201356.5s vs. the reference ~218\u2013226s), so it fails to match the key temporal grounding of both events."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge finishes telling the man he is not going to use the courtroom as a business launch, when does she instruct him to stand up for oral argument time?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 323.425,
        "end": 328.018
      },
      "pred_interval": {
        "start": 120.0,
        "end": 122.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 203.425,
        "end": 206.01799999999997,
        "average": 204.7215
      },
      "rationale_metrics": {
        "rouge_l": 0.3614457831325301,
        "text_similarity": 0.7504160404205322,
        "llm_judge_score": 1,
        "llm_judge_justification": "While the prediction correctly indicates an 'after' relationship, it gives completely incorrect timestamps for both events (hallucinated times far from the reference) and thus fails to locate the events as specified in the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the man says \"That is not a real person\", when does the judge say it would have been nice to know that information?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 154.321,
        "end": 158.401
      },
      "pred_interval": {
        "start": 160.5,
        "end": 162.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.179000000000002,
        "end": 3.5989999999999895,
        "average": 4.888999999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.29629629629629634,
        "text_similarity": 0.7095045447349548,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relation ('after'), but both event timestamps are inaccurate compared to the ground truth and the predicted timing/description of immediacy conflicts with the reference, so key factual details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge finishes describing the man's past verbal conversations with her staff, when does she say she doesn't appreciate being misled?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 177.031,
        "end": 181.351
      },
      "pred_interval": {
        "start": 175.0,
        "end": 177.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.031000000000006,
        "end": 4.350999999999999,
        "average": 3.1910000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.2784810126582279,
        "text_similarity": 0.6877022981643677,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the target utterance and the intended immediate relation, but the provided timestamps disagree substantially with the ground truth (anchor end and target start/finish times are incorrect), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the judge states that the man is not going to use the courtroom as a launch for his business, when does she command him to \"shut that off\"?",
      "video_id": "xwZ2K8b_pBw",
      "video_number": "004",
      "segment": {
        "start": 150.0,
        "end": 211.0
      },
      "gt_interval": {
        "start": 200.411,
        "end": 201.981
      },
      "pred_interval": {
        "start": 190.0,
        "end": 192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.411000000000001,
        "end": 9.980999999999995,
        "average": 10.195999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.24175824175824173,
        "text_similarity": 0.7626171708106995,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states the command follows the judge's statement, but the provided timestamps are substantially incorrect and inconsistent with the ground truth (predicted target falls within the true anchor window and is about 8\u201310s early), so the answer is factually wrong on timing."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interrogator asks what kind of objects were used, when does the witness name a toothbrush and shaving utensil?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 150.14,
        "end": 150.22
      },
      "pred_interval": {
        "start": 165.5,
        "end": 170.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.360000000000014,
        "end": 19.97999999999999,
        "average": 17.67
      },
      "rationale_metrics": {
        "rouge_l": 0.26506024096385544,
        "text_similarity": 0.7458637952804565,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the witness names a toothbrush and shaving utensil and that the answer occurs after the question, but the timestamps are substantially off (\u224810\u201320s error) and it adds unsupported audio/visual detail; thus it fails on precise temporal alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once the interrogator asks if the abuser decided to use something besides a toothbrush, when does the witness answer 'Yes'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 151.11,
        "end": 151.12
      },
      "pred_interval": {
        "start": 205.0,
        "end": 206.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.889999999999986,
        "end": 54.879999999999995,
        "average": 54.38499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.24691358024691357,
        "text_similarity": 0.7602986693382263,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the witness answers 'Yes' after the interrogator's question, but the timestamps and durations are far off (200s/205\u2013206s vs. 151.01\u2013151.12s) and it fails to capture the immediate/near-immediate timing; the added nodding detail is unverifiable and not in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "Once the interrogator asks what his mom said, when does the witness recount his mother's full response?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 153.06,
        "end": 153.23
      },
      "pred_interval": {
        "start": 310.0,
        "end": 320.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 156.94,
        "end": 167.27,
        "average": 162.10500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.20833333333333334,
        "text_similarity": 0.7948144674301147,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordinal relation ('after') but the timestamps and durations are completely incorrect (predicted ~305\u2013320s vs. ground truth ~153.03\u2013153.23s) and it adds unverified visual/audio cues; key factual timing and immediacy are thus wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the woman asks, \"What did your mom say?\", when does the man respond about his mom telling him to stop and that he was exaggerating?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 340.0
      },
      "pred_interval": {
        "start": 345.2,
        "end": 347.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.199999999999989,
        "end": 7.800000000000011,
        "average": 9.5
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.8478821516036987,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer preserves the relation ('after') and the content of the response, but the anchor and target timestamps are significantly shifted and do not match the reference timing, so the localization is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the man states he was afraid to tell anyone because his dad didn't want him to, when does he reveal his dad said it was 'our secret'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 387.0,
        "end": 389.0
      },
      "pred_interval": {
        "start": 412.5,
        "end": 414.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.5,
        "end": 25.899999999999977,
        "average": 25.69999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3636363636363636,
        "text_similarity": 0.8277519941329956,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies both statements and their temporal relation ('after'), but the provided timestamps differ substantially from the reference, so it only partially matches the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "Once the woman finishes asking, \"What did you do to your brother?\", when does the man describe taking him to the woods and playing with a toothbrush in the same way?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 427.0,
        "end": 438.0
      },
      "pred_interval": {
        "start": 487.3,
        "end": 492.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.30000000000001,
        "end": 54.0,
        "average": 57.150000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.7317241430282593,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction misidentifies both anchor and target timestamps (large discrepancies from 423\u2013438s vs 485\u2013492s) and gives a different relation ('after' vs 'once_finished'), so it fails to match the correct events and relation."
      }
    },
    {
      "question_id": "001",
      "question": "Once Lyle Menendez removes his hand from his mouth/face, when does he put his fingers back into his mouth while crying?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 515.8,
        "end": 517.7
      },
      "pred_interval": {
        "start": 522.5,
        "end": 523.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.7000000000000455,
        "end": 5.7999999999999545,
        "average": 6.25
      },
      "rationale_metrics": {
        "rouge_l": 0.2028985507246377,
        "text_similarity": 0.49578866362571716,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same visual cue (hand/fingers back to mouth) but the timestamps and duration differ substantially from the reference and it fails to state the immediate transition (saying merely 'after'), so the temporal accuracy is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "While the female voice asks, 'It stopped for you with your dad when you were eight,' when is Erik Menendez first shown on screen?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 536.0,
        "end": 579.0
      },
      "pred_interval": {
        "start": 530.0,
        "end": 531.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.0,
        "end": 48.0,
        "average": 27.0
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.8223216533660889,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly notes a camera cut to Erik Menendez, but its timestamps and durations are significantly incorrect (E1 and E2 times differ greatly from the reference) and the relationship/duration details contradict the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After Erik Menendez says 'Yes' in response to doing something about it, when does the female voice ask 'What did you do?'",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 579.0
      },
      "gt_interval": {
        "start": 560.0,
        "end": 560.8
      },
      "pred_interval": {
        "start": 560.0,
        "end": 561.0
      },
      "iou": 0.7999999999999545,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.0,
        "end": 0.20000000000004547,
        "average": 0.10000000000002274
      },
      "rationale_metrics": {
        "rouge_l": 0.3098591549295775,
        "text_similarity": 0.6579530239105225,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the events and the relative timing (E2 at 560.0s and that it occurs after E1), and E2's end is close; however E1's timestamp (558.0s) and its end time are inconsistent with the reference (557.2\u2013557.5s) and the prediction's 'immediate' follow-up slightly contradicts the reference's noted short pause."
      }
    },
    {
      "question_id": "001",
      "question": "After the camera shows Lyle Menendez crying and sniffling, when does the female voice ask about the abuse stopping when he was eight?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 533.5,
        "end": 536.5
      },
      "pred_interval": {
        "start": 520.5,
        "end": 521.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.0,
        "end": 15.0,
        "average": 14.0
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.7193002700805664,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies E1 as Lyle crying and gives a relation of 'after', but it places E2 (520.5\u2013521.5s) during E1 rather than at 533.5\u2013536.5s as in the reference, which is a major factual timing error and contradicts the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "During the female voice asking if he thought it might be happening to someone else, when is Erik Menendez shown with a distressed facial expression?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 539.0,
        "end": 545.8
      },
      "pred_interval": {
        "start": 540.0,
        "end": 542.0
      },
      "iou": 0.2941176470588255,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0,
        "end": 3.7999999999999545,
        "average": 2.3999999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.3037974683544304,
        "text_similarity": 0.711795449256897,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the female question and the 'during' relationship, but it gives inaccurate timings (E1 start off by 1s and E2 listed as 540.0\u2013542.0) and omits the key fact that Erik's distressed expression spans the entire 539.0\u2013545.8 interval."
      }
    },
    {
      "question_id": "003",
      "question": "Once the female voice finishes asking 'And who did you think it was happening to?', when does Erik Menendez answer 'Eric'?",
      "video_id": "2B_e7fvwi90",
      "video_number": "005",
      "segment": {
        "start": 510.0,
        "end": 578.75
      },
      "gt_interval": {
        "start": 551.0,
        "end": 551.5
      },
      "pred_interval": {
        "start": 560.2,
        "end": 561.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.200000000000045,
        "end": 9.5,
        "average": 9.350000000000023
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.6924756765365601,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly captures the relative relation (Erik answers immediately after the female question) and preserves the short 0.2s gap, but the absolute timestamps and the reported duration of Erik's answer are offset by ~9.2s and differ from the ground truth; the relation label differs slightly ('after' vs 'once_finished')."
      }
    },
    {
      "question_id": "001",
      "question": "After the Presiding Justice asks for the appearance of counsel, when does the appellant's counsel introduce himself?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 11.401,
        "end": 18.83
      },
      "pred_interval": {
        "start": 10.5,
        "end": 12.0
      },
      "iou": 0.07190876350540219,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.9009999999999998,
        "end": 6.829999999999998,
        "average": 3.865499999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3466666666666667,
        "text_similarity": 0.634660005569458,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation, but the timing is inaccurate and incomplete: E1 end is omitted, E2 start (10.5s vs 11.401s) and especially E2 end (12.0s vs 18.83s) are incorrect, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "During the Presiding Justice's question about public importance, when does Mr. Lifrak remain silent and attentive?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 39.5,
        "end": 103.0
      },
      "pred_interval": {
        "start": 85.0,
        "end": 87.0
      },
      "iou": 0.031496062992125984,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.5,
        "end": 16.0,
        "average": 30.75
      },
      "rationale_metrics": {
        "rouge_l": 0.3380281690140845,
        "text_similarity": 0.7768582105636597,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relationship and places Mr. Lifrak's silence within the Presiding Justice's question, but it gives inaccurate/incomplete timing for E1 (predicts 84.0s start vs correct 39.5s start and omits E1 end) and narrows E2 to a short interval rather than the full period described."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Lifrak finishes asking to reserve time for rebuttal, when does the Presiding Justice grant permission?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 109.412,
        "end": 110.2
      },
      "pred_interval": {
        "start": 190.0,
        "end": 192.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.588,
        "end": 81.8,
        "average": 81.19399999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4411764705882353,
        "text_similarity": 0.7629150152206421,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events and the 'once' relation, but the reported timestamps (and E1 interval) are significantly different from the ground truth, so the temporal localization is factually incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker (bottom left) mentions Mr. Hothi injected himself into controversies, when does he state that Mr. Hothi said very publicly on Twitter that Mr. Musk and Tesla were lying?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 196.5,
        "end": 201.5
      },
      "pred_interval": {
        "start": 180.0,
        "end": 182.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.5,
        "end": 19.0,
        "average": 17.75
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.7506084442138672,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation ('after') correct but the temporal localizations are substantially wrong\u2014both anchor and target times are much earlier than the ground truth and do not match the correct intervals or the described direct statement at 196.5\u2013201.5s."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker (bottom left) is detailing Mr. Hothi's actions, when does he mention Mr. Hothi hitting an employee with a car?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 283.6,
        "end": 285.5
      },
      "pred_interval": {
        "start": 200.0,
        "end": 202.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 83.60000000000002,
        "end": 83.5,
        "average": 83.55000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.22535211267605634,
        "text_similarity": 0.61045241355896,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives timestamps that are completely different from the reference (195\u2013202s vs. 278.5\u2013285.5s), so the temporal localization is incorrect; the relation label ('while') is roughly equivalent to 'during' but does not compensate for the incorrect times."
      }
    },
    {
      "question_id": "003",
      "question": "Once the judge (top left) finishes expressing her doubt about the relevance of Mr. Hothi's actions, when does the speaker (bottom left) begin his response?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 339.9,
        "end": 350.0
      },
      "pred_interval": {
        "start": 250.0,
        "end": 252.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.89999999999998,
        "end": 98.0,
        "average": 93.94999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.4571428571428572,
        "text_similarity": 0.7852411866188049,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general relation (speaker responds after the judge finishes) but the key factual elements\u2014the anchor and target timestamps\u2014are substantially incorrect (248.0/250.0 vs. 338.0/339.9s), and the relation label is imprecise; thus it largely fails to match the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the judge (top right) asks what if Mr. Musk made an ad hominem attack on Mr. Hothi, when does the lawyer (bottom left) respond by giving examples of unrelated attacks?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 374.2,
        "end": 380.5
      },
      "pred_interval": {
        "start": 340.5,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.69999999999999,
        "end": 25.5,
        "average": 29.599999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.8000843524932861,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer mislocates both segments\u2014especially the target (predicted 340.5\u2013355.0s vs correct 374.2\u2013380.5s)\u2014so the timings do not match the ground truth; only the 'after' relationship is consistent."
      }
    },
    {
      "question_id": "002",
      "question": "Once the judge (top center) states that the lawsuit is about Mr. Hothi almost killing Tesla employees, when does the lawyer (bottom left) clarify Mr. Musk's statements regarding Mr. Hothi harassing employees and almost killing one?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 553.0,
        "end": 561.0
      },
      "pred_interval": {
        "start": 400.0,
        "end": 415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 153.0,
        "end": 146.0,
        "average": 149.5
      },
      "rationale_metrics": {
        "rouge_l": 0.25641025641025644,
        "text_similarity": 0.7133949995040894,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially incorrect timestamps and duration (395\u2013415s vs correct 479\u2013483.317s and 553\u2013561s) and thus misstates the temporal relationship; it is therefore largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the lawyer (bottom left) finishes explaining why 'almost killed' is not a false statement of fact using a sidewalk analogy, when does the judge (top center) ask if that just means the lawyer might win the lawsuit?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 584.0,
        "end": 586.8
      },
      "pred_interval": {
        "start": 490.0,
        "end": 495.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 94.0,
        "end": 91.79999999999995,
        "average": 92.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.8483192920684814,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same speakers and a causal 'once' relation, but the timestamps are substantially incorrect and it fails to reflect that the judge's question immediately follows the lawyer's explanation, so key temporal details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker (bottom left) states the problem is that two people would be debating under different rules because Mr. Hothi is protected by anti-SLAPP, when does he state that the target of Mr. Hothi's speech would not have the same protection?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.405,
        "end": 511.559
      },
      "pred_interval": {
        "start": 515.2,
        "end": 518.6
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.7950000000000728,
        "end": 7.040999999999997,
        "average": 5.418000000000035
      },
      "rationale_metrics": {
        "rouge_l": 0.28205128205128205,
        "text_similarity": 0.7189212441444397,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the anchor and that the target follows, but the timestamps are substantially off and it contradicts the correct answer's 'immediately after' timing, so the temporal alignment is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom right) begins his turn by saying 'I mean, I think that the difficulty, Mr. Lefebvre, that I have is', when does he explain that the argument is based on Mr. Hothi entering the public sphere?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 511.597,
        "end": 512.074
      },
      "pred_interval": {
        "start": 600.7,
        "end": 605.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 89.10300000000007,
        "end": 93.82600000000002,
        "average": 91.46450000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.7833608388900757,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and segmentation are drastically different from the reference (off by ~86s and not an immediate continuation); although it identifies the same content, the timing/relationship are incorrect, so the answer is largely wrong."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker (bottom right) finishes giving the example of Mr. Musk commenting on Mr. Houthi's methodology and data, when does he question the relevance of 'almost killing someone in the parking lot'?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 512.302,
        "end": 512.387
      },
      "pred_interval": {
        "start": 670.3,
        "end": 673.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 157.99799999999993,
        "end": 160.8130000000001,
        "average": 159.40550000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.21917808219178084,
        "text_similarity": 0.7451450824737549,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are far from the ground-truth times and misplace both anchor and target events by over two minutes; while it notes an 'after' relationship, it fails to capture the immediate succession indicated in the correct answer."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker asks why this is less attenuated than in the Wilson case, when does he begin explaining the issue?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 696.0,
        "end": 703.5
      },
      "pred_interval": {
        "start": 715.5,
        "end": 725.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.5,
        "end": 21.5,
        "average": 20.5
      },
      "rationale_metrics": {
        "rouge_l": 0.28235294117647064,
        "text_similarity": 0.6797892451286316,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially different timestamps (E1/E2 shifted by ~17\u201320s and different end times) and a different temporal relation ('after' vs correct 'once_finished'), so it contradicts key temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that their analysis is relevant to 'what he did', when does he give the example of trespassing?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 764.0,
        "end": 768.7
      },
      "pred_interval": {
        "start": 760.0,
        "end": 770.0
      },
      "iou": 0.4700000000000045,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 1.2999999999999545,
        "average": 2.6499999999999773
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575344,
        "text_similarity": 0.6728965044021606,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' but all reported timestamps (E1 start, E2 start, and E2 end) substantially conflict with the ground truth, making it factually inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the presiding justice turns the floor over to the opponent for replies, when does the opponent begin speaking?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 800.0,
        "end": 802.5
      },
      "pred_interval": {
        "start": 880.0,
        "end": 890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 80.0,
        "end": 87.5,
        "average": 83.75
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.8061931133270264,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the temporal relation as 'after', it gives substantially different timestamps for E1, E2 start, and E2 end compared to the ground truth, so it is factually incorrect on the key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mr. Greenspan finishes saying that the purpose was not to monitor or harass employees, when does he begin explaining that the person was counting cars off the production line?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1053.67,
        "end": 1058.61
      },
      "pred_interval": {
        "start": 1065.0,
        "end": 1075.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.329999999999927,
        "end": 16.3900000000001,
        "average": 13.860000000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.7563333511352539,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted relation matches the intent ('immediately after' ~ 'once_finished'), but the timestamps are substantially off (E1 predicted end 1064.0s vs correct 1053.01s; E2 predicted 1065.0\u20131075.0s vs correct 1053.67\u20131058.61s), so it does not correctly align the events."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Greenspan finishes asking if there are any specific questions, when does the Presiding Justice respond by asking the panel if they have questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1131.969,
        "end": 1135.213
      },
      "pred_interval": {
        "start": 1120.0,
        "end": 1130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.969000000000051,
        "end": 5.212999999999965,
        "average": 8.591000000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.71592777967453,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation roughly right ('immediately after') but the timestamps are substantially incorrect and inconsistent with the reference (E1 and E2 start/finish times differ significantly and E2's predicted end even conflicts with the reference), so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once Mr. Liffrec finishes explaining that the Nidal case was about a university building something in a public park and that was the public debate, when does he state that the university then made statements about protesters inciting violence?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1159.099,
        "end": 1164.925
      },
      "pred_interval": {
        "start": 1205.0,
        "end": 1215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.90100000000007,
        "end": 50.075000000000045,
        "average": 47.988000000000056
      },
      "rationale_metrics": {
        "rouge_l": 0.27272727272727276,
        "text_similarity": 0.7062366008758545,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation right (events occur immediately after/once finished) but the timestamps are substantially different and incorrect compared with the reference (off by ~45s and with different durations), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's discussion about harassment, when does he mention \"extensive evidence of harassment\"?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1240.5,
        "end": 1242.0
      },
      "pred_interval": {
        "start": 1250.0,
        "end": 1255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.5,
        "end": 13.0,
        "average": 11.25
      },
      "rationale_metrics": {
        "rouge_l": 0.3389830508474576,
        "text_similarity": 0.8194090723991394,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction mentions the same events but the timestamps are substantially incorrect and the temporal relation is wrong: ground truth has E2 (1240.5\u20131242.0) occurring during E1 (1236.2\u20131246.6), while the prediction places both later and claims E2 occurs after E1."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (bottom left) concludes his argument, when does the Presiding Justice (center top) begin asking if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1340.0,
        "end": 1345.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.215999999999894,
        "end": 45.77099999999996,
        "average": 44.993499999999926
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.790869414806366,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the event order right (E2 occurs after E1) but the timestamps are significantly incorrect (predicted ~1340s vs reference E1 end 1294.2s and E2 start 1295.784s) and it falsely claims an immediate start and different end time."
      }
    },
    {
      "question_id": "003",
      "question": "After the Justice (top right) states that the Nadel case came out in 1994, when does he ask if the court would have considered things the same way with the Filmon Supreme Court's decision?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1376.0
      },
      "gt_interval": {
        "start": 1310.105,
        "end": 1318.758
      },
      "pred_interval": {
        "start": 1360.0,
        "end": 1364.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.89499999999998,
        "end": 45.24199999999996,
        "average": 47.56849999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.2051282051282051,
        "text_similarity": 0.6970857381820679,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly preserves the temporal order (E2 occurs after E1) but the timestamps are substantially incorrect and E1 is mischaracterized (start vs. end), so it fails to match the ground-truth timings."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying 'And with that, I'd submit', when does the Presiding Justice ask if there are any other questions?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1295.784,
        "end": 1299.229
      },
      "pred_interval": {
        "start": 1245.7,
        "end": 1247.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.08400000000006,
        "end": 51.72900000000004,
        "average": 50.90650000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.367816091954023,
        "text_similarity": 0.7347090244293213,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the temporal order (question occurs immediately after the speaker) but the provided timestamps are substantially incorrect (~50s earlier than ground truth) and the predicted answer omits the E2 end time, so it fails to align with the reference facts."
      }
    },
    {
      "question_id": "002",
      "question": "After the Presiding Justice asks if there are any other questions, when does Justice Sanchez start speaking about the Nadel case?",
      "video_id": "5yNgC39Q2HE",
      "video_number": "006",
      "segment": {
        "start": 1230.0,
        "end": 1375.266
      },
      "gt_interval": {
        "start": 1300.609,
        "end": 1302.492
      },
      "pred_interval": {
        "start": 1250.2,
        "end": 1251.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.40899999999988,
        "end": 50.59199999999987,
        "average": 50.500499999999874
      },
      "rationale_metrics": {
        "rouge_l": 0.26086956521739135,
        "text_similarity": 0.707638680934906,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (Sanchez speaks after the Presiding Justice) and mentions the Nadel case, but the timestamps are substantially incorrect (off by ~52 seconds) and thus factually mismatched with the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After Senator Cruz asks if the same principle applies to other protected characteristics, when does he ask if he could decide he was an Asian man?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 14.506,
        "end": 16.388
      },
      "pred_interval": {
        "start": 12.5,
        "end": 14.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0060000000000002,
        "end": 2.3880000000000017,
        "average": 2.197000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.48571428571428577,
        "text_similarity": 0.8025138974189758,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies E1 (10.0s falls within the true 8.279\u201312.323s interval) and the relation 'after', but the E2 timing is substantially incorrect (predicted 12.5\u201314.0s vs true 14.506\u201316.388s) with no overlap, so it misses the target segment."
      }
    },
    {
      "question_id": "002",
      "question": "Once Judge Jackson finishes explaining that she is not able to answer hypotheticals, when does Senator Cruz interrupt to re-ask his question about assessing standing?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 30.6,
        "end": 32.439
      },
      "pred_interval": {
        "start": 25.0,
        "end": 27.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.600000000000001,
        "end": 5.439,
        "average": 5.519500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.8175642490386963,
        "llm_judge_score": 0,
        "llm_judge_justification": "All predicted timestamps for E1 and E2 are substantially different from the ground truth, and the relation label ('after') does not match the correct 'once_finished', so the prediction is incorrect on all key elements."
      }
    },
    {
      "question_id": "003",
      "question": "While Judge Jackson is explaining how she would assess standing, when does she state that she would 'consider the relevant precedents'?",
      "video_id": "9U_cQz-7sT4",
      "video_number": "007",
      "segment": {
        "start": 0.0,
        "end": 54.0
      },
      "gt_interval": {
        "start": 45.0,
        "end": 47.8
      },
      "pred_interval": {
        "start": 40.0,
        "end": 42.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.0,
        "end": 5.799999999999997,
        "average": 5.399999999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.765241265296936,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relation ('during') and notes when she mentions precedents, but the provided timestamps are materially inaccurate and incomplete (anchor end missing and target times differ substantially from the reference), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain why that detective asked her to do it?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 37.335,
        "end": 44.121
      },
      "pred_interval": {
        "start": 12.5,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.835,
        "end": 29.121000000000002,
        "average": 26.978
      },
      "rationale_metrics": {
        "rouge_l": 0.379746835443038,
        "text_similarity": 0.6663566827774048,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but it mislocates both events with substantially incorrect timestamps and adds extraneous audio cue detail, so it fails on factual timing alignment."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mickey Haller finishes stating that the witness pointed to Detective Lee Langford, when does Langford begin his angry outburst?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 66.887,
        "end": 72.795
      },
      "pred_interval": {
        "start": 30.0,
        "end": 32.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.887,
        "end": 40.795,
        "average": 38.841
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.607069730758667,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the qualitative relation (an immediate reaction) but the timestamps are drastically incorrect and do not match the reference events, so it fails on the key factual elements (timing)."
      }
    },
    {
      "question_id": "003",
      "question": "After Detective Lee Langford's angry outburst about the witness, when does the judge declare the court in recess?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "008",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 82.826,
        "end": 85.59
      },
      "pred_interval": {
        "start": 45.0,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.82599999999999,
        "end": 38.59,
        "average": 38.208
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.699396550655365,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gets the temporal relation right but the event timestamps are significantly incorrect compared to the ground truth and it adds unverifiable audio cues; thus it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Mickey Haller finishes asking Detective Pettis if she was in the Bonaventure Hotel on October 20th, when does Detective Pettis answer?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 16.239,
        "end": 16.76
      },
      "pred_interval": {
        "start": 10.5,
        "end": 12.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.739000000000001,
        "end": 4.760000000000002,
        "average": 5.249500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8411821722984314,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relationship as immediate, but the anchor and target timestamps are substantially incorrect compared to the reference (9.5/10.5\u201312.0s vs. 16.219/16.239\u201316.76s), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "002",
      "question": "After Detective Pettis states she was asked to go to the hotel by another cop, when does she explain her personal motivation for complying?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 46.707,
        "end": 55.417
      },
      "pred_interval": {
        "start": 25.0,
        "end": 28.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.707,
        "end": 27.417,
        "average": 24.562
      },
      "rationale_metrics": {
        "rouge_l": 0.43243243243243246,
        "text_similarity": 0.8503551483154297,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the anchor and target timestamps are substantially incorrect compared to the ground truth (22.0/25.0\u201328.0s vs. 32.008s/46.707\u201355.417s), so it fails on key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After Mickey Haller asks if the man Detective Pettis made a deal with is in the courtroom, when does she point him out?",
      "video_id": "gTBoJ9W8zQ8",
      "video_number": "010",
      "segment": {
        "start": 0.0,
        "end": 89.0
      },
      "gt_interval": {
        "start": 61.92,
        "end": 62.701
      },
      "pred_interval": {
        "start": 45.5,
        "end": 47.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.42,
        "end": 15.701,
        "average": 16.0605
      },
      "rationale_metrics": {
        "rouge_l": 0.21917808219178084,
        "text_similarity": 0.7887873649597168,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives substantially incorrect timestamps and an incorrect temporal relation ('immediate'); it contradicts the ground-truth timing (anchor at ~57.56s, target at ~61.92s) and thus is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chaturved introduces Mr. Uday Hula, when does he state that Mr. Uday Hula has become a popular name pan India and beyond?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 41.33,
        "end": 43.1
      },
      "pred_interval": {
        "start": 15.5,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.83,
        "end": 26.1,
        "average": 25.965
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.7778816223144531,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but both event timestamps are incorrect versus the reference (E1 should start ~19.99s, predicted 12.0s; E2 should be ~41.33\u201343.1s, predicted 15.5\u201317.0s), so the key factual temporal locations are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once Mr. Vikas Chaturved says 'Over to you Mr. Trikram', when does Mr. Trikram start speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 153.433,
        "end": 154.776
      },
      "pred_interval": {
        "start": 45.0,
        "end": 46.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 108.43299999999999,
        "end": 108.77600000000001,
        "average": 108.6045
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444445,
        "text_similarity": 0.7597448825836182,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps are wildly incorrect (44.0/45.0/46.0 vs the correct 151.953/153.433/154.776), and the temporal relation is mischaracterized; overall it fails to match the key factual timing information."
      }
    },
    {
      "question_id": "003",
      "question": "After Mr. Trikram finishes his welcome, when does Mr. Uday Holla begin his speech?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.0,
        "end": 172.0
      },
      "pred_interval": {
        "start": 120.5,
        "end": 122.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.5,
        "end": 50.0,
        "average": 49.25
      },
      "rationale_metrics": {
        "rouge_l": 0.39999999999999997,
        "text_similarity": 0.7559772729873657,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the relative relation ('after') correct, its timestamps are substantially wrong (predicts E1 ends at 120.0s and E2 at 120.5\u2013122.0s vs. ground truth 147.207s and 169\u2013172s), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker emphasizes that facts are important, when does he state that a lawyer must have the patience of a crane?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 352.0,
        "end": 354.8
      },
      "pred_interval": {
        "start": 350.0,
        "end": 355.0
      },
      "iou": 0.5600000000000023,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.0,
        "end": 0.19999999999998863,
        "average": 1.0999999999999943
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7111316919326782,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the event order and E2 timing approximately right, but it mislocates E1 (340.0s vs. the correct 345.6\u2013348.2s) and gives a single timestamp instead of the correct interval; it also introduces an unsupported gesture cue. These inaccuracies reduce temporal correctness."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions that appearing for government becomes more difficult, when does he recount his personal experience as Advocate General?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 404.1,
        "end": 412.9
      },
      "pred_interval": {
        "start": 400.0,
        "end": 410.0
      },
      "iou": 0.45736434108527035,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.100000000000023,
        "end": 2.8999999999999773,
        "average": 3.5
      },
      "rationale_metrics": {
        "rouge_l": 0.24324324324324323,
        "text_similarity": 0.6282381415367126,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the event order and the E2 interval approximately right and adds a plausible cue, but E1's timestamp is notably off (~10s later than the reference) and E2's boundaries are slightly shifted, so the temporal alignment is imperfect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a lawyer must know the law to draft better, when does he offer an illustration?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 504.9,
        "end": 506.3
      },
      "pred_interval": {
        "start": 500.0,
        "end": 510.0
      },
      "iou": 0.1400000000000034,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.899999999999977,
        "end": 3.6999999999999886,
        "average": 4.299999999999983
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.6689229011535645,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relationship, but the timestamps are noticeably off (especially E2's start/end) and it adds an unsupported visual/audio cue (hand movements), which is a mild hallucination."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that there has been an amendment, when does he state that everment is taken out?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 529.3,
        "end": 532.0
      },
      "pred_interval": {
        "start": 550.5,
        "end": 553.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.200000000000045,
        "end": 21.200000000000045,
        "average": 21.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.7204310894012451,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events and correctly says the target occurs after the anchor, but the provided timestamps are substantially different from the ground truth (off by ~17\u201321s) and it fails to reflect that the target immediately follows the anchor."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that readiness and willingness was mandatory prior to 2018, when does he explicitly state that the suit was liable to be dismissed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 579.568,
        "end": 583.193
      },
      "pred_interval": {
        "start": 610.3,
        "end": 612.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.73199999999997,
        "end": 29.507000000000062,
        "average": 30.119500000000016
      },
      "rationale_metrics": {
        "rouge_l": 0.4761904761904762,
        "text_similarity": 0.7862696647644043,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction gets the order ('after') right, the reported timestamps are substantially incorrect for both events (anchor should be ~533.4\u2013553.9s but is given as 605.0s; target should be ~579.6\u2013583.2s but is given as 610.3\u2013612.7s), so it fails on factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker describes the balance of the purchase amount, when does he explain the necessity for the plaintiff to prove they had the necessary funds?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 634.565,
        "end": 644.056
      },
      "pred_interval": {
        "start": 670.8,
        "end": 674.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.2349999999999,
        "end": 29.94399999999996,
        "average": 33.08949999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.34210526315789475,
        "text_similarity": 0.6647416949272156,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the relative order ('after') but the reported timestamps are substantially different from the reference (anchor and target are ~36s later and target end time is far off), and it fails to capture that the target immediately follows the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a well-drafted plaint helps portray personality to the judge, when does he begin talking about the second benefit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 700.9,
        "end": 708.7
      },
      "pred_interval": {
        "start": 715.0,
        "end": 720.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.100000000000023,
        "end": 11.299999999999955,
        "average": 12.699999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.39080459770114945,
        "text_similarity": 0.8156566619873047,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction contradicts the correct timestamps and event boundaries (mixes up start/end times and gives much later times) and adds an unsupported phrase, so it is largely incorrect; it only correctly reflects the vague 'after' relationship."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a civil case lingers for a number of years, when does he explain that one becomes a senior by the time the case comes up for evidence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 719.5,
        "end": 724.9
      },
      "pred_interval": {
        "start": 780.0,
        "end": 785.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.5,
        "end": 60.10000000000002,
        "average": 60.30000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.28235294117647064,
        "text_similarity": 0.7006118297576904,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relation (that one becomes a senior by the time evidence is presented) but the provided anchor/target timestamps are incorrect and do not match the reference intervals, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes discussing client satisfaction with plaint length and associated fees, when does he introduce the strategy of putting facts at the beginning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 795.264,
        "end": 805.511
      },
      "pred_interval": {
        "start": 850.0,
        "end": 855.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.73599999999999,
        "end": 49.48900000000003,
        "average": 52.11250000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.30952380952380953,
        "text_similarity": 0.746967077255249,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted timestamps are substantially off (\u224850s later than the correct 794.0s/795.264s) and it adds an unsupported quoted phrasing, so although it preserves the sequence ('once') the timing and content are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the Supreme Court, when does he state the specific paragraph number of the judgment?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 925.404,
        "end": 950.0
      },
      "pred_interval": {
        "start": 880.5,
        "end": 882.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.903999999999996,
        "end": 68.0,
        "average": 56.452
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8554000854492188,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted answer is incorrect: it gives entirely different timestamps and a different paragraph ('23' vs the correct '240'), so it does not match the anchor/target content or timing despite both labeling the relation as 'after'."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is discussing Justice Sanjay Kishan Kaul's judgment, when does he mention the 'Ren and Martin principle of pressie writing'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 986.581,
        "end": 990.021
      },
      "pred_interval": {
        "start": 945.0,
        "end": 947.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.58100000000002,
        "end": 43.02099999999996,
        "average": 42.30099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3908045977011494,
        "text_similarity": 0.863796591758728,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer matches the relationship ('while') but the anchor and target timestamps are substantially incorrect and do not align with the reference intervals, so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using 'short paragraphs, short sentences, direct sentences' in a plaint, when does he warn that a judge will lose patience with lengthy paragraphs?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1005.669,
        "end": 1012.012
      },
      "pred_interval": {
        "start": 1020.0,
        "end": 1023.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.331000000000017,
        "end": 11.488000000000056,
        "average": 12.909500000000037
      },
      "rationale_metrics": {
        "rouge_l": 0.35000000000000003,
        "text_similarity": 0.7574865221977234,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relationship ('after') but mislocalizes both the anchor and target by a substantial margin (each ~10\u201317s later) and thus fails to match the ground-truth timestamps."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that a civil dispute is always before a civil court, when does he emphasize that the lawyer must be thorough with the civil procedure code?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1077.5,
        "end": 1083.7
      },
      "pred_interval": {
        "start": 1085.0,
        "end": 1100.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.5,
        "end": 16.299999999999955,
        "average": 11.899999999999977
      },
      "rationale_metrics": {
        "rouge_l": 0.2972972972972973,
        "text_similarity": 0.7632212042808533,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and roughly locates the anchor, but it omits the anchor end time and gives substantially incorrect target start/end times (hallucinated later end), so the timestamps are not sufficiently accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that settlement will not benefit in the short term, when does he explain how it will benefit in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1250.671,
        "end": 1254.734
      },
      "pred_interval": {
        "start": 1150.0,
        "end": 1165.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 100.67100000000005,
        "end": 89.73399999999992,
        "average": 95.20249999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.8535138368606567,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the timestamps for both anchor and target differ substantially from the ground truth, so it is factually incorrect on the key time information."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that the entire procedure for civil disputes is in the civil procedure code, when does he mention that states also have civil rules of practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1098.4,
        "end": 1101.7
      },
      "pred_interval": {
        "start": 1220.0,
        "end": 1235.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 121.59999999999991,
        "end": 133.29999999999995,
        "average": 127.44999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.7716310024261475,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation right ('after') but the timestamps are incorrect and inconsistent with the ground truth (anchor end time is missing and both anchor/target times are shifted and the target duration differs), so key factual details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how settlement will benefit you, when does he state that it will definitely help in the long term?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1238.9,
        "end": 1241.9
      },
      "pred_interval": {
        "start": 1250.0,
        "end": 1265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.099999999999909,
        "end": 23.09999999999991,
        "average": 17.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.2285714285714286,
        "text_similarity": 0.7051960229873657,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the general order right but misstates both event timestamps substantially and places E2 much later, failing to reflect that the target immediately follows the anchor; thus it is largely incorrect despite the correct direction and quoted content."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker first mentions there's a saying in Kannada, when does he explain the saying about winners and losers in court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1273.6,
        "end": 1278.4
      },
      "pred_interval": {
        "start": 1300.0,
        "end": 1320.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.40000000000009,
        "end": 41.59999999999991,
        "average": 34.0
      },
      "rationale_metrics": {
        "rouge_l": 0.1846153846153846,
        "text_similarity": 0.6799211502075195,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps are substantially incorrect (both events are shifted ~26\u201330s later), E1 end time is omitted, and E2 duration is overextended, so it fails on factual alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker begins discussing the drafting of the plaint, when does he advise to draft it in a professional manner?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1319.385,
        "end": 1344.757
      },
      "pred_interval": {
        "start": 1380.0,
        "end": 1395.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.61500000000001,
        "end": 50.24299999999994,
        "average": 55.428999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.2153846153846154,
        "text_similarity": 0.8641647100448608,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the correct ones (off by ~54\u201365 seconds) and mislocate both anchor and target; the relationship and timing therefore do not align with the reference."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker states 'You have order two, rule two, which specifies that', when does he elaborate on what it specifies?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1439.147,
        "end": 1451.484
      },
      "pred_interval": {
        "start": 1425.0,
        "end": 1432.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.146999999999935,
        "end": 19.483999999999924,
        "average": 16.81549999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.7846449613571167,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gives completely different timestamps for both the anchor and elaboration compared to the reference and thus fails to match the key factual timing; while both assert the elaboration follows the anchor, the predicted start/end times contradict the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions 'life of the lawyer becomes easier', when does he list Order Six, Seven, and Eight?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1460.096,
        "end": 1410.578
      },
      "pred_interval": {
        "start": 1505.0,
        "end": 1512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.903999999999996,
        "end": 101.42200000000003,
        "average": 73.16300000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.38805970149253727,
        "text_similarity": 0.7021270990371704,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gets the qualitative relationship ('short pause') but the timestamps are substantially incorrect (predicted ~1490\u20131512s vs correct ~1460\u20131468s) and it misrepresents E1 as starting rather than the correct finish time, so it fails on factual timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Then comes the stage of evidence', when does he explain that a lawyer must sit with clients to understand what kind of evidence to lead?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1553.662,
        "end": 1566.557
      },
      "pred_interval": {
        "start": 1570.0,
        "end": 1578.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.337999999999965,
        "end": 11.442999999999984,
        "average": 13.890499999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.20689655172413796,
        "text_similarity": 0.7657045722007751,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the rough ordering (E2 after E1) and similar phrasing, but the timestamps for both E1 and E2 are significantly off from the reference and the described relationship ('after a brief pause') does not match the reference context; key timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions the written statement, when does he begin talking about Order 8?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1615.811,
        "end": 1624.02
      },
      "pred_interval": {
        "start": 1605.5,
        "end": 1612.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.310999999999922,
        "end": 12.019999999999982,
        "average": 11.165499999999952
      },
      "rationale_metrics": {
        "rouge_l": 0.36666666666666664,
        "text_similarity": 0.7876486778259277,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events and their temporal relation ('after'), but it misstates key timestamps (E1 missing an end time and shifted start; E2 start/end times differ substantially from the reference), so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that a general denial is not sufficient, when does he mention the mistake lawyers normally commit?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1651.02,
        "end": 1671.11
      },
      "pred_interval": {
        "start": 1650.0,
        "end": 1660.0
      },
      "iou": 0.4253908100426367,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.0199999999999818,
        "end": 11.1099999999999,
        "average": 6.064999999999941
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.7409701347351074,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the relation right and roughly identifies the same segments, but the anchor timing is substantially off (1640.0s vs 1650.5s) and the target interval is shortened and shifted (1650.0\u20131660.0 vs 1651.02\u20131671.11), so key temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker emphasizes that a lawyer must know the law, when does he explain the specific areas a lawyer should be thorough with?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1758.607,
        "end": 1763.816
      },
      "pred_interval": {
        "start": 1720.0,
        "end": 1735.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.60699999999997,
        "end": 28.81600000000003,
        "average": 33.7115
      },
      "rationale_metrics": {
        "rouge_l": 0.34210526315789475,
        "text_similarity": 0.6798481941223145,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the events and their 'after' relation and paraphrases the content, but the provided timestamps for both E1 and E2 are substantially different from the reference, so the answer is factually incorrect on the requested timing."
      }
    },
    {
      "question_id": "001",
      "question": "When is the next time the speaker mentions an 'Order six, Rule' after mentioning 'Order six, Rule four'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1827.2,
        "end": 1830.9
      },
      "pred_interval": {
        "start": 1820.0,
        "end": 1825.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.2000000000000455,
        "end": 5.900000000000091,
        "average": 6.550000000000068
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.7142677307128906,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relationship ('after') right but misidentifies both timestamps (1800.0 vs 1773.5\u20131775.0 and 1820.0\u20131825.0 vs 1827.2\u20131830.9) and fails to name the second mention as 'Rule eight', so it is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker explains that fraud and undue influence require specific pleas, when does he state that a general plea is not sufficient?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1802.1,
        "end": 1806.5
      },
      "pred_interval": {
        "start": 1890.0,
        "end": 1895.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 87.90000000000009,
        "end": 88.5,
        "average": 88.20000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.25,
        "text_similarity": 0.6682506799697876,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct temporal relation ('after') but the event timestamps are significantly shifted (~90 seconds later) and do not match the ground truth intervals, so the answer is largely incorrect. The anchor/target timing discrepancy makes it unacceptable despite correct relation."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises using short sentences and small paragraphs in a written statement, when does he transition to discussing 'evidence'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1908.4,
        "end": 1914.4
      },
      "pred_interval": {
        "start": 1950.0,
        "end": 1955.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.59999999999991,
        "end": 40.59999999999991,
        "average": 41.09999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.15625,
        "text_similarity": 0.6350635886192322,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the 'evidence' segment follows the drafting-advice segment, but the provided timestamps and durations are substantially incorrect compared with the ground truth, so the alignment is poor."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that leading questions are to be eschewed, when does he advise on how to prepare for examination?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1964.967,
        "end": 1965.937
      },
      "pred_interval": {
        "start": 2000.5,
        "end": 2005.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.5329999999999,
        "end": 39.0630000000001,
        "average": 37.298
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444445,
        "text_similarity": 0.7037798762321472,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the provided timestamps are substantially inaccurate (off by ~30\u201335 seconds for both E1 and E2) and the event boundaries/durations do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes how some lawyers approach cross-examination without preparation, when does he explain what a good lawyer always does?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2010.4,
        "end": 2018.651
      },
      "pred_interval": {
        "start": 2050.0,
        "end": 2055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.59999999999991,
        "end": 36.34899999999993,
        "average": 37.97449999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.39436619718309857,
        "text_similarity": 0.7516005039215088,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives completely different time spans (2045\u20132055s vs correct ~2008.74\u20132018.65s) and an incorrect relation label; while both imply a temporal order, the predicted timestamps and relation do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's explanation of pitfalls when unprepared for cross-examination, when does he mention forgetting to ask relevant questions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2044.393,
        "end": 2049.878
      },
      "pred_interval": {
        "start": 2105.5,
        "end": 2110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 61.10699999999997,
        "end": 60.121999999999844,
        "average": 60.61449999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.7163577079772949,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the relationship ('during') is correct, the predicted timestamps are substantially incorrect and incomplete (E1 lacks an end time and both E1/E2 times are ~60s later than the reference), so it fails to match the ground-truth temporal boundaries."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says that cross-examination is an art by itself, when does he state that watching a good cross-examiner will help enormously?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2187.557,
        "end": 2201.817
      },
      "pred_interval": {
        "start": 2205.0,
        "end": 2212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.44300000000021,
        "end": 10.182999999999993,
        "average": 13.813000000000102
      },
      "rationale_metrics": {
        "rouge_l": 0.3,
        "text_similarity": 0.8378012180328369,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the semantic relation and quotes the relevant phrase, but both anchor and target timestamps are substantially incorrect (misaligned with the ground truth), so the answer is factually wrong on the key timing details."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker describes law as a noble profession, when does he mention lawyers dedicating themselves to clients to ensure justice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2222.5,
        "end": 2233.2
      },
      "pred_interval": {
        "start": 2250.0,
        "end": 2260.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.5,
        "end": 26.800000000000182,
        "average": 27.15000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7795225381851196,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and temporal relation are incorrect and contradict the reference: the correct E2 overlaps with and explains E1 around 2222\u20132235s, whereas the prediction places E2 much later (2250\u20132260s) and labeled it 'after', so it fails to match key temporal facts."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that delays are endemic, when does he provide the reason as a call for settlement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2338.838,
        "end": 2346.208
      },
      "pred_interval": {
        "start": 2305.0,
        "end": 2315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.83800000000019,
        "end": 31.208000000000084,
        "average": 32.52300000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8611265420913696,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the semantic relation (reason follows the 'delays are endemic' statement) but the timestamps are substantially incorrect and do not match the immediate follow-up specified in the reference, so the temporal alignment is largely wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker states that delays are endemic, when does he ask to ensure settlements occur?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2340.0,
        "end": 2346.0
      },
      "pred_interval": {
        "start": 2345.2,
        "end": 2348.7
      },
      "iou": 0.09195402298852857,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.199999999999818,
        "end": 2.699999999999818,
        "average": 3.949999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.35294117647058826,
        "text_similarity": 0.8388659954071045,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct ordering (target after anchor) and captures the content, but the anchor and target timestamps differ substantially from the reference and the relationship label ('when') is ambiguous rather than explicitly 'after', so it is only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker notes it's already 40 minutes, when does he state that he will give some time for questioning?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2367.8,
        "end": 2371.0
      },
      "pred_interval": {
        "start": 2423.0,
        "end": 2426.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 55.19999999999982,
        "end": 55.5,
        "average": 55.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.4615384615384615,
        "text_similarity": 0.8734838962554932,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two utterances, but the provided timestamps are substantially shifted (~55 seconds later) compared to the ground truth and the relation is labeled generically as 'when' rather than the correct 'immediately follows', so the temporal localization is inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker mentions postponing because he was busy, when does he thank someone for 'pestering' him to educate the lawyer community?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2391.454,
        "end": 2399.123
      },
      "pred_interval": {
        "start": 2505.4,
        "end": 2509.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 113.94599999999991,
        "end": 109.87699999999995,
        "average": 111.91149999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.37974683544303806,
        "text_similarity": 0.8074439764022827,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the correct events and their temporal relation, but the timestamps are substantially incorrect (off by ~112 seconds) and the event boundaries differ from the ground truth, so it fails factual alignment."
      }
    },
    {
      "question_id": "001",
      "question": "How long does the speaker talk about the importance of reading entire judgments, not just highlighted portions?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2578.041,
        "end": 2584.494
      },
      "pred_interval": {
        "start": 2520.0,
        "end": 2540.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.04100000000017,
        "end": 44.49400000000014,
        "average": 51.267500000000155
      },
      "rationale_metrics": {
        "rouge_l": 0.253968253968254,
        "text_similarity": 0.5863713026046753,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly labels the relationship as 'during' but gives substantially incorrect start/end timestamps and duration (2520.0\u20132540.0 vs. reference 2568.041\u20132578.041), so it fails on factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes saying he will take questions, when does Mr. Vikas begin speaking?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2614.902,
        "end": 2617.184
      },
      "pred_interval": {
        "start": 2585.0,
        "end": 2587.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.902000000000044,
        "end": 30.184000000000196,
        "average": 30.04300000000012
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.7968556880950928,
        "llm_judge_score": 0,
        "llm_judge_justification": "The predicted timestamps contradict the correct ones by large margins (both E1 and E2 times are incorrect) and incorrectly asserts an immediate transition, whereas the correct E2 starts much later; key factual elements are wrong."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that online law lexicons are not sufficient, when does he directly say 'You must read. A lawyer must read.'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2522.2,
        "end": 2525.3
      },
      "pred_interval": {
        "start": 2660.0,
        "end": 2663.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 137.80000000000018,
        "end": 137.69999999999982,
        "average": 137.75
      },
      "rationale_metrics": {
        "rouge_l": 0.45,
        "text_similarity": 0.7449888586997986,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the order and short temporal relation correct (E2 follows E1 and is 'shortly after'), but the absolute timestamps are materially wrong compared to the reference, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he would fall asleep reading the Civil Procedure Code as it is, when does he explain that he would be enthusiastic if he had a case on hand?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2688.6,
        "end": 2699.0
      },
      "pred_interval": {
        "start": 2705.0,
        "end": 2715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.40000000000009,
        "end": 16.0,
        "average": 16.200000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.25882352941176473,
        "text_similarity": 0.7288033962249756,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the basic order and semantic content (enthusiasm follows the comment about falling asleep) but the timestamps differ substantially from the reference, the relation is overstated as 'immediately after', and it adds visual/audio cues not present in the ground truth, so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker asks 'Specific relief act, what are the sections?', when does he advise to go to the AR manual?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2720.5,
        "end": 2722.3
      },
      "pred_interval": {
        "start": 2780.0,
        "end": 2790.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 59.5,
        "end": 67.69999999999982,
        "average": 63.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.37209302325581395,
        "text_similarity": 0.817112386226654,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after'), but the anchor/target timestamps and durations are substantially wrong compared to the reference and it adds a hallucinated visual cue (gesture toward a book), so it is mostly incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions he suffered from COVID, when does he start describing a doctor's experience during the peak of COVID?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2807.119,
        "end": 2850.7
      },
      "pred_interval": {
        "start": 2850.0,
        "end": 2870.0
      },
      "iou": 0.011132138483799871,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 42.88099999999986,
        "end": 19.300000000000182,
        "average": 31.09050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.21686746987951808,
        "text_similarity": 0.7283673286437988,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the ordering (anchor then target) but the timestamps are substantially incorrect compared to the reference and it adds unsupported visual/audio cues; key factual elements (exact times and durations) are wrong, so it is largely inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once Vikas Chatrath finishes asking how to structure arguments, when does Udaya Holla begin to explain what one must always remember?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2916.46,
        "end": 2963.1
      },
      "pred_interval": {
        "start": 2870.5,
        "end": 2872.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 45.960000000000036,
        "end": 91.09999999999991,
        "average": 68.52999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.31578947368421056,
        "text_similarity": 0.6841923594474792,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer preserves the correct temporal relation ('immediately after') but gives substantially incorrect anchor and target timestamps (off by ~46 seconds), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After Udaya Holla suggests preparing a list of dates and synopsis, when does he mention that the High Court has adopted this practice?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2941.0,
        "end": 2942.8
      },
      "pred_interval": {
        "start": 2905.0,
        "end": 2907.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.0,
        "end": 35.80000000000018,
        "average": 35.90000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.3448275862068966,
        "text_similarity": 0.789918065071106,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies that the target occurs after the anchor, but the provided timestamps are substantially incorrect compared to the reference (2900/2905s vs. 2929.5/2941\u20132942.8s) and it adds an unsupported detail ('brief pause'). These factual timing errors warrant a low score despite the correct ordering."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes asking how to specifically plead in a summary suit, when does Udaya Holla ask for clarification 'In a summary suit, is it?'",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2999.596,
        "end": 3000.717
      },
      "pred_interval": {
        "start": 3030.0,
        "end": 3031.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.403999999999996,
        "end": 30.7829999999999,
        "average": 30.59349999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.6839185953140259,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures that Udaya's question is an immediate response to Vikas, but the reported timestamps are off by about 29\u201330 seconds from the reference, so the timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes lawyers arguing endlessly for hours on end, when does he state that the judge then sleeps?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3046.2,
        "end": 3047.7
      },
      "pred_interval": {
        "start": 3100.0,
        "end": 3110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 53.80000000000018,
        "end": 62.30000000000018,
        "average": 58.05000000000018
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.8363445997238159,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the event timestamps are significantly off (predicted ~3080\u20133110s vs. ground truth ~3040\u20133047s), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker confirms the order number as '37, correct. Sorry.', when does the other speaker (Vikas Chaprath) begin to introduce a common question?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3157.242,
        "end": 3163.028
      },
      "pred_interval": {
        "start": 3165.0,
        "end": 3175.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.757999999999811,
        "end": 11.972000000000207,
        "average": 9.865000000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.8127405643463135,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the 'after' relationship, but the predicted E2 timing (3165.0\u20133175.0s) is substantially later than the ground truth (3157.242\u20133163.028s), so the key temporal details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker explains that preliminary objections concerning maintainability must appear in the beginning of a written statement, when does he begin describing how lawyers often present their defense?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3301.7,
        "end": 3309.9
      },
      "pred_interval": {
        "start": 3200.0,
        "end": 3210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.69999999999982,
        "end": 99.90000000000009,
        "average": 100.79999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.27499999999999997,
        "text_similarity": 0.7821524143218994,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation correct ('after') but the event timestamps are substantially inaccurate (predicted ~3190\u20133210s vs ground truth 3281\u20133309s), misplacing both anchor and target by ~90+ seconds and mismatching durations."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker recommends setting out your own facts in the written statement, when does he mention preliminary objections?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3223.3,
        "end": 3224.548
      },
      "pred_interval": {
        "start": 3250.0,
        "end": 3265.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.699999999999818,
        "end": 40.452000000000226,
        "average": 33.57600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.7150648236274719,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation but the provided timestamps are substantially inaccurate (off by ~20\u201340 seconds) compared to the reference, so the intervals do not match the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions misjoinder or non-joinder of parties, when does he list territorial lack of jurisdiction as an objection?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3254.547,
        "end": 3258.914
      },
      "pred_interval": {
        "start": 3305.0,
        "end": 3315.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 50.452999999999975,
        "end": 56.085999999999785,
        "average": 53.26949999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.3333333333333333,
        "text_similarity": 0.7034938931465149,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events (misjoinder/non-joinder and territorial lack of jurisdiction) but the timestamps are substantially off (shifted by ~40\u201360s) and the relation 'after' does not match the precise 'next' (immediate succession) in the reference, so it is largely temporally incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Vikas finishes asking his question about the Advocate General's experience, when does Udaya Holla begin to respond about the State Council's disabilities?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3417.766,
        "end": 3429.231
      },
      "pred_interval": {
        "start": 3370.0,
        "end": 3385.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.766000000000076,
        "end": 44.23100000000022,
        "average": 45.99850000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.6560485363006592,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly labels the relation as 'after' but the timestamps are substantially different from the ground truth (E1 and E2 starts off by ~47\u201358s and E2 end is ~44s earlier), so it fails to match the key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes saying the colloquial Kannada phrase, when does he translate it to English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3413.2,
        "end": 3417.7
      },
      "pred_interval": {
        "start": 3440.0,
        "end": 3445.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.800000000000182,
        "end": 27.300000000000182,
        "average": 27.050000000000182
      },
      "rationale_metrics": {
        "rouge_l": 0.21875000000000003,
        "text_similarity": 0.7823352813720703,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the rough ordering (phrase then translation) but gives substantially different timestamps and a different relation ('after' instead of 'once_finished'), failing to match the key timing and immediacy details in the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "Once the first speaker finishes talking about the difficulties faced by state councils in tough situations, when does the second speaker say 'Vikram'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3471.82,
        "end": 3472.161
      },
      "pred_interval": {
        "start": 3500.0,
        "end": 3501.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.179999999999836,
        "end": 29.338999999999942,
        "average": 28.75949999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.2580645161290323,
        "text_similarity": 0.7061607837677002,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets only the coarse ordering (second speaker speaks after the first) but the timestamps are substantially different from the reference and the relation is imprecise ('after' vs. 'once_finished'), so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises Kannada medium students to acquire more knowledge of English, when does he suggest joining websites for better English?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3527.318,
        "end": 3535.0
      },
      "pred_interval": {
        "start": 3555.0,
        "end": 3560.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.68199999999979,
        "end": 25.0,
        "average": 26.340999999999894
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6485239267349243,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct 'after' relation, but the event timestamps are significantly misaligned with the ground truth (offset by ~50\u201360s), so the temporal localization is incorrect despite matching the ordering."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker talks about drafting in Kannada, when does he emphasize having mastery over the Kannada language?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3590.3,
        "end": 3592.0
      },
      "pred_interval": {
        "start": 3620.5,
        "end": 3625.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.199999999999818,
        "end": 33.0,
        "average": 31.59999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.46913580246913583,
        "text_similarity": 0.9322478175163269,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events and their 'after' relation, but the timestamps are substantially incorrect and it fails to reflect that the target occurs immediately after the anchor, so the answer is largely inaccurate for a video-timing task."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks about scheduling a lawyer's day, when does the first speaker refer to the question as a 'multi-million dollar question'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3696.0,
        "end": 3697.2
      },
      "pred_interval": {
        "start": 3680.0,
        "end": 3685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.0,
        "end": 12.199999999999818,
        "average": 14.099999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.31999999999999995,
        "text_similarity": 0.7886026501655579,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction misreports both timestamp ranges (anchor and target differ substantially from the ground truth and the anchor end time is omitted), so the temporal alignment is incorrect despite labeling the relationship as 'after'."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises taking books on management, when does he mention keeping one's wife happy at home?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3701.248,
        "end": 3706.6
      },
      "pred_interval": {
        "start": 3750.0,
        "end": 3755.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.75199999999995,
        "end": 48.40000000000009,
        "average": 48.57600000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.35714285714285715,
        "text_similarity": 0.8114172220230103,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction captures the 'after' relationship and the content (keeping wife happy) but its timestamps are substantially incorrect (anchor start/end mismatch and target ~50s off) and it omits the anchor end time, so it is factually inaccurate and incomplete."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises lawyers on how to handle client pleas, when does he suggest setting out facts in the client's presence?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.2,
        "end": 3750.22
      },
      "pred_interval": {
        "start": 3800.0,
        "end": 3815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.80000000000018,
        "end": 64.7800000000002,
        "average": 57.29000000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7035235166549683,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation right ('after') but the event timestamps and durations are materially incorrect (off by tens of seconds and an inflated target duration), so it fails to match the reference timing accurately."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that a judge would have three drafts of his judgment, when does he elaborate on what the first draft is for?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3750.51,
        "end": 3750.56
      },
      "pred_interval": {
        "start": 3850.0,
        "end": 3865.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.48999999999978,
        "end": 114.44000000000005,
        "average": 106.96499999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.7817990779876709,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures only the coarse temporal relation ('after') but the anchor and target timestamps differ substantially from the ground truth (off by tens of seconds to minutes) and do not match the precise intervals, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions Palkiwala's initial lack of work, when does he refer to Justice Chawla's memoir?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3903.539,
        "end": 3917.722
      },
      "pred_interval": {
        "start": 3900.0,
        "end": 3910.0
      },
      "iou": 0.36457510439000734,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 3.5390000000002146,
        "end": 7.722000000000207,
        "average": 5.630500000000211
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.8406296968460083,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted anchor time (3880s) is far from the correct 3912.21s, and the predicted relation 'after' is incorrect since the reference target (3903.539\u20133917.722s) encompasses the anchor. The predicted target interval only partially overlaps and its boundaries do not match the ground truth."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Make it a habit', when does he mention biting the nail?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3936.731,
        "end": 3942.304
      },
      "pred_interval": {
        "start": 3965.0,
        "end": 3968.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.268999999999778,
        "end": 25.695999999999913,
        "average": 26.982499999999845
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.7692134976387024,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the nail-biting mention comes after the anchor, but the timestamps are substantially incorrect (off by many seconds) and it adds a brief pause, contradicting the reference's 'directly after' relation; this omission/misalignment warrants a very low score."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker finishes requesting the first speaker to talk about staying fit, when does the first speaker repeat 'How to stay fit, physically fit, sir'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3986.204,
        "end": 3987.967
      },
      "pred_interval": {
        "start": 4050.0,
        "end": 4055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 63.79599999999982,
        "end": 67.0329999999999,
        "average": 65.41449999999986
      },
      "rationale_metrics": {
        "rouge_l": 0.1791044776119403,
        "text_similarity": 0.6773096323013306,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the qualitative relationship ('after') right but the timestamps are factually incorrect (all times shifted and durations mismatched) and it adds an unsupported audio-cue detail; thus key factual elements are wrong. "
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that 'fitness is absolutely essential in any profession', when does he explain that 'our work transcends the entire waking hours of our time'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4056.898,
        "end": 4064.789
      },
      "pred_interval": {
        "start": 4100.0,
        "end": 4105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.10199999999986,
        "end": 40.210999999999785,
        "average": 41.656499999999824
      },
      "rationale_metrics": {
        "rouge_l": 0.3116883116883117,
        "text_similarity": 0.6949465274810791,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relationship ('after') and continuation of discussion, but the anchor and target timestamps (off by ~40\u201350s) and target duration do not match the ground truth, so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the judge smiling for the first time, when does he explain why the judge smiled?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4157.778,
        "end": 4164.121
      },
      "pred_interval": {
        "start": 4125.0,
        "end": 4130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.77800000000025,
        "end": 34.121000000000095,
        "average": 33.44950000000017
      },
      "rationale_metrics": {
        "rouge_l": 0.2708333333333333,
        "text_similarity": 0.7060163617134094,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the predicted timestamps are substantially incorrect (off by ~30+ seconds) and the predicted event durations contradict the ground truth; it also introduces unsupported visual/audio cues, so it is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker uses a cricket analogy involving Kumble, when does he advise the audience to 'Go and observe'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4289.867,
        "end": 4291.509
      },
      "pred_interval": {
        "start": 4190.0,
        "end": 4195.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 99.86700000000019,
        "end": 96.50900000000001,
        "average": 98.1880000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2916666666666667,
        "text_similarity": 0.7822680473327637,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but gives substantially incorrect timestamps for both events and adds unsupported visual/audio cues, so it fails on factual precision and includes hallucinated details."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker recommends reading Dale Carnegie's book, when does he mention other books on management techniques?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4206.04,
        "end": 4211.851
      },
      "pred_interval": {
        "start": 4265.0,
        "end": 4270.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 58.960000000000036,
        "end": 58.14900000000034,
        "average": 58.55450000000019
      },
      "rationale_metrics": {
        "rouge_l": 0.4040404040404041,
        "text_similarity": 0.8421347737312317,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the target occurs after the anchor, but the reported timestamps are substantially shifted and the event durations differ from the ground truth; it also adds visual/audio cues not present in the reference, indicating low temporal accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker advises lawyers not to sit in the canteen, when does he instruct them to sit in a court?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4301.616,
        "end": 4305.419
      },
      "pred_interval": {
        "start": 4345.0,
        "end": 4350.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.384000000000015,
        "end": 44.58100000000013,
        "average": 43.98250000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.37500000000000006,
        "text_similarity": 0.6997982859611511,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two events (advice vs. instruction) and their order, but the reported timestamps are significantly off from the ground truth (by ~29\u201344s) and do not cover the correct intervals, so the match is largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once Nitika finishes asking her question on YouTube, when does the speaker ask for the question to be repeated?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4378.889,
        "end": 4380.233
      },
      "pred_interval": {
        "start": 4405.0,
        "end": 4410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.110999999999876,
        "end": 29.766999999999825,
        "average": 27.93899999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.27586206896551724,
        "text_similarity": 0.6993861198425293,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only preserves the relative order ('after') but the timestamps are significantly off (by ~23\u201326s), it omits the E2 end time, and thus fails to match the precise timing and completeness of the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks 'Then which are the sections?' related to the Contract Act, when does he begin describing the illustration about a property purchase agreement?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4437.734,
        "end": 4450.995
      },
      "pred_interval": {
        "start": 4470.0,
        "end": 4475.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.26599999999962,
        "end": 24.00500000000011,
        "average": 28.135499999999865
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.6659789681434631,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction gets the order ('after') right, the timestamps are substantially incorrect compared to the ground truth (E1 ends at 4402.161s; E2 starts 4437.734s and ends 4450.995s), so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says he closes his case, when does he explain why?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4475.036,
        "end": 4480.501
      },
      "pred_interval": {
        "start": 4532.0,
        "end": 4545.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 56.96399999999994,
        "end": 64.4989999999998,
        "average": 60.73149999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.6774641871452332,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the timestamps are substantially shifted later (~55\u201365s) and the event durations/intervals do not match the ground truth, so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises lawyers to practice in trial court, when does he explain the main skill acquired there?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4559.082,
        "end": 4593.185
      },
      "pred_interval": {
        "start": 4600.0,
        "end": 4610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.917999999999665,
        "end": 16.8149999999996,
        "average": 28.866499999999633
      },
      "rationale_metrics": {
        "rouge_l": 0.28125000000000006,
        "text_similarity": 0.6533350348472595,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the provided timestamps are substantially misaligned with the ground truth (each interval is ~40\u201350 seconds later), so the key factual timing information is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that 'habit would be derived of five words', when does he proceed to list and explain those words?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4629.048,
        "end": 4640.287
      },
      "pred_interval": {
        "start": 4650.0,
        "end": 4670.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.952000000000226,
        "end": 29.712999999999738,
        "average": 25.332499999999982
      },
      "rationale_metrics": {
        "rouge_l": 0.38461538461538464,
        "text_similarity": 0.7499496936798096,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the high-level sequence (anchor then listing) but the reported timestamps for both E1 and E2 are substantially different from the ground truth (off by ~40s and ~20\u201330s respectively), and the relation label is less precise than 'once_finished', so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking about working parallelly in High Court and Trial Court, when does another speaker give an affirmative answer?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4668.869,
        "end": 4673.473
      },
      "pred_interval": {
        "start": 4670.0,
        "end": 4675.0
      },
      "iou": 0.5664655031805218,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.1310000000003129,
        "end": 1.5270000000000437,
        "average": 1.3290000000001783
      },
      "rationale_metrics": {
        "rouge_l": 0.27397260273972607,
        "text_similarity": 0.826288104057312,
        "llm_judge_score": 4,
        "llm_judge_justification": "The predicted answer gets the relation correct and E2 timings roughly close, but it gives an incorrect E1 timestamp (off by ~5s) and omits E1's end time, and the E2 times differ by ~1\u20131.5s from the reference, so it is only partially aligned."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains how a youngster can gain practical experience in a smaller trial court office, when does he state this would not be possible in a larger office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4722.411,
        "end": 4727.419
      },
      "pred_interval": {
        "start": 4735.0,
        "end": 4742.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.588999999999942,
        "end": 14.581000000000131,
        "average": 13.585000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488375,
        "text_similarity": 0.7361413836479187,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction roughly captures that E2 follows E1 in wording, but the timestamps are substantially incorrect for both events (E1 start misreported and E2 shifted well past the true 4722\u20134727s window), and it misrepresents the actual temporal overlap/placement described in the reference."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes sharing the Japanese quote about a frog in a well, when does another speaker interject with a related Sanskrit saying?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4760.624,
        "end": 4763.847
      },
      "pred_interval": {
        "start": 4810.0,
        "end": 4815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.376000000000204,
        "end": 51.15300000000025,
        "average": 50.264500000000226
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.8002775311470032,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering and nature of the interjection right (anchor then target, 'shortly after'), but the timestamps are substantially incorrect (offset by ~48\u201352s from the ground truth), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks how a new associate creates space in a big office with many existing associates, when does the host add that the guest also has a very big office?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4866.42,
        "end": 4872.823
      },
      "pred_interval": {
        "start": 4850.0,
        "end": 4855.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.420000000000073,
        "end": 17.82300000000032,
        "average": 17.121500000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.28169014084507044,
        "text_similarity": 0.700206995010376,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two utterances and the 'after' relation, but the temporal boundaries are substantially incorrect (both anchor and target times differ by many seconds from the ground truth), so the timing information is unreliable."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker (guest) finishes explaining that a junior lawyer catches the senior's eye by preparing research and synopses, when does the speaker pose a rhetorical question about how a junior lawyer could get attention by arriving late?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4940.902,
        "end": 4951.577
      },
      "pred_interval": {
        "start": 4920.0,
        "end": 4925.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.902000000000044,
        "end": 26.577000000000226,
        "average": 23.739500000000135
      },
      "rationale_metrics": {
        "rouge_l": 0.14925373134328357,
        "text_similarity": 0.7486572265625,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the semantic content and 'after' relation correct (anchor explaining preparation and a following rhetorical question), but the temporal boundaries are significantly incorrect and incomplete compared to the reference timestamps, so it is largely factually mismatched."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker (guest) emphasizes that hard work is the most important thing in the legal profession, when does the speaker begin to describe a scenario where a client observes their lawyer arguing a case diligently?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4985.39,
        "end": 4996.139
      },
      "pred_interval": {
        "start": 5000.0,
        "end": 5005.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.609999999999673,
        "end": 8.860999999999876,
        "average": 11.735499999999774
      },
      "rationale_metrics": {
        "rouge_l": 0.2571428571428571,
        "text_similarity": 0.8449462056159973,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the 'after' relation and the client-observation scenario, but its timestamps are substantially misaligned with the ground truth (both E1 and E2 times are off by many seconds), so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker emphasizes that youngsters must do hard work, when does the second speaker agree with the point about hard work?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5024.62,
        "end": 5033.21
      },
      "pred_interval": {
        "start": 5050.0,
        "end": 5055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.38000000000011,
        "end": 21.789999999999964,
        "average": 23.585000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.2941176470588235,
        "text_similarity": 0.7347370386123657,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') and that the second speaker agrees, but the provided timestamps are substantially different from the reference (off by ~11\u201326 seconds) and the event durations/alignments do not match, so it is largely temporally incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the second speaker asks how to master lower court procedures without much practice, when does the first speaker suggest sitting in courts to learn?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5044.49,
        "end": 5051.81
      },
      "pred_interval": {
        "start": 5105.0,
        "end": 5110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.51000000000022,
        "end": 58.1899999999996,
        "average": 59.34999999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.2608695652173913,
        "text_similarity": 0.6827079057693481,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the ground-truth (off by ~56\u201360s) and do not match the correct segment boundaries; although it notes the 'after' relationship, the timing information is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the second speaker asks if one should have High Court experience before practicing in the Supreme Court, when does the first speaker suggest that one is better off going to the trial court first?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5010.0,
        "end": 5220.0
      },
      "gt_interval": {
        "start": 5126.422,
        "end": 5141.99
      },
      "pred_interval": {
        "start": 5150.0,
        "end": 5158.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.57800000000043,
        "end": 16.01000000000022,
        "average": 19.794000000000324
      },
      "rationale_metrics": {
        "rouge_l": 0.2465753424657534,
        "text_similarity": 0.6876405477523804,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'after' relationship but the timestamps and spans are significantly off from the ground truth (anchor and target intervals do not match the correct start/end times), so it is largely incorrect despite the correct ordering."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes explaining the difference between 'learn' and 'earn', when does he say, 'So thank you everyone'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5198.8,
        "end": 5199.7
      },
      "pred_interval": {
        "start": 5205.5,
        "end": 5206.8
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.699999999999818,
        "end": 7.100000000000364,
        "average": 6.900000000000091
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985916,
        "text_similarity": 0.6885342001914978,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction identifies the correct events but the timestamps differ significantly from the ground truth (several seconds off) and it mislabels the relation as 'after' instead of the immediate 'once_finished', so it is only partially correct."
      }
    },
    {
      "question_id": "002",
      "question": "After the main speaker thanks 'Mr. Hola' and mentions it's always a pleasure connecting, when does the second speaker say 'Thank you very much'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5219.7,
        "end": 5221.2
      },
      "pred_interval": {
        "start": 5220.3,
        "end": 5221.5
      },
      "iou": 0.49999999999974737,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6000000000003638,
        "end": 0.3000000000001819,
        "average": 0.45000000000027285
      },
      "rationale_metrics": {
        "rouge_l": 0.5675675675675675,
        "text_similarity": 0.7886953353881836,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and gives reasonably close E2 timings, but it misreports E1 by giving a start time (5215.0s) instead of the correct finish (5219.6s) and omits E1's end, causing an important timing inaccuracy."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker says 'You are the educator of lawyers', when is the next time the first speaker says 'Thank you'?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5236.0
      },
      "gt_interval": {
        "start": 5224.9,
        "end": 5226.9
      },
      "pred_interval": {
        "start": 5230.2,
        "end": 5231.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.300000000000182,
        "end": 4.600000000000364,
        "average": 4.950000000000273
      },
      "rationale_metrics": {
        "rouge_l": 0.44705882352941173,
        "text_similarity": 0.7641274929046631,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction fails to match the correct event timestamps and misses the actual next 'Thank you' from the first speaker (5224.9\u20135226.9), instead giving incorrect times (5225.0 and 5230.2\u20135231.5); only the vague 'after' relation is preserved."
      }
    },
    {
      "question_id": "001",
      "question": "After Trivikram thanks Mr. Vikas Chaturvedi, when does he extend a warm welcome to Mr. Udaya Holla?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.288,
        "end": 166.858
      },
      "pred_interval": {
        "start": 180.0,
        "end": 182.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.71199999999999,
        "end": 15.641999999999996,
        "average": 16.176999999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.6661134958267212,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the reported event timestamps are significantly incorrect compared to the ground truth (both E1 and E2 are shifted later), so it fails on factual timing details required by the question."
      }
    },
    {
      "question_id": "002",
      "question": "During his discussion of strategies for a civil dispute, when does Mr. Udaya Holla state that preparation is what counts the most?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 251.8,
        "end": 254.67
      },
      "pred_interval": {
        "start": 240.0,
        "end": 243.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.800000000000011,
        "end": 11.169999999999987,
        "average": 11.485
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.5810602307319641,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'during' relationship, but the provided timestamps are substantially off (~10\u201312 seconds) and do not overlap the ground-truth intervals, so the answer is largely incorrect on key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After Vikas Chatrath finishes explaining the difference between 'learn' and 'earn', when does he thank everyone and wish them to stay safe and blessed?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5198.086,
        "end": 5203.11
      },
      "pred_interval": {
        "start": 5205.2,
        "end": 5208.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.113999999999578,
        "end": 4.890000000000327,
        "average": 6.001999999999953
      },
      "rationale_metrics": {
        "rouge_l": 0.2417582417582418,
        "text_similarity": 0.7529076337814331,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') and the concluding tone, but the event timestamps are significantly misaligned with the reference (predicted E1/E2 are ~7\u201312s later than the ground truth) and thus the predicted event boundaries and durations contradict the correct answer."
      }
    },
    {
      "question_id": "002",
      "question": "During the announcement of the next session, when does Vikas Chatrath mention Mr. Shingar Murali?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5207.213,
        "end": 5209.213
      },
      "pred_interval": {
        "start": 5215.5,
        "end": 5217.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.287000000000262,
        "end": 8.48700000000008,
        "average": 8.387000000000171
      },
      "rationale_metrics": {
        "rouge_l": 0.30952380952380953,
        "text_similarity": 0.7016289830207825,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies that the mention occurs during the announcement, but its timestamps are substantially off (prediction ~5215.5\u20135217.7s vs ground truth 5207.213\u20135209.213s), so it fails to match the key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath finishes telling everyone to stay safe and blessed, when does he say it's always a pleasure connecting with Mr. Hola?",
      "video_id": "hXN6cDKyk4g",
      "video_number": "009",
      "segment": {
        "start": 5190.0,
        "end": 5235.05
      },
      "gt_interval": {
        "start": 5201.609,
        "end": 5204.971
      },
      "pred_interval": {
        "start": 5208.0,
        "end": 5210.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.390999999999622,
        "end": 5.029000000000451,
        "average": 5.710000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.23255813953488372,
        "text_similarity": 0.6273831129074097,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the sequence and that the remark follows immediately ('once'), but the timestamps are off by several seconds (5208.0/5210.0 vs. 5201.609\u20135204.971) and it omits the additional phrase 'and Thrikram and associates,' so it is factually inaccurate on key details."
      }
    },
    {
      "question_id": "001",
      "question": "Once the prosecutor finishes introducing his opening statement, when does he explain why he gets to go first?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 43.329,
        "end": 50.118
      },
      "pred_interval": {
        "start": 12.5,
        "end": 18.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.829,
        "end": 32.118,
        "average": 31.4735
      },
      "rationale_metrics": {
        "rouge_l": 0.21875000000000003,
        "text_similarity": 0.758617103099823,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the qualitative relationship ('after') but the anchor and target timestamps are completely incorrect and do not match the ground-truth times or durations, so it fails to accurately locate the events."
      }
    },
    {
      "question_id": "002",
      "question": "After the prosecutor mentions the bartender, John Thomas, and the server, Roberta Jones, when does John observe Mr. Miller pass a small glass bottle?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 150.298,
        "end": 158.469
      },
      "pred_interval": {
        "start": 75.0,
        "end": 80.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.298,
        "end": 78.469,
        "average": 76.8835
      },
      "rationale_metrics": {
        "rouge_l": 0.2933333333333334,
        "text_similarity": 0.7007832527160645,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the timestamps are drastically incorrect for both E1 and E2 compared to the reference (predicted ~65\u201380s vs. ground truth 134.772s and 150.298\u2013158.469s), so it fails on factual timing and alignment."
      }
    },
    {
      "question_id": "003",
      "question": "Once John observes the unknown man pull a gun on the defendant, when does the gunman pull out his shield and shout 'Police! You are under arrest!'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 176.477,
        "end": 185.645
      },
      "pred_interval": {
        "start": 150.5,
        "end": 155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 25.977000000000004,
        "end": 30.64500000000001,
        "average": 28.311000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.37499999999999994,
        "text_similarity": 0.8311910629272461,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the correct events and labels the relation as 'after', but it gives completely incorrect timestamps (E1 missing its end time and placed much earlier, E2 at 150.5\u2013155.0 vs correct 176.477\u2013185.645), so it fails to match the ground truth timing."
      }
    },
    {
      "question_id": "001",
      "question": "After John observes Mr. Miller pass a small glass bottle with white powder, when does John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 163.6,
        "end": 166.8
      },
      "pred_interval": {
        "start": 175.5,
        "end": 177.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 11.900000000000006,
        "end": 10.199999999999989,
        "average": 11.049999999999997
      },
      "rationale_metrics": {
        "rouge_l": 0.31460674157303375,
        "text_similarity": 0.8335587978363037,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly labels the temporal relation as 'after', it gives substantially different event times and misidentifies the target event (decision to call 911 occurs earlier in the reference), so key temporal boundaries and the event definition are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the officer gives chase and the defendant pushes him, when does the officer trip and hit his head, opening a big gash on his forehead?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 216.0,
        "end": 226.824
      },
      "pred_interval": {
        "start": 240.0,
        "end": 242.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 15.175999999999988,
        "average": 19.587999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.37623762376237624,
        "text_similarity": 0.899189829826355,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the 'after' relationship, its anchor and target timestamps are significantly shifted and contradict the reference intervals, omitting the correct temporal boundaries and thus failing key factual details."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes sees the defendant exiting the saloon like a bat out of hell, when does she decide she should get back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 334.0,
        "end": 343.5
      },
      "pred_interval": {
        "start": 310.0,
        "end": 312.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.0,
        "end": 31.5,
        "average": 27.75
      },
      "rationale_metrics": {
        "rouge_l": 0.36842105263157887,
        "text_similarity": 0.8738704919815063,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the anchor event and the temporal relation ('after') but the timestamps are substantially inaccurate and the target event content is wrong/partially hallucinated (omits the quoted speech and actual target timing). Key factual elements and timings from the correct answer are missing or contradicted."
      }
    },
    {
      "question_id": "001",
      "question": "Once Dr. Reyes finishes thinking 'something seems wrong', when does she think 'I should get back to that bar'?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 337.3,
        "end": 340.5
      },
      "pred_interval": {
        "start": 350.5,
        "end": 352.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.199999999999989,
        "end": 11.5,
        "average": 12.349999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.40909090909090906,
        "text_similarity": 0.8511062264442444,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies a subsequent relation (after) but the anchor and target timestamps are substantially incorrect (off by ~13 seconds and differing durations) compared to the ground truth, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states the car was seized, when does he mention a forensic technician finding cocaine residue?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 420.1,
        "end": 428.2
      },
      "pred_interval": {
        "start": 470.0,
        "end": 472.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.89999999999998,
        "end": 44.30000000000001,
        "average": 47.099999999999994
      },
      "rationale_metrics": {
        "rouge_l": 0.3725490196078431,
        "text_similarity": 0.9309802651405334,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the two events and the 'after' relation, but the reported timestamps and durations are substantially different from the reference (off by ~50s), so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker specifies the time and date of the incident, when does he mention the car being seized?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 415.9,
        "end": 419.1
      },
      "pred_interval": {
        "start": 455.0,
        "end": 457.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.10000000000002,
        "end": 37.89999999999998,
        "average": 38.5
      },
      "rationale_metrics": {
        "rouge_l": 0.3870967741935484,
        "text_similarity": 0.9243744015693665,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the two events and their 'after' relation, but the provided timestamps are significantly inaccurate compared to the reference (off by ~40s), which is a key factual error."
      }
    },
    {
      "question_id": "001",
      "question": "After the prosecution states that the defendant Carl Miller was meeting with his business clients, when does the evidence show that the defendant was in the saloon at 3:55 p.m.?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 510.41,
        "end": 510.45
      },
      "pred_interval": {
        "start": 530.0,
        "end": 535.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.589999999999975,
        "end": 24.55000000000001,
        "average": 22.069999999999993
      },
      "rationale_metrics": {
        "rouge_l": 0.325,
        "text_similarity": 0.7707029581069946,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction does not match the precise timestamps (correct E1: 510.31\u2013510.38, E2: 510.41\u2013510.45); it omits E1 timing, gives E2 as 530\u2013535s and adds unsupported detail about video footage. Although it states 'after', the timing and factual mismatches make it largely incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once John observed Mr. Miller pass a small glass bottle with a white powder in it to his buddy, when did John decide to call 911?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 631.0,
        "end": 638.07
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.0,
        "end": 33.07000000000005,
        "average": 32.035000000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.3157894736842105,
        "text_similarity": 0.7749179601669312,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction gives entirely different timestamps (600\u2013605s for the call vs correct 631\u2013638.07s) and does not match the anchor timing (616.51\u2013617.0s), erroneously reversing the temporal relation, so it is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the gunman shouted, 'Police! You are under arrest!', when did the defendant immediately take off running towards the exit?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 673.544,
        "end": 683.655
      },
      "pred_interval": {
        "start": 660.0,
        "end": 665.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.543999999999983,
        "end": 18.654999999999973,
        "average": 16.099499999999978
      },
      "rationale_metrics": {
        "rouge_l": 0.411764705882353,
        "text_similarity": 0.8160393238067627,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction omits the anchor timestamps and provides incorrect target times (660.0\u2013665.0s) that conflict with the reference (673.544\u2013683.655s), and thus wrongly asserts an 'immediate' relation; major factual mismatch."
      }
    },
    {
      "question_id": "001",
      "question": "After Dr. Reyes sees the defendant exiting the saloon, when does she wonder if something is wrong?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.4,
        "end": 751.6
      },
      "pred_interval": {
        "start": 710.0,
        "end": 715.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 36.39999999999998,
        "end": 36.60000000000002,
        "average": 36.5
      },
      "rationale_metrics": {
        "rouge_l": 0.5194805194805195,
        "text_similarity": 0.9266869425773621,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation and paraphrases E2's mental state, but the timestamps are substantially inaccurate (both E1 and E2 are dozens of seconds earlier than the ground truth) and the E2 end time is omitted, so the answer is factually incorrect on key temporal details."
      }
    },
    {
      "question_id": "002",
      "question": "After the defendant jumped into a convertible, when does he look again at Dr. Reyes while starting the car?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 768.3,
        "end": 773.5
      },
      "pred_interval": {
        "start": 730.0,
        "end": 732.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.299999999999955,
        "end": 41.5,
        "average": 39.89999999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.5128205128205129,
        "text_similarity": 0.9490802884101868,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly identifies the same events and the 'after' relation, but the timestamps are substantially wrong (E1 and E2 times differ from the reference by ~36\u201341s) and E2's end time contradicts the ground truth, so it's semantically similar but factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After Dr. Reyes writes down the Mustang's plate number, when does she decide to go back to the bar?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 797.9,
        "end": 808.8
      },
      "pred_interval": {
        "start": 750.0,
        "end": 755.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.89999999999998,
        "end": 53.799999999999955,
        "average": 50.849999999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.3764705882352941,
        "text_similarity": 0.8516806364059448,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although both answers agree the relation is 'after', the predicted timings and event boundaries (much earlier timestamps and different start/end descriptions) contradict the reference and misidentify the key moments, so it is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, \"You will not hear one single person,\" when does the speaker mention the date \"September 8th, 2020\"?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 882.8,
        "end": 885.6
      },
      "pred_interval": {
        "start": 885.3,
        "end": 887.5
      },
      "iou": 0.06382978723405645,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.5,
        "end": 1.8999999999999773,
        "average": 2.1999999999999886
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.7783506512641907,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies both events, the quoted phrase, and the 'after' relationship, but the anchor timing is imprecise (start ~3.7s early and no end given) and the target times are shifted later by ~2.5s start / ~1.9s end, so key timing details are inaccurate or omitted."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that the car was seized and taken to the crime lab, when does the speaker begin describing the finding of cocaine residue by a forensic technician?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 891.0,
        "end": 904.4
      },
      "pred_interval": {
        "start": 912.7,
        "end": 915.2
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 21.700000000000045,
        "end": 10.800000000000068,
        "average": 16.250000000000057
      },
      "rationale_metrics": {
        "rouge_l": 0.36619718309859156,
        "text_similarity": 0.8382009863853455,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the coarse ordering ('after') but the timestamps are substantially incorrect and it fails to note the target began immediately after the anchor; therefore the answer is mostly wrong despite matching the general order."
      }
    },
    {
      "question_id": "003",
      "question": "During the speaker's statement about finding the defendant Carl Miller guilty of felonies, when is \"fleeing and eluding\" specifically mentioned?",
      "video_id": "DelhQUg8eH4",
      "video_number": "011",
      "segment": {
        "start": 870.0,
        "end": 958.0
      },
      "gt_interval": {
        "start": 930.8,
        "end": 947.9
      },
      "pred_interval": {
        "start": 940.8,
        "end": 942.0
      },
      "iou": 0.07017543859649379,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.0,
        "end": 5.899999999999977,
        "average": 7.949999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.8247944116592407,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies that 'fleeing and eluding' occurs during the guilty-felony statement and both events overlap, but it provides less precise/complete timing (gives a single anchor time instead of the full anchor interval and different start/end times for E2) than the reference."
      }
    },
    {
      "question_id": "001",
      "question": "After the male speaker asks the witness to state her name and spell her last name, when does the witness spell out her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.561,
        "end": 36.805
      },
      "pred_interval": {
        "start": 15.0,
        "end": 17.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.561,
        "end": 19.805,
        "average": 19.183
      },
      "rationale_metrics": {
        "rouge_l": 0.3255813953488372,
        "text_similarity": 0.6789999008178711,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the timestamps are substantially incorrect and it adds a false claim about the witness speaking immediately after the question; this omits and contradicts key factual timing details from the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the male speaker asks the witness if she was working as a plumber at a construction site on the evening of October 27th, 2020, when does the witness describe finding the broken window of her vehicle?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 69.075,
        "end": 74.643
      },
      "pred_interval": {
        "start": 65.0,
        "end": 70.0
      },
      "iou": 0.09592450482215048,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.075000000000003,
        "end": 4.643000000000001,
        "average": 4.359000000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3218390804597701,
        "text_similarity": 0.744957447052002,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the relative ordering ('after') and gives an approximate E1 timing, but the E2 start/end times are significantly off compared to the reference and it adds an unverified audio/visual cue, so it is only partially correct."
      }
    },
    {
      "question_id": "003",
      "question": "After the male speaker asks the witness to describe the area where she parked her car and if she was worried about its safety, when does the witness explain that she was not worried because she always parked near a bar?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 110.14,
        "end": 128.77
      },
      "pred_interval": {
        "start": 120.0,
        "end": 125.0
      },
      "iou": 0.2683843263553407,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.86,
        "end": 3.7700000000000102,
        "average": 6.815000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.23300970873786406,
        "text_similarity": 0.6684079170227051,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the content and that the witness's explanation comes after the question, but the reported timestamps deviate substantially from the ground truth (E1 is ~12s later and E2 start ~10s later) and it adds an unsupported audio cue, so the temporal alignment and some details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Ms. Mendoza reports that her tablet and ice skates were stolen, when does the lawyer ask her if she made any attempt to look for the perpetrators or immediately contacted the police?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 164.191,
        "end": 172.955
      },
      "pred_interval": {
        "start": 180.5,
        "end": 184.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.308999999999997,
        "end": 11.044999999999987,
        "average": 13.676999999999992
      },
      "rationale_metrics": {
        "rouge_l": 0.24096385542168675,
        "text_similarity": 0.6889381408691406,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the events and the 'after' relation, but the provided timestamps are significantly offset from the ground truth (both E1 and E2 are placed much later), so the timing is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza explains that she noticed a suspicious man around her car while on the phone with the operator, when does the lawyer ask her what the officer did?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 261.353,
        "end": 262.466
      },
      "pred_interval": {
        "start": 210.0,
        "end": 213.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 51.35300000000001,
        "end": 48.96600000000001,
        "average": 50.15950000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.2558139534883721,
        "text_similarity": 0.7244119048118591,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the events and their temporal relation ('after'), but the provided timestamps are substantially different from the ground truth (predicted times are much earlier and misaligned), so it is largely incorrect despite capturing the relation."
      }
    },
    {
      "question_id": "003",
      "question": "After Ms. Mendoza states that the thief got out of the car and started running down the street, when does she describe the thief punching the arresting officer?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 321.13,
        "end": 331.68
      },
      "pred_interval": {
        "start": 245.0,
        "end": 248.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 76.13,
        "end": 83.68,
        "average": 79.905
      },
      "rationale_metrics": {
        "rouge_l": 0.20779220779220778,
        "text_similarity": 0.6272151470184326,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the qualitative relation ('after') correct, the timestamps for both E1 and E2 are substantially incorrect compared to the reference, so it fails on key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza to point out the man who assaulted the officer, when does Ms. Mendoza describe him?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 349.986,
        "end": 355.253
      },
      "pred_interval": {
        "start": 350.0,
        "end": 355.0
      },
      "iou": 0.9493070058857042,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.014000000000010004,
        "end": 0.2529999999999859,
        "average": 0.13349999999999795
      },
      "rationale_metrics": {
        "rouge_l": 0.4666666666666667,
        "text_similarity": 0.7499159574508667,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the anchor timing and that E2 occurs after E1, and gives a reasonable E2 time window, but it omits the key factual content of Ms. Mendoza's description ('skinny and with gray hair') and has slight timestamp differences."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking if the man settled down and cooperated after being put in the patrol car, when does Ms. Mendoza reply that he did not?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 457.548,
        "end": 460.233
      },
      "pred_interval": {
        "start": 420.0,
        "end": 423.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.548,
        "end": 37.233000000000004,
        "average": 37.3905
      },
      "rationale_metrics": {
        "rouge_l": 0.3191489361702128,
        "text_similarity": 0.7274868488311768,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two events but the timestamps are significantly off (each ~35\u201338s earlier than the reference) and the relation label ('after') differs from the more specific 'once_finished', so the answer is largely incorrect despite matching event types."
      }
    },
    {
      "question_id": "003",
      "question": "After the new lawyer says 'Good morning', when does he ask Ms. Mendoza to state and spell her last name?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 502.241,
        "end": 504.904
      },
      "pred_interval": {
        "start": 510.0,
        "end": 512.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.7590000000000146,
        "end": 7.096000000000004,
        "average": 7.427500000000009
      },
      "rationale_metrics": {
        "rouge_l": 0.47500000000000003,
        "text_similarity": 0.8019733428955078,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relation, but the provided timestamps differ noticeably from the reference (several seconds later), so it's mostly correct but not precisely aligned temporally."
      }
    },
    {
      "question_id": "001",
      "question": "After the lawyer asks Ms. Mendoza if she prefers to testify with an interpreter, when does Ms. Mendoza confirm her preference?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 525.733,
        "end": 528.876
      },
      "pred_interval": {
        "start": 525.0,
        "end": 530.0
      },
      "iou": 0.6286000000000058,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.7329999999999472,
        "end": 1.1240000000000236,
        "average": 0.9284999999999854
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.6200487613677979,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and gives approximate timings and relevant cues, but the timestamps are notably imprecise (E1 predicted at 520.0s is ~4.6s earlier than the ground truth 524.632s, and E2 start/end are slightly shifted), so it is not an exact match."
      }
    },
    {
      "question_id": "002",
      "question": "After Ms. Mendoza states that she saw someone inside her car through the broken window, when does the lawyer acknowledge her statement?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 559.358,
        "end": 561.878
      },
      "pred_interval": {
        "start": 600.0,
        "end": 605.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 40.64200000000005,
        "end": 43.12199999999996,
        "average": 41.882000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2650602409638554,
        "text_similarity": 0.6988437175750732,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer preserves the 'after' relation and that the lawyer acknowledged verbally, it is factually incorrect about the event timestamps (off by ~35\u201340s) and adds unverified visual detail (nodding); key temporal facts from the correct answer are omitted/mismatched."
      }
    },
    {
      "question_id": "003",
      "question": "After the lawyer asks Ms. Mendoza if anything was removed from her vehicle, when does Ms. Mendoza list the stolen items?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 622.801,
        "end": 634.921
      },
      "pred_interval": {
        "start": 650.0,
        "end": 660.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 27.198999999999955,
        "end": 25.07899999999995,
        "average": 26.138999999999953
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7249492406845093,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation but gives substantially different timestamps than the ground truth (off by ~25\u201330s) and adds unverified visual/audio details; key factual timing information is incorrect, so it is largely mismatched."
      }
    },
    {
      "question_id": "001",
      "question": "After the witness finishes describing the man looking through the car window, when does the lawyer state that a deputy was arriving?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 711.914,
        "end": 714.633
      },
      "pred_interval": {
        "start": 710.5,
        "end": 712.0
      },
      "iou": 0.020808129687880945,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 1.4139999999999873,
        "end": 2.633000000000038,
        "average": 2.0235000000000127
      },
      "rationale_metrics": {
        "rouge_l": 0.22784810126582278,
        "text_similarity": 0.7107059955596924,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the correct temporal relation ('after') but the timestamp estimates are substantially off from the reference (predicted E1 at 708.0s vs 711.567s; predicted E2 710.5\u2013712.0s vs 711.914\u2013714.633s), so it is factually inaccurate about timing."
      }
    },
    {
      "question_id": "002",
      "question": "Once the lawyer finishes asking what the officer did, when does the witness describe the officer speaking into his radio and telling her to stay put?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 742.233,
        "end": 766.142
      },
      "pred_interval": {
        "start": 755.0,
        "end": 757.5
      },
      "iou": 0.104563135221046,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.767000000000053,
        "end": 8.642000000000053,
        "average": 10.704500000000053
      },
      "rationale_metrics": {
        "rouge_l": 0.28865979381443296,
        "text_similarity": 0.7079578638076782,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the described event (witness saying the officer spoke into his radio and told her to stay), but the timestamps and temporal relation contradict the reference (predicted times are ~13\u201315s off and relation labeled 'after' versus 'once_finished'), so key factual details are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the witness finishes stating she is absolutely sure about identifying the defendant, when does the lawyer ask about the suspect's behavior after being cuffed?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 851.101,
        "end": 861.493
      },
      "pred_interval": {
        "start": 865.0,
        "end": 867.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 13.899000000000001,
        "end": 5.506999999999948,
        "average": 9.702999999999975
      },
      "rationale_metrics": {
        "rouge_l": 0.2736842105263158,
        "text_similarity": 0.7465881109237671,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and event roles but gives substantially incorrect timestamps (both E1 and E2 are shifted later by many seconds), so it fails to match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the male lawyer asks what happened when they got to the patrol car, when does Ms. Mendoza describe the officers searching the suspect?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 885.63,
        "end": 894.463
      },
      "pred_interval": {
        "start": 880.5,
        "end": 884.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.1299999999999955,
        "end": 10.462999999999965,
        "average": 7.7964999999999804
      },
      "rationale_metrics": {
        "rouge_l": 0.3055555555555555,
        "text_similarity": 0.6115105152130127,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the correct events (lawyer question and Ms. Mendoza describing a search) but the time spans are substantially incorrect and inconsistent with the ground truth (E2 is placed earlier and does not cover the complete item list), and the asserted 'after' relation is not supported by the predicted timestamps."
      }
    },
    {
      "question_id": "002",
      "question": "Once Ms. Mendoza finishes explaining she got closer to see what was happening, when does she state that's why she remembers it well?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 919.062,
        "end": 923.688
      },
      "pred_interval": {
        "start": 910.2,
        "end": 912.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.861999999999966,
        "end": 10.987999999999943,
        "average": 9.924999999999955
      },
      "rationale_metrics": {
        "rouge_l": 0.4657534246575342,
        "text_similarity": 0.7764178514480591,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the relation and paraphrases the quoted phrase, but the crucial temporal annotations are incorrect (different start/end times and missing E1 end), so it fails to align with the ground truth timestamps."
      }
    },
    {
      "question_id": "003",
      "question": "Once the male lawyer finishes asking if the suspect settled down and cooperated in the patrol car, when does Ms. Mendoza state he did not cooperate?",
      "video_id": "k28NMpEkuRU",
      "video_number": "012",
      "segment": {
        "start": 870.0,
        "end": 962.0
      },
      "gt_interval": {
        "start": 937.892,
        "end": 940.207
      },
      "pred_interval": {
        "start": 940.1,
        "end": 943.3
      },
      "iou": 0.01978550295857486,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.20799999999997,
        "end": 3.092999999999961,
        "average": 2.6504999999999654
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.5965430736541748,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct relation and the content of Ms. Mendoza's reply, but the event timestamps are notably off (E1 slightly shifted and E2 delayed by ~2.2s with an extended end), so it is only partially accurate."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says, 'Good evening, friends,' when does he mention that their journey started in 2020 during the COVID times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 5.453,
        "end": 8.514
      },
      "pred_interval": {
        "start": 12.5,
        "end": 15.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 7.047,
        "end": 6.486000000000001,
        "average": 6.766500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.32786885245901637,
        "text_similarity": 0.6812072396278381,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer correctly identifies the temporal relationship ('after') but both the anchor and target timestamps are significantly incorrect compared to the reference, so it fails on the key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker (Vikas Chatrath) says 'I think there's some issue with the net,' when does the screen go black with his name displayed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 67.726,
        "end": 75.546
      },
      "pred_interval": {
        "start": 105.0,
        "end": 110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 37.274,
        "end": 34.45399999999999,
        "average": 35.864
      },
      "rationale_metrics": {
        "rouge_l": 0.34285714285714286,
        "text_similarity": 0.6910404562950134,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer's timestamps are largely incorrect (predicted ~100\u2013110s vs. correct 63.456s anchor end and 67.726\u201375.546s target), though it correctly states the temporal relationship ('immediately after'); overall it fails on the key factual timings."
      }
    },
    {
      "question_id": "003",
      "question": "Once Vikas Chatrath says 'Over to you, sir,' when does Mr. R.S. Cheema begin speaking?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 169.165,
        "end": 175.752
      },
      "pred_interval": {
        "start": 150.0,
        "end": 153.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.164999999999992,
        "end": 22.75200000000001,
        "average": 20.9585
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.7286911010742188,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are substantially different from the reference (predicted ~148\u2013153s vs. correct ~167\u2013175s), so the anchor/target boundaries do not match; the only loose agreement is that the target follows the anchor, but the timing is largely incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After Mr. Vikas Chatrath finishes his speech and invites Mr. Cheema, when does Mr. Cheema describe the topic as very generic and vast?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 200.048,
        "end": 204.229
      },
      "pred_interval": {
        "start": 165.0,
        "end": 170.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 35.048,
        "end": 34.22900000000001,
        "average": 34.63850000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.411764705882353,
        "text_similarity": 0.6126027703285217,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relation and that Mr. Cheema describes the topic as generic/vast, but the timestamps are grossly incorrect (\u2248160\u2013170s vs correct \u224815\u201354s), so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "After Mr. Cheema says he agreed with the choice of the broad topic, when does he explain the reason for his agreement?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 229.669,
        "end": 242.242
      },
      "pred_interval": {
        "start": 185.0,
        "end": 190.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.66900000000001,
        "end": 52.24199999999999,
        "average": 48.4555
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985915,
        "text_similarity": 0.7629320621490479,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the relation as 'after' but the provided temporal spans are far off from the ground truth (predicted ~180\u2013190s vs. true 224.6\u2013242.2s), so the key factual timing is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "During Mr. Cheema's description of going into division bench courts to hear idols and icons, when does he mention that the judges are not in a hurry?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 305.317,
        "end": 313.619
      },
      "pred_interval": {
        "start": 220.0,
        "end": 225.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 85.31700000000001,
        "end": 88.61900000000003,
        "average": 86.96800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.8501313328742981,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('during') right but both event timestamps are substantially incorrect and do not match the ground-truth anchor or target intervals, making the answer factually wrong despite the correct relation."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions he has been appearing in three High Courts, when does he state that criminal appeals have not been argued for a long time in the Punjab and Haryana High Court?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 379.0,
        "end": 385.0
      },
      "pred_interval": {
        "start": 345.0,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 34.0,
        "end": 30.0,
        "average": 32.0
      },
      "rationale_metrics": {
        "rouge_l": 0.32432432432432434,
        "text_similarity": 0.7275703549385071,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right, but both event timestamps and spans are significantly incorrect compared to the ground truth (predicted ~340\u2013355s vs. actual ~365\u2013385s), so it only partially matches."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions there is very little hearing in appeal, when does he state that the most 'scaring part' is the flood of outstanding litigation causing the hearing to get a beating?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 408.915,
        "end": 418.887
      },
      "pred_interval": {
        "start": 410.0,
        "end": 420.0
      },
      "iou": 0.801714027965721,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 1.0849999999999795,
        "end": 1.1129999999999995,
        "average": 1.0989999999999895
      },
      "rationale_metrics": {
        "rouge_l": 0.28947368421052627,
        "text_similarity": 0.789029598236084,
        "llm_judge_score": 9,
        "llm_judge_justification": "The prediction identifies the same events and the 'after' relation; timestamp intervals closely match the ground truth with only minor offsets (~0.5\u20131.1s) and no factual errors."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker says, 'Let's first also define what the appeals are', when does he begin to define criminal appeals as generally understood?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 330.0,
        "end": 540.0
      },
      "gt_interval": {
        "start": 473.605,
        "end": 487.757
      },
      "pred_interval": {
        "start": 480.0,
        "end": 490.0
      },
      "iou": 0.4731320524550176,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.394999999999982,
        "end": 2.242999999999995,
        "average": 4.318999999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.26865671641791045,
        "text_similarity": 0.7693504691123962,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the correct events and directionality but the timestamps are notably shifted (E1 ~4s late, E2 ~6s late and ends later) and the relation label ('after') is less precise than the ground truth ('once_finished'), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes stating that the real purpose of this exercise is to scuttle the path of the litigant, when does he discuss today's purpose?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 562.2,
        "end": 567.6
      },
      "pred_interval": {
        "start": 520.5,
        "end": 526.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 41.700000000000045,
        "end": 41.60000000000002,
        "average": 41.650000000000034
      },
      "rationale_metrics": {
        "rouge_l": 0.325,
        "text_similarity": 0.8471716642379761,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only matches the coarse ordering (E2 occurs after E1) but gives entirely incorrect start/end timestamps and fails to capture that E2 immediately follows E1 as stated in the reference, so it is largely factually wrong."
      }
    },
    {
      "question_id": "002",
      "question": "While the speaker is listing the second category of appeals, when does he mention the 'Essential Commodities Act'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 595.2,
        "end": 601.662
      },
      "pred_interval": {
        "start": 610.0,
        "end": 612.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.799999999999955,
        "end": 10.837999999999965,
        "average": 12.81899999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.8384160995483398,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relationship ('during') but the timestamps are substantially wrong: both the anchor and target times are shifted ~15\u201320s later than the ground truth and do not align with the correct intervals, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes stating he will confine today's discussion to the Indian Penal Code, when does he explain that he doesn't want to just touch and go?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 510.0,
        "end": 720.0
      },
      "gt_interval": {
        "start": 628.624,
        "end": 635.538
      },
      "pred_interval": {
        "start": 675.5,
        "end": 680.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.875999999999976,
        "end": 44.46199999999999,
        "average": 45.66899999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.32941176470588235,
        "text_similarity": 0.7111543416976929,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the ordering (anchor before target) but the timestamps are substantially incorrect and the correct answer indicates the target immediately follows the anchor, whereas the prediction gives later, non-contiguous times; thus it fails on key temporal details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions that their judgments are not very clear, when does he begin to explain that, unlike IPC, other acts *do* have provisions for vicarious liability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 746.063,
        "end": 753.951
      },
      "pred_interval": {
        "start": 715.0,
        "end": 730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.062999999999988,
        "end": 23.951000000000022,
        "average": 27.507000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.2631578947368421,
        "text_similarity": 0.7609800100326538,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relation but the event timestamps are substantially inaccurate (both starts and ends differ by ~30\u201340s from ground truth) and the anchor's end time is omitted, so key factual timing details are wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes describing the first clause of the provisions, when does he explain that an individual is a 'deemed accused' by virtue of their office?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 772.927,
        "end": 777.037
      },
      "pred_interval": {
        "start": 760.0,
        "end": 775.0
      },
      "iou": 0.1216763514703278,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.927000000000021,
        "end": 2.0370000000000346,
        "average": 7.482000000000028
      },
      "rationale_metrics": {
        "rouge_l": 0.40625,
        "text_similarity": 0.7844386696815491,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the relation roughly right but the timestamps deviate substantially from the reference (E1 off by ~22s, E2 start off by ~13s), so it is largely factually incorrect despite capturing the general sequence."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker states that the 'second part is most important', when does he begin describing this second part of the provisions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 690.0,
        "end": 900.0
      },
      "gt_interval": {
        "start": 786.039,
        "end": 799.875
      },
      "pred_interval": {
        "start": 820.0,
        "end": 835.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 33.96100000000001,
        "end": 35.125,
        "average": 34.543000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.42857142857142855,
        "text_similarity": 0.7723156213760376,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the causal ordering right (E2 occurs after E1) but the timestamps are substantially incorrect compared to the reference (predicted E1/E2 start and end times differ by ~24s+), and the relation label is imprecise\u2014thus largely inaccurate. "
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes asking if people are vicariously liable, when does he state that the answer is 'yes and no both'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 884.653,
        "end": 886.655
      },
      "pred_interval": {
        "start": 900.0,
        "end": 910.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.34699999999998,
        "end": 23.345000000000027,
        "average": 19.346000000000004
      },
      "rationale_metrics": {
        "rouge_l": 0.4883720930232558,
        "text_similarity": 0.6856043338775635,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction matches the content phrase but the timestamps are substantially off (predicted ~895\u2013910s vs ground truth ~882.39\u2013886.655s) and the relationship is labeled 'after' rather than the correct 'once_finished', so the timing and relation are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker explains that the Food Safety and Security Act allows a company to appoint a competent person as a nominee, when does he mention the Environment Act?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 954.035,
        "end": 958.702
      },
      "pred_interval": {
        "start": 950.0,
        "end": 960.0
      },
      "iou": 0.466700000000003,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.034999999999968,
        "end": 1.2980000000000018,
        "average": 2.666499999999985
      },
      "rationale_metrics": {
        "rouge_l": 0.367816091954023,
        "text_similarity": 0.7745795249938965,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly captures both events and the 'after' relation, but the provided timestamps deviate from the reference by several seconds (E1 ~4s earlier; E2 start ~4s earlier and end ~1.3s later), so it is not precisely aligned."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes that the rest of the appeals would always be technical, when does he introduce the topic of drafting an appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 870.0,
        "end": 1080.0
      },
      "gt_interval": {
        "start": 1052.092,
        "end": 1055.115
      },
      "pred_interval": {
        "start": 1020.0,
        "end": 1030.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.0920000000001,
        "end": 25.11500000000001,
        "average": 28.603500000000054
      },
      "rationale_metrics": {
        "rouge_l": 0.4155844155844156,
        "text_similarity": 0.6905958652496338,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relation ('after') right but the timestamps are substantially incorrect (E1 and E2 are ~24\u201332 seconds earlier than the reference), so it does not match the key factual timing details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions looking into the nuances of drafting, when does he state what he found regarding drafting and appeals?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1054.8,
        "end": 1058.3
      },
      "pred_interval": {
        "start": 1085.0,
        "end": 1095.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.200000000000045,
        "end": 36.700000000000045,
        "average": 33.450000000000045
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.8453938961029053,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship, but the provided timestamps for E1 and especially E2 differ substantially from the ground truth, so the answer is factually incorrect in key details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that criminal proceedings are not adversarial, when does he discuss how formal errors and oversights can be overcome?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1120.624,
        "end": 1125.731
      },
      "pred_interval": {
        "start": 1120.0,
        "end": 1135.0
      },
      "iou": 0.34046666666666475,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 0.6240000000000236,
        "end": 9.269000000000005,
        "average": 4.9465000000000146
      },
      "rationale_metrics": {
        "rouge_l": 0.2191780821917808,
        "text_similarity": 0.8689538240432739,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the relation ('after') and approximately locates the target's start, but the anchor start time is substantially off (1105.0s vs 1114.2s) and the predicted target end (1135.0s) overly extends beyond the ground-truth span, so timings are only partially accurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that the 313 was not correctly recorded in a Ropar case, when does he recall the accused saying he was made to sign a blank paper?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1050.0,
        "end": 1260.0
      },
      "gt_interval": {
        "start": 1194.217,
        "end": 1254.6
      },
      "pred_interval": {
        "start": 1200.0,
        "end": 1210.0
      },
      "iou": 0.16560952585992797,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.782999999999902,
        "end": 44.59999999999991,
        "average": 25.191499999999905
      },
      "rationale_metrics": {
        "rouge_l": 0.22499999999999998,
        "text_similarity": 0.8601428270339966,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') right but substantially misaligns the event timestamps and durations (E1 given as 1185.0 vs correct 1190.9\u20131193.6, E2 given as 1200.0\u20131210.0 vs correct 1194.217\u20131195.861) and omits E1's end time, so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what they should do, when does he ask if an application needs to be filed for additional evidence in the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1251.721,
        "end": 1266.539
      },
      "pred_interval": {
        "start": 1270.5,
        "end": 1275.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.778999999999996,
        "end": 8.461000000000013,
        "average": 13.620000000000005
      },
      "rationale_metrics": {
        "rouge_l": 0.3513513513513513,
        "text_similarity": 0.6907221078872681,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction preserves the events and their 'after' relationship, but the timestamps are substantially off for both events (E1 and E2 occur ~10\u201330s later than the ground truth), so it is factually inaccurate on the crucial timing detail."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker states that the court made a mistake by not formally proving a cassette recording, when does he explain that they had to make an application for evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1291.0,
        "end": 1294.14
      },
      "pred_interval": {
        "start": 1335.0,
        "end": 1342.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.0,
        "end": 47.8599999999999,
        "average": 45.92999999999995
      },
      "rationale_metrics": {
        "rouge_l": 0.30769230769230765,
        "text_similarity": 0.6323487758636475,
        "llm_judge_score": 1,
        "llm_judge_justification": "The prediction identifies the same two events but the time spans are significantly offset (~35\u201345s later than the reference) and the temporal relation ('after') contradicts the correct 'once_finished' relation, so it is essentially incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says that first reactions may never come again if not noted down, when does he state that he made a practice of noting his reactions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1230.0,
        "end": 1440.0
      },
      "gt_interval": {
        "start": 1394.269,
        "end": 1401.682
      },
      "pred_interval": {
        "start": 1400.0,
        "end": 1405.5
      },
      "iou": 0.14976404594426296,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.7309999999999945,
        "end": 3.8179999999999836,
        "average": 4.774499999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7828750014305115,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction captures the correct semantic relation ('after') and roughly the same region for E2, but the provided timestamps do not match the ground truth (E1 is shifted later and E2 extends beyond the true end), so it is only partially correct."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says one should not put the detail in an appeal, when does he suggest that subtle points may be hinted at when drafting grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1444.582,
        "end": 1452.109
      },
      "pred_interval": {
        "start": 1435.0,
        "end": 1445.0
      },
      "iou": 0.024431585715114532,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.582000000000107,
        "end": 7.108999999999924,
        "average": 8.345500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.23333333333333334,
        "text_similarity": 0.7777277231216431,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies the 'after' relationship and the general temporal ordering, but the reported time stamps deviate noticeably from the reference (especially the target start/end), so it is only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker asks 'What is the key question?', when does he compare a trial or appeal to writing a novel?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1536.909,
        "end": 1545.661
      },
      "pred_interval": {
        "start": 1510.0,
        "end": 1525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.909000000000106,
        "end": 20.661000000000058,
        "average": 23.785000000000082
      },
      "rationale_metrics": {
        "rouge_l": 0.26229508196721313,
        "text_similarity": 0.6914864182472229,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps and temporal relationship are largely incorrect\u2014both anchor and target times differ substantially from the reference and the relation 'after' contradicts the reference's immediate/direct follow; only the mention of the anchor question somewhat aligns."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker mentions keeping the core issue in mind, when does he advise that the first reading of an appeal should be relaxed?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1410.0,
        "end": 1620.0
      },
      "gt_interval": {
        "start": 1594.74,
        "end": 1607.579
      },
      "pred_interval": {
        "start": 1605.0,
        "end": 1615.0
      },
      "iou": 0.1272951628825248,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.259999999999991,
        "end": 7.421000000000049,
        "average": 8.84050000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.8406602144241333,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction incorrectly reports the event boundaries (E1 timing and E2 start/end are both substantially off and E1 is given as a start rather than the correct finish), though it correctly captures that the advice occurs after the anchor; overall the timestamps are inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker says that the first reading of an appeal should be relaxed, when does he compare it to watching a soap opera, reading a novel, or listening to music?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1608.151,
        "end": 1615.0
      },
      "pred_interval": {
        "start": 1620.5,
        "end": 1628.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.348999999999933,
        "end": 13.0,
        "average": 12.674499999999966
      },
      "rationale_metrics": {
        "rouge_l": 0.17948717948717952,
        "text_similarity": 0.6897974014282227,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly captures the semantic relation (the comparison happens after/once the first reading), and the anchor timing is close, but the target timestamps are notably off from the reference and the predicted end time introduces unsupported detail, so it's only partially accurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker advises being as neutral as possible, when does he explain that this enables objectivity and unbiased case knowledge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1638.375,
        "end": 1647.567
      },
      "pred_interval": {
        "start": 1685.0,
        "end": 1692.3
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.625,
        "end": 44.73299999999995,
        "average": 45.678999999999974
      },
      "rationale_metrics": {
        "rouge_l": 0.34782608695652173,
        "text_similarity": 0.6877155303955078,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction correctly identifies the same two events and the 'after' relationship (content and relation match), but the provided timestamps differ substantially from the ground-truth intervals, so the temporal alignment is factually incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker asks if he has a good case, when does he follow up by asking if he has a bad case or one in between?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1590.0,
        "end": 1800.0
      },
      "gt_interval": {
        "start": 1674.0,
        "end": 1681.0
      },
      "pred_interval": {
        "start": 1750.7,
        "end": 1758.9
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 76.70000000000005,
        "end": 77.90000000000009,
        "average": 77.30000000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837834,
        "text_similarity": 0.7059783339500427,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same two utterances in the correct order, but the timestamps are substantially different (off by ~75s) and the temporal relation label ('after' vs 'next') is less precise; therefore it is largely incorrect despite matching the event content."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker mentions that three to four things would matter when preparing an appeal, what is the first thing he suggests?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1824.432,
        "end": 1828.85
      },
      "pred_interval": {
        "start": 1775.5,
        "end": 1780.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 48.932000000000016,
        "end": 48.84999999999991,
        "average": 48.89099999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.3174603174603175,
        "text_similarity": 0.773806631565094,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'immediately after' relationship but the anchor/target timestamps do not match the reference (off by ~48s and with different boundaries), so key factual elements are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker emphasizes that stages in an appeal should be clear in one's mind, when does he start talking about the 'finest lawyers'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1890.082,
        "end": 1904.09
      },
      "pred_interval": {
        "start": 1795.0,
        "end": 1800.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 95.08200000000011,
        "end": 104.08999999999992,
        "average": 99.58600000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.273972602739726,
        "text_similarity": 0.7238109111785889,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering and relative relation ('shortly after') right, but the timestamps are substantially incorrect (off by about 100s) and do not match the precise anchor/target times in the correct answer."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes his point about preparation, when does he announce moving to the next phase of the appeal?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1770.0,
        "end": 1980.0
      },
      "gt_interval": {
        "start": 1921.825,
        "end": 1924.428
      },
      "pred_interval": {
        "start": 1820.0,
        "end": 1825.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 101.82500000000005,
        "end": 99.42800000000011,
        "average": 100.62650000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.3235294117647059,
        "text_similarity": 0.8314478993415833,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives times that are hundreds of seconds off and asserts a brief pause, whereas the reference has specific times ~1918\u20131924s with the target immediately following the anchor; thus the prediction is largely incorrect. "
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker explains how a judge will eventually respond to an appeal, when does he mention being placed before a newly transferred judge?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 1984.778,
        "end": 1991.706
      },
      "pred_interval": {
        "start": 2005.0,
        "end": 2010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 20.22199999999998,
        "end": 18.294000000000096,
        "average": 19.258000000000038
      },
      "rationale_metrics": {
        "rouge_l": 0.3294117647058823,
        "text_similarity": 0.8279670476913452,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets only the overall 'after' relation right but the event timings and content boundaries are significantly incorrect (timestamps off by ~10\u201320s and the stated E2 content/start do not match the reference), so it fails to align with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker describes how he handles a client's ticklish case by listening to their story, when does he next explain that judges would analyze in a similar way and evolve their style?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2000.528,
        "end": 2006.954
      },
      "pred_interval": {
        "start": 2050.0,
        "end": 2055.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 49.47199999999998,
        "end": 48.04600000000005,
        "average": 48.759000000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.32,
        "text_similarity": 0.7078193426132202,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the judge-explanation follows the ticklish-case remark but mislocalizes both events by ~40\u201350s and labels the relation as 'after' rather than the immediate 'next'\u2014so the timing and relation are largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker states that a diligent lawyer would form and correct their opinion about the bench every day, when does he next say that this practice saves the court's time?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 1950.0,
        "end": 2160.0
      },
      "gt_interval": {
        "start": 2071.617,
        "end": 2074.641
      },
      "pred_interval": {
        "start": 2100.0,
        "end": 2105.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 28.38299999999981,
        "end": 30.358999999999924,
        "average": 29.370999999999867
      },
      "rationale_metrics": {
        "rouge_l": 0.3376623376623377,
        "text_similarity": 0.7354394197463989,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures the general idea that the saving-the-court-time remark follows the 'every day' statement, but the timestamps and span boundaries are substantially incorrect and the relation labeled 'after' does not match the more precise 'next' relation; the answer thus contains fabricated timing details and incorrect alignment."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks what makes a good lawyer, when does he begin to answer by describing a man who has prepared his brief fully?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2192.444,
        "end": 2200.017
      },
      "pred_interval": {
        "start": 2145.0,
        "end": 2155.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 47.44399999999996,
        "end": 45.016999999999825,
        "average": 46.23049999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.24999999999999994,
        "text_similarity": 0.7690491080284119,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although the predicted answer gets the relative 'after' relation right, its timestamps are substantially incorrect (off by ~40+ seconds) and it fails to capture the correct end/completion time (2200.017s), so it does not match the reference timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker describes a fresh judge saying 'no reading of evidence', when does he express his opinion that it's illegal and unconstitutional?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2237.752,
        "end": 2243.727
      },
      "pred_interval": {
        "start": 2205.0,
        "end": 2212.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.75199999999995,
        "end": 31.72699999999986,
        "average": 32.23949999999991
      },
      "rationale_metrics": {
        "rouge_l": 0.3013698630136986,
        "text_similarity": 0.7462781071662903,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right, but the reported timestamps for both E1 and E2 are far off from the ground-truth intervals (offset by ~34\u201345s) and the E2 boundaries do not match the reference, so the answer is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker instructs to make the judge read Section 54 of the Evidence Act, when does he explain its content regarding the previous character of an accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2130.0,
        "end": 2340.0
      },
      "gt_interval": {
        "start": 2302.58,
        "end": 2310.354
      },
      "pred_interval": {
        "start": 2270.0,
        "end": 2280.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.57999999999993,
        "end": 30.353999999999814,
        "average": 31.46699999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.7796434164047241,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering ('after') but the timestamps are substantially incorrect (E1 and E2 times differ by ~18\u201340s from the reference) and the anchor end time is omitted, so it fails to match the correct intervals."
      }
    },
    {
      "question_id": "001",
      "question": "During the speaker's introduction of the second question as a heinous crime, when does he describe specific examples like 'chopped off the hand'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2354.498,
        "end": 2357.601
      },
      "pred_interval": {
        "start": 2345.0,
        "end": 2355.0
      },
      "iou": 0.03983810808665568,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 9.498000000000047,
        "end": 2.6010000000001128,
        "average": 6.04950000000008
      },
      "rationale_metrics": {
        "rouge_l": 0.4155844155844156,
        "text_similarity": 0.8789510726928711,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction preserves the content and 'during' relation (mentions the heinous crime and the 'chopped off the hand' example), but the timestamps are substantially inaccurate (anchor ~7s early, target start ~9s early), so it is semantically correct but temporally imprecise."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states 'this is the second kind of roadblock', when does he introduce the 'Third kind of roadblock'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2414.579,
        "end": 2418.664
      },
      "pred_interval": {
        "start": 2400.0,
        "end": 2410.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 14.579000000000178,
        "end": 8.664000000000215,
        "average": 11.621500000000196
      },
      "rationale_metrics": {
        "rouge_l": 0.35135135135135137,
        "text_similarity": 0.8001782894134521,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction only captures that E2 follows E1 but gives substantially incorrect timestamps (both anchor and target off by ~10\u201320s) and mislabels the relation as 'after' rather than immediate 'next', so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker concludes explaining 'these are the questions which are put to you', when does he transition to discussing myths about 'a dying declaration'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2310.0,
        "end": 2520.0
      },
      "gt_interval": {
        "start": 2462.739,
        "end": 2468.126
      },
      "pred_interval": {
        "start": 2480.0,
        "end": 2490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.260999999999967,
        "end": 21.873999999999796,
        "average": 19.56749999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7857306599617004,
        "llm_judge_score": 3,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the anchor and target timestamps are significantly off from the ground truth (both shifted by ~13\u201318s), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker extensively discusses the 1925 Lahore Bakshish Singh judgment, when does he bring up the Lakshmi versus Om Prakash case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2549.448,
        "end": 2555.294
      },
      "pred_interval": {
        "start": 2510.0,
        "end": 2515.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 39.447999999999865,
        "end": 40.29399999999987,
        "average": 39.87099999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.4000000000000001,
        "text_similarity": 0.7982439994812012,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer correctly identifies the relation as 'after', both event timestamps are significantly incorrect compared to the ground truth (E1 and E2 are misplaced by tens of seconds), so the core factual timing is largely wrong."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying, 'These are the cases when you have opened the door of a hearing', when does he state, 'These are situations which we must know'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2605.712,
        "end": 2610.678
      },
      "pred_interval": {
        "start": 2605.0,
        "end": 2610.0
      },
      "iou": 0.7551954913702181,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 1,
        "R@0.7": 1
      },
      "mae": {
        "start": 0.7119999999999891,
        "end": 0.6779999999998836,
        "average": 0.6949999999999363
      },
      "rationale_metrics": {
        "rouge_l": 0.3409090909090909,
        "text_similarity": 0.5972994565963745,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction gets the order and approximate timing of E2 and correctly labels the relationship as 'once', but it fails to provide the E1 end time (giving only an E1 start) and has small but nontrivial timestamp discrepancies versus the reference. These omissions reduce completeness and precision."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises, 'But when you go to this, don't offend the judge', when does he describe making notes in 'three phases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2490.0,
        "end": 2700.0
      },
      "gt_interval": {
        "start": 2647.594,
        "end": 2653.382
      },
      "pred_interval": {
        "start": 2680.0,
        "end": 2685.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.40599999999995,
        "end": 31.617999999999938,
        "average": 32.011999999999944
      },
      "rationale_metrics": {
        "rouge_l": 0.4473684210526316,
        "text_similarity": 0.7343551516532898,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but the event timestamps are substantially off (each is ~28\u201334 seconds later than the reference), so the timing information is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker advises a lawyer to stop if their best argument isn't selling, when does he suggest the lawyer might say, 'Your lordship may hear me on other issues'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2687.877,
        "end": 2690.499
      },
      "pred_interval": {
        "start": 2700.0,
        "end": 2705.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.123000000000047,
        "end": 14.501000000000204,
        "average": 13.312000000000126
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.7719107270240784,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the target phrase and that E2 occurs after E1, but the provided timestamps are significantly shifted from the reference (off by roughly 8\u201315 seconds), so the temporal alignment is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes explaining how language helps with flexibility and playing cool in arguments, when does he introduce 'sense of humor' as an important quality?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2718.862,
        "end": 2725.929
      },
      "pred_interval": {
        "start": 2750.0,
        "end": 2755.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.13799999999992,
        "end": 29.070999999999913,
        "average": 30.104499999999916
      },
      "rationale_metrics": {
        "rouge_l": 0.37837837837837834,
        "text_similarity": 0.8096638917922974,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the introduction of 'sense of humor' but the timestamps are substantially shifted (~27\u201332s later) compared to the reference and it fails to state that E2 follows immediately after E1, so it is temporally inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker identifies 'trap cases' and 'DA cases' as the majority of Corruption Act cases, when does he introduce 'scam cases'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2670.0,
        "end": 2880.0
      },
      "gt_interval": {
        "start": 2786.395,
        "end": 2789.04
      },
      "pred_interval": {
        "start": 2810.0,
        "end": 2815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 23.605000000000018,
        "end": 25.960000000000036,
        "average": 24.782500000000027
      },
      "rationale_metrics": {
        "rouge_l": 0.3225806451612903,
        "text_similarity": 0.8513339757919312,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly states that the 'scam cases' segment follows the anchor, but the provided timestamps are substantially incorrect compared to the reference (2781.409\u20132789.04), so it fails on factual accuracy of key details."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks if a busy or young lawyer needs a register of important judgments, when does he directly ask if it is a necessary tool?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2898.935,
        "end": 2905.299
      },
      "pred_interval": {
        "start": 2860.5,
        "end": 2865.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 38.434999999999945,
        "end": 40.29899999999998,
        "average": 39.36699999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.29333333333333333,
        "text_similarity": 0.8237636685371399,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the prediction correctly identifies the relative relation ('after') and the two utterances, the provided timestamps are substantially incorrect (both anchor and target times differ markedly from the reference), the anchor end time is omitted, and the predicted temporal spacing contradicts the described clear pause\u2014hence poor alignment with the ground truth."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes emphasizing the need to know, read, and re-read foundational judgments, when does he state he will recount a few of them?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 2930.268,
        "end": 2935.553
      },
      "pred_interval": {
        "start": 2920.0,
        "end": 2925.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.268000000000029,
        "end": 10.052999999999884,
        "average": 10.160499999999956
      },
      "rationale_metrics": {
        "rouge_l": 0.29850746268656714,
        "text_similarity": 0.7760599255561829,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly indicates that E2 follows E1, but the provided start/end timestamps are substantially different from the reference and contain factual inaccuracies, so it fails to match the correct answer's detailed timing."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes listing the question about a false defense or wrong answer given by the accused, when does he explicitly state his suggestion and request for foundational judgments?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 2850.0,
        "end": 3060.0
      },
      "gt_interval": {
        "start": 3029.268,
        "end": 3033.995
      },
      "pred_interval": {
        "start": 3005.0,
        "end": 3010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.26800000000003,
        "end": 23.99499999999989,
        "average": 24.13149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2686567164179105,
        "text_similarity": 0.7755736112594604,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted answer gets the temporal relation 'after' correct, its timestamps for both events are substantially different from the reference and it omits the note about intervening sentences, so the factual timing is incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker mentions 'Virsar Singh's judgment, 1958', when does he mention 'Vivian Bose's judgment, a classic'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3045.106,
        "end": 3052.317
      },
      "pred_interval": {
        "start": 3075.0,
        "end": 3082.5
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.893999999999778,
        "end": 30.182999999999993,
        "average": 30.038499999999885
      },
      "rationale_metrics": {
        "rouge_l": 0.32727272727272727,
        "text_similarity": 0.6786273717880249,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same two mentions and the 'after' relation, but the anchor and target timestamps are substantially off (anchor ~+19.8s, target start/end ~+29.9s/+30.2s), so the timing is inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker refers to the case 'Prabhu versus the Emperor, 1941', when does he ask if the initial onus on the prosecution transfers itself?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3120.51,
        "end": 3129.351
      },
      "pred_interval": {
        "start": 3105.0,
        "end": 3110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.510000000000218,
        "end": 19.351000000000113,
        "average": 17.430500000000166
      },
      "rationale_metrics": {
        "rouge_l": 0.32727272727272727,
        "text_similarity": 0.7771458625793457,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the 'after' relation right but the reported anchor and target timestamps are substantially misaligned with the ground truth (off by ~15\u201325+ seconds), so the key temporal elements are incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes saying '3.', when does he explain that the court has a 'broader picture' with the accused's evidence?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3030.0,
        "end": 3240.0
      },
      "gt_interval": {
        "start": 3158.541,
        "end": 3164.576
      },
      "pred_interval": {
        "start": 3205.0,
        "end": 3215.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 46.45899999999983,
        "end": 50.42399999999998,
        "average": 48.441499999999905
      },
      "rationale_metrics": {
        "rouge_l": 0.2711864406779661,
        "text_similarity": 0.7693619132041931,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies an anchor and target but the timestamps are substantially off (by ~36\u201346s) and the relation is labeled 'after' instead of the correct immediate 'once_finished', so it fails on factual timing and relational accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker introduces the 1954 Supreme Court case, when does he detail the specific allegation and offense?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3228.663,
        "end": 3239.981
      },
      "pred_interval": {
        "start": 3260.0,
        "end": 3280.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.33699999999999,
        "end": 40.01899999999978,
        "average": 35.677999999999884
      },
      "rationale_metrics": {
        "rouge_l": 0.36363636363636365,
        "text_similarity": 0.8872860074043274,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') correct but significantly misplaces both event timestamps (E1 start is ~26+ seconds late and E1 end is omitted; E2 is shifted and much longer), so it does not match the reference timing or coverage."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes saying that Tanu Bedi is present, when does he begin describing the case involving an Afghan Airlines pilot?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3267.18,
        "end": 3280.178
      },
      "pred_interval": {
        "start": 3350.0,
        "end": 3370.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.82000000000016,
        "end": 89.82200000000012,
        "average": 86.32100000000014
      },
      "rationale_metrics": {
        "rouge_l": 0.345679012345679,
        "text_similarity": 0.8624380826950073,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same anchor and target events but the timestamps are off by ~80\u201390 seconds and the relation 'after' is less precise than the correct 'once_finished', so the answer is largely incorrect despite matching event descriptions."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker asks if they can continue for 10 minutes, when does someone respond with 'Yes, of course, of course'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3210.0,
        "end": 3420.0
      },
      "gt_interval": {
        "start": 3407.405,
        "end": 3409.588
      },
      "pred_interval": {
        "start": 3405.0,
        "end": 3410.0
      },
      "iou": 0.43659999999999854,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 2.4050000000002,
        "end": 0.4119999999998072,
        "average": 1.4085000000000036
      },
      "rationale_metrics": {
        "rouge_l": 0.35443037974683544,
        "text_similarity": 0.8917765021324158,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction identifies the same events and roughly similar times for the response, but the anchor time is off by ~10s, the target interval is shifted/extended, and the relation is mislabeled ('after' vs direct 'once_finished'), so key temporal and relational details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker asks how long they can continue, when does he ask if they can go on for 10 minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3405.21,
        "end": 3408.57
      },
      "pred_interval": {
        "start": 3450.0,
        "end": 3455.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 44.789999999999964,
        "end": 46.429999999999836,
        "average": 45.6099999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.13333333333333333,
        "text_similarity": 0.6201419830322266,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the provided timestamps are significantly different from the ground truth and it introduces an unsupported audio-cue detail (questioning inflection). These mismatches and the hallucinated cue make the answer largely incorrect despite the correct ordering."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker mentions the judge saying 'no, you are convicted', when does he recall playing basketball?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3490.94,
        "end": 3501.65
      },
      "pred_interval": {
        "start": 3520.0,
        "end": 3525.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 29.059999999999945,
        "end": 23.34999999999991,
        "average": 26.204999999999927
      },
      "rationale_metrics": {
        "rouge_l": 0.18181818181818182,
        "text_similarity": 0.7935259342193604,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation right (target after anchor) but the provided timestamps are substantially different from the ground truth and thus factually incorrect, and it adds an unsupported visual cue (shooting gesture). These major factual errors warrant a low score despite correct ordering."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker advises that 'Your case must bother you', when does he begin telling another interesting case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3390.0,
        "end": 3600.0
      },
      "gt_interval": {
        "start": 3552.29,
        "end": 3555.63
      },
      "pred_interval": {
        "start": 3570.0,
        "end": 3578.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 17.710000000000036,
        "end": 22.36999999999989,
        "average": 20.039999999999964
      },
      "rationale_metrics": {
        "rouge_l": 0.1794871794871795,
        "text_similarity": 0.7086281776428223,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the qualitative relation ('after') right but the reported time intervals are substantially incorrect compared to the reference, and it adds an unsupported audio-cue detail (hallucination). Key factual elements (accurate timestamps) are missing/incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker finishes describing the detailed injury on the forehead, when does he state that the benefit of doubt went to the accused?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3636.678,
        "end": 3640.261
      },
      "pred_interval": {
        "start": 3605.0,
        "end": 3610.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.677999999999884,
        "end": 30.260999999999967,
        "average": 30.969499999999925
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.7461322546005249,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction identifies the same events but gives timestamps that substantially conflict with the ground truth (E2 is placed much earlier and overlapping E1 rather than after), so despite matching the relation label the temporal details are incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "During the speaker's description of the trickster character from Maupassant's story and his famous acts, when does he specifically mention the trickster showing different tricks to people?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3732.928,
        "end": 3736.294
      },
      "pred_interval": {
        "start": 3650.0,
        "end": 3655.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 82.92799999999988,
        "end": 81.29399999999987,
        "average": 82.11099999999988
      },
      "rationale_metrics": {
        "rouge_l": 0.23529411764705882,
        "text_similarity": 0.7801496982574463,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted answer gives anchor/target time spans that are far earlier (~87s) than the ground truth and do not overlap the correct intervals; although it states the same 'during' relation, the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "Once the speaker finishes introducing that there are 'two more cases', when does he name the location of the first of these new cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3570.0,
        "end": 3780.0
      },
      "gt_interval": {
        "start": 3665.138,
        "end": 3668.806
      },
      "pred_interval": {
        "start": 3720.0,
        "end": 3725.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 54.86200000000008,
        "end": 56.19399999999996,
        "average": 55.52800000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.21212121212121213,
        "text_similarity": 0.5582341551780701,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer captures the general ordering ('once') but the timestamps are substantially incorrect (off by ~55 seconds) and it states the naming occurs much later rather than 'almost immediately'; the relation label is also less precise than 'once_finished'."
      }
    },
    {
      "question_id": "001",
      "question": "Once the main speaker finishes stating that 'So that persuaded the court', when does he begin talking about other interesting cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3833.541,
        "end": 3838.467
      },
      "pred_interval": {
        "start": 3802.0,
        "end": 3815.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 31.541000000000167,
        "end": 23.4670000000001,
        "average": 27.504000000000133
      },
      "rationale_metrics": {
        "rouge_l": 0.25396825396825395,
        "text_similarity": 0.7833478450775146,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the ordering (E2 after E1) but the timestamps are significantly wrong (E1 end and E2 start differ by >10s and >30s from the reference), it omits E2's end time, and uses a less precise relation than 'once_finished'."
      }
    },
    {
      "question_id": "002",
      "question": "After the host says 'Thank you, sir', when does he describe how the session shifted to English?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3905.664,
        "end": 3912.762
      },
      "pred_interval": {
        "start": 3875.0,
        "end": 3890.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.664000000000215,
        "end": 22.76200000000017,
        "average": 26.713000000000193
      },
      "rationale_metrics": {
        "rouge_l": 0.3571428571428571,
        "text_similarity": 0.7809655666351318,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the temporal relation ('after') correct but the timestamps are substantially different from the reference (both events' start times are incorrect by several seconds and E2's timing is ~30s early) and it omits end times, so it is largely factually incorrect/incomplete."
      }
    },
    {
      "question_id": "003",
      "question": "After the host announces that there are questions received on the chat, when does he ask the first question about supplementary grounds?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3750.0,
        "end": 3960.0
      },
      "gt_interval": {
        "start": 3949.503,
        "end": 3955.411
      },
      "pred_interval": {
        "start": 3930.0,
        "end": 3940.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.503000000000156,
        "end": 15.411000000000058,
        "average": 17.457000000000107
      },
      "rationale_metrics": {
        "rouge_l": 0.2647058823529412,
        "text_similarity": 0.7866700887680054,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the announcement and the subsequent question events but gives substantially incorrect timestamps and a different relation label ('after' vs. the direct 'next'), so it fails to match the reference precisely."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker says they can combine the two questions, when does he state that all questions of fact can be raised?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 3972.976,
        "end": 3975.039
      },
      "pred_interval": {
        "start": 3950.0,
        "end": 3960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 22.976000000000113,
        "end": 15.039000000000215,
        "average": 19.007500000000164
      },
      "rationale_metrics": {
        "rouge_l": 0.2439024390243902,
        "text_similarity": 0.7300094366073608,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the temporal relation right (target occurs after anchor) but the reported timestamps are substantially incorrect compared to the reference and it adds an unsupported visual/audio cue, so the answer is largely inaccurate."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes stating that supplementary grounds are not needed in a normal case, when does he explain that all questions of fact and law are open in a normal case?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4030.938,
        "end": 4036.242
      },
      "pred_interval": {
        "start": 4000.0,
        "end": 4010.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.938000000000102,
        "end": 26.24200000000019,
        "average": 28.590000000000146
      },
      "rationale_metrics": {
        "rouge_l": 0.20930232558139533,
        "text_similarity": 0.6867276430130005,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the temporal relation (E2 occurs after E1) but the provided timestamps are substantially incorrect compared with the reference, and it introduces an unverified visual/audio cue (a pause). These factual timing errors reduce accuracy significantly."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker finishes explaining how a long note was required in the Sajan Kumar case, when does he describe the judge quoting his written note 80 times?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 3930.0,
        "end": 4140.0
      },
      "gt_interval": {
        "start": 4130.358,
        "end": 4139.64
      },
      "pred_interval": {
        "start": 4100.0,
        "end": 4110.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 30.358000000000175,
        "end": 29.640000000000327,
        "average": 29.99900000000025
      },
      "rationale_metrics": {
        "rouge_l": 0.2626262626262626,
        "text_similarity": 0.7753303050994873,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('after') right but misstates the anchor/target timestamps by large margins relative to the ground truth and adds an unsupported audio cue; thus it is largely factually incorrect despite the correct ordering."
      }
    },
    {
      "question_id": "001",
      "question": "After the host finishes asking about making an opening statement to impress the court, when does the main speaker begin explaining that a prepared lawyer can present their case in two minutes?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4186.419,
        "end": 4209.102
      },
      "pred_interval": {
        "start": 4120.5,
        "end": 4130.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 65.91899999999987,
        "end": 79.10199999999986,
        "average": 72.51049999999987
      },
      "rationale_metrics": {
        "rouge_l": 0.23880597014925373,
        "text_similarity": 0.730675458908081,
        "llm_judge_score": 2,
        "llm_judge_justification": "Although the predicted relation ('after') matches, the predicted timestamps are far from the ground truth (predicted ~4115\u20134130s vs correct ~4187\u20134209s), so it fails to identify the correct segments and does not capture the referenced explanation."
      }
    },
    {
      "question_id": "002",
      "question": "During the main speaker's narration of the Churchill story, when does he say that Churchill responded he needed '15 days before' to prepare for a two-minute speech?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4275.956,
        "end": 4281.618
      },
      "pred_interval": {
        "start": 4200.0,
        "end": 4205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 75.95600000000013,
        "end": 76.6180000000004,
        "average": 76.28700000000026
      },
      "rationale_metrics": {
        "rouge_l": 0.30303030303030304,
        "text_similarity": 0.830507755279541,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer correctly identifies the 'during' relationship but provides substantially incorrect timestamps (and omits E1's end time), so the temporal alignment and factual details do not match the ground truth."
      }
    },
    {
      "question_id": "003",
      "question": "After the main speaker finishes explaining that the less one has to speak, the more deeply one has to prepare, when does he begin defining a summary as a test of a lawyer's ability?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4110.0,
        "end": 4320.0
      },
      "gt_interval": {
        "start": 4306.932,
        "end": 4319.831
      },
      "pred_interval": {
        "start": 4300.0,
        "end": 4310.0
      },
      "iou": 0.15470727648631893,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.931999999999789,
        "end": 9.831000000000131,
        "average": 8.38149999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.26666666666666666,
        "text_similarity": 0.695805013179779,
        "llm_judge_score": 6,
        "llm_judge_justification": "The prediction gets the temporal relation right and provides approximate timings, but the timestamps deviate from the ground truth by about 6\u201310 seconds (E1 off by ~10s, E2 start off by ~7s), so it is imprecise."
      }
    },
    {
      "question_id": "001",
      "question": "After the host asks about the most challenging situations, when does the guest start explaining the amount of preparation needed for short vs long speeches?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4289.354,
        "end": 4303.141
      },
      "pred_interval": {
        "start": 4350.0,
        "end": 4365.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 60.64599999999973,
        "end": 61.85900000000038,
        "average": 61.252500000000055
      },
      "rationale_metrics": {
        "rouge_l": 0.28125,
        "text_similarity": 0.8213424682617188,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the two events and that the guest's explanation occurs after the host's question, but the provided timestamps are substantially off from the ground truth (roughly 55\u201365 seconds later) and the event durations do not match, so the temporal details are factually incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "Once the host finishes rephrasing the question about the impact of oral advocacy versus drafting, when does the guest immediately state that oral advocacy is more impactful?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4347.622,
        "end": 4350.188
      },
      "pred_interval": {
        "start": 4405.0,
        "end": 4415.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 57.3779999999997,
        "end": 64.8119999999999,
        "average": 61.0949999999998
      },
      "rationale_metrics": {
        "rouge_l": 0.3287671232876712,
        "text_similarity": 0.83466637134552,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction preserves the 'immediately after' relationship but the timestamps are substantially different from the ground truth (offset by ~54s and a longer target duration), so it is largely incorrect on the key temporal details."
      }
    },
    {
      "question_id": "003",
      "question": "After the guest confirms that the framing of questions of law is 'all important', when does he state that 'manifest injustice' is the category for most cases?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4290.0,
        "end": 4500.0
      },
      "gt_interval": {
        "start": 4407.975,
        "end": 4412.261
      },
      "pred_interval": {
        "start": 4470.0,
        "end": 4480.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 62.024999999999636,
        "end": 67.73899999999958,
        "average": 64.8819999999996
      },
      "rationale_metrics": {
        "rouge_l": 0.2777777777777778,
        "text_similarity": 0.8607710003852844,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship between events, but the provided timestamps are significantly off from the ground truth (roughly 60\u201370 seconds later) and include an extra end time not matching the reference, so the timing details are inaccurate."
      }
    },
    {
      "question_id": "001",
      "question": "Once the interviewer finishes asking how the speaker's different fields and readings affect their argument style, when does the speaker respond by saying 'Sir, you're absolutely right'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4537.667,
        "end": 4541.776
      },
      "pred_interval": {
        "start": 4485.0,
        "end": 4490.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.66700000000037,
        "end": 51.77599999999984,
        "average": 52.221500000000106
      },
      "rationale_metrics": {
        "rouge_l": 0.24657534246575347,
        "text_similarity": 0.8271633982658386,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps differ substantially from the reference (around 60s earlier) and the temporal relation is incorrect \u2014 the reference shows the target spanning the anchor's end (immediate occurrence), while the prediction places the response several seconds after; thus it fails to match key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker states that 'Advocacy is a communicative skill', when does he elaborate that 'One part of a communicative skill is language'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4562.343,
        "end": 4567.786
      },
      "pred_interval": {
        "start": 4550.0,
        "end": 4555.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 12.342999999999847,
        "end": 12.786000000000058,
        "average": 12.564499999999953
      },
      "rationale_metrics": {
        "rouge_l": 0.28571428571428575,
        "text_similarity": 0.760790228843689,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the two utterances but gives substantially incorrect timestamps and the wrong temporal relation ('after' vs. the ground-truth overlapping/direct elaboration). These timing and relation errors make the answer largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the interviewer mentions watching Justice Muralidhar's YouTube, when does the interviewer give an example from him about being asked 'did you murder?'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4470.0,
        "end": 4680.0
      },
      "gt_interval": {
        "start": 4617.635,
        "end": 4624.683
      },
      "pred_interval": {
        "start": 4650.0,
        "end": 4660.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 32.36499999999978,
        "end": 35.31700000000001,
        "average": 33.840999999999894
      },
      "rationale_metrics": {
        "rouge_l": 0.4931506849315069,
        "text_similarity": 0.7482073903083801,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction gets the relative relation ('after') right but the timestamps are substantially incorrect (both E1 and E2 are shifted ~25\u201335s later and E2 does not directly follow E1 as in the reference), so it fails on factual timing accuracy."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker defines the appeal as an 'extension of the trial', when does he explain that notes can 'awaken you to the subtleties of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4673.51,
        "end": 4680.899
      },
      "pred_interval": {
        "start": 4680.0,
        "end": 4685.0
      },
      "iou": 0.07824194952135413,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.489999999999782,
        "end": 4.100999999999658,
        "average": 5.29549999999972
      },
      "rationale_metrics": {
        "rouge_l": 0.19444444444444445,
        "text_similarity": 0.7284385561943054,
        "llm_judge_score": 5,
        "llm_judge_justification": "The prediction correctly identifies the relative order (target after anchor) but the timestamps are inconsistent with the ground truth: E1 end time is omitted and its start is shifted, while E2 is shifted later and extended beyond the correct interval, so it is only a partial match."
      }
    },
    {
      "question_id": "002",
      "question": "Once the speaker finishes advising how to objectively approach an appeal, when does he state that 'You have to absorb all the witnesses of the surrounding environment'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4746.348,
        "end": 4752.661
      },
      "pred_interval": {
        "start": 4720.0,
        "end": 4730.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.347999999999956,
        "end": 22.661000000000058,
        "average": 24.504500000000007
      },
      "rationale_metrics": {
        "rouge_l": 0.2162162162162162,
        "text_similarity": 0.7887207269668579,
        "llm_judge_score": 1,
        "llm_judge_justification": "The predicted timestamps are significantly different from the ground truth and misplace both anchor and target segments; the described relationship ('once') does not reflect the correct immediate-following/continuation relation, so the prediction is largely incorrect."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'then I become a lawyer', when does he describe the 'story of the lawyer of the appeal'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4650.0,
        "end": 4860.0
      },
      "gt_interval": {
        "start": 4805.697,
        "end": 4824.178
      },
      "pred_interval": {
        "start": 4800.0,
        "end": 4810.0
      },
      "iou": 0.1779717098188396,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 5.697000000000116,
        "end": 14.177999999999884,
        "average": 9.9375
      },
      "rationale_metrics": {
        "rouge_l": 0.17910447761194026,
        "text_similarity": 0.7037492394447327,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly captures the 'after' relationship and general temporal region, but the timestamps are inaccurate and incomplete (E1 end missing; E2 starts earlier and ends much earlier than the reference), so key factual timing details are incorrect."
      }
    },
    {
      "question_id": "001",
      "question": "After the speaker describes the 'art of communication' as the 'foundation of law', when does he state that its effectiveness depends on the 'quality of your preparation'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4848.488,
        "end": 4862.369
      },
      "pred_interval": {
        "start": 4875.0,
        "end": 4885.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 26.511999999999716,
        "end": 22.631000000000313,
        "average": 24.571500000000015
      },
      "rationale_metrics": {
        "rouge_l": 0.4444444444444444,
        "text_similarity": 0.8855800032615662,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction preserves the correct semantic relation ('after') and the quoted phrases, but the timestamps are significantly misaligned (anchor placed ~22s late and target ~26s late), even placing the predicted anchor inside the correct target window, so the temporal grounding is incorrect."
      }
    },
    {
      "question_id": "002",
      "question": "After the initial speaker says 'Thank you, sir', when does the second speaker begin apologizing for not taking all questions?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4960.598,
        "end": 4970.611
      },
      "pred_interval": {
        "start": 4950.0,
        "end": 4960.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.597999999999956,
        "end": 10.610999999999876,
        "average": 10.604499999999916
      },
      "rationale_metrics": {
        "rouge_l": 0.3076923076923077,
        "text_similarity": 0.8720223307609558,
        "llm_judge_score": 3,
        "llm_judge_justification": "While the predicted relation ('after') matches the reference, the predicted time spans are substantially inaccurate (E1 start is ~19s early and lacks an end; E2 is ~10s early and ends ~10s before the true end), so key temporal details are incorrect or omitted."
      }
    },
    {
      "question_id": "003",
      "question": "After the host refers to 'Q and Q, quality and quantity insurance', when does he mention the three important judgments: Prabhu, Rishikesh, and Vijay?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 4830.0,
        "end": 5040.0
      },
      "gt_interval": {
        "start": 4996.141,
        "end": 5009.076
      },
      "pred_interval": {
        "start": 5005.0,
        "end": 5020.0
      },
      "iou": 0.1708370007125176,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 8.859000000000378,
        "end": 10.923999999999978,
        "average": 9.891500000000178
      },
      "rationale_metrics": {
        "rouge_l": 0.5454545454545454,
        "text_similarity": 0.9161685705184937,
        "llm_judge_score": 3,
        "llm_judge_justification": "The prediction gets the relation ('after') correct but the timestamps are substantially off and mislabeled (anchor timing is given as a start instead of the correct end at 4994.478s, and the target is shifted ~9s\u201311s later than the reference), so it does not accurately match the correct timings."
      }
    },
    {
      "question_id": "001",
      "question": "Once the speaker finishes quoting 'abhi na jao chhod ke, dil abhi bhara nahi', when does he state 'That's what the common sense is'?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5019.2,
        "end": 5022.6
      },
      "pred_interval": {
        "start": 5025.4,
        "end": 5027.1
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 6.199999999999818,
        "end": 4.5,
        "average": 5.349999999999909
      },
      "rationale_metrics": {
        "rouge_l": 0.5238095238095238,
        "text_similarity": 0.7643880844116211,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets E1 roughly correct (5020.0s is within the reference E1 window) but mislocates E2 by several seconds (starts at 5025.4s vs 5019.2s) and therefore assigns the wrong temporal relation ('after' vs 'once_finished'), so it fails to match the key timing and relation details."
      }
    },
    {
      "question_id": "002",
      "question": "After the speaker expresses surprise about people increasing viewership on YouTube, when does he mention everybody was viewed in the Zoom chat?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5030.3,
        "end": 5032.8
      },
      "pred_interval": {
        "start": 5040.5,
        "end": 5042.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 10.199999999999818,
        "end": 9.199999999999818,
        "average": 9.699999999999818
      },
      "rationale_metrics": {
        "rouge_l": 0.3142857142857143,
        "text_similarity": 0.8195455074310303,
        "llm_judge_score": 7,
        "llm_judge_justification": "The prediction correctly identifies both events and the 'after' relation, but the provided timestamps are noticeably shifted later (~12\u201315 seconds) from the ground truth, so the timing details are inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "After the speaker says 'Thank you, sir' for the first time, when does he specifically thank Tanu Bedi?",
      "video_id": "RHq_CJJDpUY",
      "video_number": "013",
      "segment": {
        "start": 5010.0,
        "end": 5060.0
      },
      "gt_interval": {
        "start": 5046.2,
        "end": 5049.1
      },
      "pred_interval": {
        "start": 5050.2,
        "end": 5051.7
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 2.5999999999994543,
        "average": 3.299999999999727
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.8259907960891724,
        "llm_judge_score": 4,
        "llm_judge_justification": "The prediction correctly identifies the temporal relation ('after') but the timestamps for both E1 and E2 deviate substantially from the reference, misplacing the events and thus failing to match key factual details."
      }
    },
    {
      "question_id": "001",
      "question": "Once Alex finishes asking about the key challenges facing individuals who are called to give evidence, when does Paul begin to talk about nervousness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 33.237,
        "end": 36.762
      },
      "pred_interval": {
        "start": 15.0,
        "end": 30.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 18.237000000000002,
        "end": 6.7620000000000005,
        "average": 12.499500000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3125,
        "text_similarity": 0.6619757413864136,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the qualitative relation ('immediately after') right but the timestamps are substantially incorrect (predicted ~14.5\u201330.0s vs. correct ~33.216\u201336.762s), so it fails on key factual timing details."
      }
    },
    {
      "question_id": "002",
      "question": "Once Alex finishes asking what makes the actual process of giving evidence so difficult, when does Paul begin to describe the cross-examination process?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 84.92,
        "end": 92.191
      },
      "pred_interval": {
        "start": 60.0,
        "end": 75.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 24.92,
        "end": 17.191000000000003,
        "average": 21.055500000000002
      },
      "rationale_metrics": {
        "rouge_l": 0.3188405797101449,
        "text_similarity": 0.7079848051071167,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies the target as occurring immediately after the anchor, but the timestamps are substantially incorrect (off by ~24s for the anchor and ~25s for the target) and the event durations differ, so it is largely inaccurate."
      }
    },
    {
      "question_id": "003",
      "question": "Once Paul finishes describing the anecdote about the expert witness losing credibility, when does he start describing the other example of a fact witness?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 0.0,
        "end": 210.0
      },
      "gt_interval": {
        "start": 172.683,
        "end": 180.236
      },
      "pred_interval": {
        "start": 120.0,
        "end": 135.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.68299999999999,
        "end": 45.23599999999999,
        "average": 48.95949999999999
      },
      "rationale_metrics": {
        "rouge_l": 0.3888888888888889,
        "text_similarity": 0.8657554984092712,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer's timestamps substantially contradict the reference (119.5s/120.0\u2013135.0s vs. 171.923s/172.683\u2013180.236s), so it is largely incorrect, though both agree the target follows immediately after the anchor."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker describes the sarcastic comments made about expert witness answers, when does he recount the witness's emotional reaction?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 157.3,
        "end": 164.2
      },
      "pred_interval": {
        "start": 200.5,
        "end": 205.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 43.19999999999999,
        "end": 40.80000000000001,
        "average": 42.0
      },
      "rationale_metrics": {
        "rouge_l": 0.5185185185185185,
        "text_similarity": 0.9225702285766602,
        "llm_judge_score": 2,
        "llm_judge_justification": "The predicted answer gets the temporal relation ('after') right but the anchor and target time spans are substantially incorrect and do not match the correct event boundaries, so it fails on key factual details."
      }
    },
    {
      "question_id": "002",
      "question": "Once the second speaker asks what witness familiarisation actually means, when does the first speaker begin to define the term?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 197.7,
        "end": 204.0
      },
      "pred_interval": {
        "start": 250.0,
        "end": 255.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 52.30000000000001,
        "end": 51.0,
        "average": 51.650000000000006
      },
      "rationale_metrics": {
        "rouge_l": 0.3384615384615384,
        "text_similarity": 0.7854088544845581,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction correctly identifies that the definition follows the question, but the timestamps and durations differ substantially from the reference (big time offsets) and the relation label is less precise ('after' vs 'once_finished'), so it does not match the ground truth factually."
      }
    },
    {
      "question_id": "003",
      "question": "After the first speaker finishes describing the benefits of witness familiarisation for individuals, when does he next outline the benefits for instructing solicitors?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 150.0,
        "end": 360.0
      },
      "gt_interval": {
        "start": 300.4,
        "end": 308.0
      },
      "pred_interval": {
        "start": 320.0,
        "end": 330.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 19.600000000000023,
        "end": 22.0,
        "average": 20.80000000000001
      },
      "rationale_metrics": {
        "rouge_l": 0.3661971830985916,
        "text_similarity": 0.843399167060852,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction captures that the second segment follows the first, but the timestamps are substantially incorrect (off by ~15\u201320s) and the relation label ('after') does not match the specific 'next' relation and immediacy in the reference; key factual timing details are wrong."
      }
    },
    {
      "question_id": "001",
      "question": "After the first speaker explains why they don't jump straight into cross-examination, when does he start talking about the need for theory?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 356.6,
        "end": 365.0
      },
      "pred_interval": {
        "start": 340.5,
        "end": 355.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 16.100000000000023,
        "end": 10.0,
        "average": 13.050000000000011
      },
      "rationale_metrics": {
        "rouge_l": 0.29411764705882354,
        "text_similarity": 0.7796673774719238,
        "llm_judge_score": 2,
        "llm_judge_justification": "While the predicted answer gets the qualitative relation ('after') right, both event timestamps are substantially different from the ground truth (E1 and E2 times are off by many seconds), so the prediction is factually inaccurate. This omission of correct temporal details warrants a very low score."
      }
    },
    {
      "question_id": "002",
      "question": "During the period when the first speaker is discussing the differences in virtual hearings, when does he mention judges requiring witnesses to demonstrate they are alone?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 394.0,
        "end": 398.0
      },
      "pred_interval": {
        "start": 390.0,
        "end": 400.0
      },
      "iou": 0.4,
      "recall_at_threshold": {
        "R@0.3": 1,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 4.0,
        "end": 2.0,
        "average": 3.0
      },
      "rationale_metrics": {
        "rouge_l": 0.393939393939394,
        "text_similarity": 0.8251672983169556,
        "llm_judge_score": 8,
        "llm_judge_justification": "The prediction correctly identifies the 'during' relation and its E2 interval substantially overlaps the reference (390\u2013400s vs 394\u2013398s). However, the E1 timing is imprecise (starts earlier at 370.0s and omits an end time) compared to the reference window 375.5\u2013416.8s."
      }
    },
    {
      "question_id": "003",
      "question": "Once the second speaker finishes thanking Paul, when does he mention sharing some of his insights?",
      "video_id": "L_dJ23CLzTo",
      "video_number": "014",
      "segment": {
        "start": 330.0,
        "end": 458.0
      },
      "gt_interval": {
        "start": 435.4,
        "end": 437.5
      },
      "pred_interval": {
        "start": 420.0,
        "end": 430.0
      },
      "iou": 0.0,
      "recall_at_threshold": {
        "R@0.3": 0,
        "R@0.5": 0,
        "R@0.7": 0
      },
      "mae": {
        "start": 15.399999999999977,
        "end": 7.5,
        "average": 11.449999999999989
      },
      "rationale_metrics": {
        "rouge_l": 0.4406779661016949,
        "text_similarity": 0.8306517601013184,
        "llm_judge_score": 2,
        "llm_judge_justification": "The prediction fails on key facts: both event timestamps (E1: 415.0 vs 420.0; E2: 420.0\u2013430.0 vs 435.4\u2013437.5) and the relation label ('after' vs 'once_finished') are incorrect; it only preserves the vague ordering."
      }
    }
  ]
}