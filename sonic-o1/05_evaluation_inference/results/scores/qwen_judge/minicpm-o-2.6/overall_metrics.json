{
  "model": "minicpm-o-2.6",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.15791989379902355,
            "rouge_l_std": 0.02783449281433809,
            "text_similarity_mean": 0.6950965002179146,
            "text_similarity_std": 0.06941687450134958,
            "llm_judge_score_mean": 4.625,
            "llm_judge_score_std": 1.8328597873268975
          },
          "short": {
            "rouge_l_mean": 0.10185036281569146,
            "rouge_l_std": 0.04036852297885348,
            "text_similarity_mean": 0.5717672556638718,
            "text_similarity_std": 0.13978580123165915,
            "llm_judge_score_mean": 3.8125,
            "llm_judge_score_std": 1.2358574958303243
          },
          "cider": {
            "cider_detailed": 0.029803928942011153,
            "cider_short": 9.062776208724314e-07
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.1549943723190393,
            "rouge_l_std": 0.03324691436681503,
            "text_similarity_mean": 0.5869202670596895,
            "text_similarity_std": 0.14402794967669835,
            "llm_judge_score_mean": 4.428571428571429,
            "llm_judge_score_std": 1.498298354528788
          },
          "short": {
            "rouge_l_mean": 0.10229786480470397,
            "rouge_l_std": 0.05387440725534158,
            "text_similarity_mean": 0.5056423005603609,
            "text_similarity_std": 0.15286035350399393,
            "llm_judge_score_mean": 3.4761904761904763,
            "llm_judge_score_std": 1.2196427118919713
          },
          "cider": {
            "cider_detailed": 0.011264094503831158,
            "cider_short": 0.00042254553762291627
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.13835375529728072,
            "rouge_l_std": 0.02554163460765692,
            "text_similarity_mean": 0.601847805082798,
            "text_similarity_std": 0.17075743569889865,
            "llm_judge_score_mean": 4.444444444444445,
            "llm_judge_score_std": 1.6405358955814888
          },
          "short": {
            "rouge_l_mean": 0.10018371400146345,
            "rouge_l_std": 0.05311590060554425,
            "text_similarity_mean": 0.5315046509106954,
            "text_similarity_std": 0.1988854765467706,
            "llm_judge_score_mean": 3.4444444444444446,
            "llm_judge_score_std": 1.2120791238484128
          },
          "cider": {
            "cider_detailed": 0.0037781294854639536,
            "cider_short": 2.1358718782126128e-11
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.14660879809656127,
            "rouge_l_std": 0.02446921990129999,
            "text_similarity_mean": 0.5908027152220409,
            "text_similarity_std": 0.10169345911239219,
            "llm_judge_score_mean": 4.2,
            "llm_judge_score_std": 1.3759844960366863
          },
          "short": {
            "rouge_l_mean": 0.08384693389700883,
            "rouge_l_std": 0.03348199698598676,
            "text_similarity_mean": 0.4694525440533956,
            "text_similarity_std": 0.14275780337477761,
            "llm_judge_score_mean": 2.6,
            "llm_judge_score_std": 0.7118052168020874
          },
          "cider": {
            "cider_detailed": 6.458853205345195e-08,
            "cider_short": 1.730930546237192e-06
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.14245901788663878,
            "rouge_l_std": 0.04681752718690547,
            "text_similarity_mean": 0.48581524995657116,
            "text_similarity_std": 0.20214316745229144,
            "llm_judge_score_mean": 3.923076923076923,
            "llm_judge_score_std": 2.0176733920929233
          },
          "short": {
            "rouge_l_mean": 0.07825050779295131,
            "rouge_l_std": 0.06041955278833854,
            "text_similarity_mean": 0.452728013579662,
            "text_similarity_std": 0.1811997802896769,
            "llm_judge_score_mean": 2.8461538461538463,
            "llm_judge_score_std": 1.0986812966989
          },
          "cider": {
            "cider_detailed": 0.0002678973439734874,
            "cider_short": 7.858356457201233e-07
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.15970611760314207,
            "rouge_l_std": 0.039306666274795066,
            "text_similarity_mean": 0.5785381674766541,
            "text_similarity_std": 0.15732447600478286,
            "llm_judge_score_mean": 4.5,
            "llm_judge_score_std": 2.0371548787463363
          },
          "short": {
            "rouge_l_mean": 0.10511072384781953,
            "rouge_l_std": 0.04607016299073092,
            "text_similarity_mean": 0.4981545329093933,
            "text_similarity_std": 0.13514985392046683,
            "llm_judge_score_mean": 3.45,
            "llm_judge_score_std": 1.16081867662439
          },
          "cider": {
            "cider_detailed": 0.02629862731160364,
            "cider_short": 0.0018287703426092667
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.14239363983121875,
            "rouge_l_std": 0.022572253324974857,
            "text_similarity_mean": 0.5229383472885404,
            "text_similarity_std": 0.1136727153298864,
            "llm_judge_score_mean": 3.2142857142857144,
            "llm_judge_score_std": 1.3190132366156704
          },
          "short": {
            "rouge_l_mean": 0.1060891025068583,
            "rouge_l_std": 0.08054667806783257,
            "text_similarity_mean": 0.4302914972816195,
            "text_similarity_std": 0.14763039549209356,
            "llm_judge_score_mean": 2.5714285714285716,
            "llm_judge_score_std": 1.3477115902938006
          },
          "cider": {
            "cider_detailed": 0.003256004929427736,
            "cider_short": 1.9099983647360507e-10
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.13387894417544385,
            "rouge_l_std": 0.030478174709726944,
            "text_similarity_mean": 0.5082657085731626,
            "text_similarity_std": 0.22233077541518365,
            "llm_judge_score_mean": 3.9166666666666665,
            "llm_judge_score_std": 1.7539637649874324
          },
          "short": {
            "rouge_l_mean": 0.08685722465094527,
            "rouge_l_std": 0.048078628925964505,
            "text_similarity_mean": 0.4349144430210193,
            "text_similarity_std": 0.2051905809219237,
            "llm_judge_score_mean": 2.5,
            "llm_judge_score_std": 1.118033988749895
          },
          "cider": {
            "cider_detailed": 0.014136992899731714,
            "cider_short": 1.464802480303815e-16
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.14917350622309739,
            "rouge_l_std": 0.02792367297953055,
            "text_similarity_mean": 0.49712598447998363,
            "text_similarity_std": 0.1280016933075071,
            "llm_judge_score_mean": 4.541666666666667,
            "llm_judge_score_std": 1.4135996211406145
          },
          "short": {
            "rouge_l_mean": 0.08603708557938777,
            "rouge_l_std": 0.03854069621494866,
            "text_similarity_mean": 0.42378266838689643,
            "text_similarity_std": 0.10446566309180216,
            "llm_judge_score_mean": 3.375,
            "llm_judge_score_std": 1.1110243021644486
          },
          "cider": {
            "cider_detailed": 0.004462975337730058,
            "cider_short": 0.00206350478043158
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.1529563081435949,
            "rouge_l_std": 0.0219173461246786,
            "text_similarity_mean": 0.5957852537217347,
            "text_similarity_std": 0.16173437913207828,
            "llm_judge_score_mean": 4.565217391304348,
            "llm_judge_score_std": 1.3776503928482486
          },
          "short": {
            "rouge_l_mean": 0.10123110298060381,
            "rouge_l_std": 0.03760244945673548,
            "text_similarity_mean": 0.4848004417574924,
            "text_similarity_std": 0.18310798133173303,
            "llm_judge_score_mean": 3.391304347826087,
            "llm_judge_score_std": 1.0103434816193262
          },
          "cider": {
            "cider_detailed": 0.01306508676711751,
            "cider_short": 0.0002474674292481541
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.15629854881736388,
            "rouge_l_std": 0.038096567378384565,
            "text_similarity_mean": 0.6306699904111716,
            "text_similarity_std": 0.11735839102366534,
            "llm_judge_score_mean": 4.461538461538462,
            "llm_judge_score_std": 1.3367805536140587
          },
          "short": {
            "rouge_l_mean": 0.11846576791318457,
            "rouge_l_std": 0.04280399018490756,
            "text_similarity_mean": 0.6219335473500766,
            "text_similarity_std": 0.09803272455242235,
            "llm_judge_score_mean": 3.8461538461538463,
            "llm_judge_score_std": 1.2917581249035894
          },
          "cider": {
            "cider_detailed": 2.543817367804251e-06,
            "cider_short": 0.016515715880062655
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.11703033161148202,
            "rouge_l_std": 0.021318274095431702,
            "text_similarity_mean": 0.3902517110109329,
            "text_similarity_std": 0.14788265339590032,
            "llm_judge_score_mean": 3.111111111111111,
            "llm_judge_score_std": 1.1493422703098446
          },
          "short": {
            "rouge_l_mean": 0.06471491586930551,
            "rouge_l_std": 0.04248829431574872,
            "text_similarity_mean": 0.34109233174886966,
            "text_similarity_std": 0.1705667845862657,
            "llm_judge_score_mean": 2.388888888888889,
            "llm_judge_score_std": 0.48749802152178456
          },
          "cider": {
            "cider_detailed": 0.003523326101384092,
            "cider_short": 3.961172493444511e-11
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.16522450966637697,
            "rouge_l_std": 0.03419405286773445,
            "text_similarity_mean": 0.6307390591372615,
            "text_similarity_std": 0.1226480346274087,
            "llm_judge_score_mean": 4.869565217391305,
            "llm_judge_score_std": 1.2616189676997311
          },
          "short": {
            "rouge_l_mean": 0.10654207694403721,
            "rouge_l_std": 0.04519422966725866,
            "text_similarity_mean": 0.5131650515224623,
            "text_similarity_std": 0.1531535701292537,
            "llm_judge_score_mean": 3.391304347826087,
            "llm_judge_score_std": 1.010343481619326
          },
          "cider": {
            "cider_detailed": 0.05674520092648377,
            "cider_short": 0.009568560215617344
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.14746136488232794,
          "text_similarity_mean": 0.5626766738183427,
          "llm_judge_score_mean": 4.2154726173120824
        },
        "short": {
          "rouge_l_mean": 0.09549826027722777,
          "text_similarity_mean": 0.48301763682660115,
          "llm_judge_score_mean": 3.1610283668394037
        },
        "cider": {
          "cider_detailed_mean": 0.012815759458050625,
          "cider_short_mean": 0.0023576913447211673
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9215686274509803,
          "correct": 94,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.2702236888059469,
            "rouge_l_std": 0.09402658995872817,
            "text_similarity_mean": 0.7420175335570878,
            "text_similarity_std": 0.10482143925326806,
            "llm_judge_score_mean": 8.264705882352942,
            "llm_judge_score_std": 1.2440168225033295
          },
          "rationale_cider": 0.16609688056183636
        },
        "02_Job_Interviews": {
          "accuracy": 0.95,
          "correct": 95,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2580216803176492,
            "rouge_l_std": 0.06995559903765076,
            "text_similarity_mean": 0.7248580968379974,
            "text_similarity_std": 0.09637647149682914,
            "llm_judge_score_mean": 8.43,
            "llm_judge_score_std": 1.0320368210485515
          },
          "rationale_cider": 0.18434510455174494
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.9260869565217391,
          "correct": 213,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.2529846254537641,
            "rouge_l_std": 0.07372769561630109,
            "text_similarity_mean": 0.7303403461756913,
            "text_similarity_std": 0.10658679927247929,
            "llm_judge_score_mean": 8.152173913043478,
            "llm_judge_score_std": 1.2223888107050316
          },
          "rationale_cider": 0.14464254670935234
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.8461538461538461,
          "correct": 33,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.2467319569191433,
            "rouge_l_std": 0.08065091457964564,
            "text_similarity_mean": 0.7324834825136722,
            "text_similarity_std": 0.09892968985686369,
            "llm_judge_score_mean": 7.538461538461538,
            "llm_judge_score_std": 1.4995068222783021
          },
          "rationale_cider": 0.14067630183783786
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.8695652173913043,
          "correct": 100,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2571555635634705,
            "rouge_l_std": 0.07778907899812351,
            "text_similarity_mean": 0.7316236217870661,
            "text_similarity_std": 0.12398469752819963,
            "llm_judge_score_mean": 8.034782608695652,
            "llm_judge_score_std": 1.5089900727630858
          },
          "rationale_cider": 0.09297703241999312
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.8505747126436781,
          "correct": 74,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.23700345914301105,
            "rouge_l_std": 0.07918765502556697,
            "text_similarity_mean": 0.7173822181320738,
            "text_similarity_std": 0.12940185066269563,
            "llm_judge_score_mean": 7.2988505747126435,
            "llm_judge_score_std": 1.807487253917544
          },
          "rationale_cider": 0.08461818159607958
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.8235294117647058,
          "correct": 42,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.22623678455117516,
            "rouge_l_std": 0.05350260605226065,
            "text_similarity_mean": 0.7124962514522029,
            "text_similarity_std": 0.12390644457077324,
            "llm_judge_score_mean": 7.117647058823529,
            "llm_judge_score_std": 1.7673183277245097
          },
          "rationale_cider": 0.14832438125394584
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 0.9552238805970149,
          "correct": 64,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.23552484750566724,
            "rouge_l_std": 0.0597364297163882,
            "text_similarity_mean": 0.7568402463820443,
            "text_similarity_std": 0.11427472066440154,
            "llm_judge_score_mean": 8.08955223880597,
            "llm_judge_score_std": 1.2060957522887714
          },
          "rationale_cider": 0.11031581491264011
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.8062015503875969,
          "correct": 104,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.24273759571834475,
            "rouge_l_std": 0.0714259779453791,
            "text_similarity_mean": 0.7013135284416435,
            "text_similarity_std": 0.13461595170779772,
            "llm_judge_score_mean": 7.558139534883721,
            "llm_judge_score_std": 1.7604922529730367
          },
          "rationale_cider": 0.09348317477100772
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.8235294117647058,
          "correct": 70,
          "total": 85,
          "rationale": {
            "rouge_l_mean": 0.24810578037459208,
            "rouge_l_std": 0.07293413397689982,
            "text_similarity_mean": 0.743838882446289,
            "text_similarity_std": 0.11870944186313181,
            "llm_judge_score_mean": 7.811764705882353,
            "llm_judge_score_std": 1.575504532799564
          },
          "rationale_cider": 0.18106493451975214
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 0.8923076923076924,
          "correct": 58,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.23208682620963894,
            "rouge_l_std": 0.06470240158302064,
            "text_similarity_mean": 0.7443461303527539,
            "text_similarity_std": 0.0991891271360038,
            "llm_judge_score_mean": 8.061538461538461,
            "llm_judge_score_std": 1.263413241086019
          },
          "rationale_cider": 0.08576200446960208
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.9206349206349206,
          "correct": 174,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.2427443007104362,
            "rouge_l_std": 0.07564357871972868,
            "text_similarity_mean": 0.7094581677484765,
            "text_similarity_std": 0.10242621782337322,
            "llm_judge_score_mean": 7.91005291005291,
            "llm_judge_score_std": 1.4019466803537939
          },
          "rationale_cider": 0.09217741399734292
        },
        "13_Olympics": {
          "accuracy": 0.782608695652174,
          "correct": 54,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.22297174990263366,
            "rouge_l_std": 0.07493692451865161,
            "text_similarity_mean": 0.7318898897240127,
            "text_similarity_std": 0.10015861616948918,
            "llm_judge_score_mean": 7.202898550724638,
            "llm_judge_score_std": 1.7074407507273206
          },
          "rationale_cider": 0.11820325915752843
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8744603787131044,
        "rationale": {
          "rouge_l_mean": 0.2440406814750364,
          "text_similarity_mean": 0.7291452611962317,
          "llm_judge_score_mean": 7.805428305998295
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.014482665099741129,
          "std_iou": 0.06561070667351358,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.015267175572519083,
            "count": 4,
            "total": 262
          },
          "R@0.5": {
            "recall": 0.003816793893129771,
            "count": 1,
            "total": 262
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 262
          },
          "mae": {
            "start_mean": 658.3624847328246,
            "end_mean": 4226.351893129771,
            "average_mean": 2442.357188931298
          },
          "rationale": {
            "rouge_l_mean": 0.21592267264889473,
            "rouge_l_std": 0.09984178384327684,
            "text_similarity_mean": 0.429159315270976,
            "text_similarity_std": 0.18535207180346194,
            "llm_judge_score_mean": 5.3816793893129775,
            "llm_judge_score_std": 1.60846052834833
          },
          "rationale_cider": 0.24760826300201494
        },
        "02_Job_Interviews": {
          "mean_iou": 0.03270429233420023,
          "std_iou": 0.1169718734737444,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.052845528455284556,
            "count": 13,
            "total": 246
          },
          "R@0.5": {
            "recall": 0.016260162601626018,
            "count": 4,
            "total": 246
          },
          "R@0.7": {
            "recall": 0.0040650406504065045,
            "count": 1,
            "total": 246
          },
          "mae": {
            "start_mean": 375.5312520325203,
            "end_mean": 368.0999878048781,
            "average_mean": 371.8156199186992
          },
          "rationale": {
            "rouge_l_mean": 0.19791591300200187,
            "rouge_l_std": 0.0911502499817805,
            "text_similarity_mean": 0.3836054064303152,
            "text_similarity_std": 0.1751655011792324,
            "llm_judge_score_mean": 5.40650406504065,
            "llm_judge_score_std": 1.7124004849085925
          },
          "rationale_cider": 0.24711940037125188
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.02101621231123869,
          "std_iou": 0.09365195083048253,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.030828516377649325,
            "count": 16,
            "total": 519
          },
          "R@0.5": {
            "recall": 0.015414258188824663,
            "count": 8,
            "total": 519
          },
          "R@0.7": {
            "recall": 0.0019267822736030828,
            "count": 1,
            "total": 519
          },
          "mae": {
            "start_mean": 1042.4920211946048,
            "end_mean": 1040.3418901734103,
            "average_mean": 1041.4169556840077
          },
          "rationale": {
            "rouge_l_mean": 0.18424253949729022,
            "rouge_l_std": 0.09147001101708899,
            "text_similarity_mean": 0.38460938602341393,
            "text_similarity_std": 0.18160557040979336,
            "llm_judge_score_mean": 5.470134874759152,
            "llm_judge_score_std": 1.6973781723694221
          },
          "rationale_cider": 0.23368982975716915
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.02139003715609796,
          "std_iou": 0.10943055202629653,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.043478260869565216,
            "count": 5,
            "total": 115
          },
          "R@0.5": {
            "recall": 0.008695652173913044,
            "count": 1,
            "total": 115
          },
          "R@0.7": {
            "recall": 0.008695652173913044,
            "count": 1,
            "total": 115
          },
          "mae": {
            "start_mean": 123.35614782608694,
            "end_mean": 123.16967826086955,
            "average_mean": 123.26291304347826
          },
          "rationale": {
            "rouge_l_mean": 0.18758158640318182,
            "rouge_l_std": 0.0966120418958786,
            "text_similarity_mean": 0.4038185607156028,
            "text_similarity_std": 0.1592596956258718,
            "llm_judge_score_mean": 5.6173913043478265,
            "llm_judge_score_std": 1.3678457114598737
          },
          "rationale_cider": 0.30456482296296716
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.022559954737346826,
          "std_iou": 0.09930309494152856,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.026239067055393587,
            "count": 9,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.014577259475218658,
            "count": 5,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0058309037900874635,
            "count": 2,
            "total": 343
          },
          "mae": {
            "start_mean": 654.3455714285714,
            "end_mean": 655.5992099125364,
            "average_mean": 654.9723906705539
          },
          "rationale": {
            "rouge_l_mean": 0.19103853338987728,
            "rouge_l_std": 0.09828893583833948,
            "text_similarity_mean": 0.37901132975218943,
            "text_similarity_std": 0.20043368543184176,
            "llm_judge_score_mean": 5.311953352769679,
            "llm_judge_score_std": 1.6159021988323992
          },
          "rationale_cider": 0.21764524994225015
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.010637065084053106,
          "std_iou": 0.050960788268986874,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.00796812749003984,
            "count": 2,
            "total": 251
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 251
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 251
          },
          "mae": {
            "start_mean": 238.0745378486056,
            "end_mean": 272.88820318725107,
            "average_mean": 255.48137051792833
          },
          "rationale": {
            "rouge_l_mean": 0.20688072132670066,
            "rouge_l_std": 0.10096164110830645,
            "text_similarity_mean": 0.4355150994907812,
            "text_similarity_std": 0.16464813130063755,
            "llm_judge_score_mean": 5.374501992031872,
            "llm_judge_score_std": 1.4920730955640182
          },
          "rationale_cider": 0.34858011435224673
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.003248835436044256,
          "std_iou": 0.02279342644562709,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 101
          },
          "mae": {
            "start_mean": 269.7770693069307,
            "end_mean": 256.21581188118813,
            "average_mean": 262.99644059405944
          },
          "rationale": {
            "rouge_l_mean": 0.16721434109517763,
            "rouge_l_std": 0.08443324034445797,
            "text_similarity_mean": 0.41039216946257223,
            "text_similarity_std": 0.17042061545014478,
            "llm_judge_score_mean": 5.306930693069307,
            "llm_judge_score_std": 1.7951671879856517
          },
          "rationale_cider": 0.25145116809410545
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.015530412205444426,
          "std_iou": 0.06830434192614128,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.006622516556291391,
            "count": 1,
            "total": 151
          },
          "R@0.5": {
            "recall": 0.006622516556291391,
            "count": 1,
            "total": 151
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 151
          },
          "mae": {
            "start_mean": 301.70353642384106,
            "end_mean": 398.23956291390726,
            "average_mean": 349.9715496688742
          },
          "rationale": {
            "rouge_l_mean": 0.20637120756002944,
            "rouge_l_std": 0.10181989871568056,
            "text_similarity_mean": 0.41927941807157154,
            "text_similarity_std": 0.18894329781785435,
            "llm_judge_score_mean": 5.410596026490066,
            "llm_judge_score_std": 1.640834560627137
          },
          "rationale_cider": 0.2891809943615627
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.02195987237121976,
          "std_iou": 0.09222722937561278,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.028423772609819122,
            "count": 11,
            "total": 387
          },
          "R@0.5": {
            "recall": 0.012919896640826873,
            "count": 5,
            "total": 387
          },
          "R@0.7": {
            "recall": 0.002583979328165375,
            "count": 1,
            "total": 387
          },
          "mae": {
            "start_mean": 170.82260465116278,
            "end_mean": 171.84018863049096,
            "average_mean": 171.3313966408269
          },
          "rationale": {
            "rouge_l_mean": 0.2098887111144131,
            "rouge_l_std": 0.1000693008660326,
            "text_similarity_mean": 0.4159569602738825,
            "text_similarity_std": 0.1757787462658677,
            "llm_judge_score_mean": 5.516795865633075,
            "llm_judge_score_std": 1.5373395816594693
          },
          "rationale_cider": 0.31747473592305847
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.018502921580203762,
          "std_iou": 0.08526821294456159,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.02631578947368421,
            "count": 5,
            "total": 190
          },
          "R@0.5": {
            "recall": 0.005263157894736842,
            "count": 1,
            "total": 190
          },
          "R@0.7": {
            "recall": 0.005263157894736842,
            "count": 1,
            "total": 190
          },
          "mae": {
            "start_mean": 199.97283684210527,
            "end_mean": 231.63778421052635,
            "average_mean": 215.8053105263158
          },
          "rationale": {
            "rouge_l_mean": 0.17715213994727455,
            "rouge_l_std": 0.09483980121028213,
            "text_similarity_mean": 0.36601856495008656,
            "text_similarity_std": 0.1980212222240642,
            "llm_judge_score_mean": 5.426315789473684,
            "llm_judge_score_std": 1.7747771113618325
          },
          "rationale_cider": 0.2050814544576896
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.028428613740708996,
          "std_iou": 0.1129337342340637,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.041379310344827586,
            "count": 6,
            "total": 145
          },
          "R@0.5": {
            "recall": 0.006896551724137931,
            "count": 1,
            "total": 145
          },
          "R@0.7": {
            "recall": 0.006896551724137931,
            "count": 1,
            "total": 145
          },
          "mae": {
            "start_mean": 206.57364137931035,
            "end_mean": 208.70534482758623,
            "average_mean": 207.63949310344827
          },
          "rationale": {
            "rouge_l_mean": 0.1782191645542518,
            "rouge_l_std": 0.08675831569962948,
            "text_similarity_mean": 0.42324410276808616,
            "text_similarity_std": 0.18930155921265673,
            "llm_judge_score_mean": 5.517241379310345,
            "llm_judge_score_std": 1.5761299100836095
          },
          "rationale_cider": 0.20793654023139063
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.021754607783230472,
          "std_iou": 0.08153436979421272,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.029411764705882353,
            "count": 13,
            "total": 442
          },
          "R@0.5": {
            "recall": 0.004524886877828055,
            "count": 2,
            "total": 442
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 442
          },
          "mae": {
            "start_mean": 484.80385520361995,
            "end_mean": 454.64426470588234,
            "average_mean": 469.72405995475117
          },
          "rationale": {
            "rouge_l_mean": 0.18236998367897836,
            "rouge_l_std": 0.10398725324135792,
            "text_similarity_mean": 0.36370570317534434,
            "text_similarity_std": 0.1949182771607262,
            "llm_judge_score_mean": 5.334841628959276,
            "llm_judge_score_std": 1.7073460120265216
          },
          "rationale_cider": 0.18459453101621895
        },
        "13_Olympics": {
          "mean_iou": 0.0032380463892322134,
          "std_iou": 0.021010887840272236,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 85
          },
          "mae": {
            "start_mean": 70.7790705882353,
            "end_mean": 74.8578117647059,
            "average_mean": 72.8184411764706
          },
          "rationale": {
            "rouge_l_mean": 0.20187987505217833,
            "rouge_l_std": 0.09910401779046918,
            "text_similarity_mean": 0.45833102740785653,
            "text_similarity_std": 0.20527056102192195,
            "llm_judge_score_mean": 4.952941176470588,
            "llm_judge_score_std": 1.4785197872350826
          },
          "rationale_cider": 0.16894062287262598
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.018111810479135525,
        "mae_average": 510.7379331100547,
        "R@0.3": 0.023752294577765867,
        "R@0.5": 0.0073070104635794796,
        "R@0.7": 0.0027124667565423264,
        "rationale": {
          "rouge_l_mean": 0.19282133763617304,
          "text_similarity_mean": 0.40558823413789835,
          "llm_judge_score_mean": 5.3867559644360385
        }
      }
    }
  }
}