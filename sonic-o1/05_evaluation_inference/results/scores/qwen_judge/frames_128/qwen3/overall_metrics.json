{
  "model": "qwen3",
  "experiment_name": "frames_128",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.24810079147226405,
            "rouge_l_std": 0.0633950347455336,
            "text_similarity_mean": 0.7757610492408276,
            "text_similarity_std": 0.05723739721777735,
            "llm_judge_score_mean": 7.8125,
            "llm_judge_score_std": 1.1301963325015703
          },
          "short": {
            "rouge_l_mean": 0.23864162339783926,
            "rouge_l_std": 0.062436060007890475,
            "text_similarity_mean": 0.7442481704056263,
            "text_similarity_std": 0.1109145964680579,
            "llm_judge_score_mean": 6.9375,
            "llm_judge_score_std": 1.0288798520721456
          },
          "cider": {
            "cider_detailed": 0.014172862553765384,
            "cider_short": 0.01882904370168776
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.253916958461574,
            "rouge_l_std": 0.059934644532716956,
            "text_similarity_mean": 0.7623217446463448,
            "text_similarity_std": 0.0799689340866205,
            "llm_judge_score_mean": 8.666666666666666,
            "llm_judge_score_std": 0.8357108940373449
          },
          "short": {
            "rouge_l_mean": 0.2401535606086788,
            "rouge_l_std": 0.07802329108178602,
            "text_similarity_mean": 0.704269948459807,
            "text_similarity_std": 0.12563366358275654,
            "llm_judge_score_mean": 6.761904761904762,
            "llm_judge_score_std": 1.1914281907806479
          },
          "cider": {
            "cider_detailed": 0.07976783563687653,
            "cider_short": 0.02400062350708028
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.23035561169460522,
            "rouge_l_std": 0.08016438004888712,
            "text_similarity_mean": 0.7039525210857391,
            "text_similarity_std": 0.14334198779113144,
            "llm_judge_score_mean": 6.538461538461538,
            "llm_judge_score_std": 1.9060787220559796
          },
          "short": {
            "rouge_l_mean": 0.19574479459839497,
            "rouge_l_std": 0.08442113891179151,
            "text_similarity_mean": 0.6480395140556189,
            "text_similarity_std": 0.19369745790623968,
            "llm_judge_score_mean": 5.384615384615385,
            "llm_judge_score_std": 1.8203322409537284
          },
          "cider": {
            "cider_detailed": 0.011135545211546463,
            "cider_short": 0.019480670336388124
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.24412445387614778,
          "text_similarity_mean": 0.7473451049909704,
          "llm_judge_score_mean": 7.672542735042735
        },
        "short": {
          "rouge_l_mean": 0.224846659534971,
          "text_similarity_mean": 0.6988525443070174,
          "llm_judge_score_mean": 6.361340048840049
        },
        "cider": {
          "cider_detailed_mean": 0.035025414467396127,
          "cider_short_mean": 0.020770112515052053
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9607843137254902,
          "correct": 98,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.38929045562953307,
            "rouge_l_std": 0.11726338087077823,
            "text_similarity_mean": 0.7851902614621555,
            "text_similarity_std": 0.0852064389041572,
            "llm_judge_score_mean": 9.03921568627451,
            "llm_judge_score_std": 0.9794112740189944
          },
          "rationale_cider": 0.4180748621621745
        },
        "02_Job_Interviews": {
          "accuracy": 0.99,
          "correct": 99,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.37963073498711014,
            "rouge_l_std": 0.12165116977207527,
            "text_similarity_mean": 0.7629166239500046,
            "text_similarity_std": 0.09833830408994229,
            "llm_judge_score_mean": 9.17,
            "llm_judge_score_std": 0.895041898460625
          },
          "rationale_cider": 0.21851867948557782
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9826086956521739,
          "correct": 113,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.3766005656212153,
            "rouge_l_std": 0.11877266780459371,
            "text_similarity_mean": 0.7902623705241991,
            "text_similarity_std": 0.11311557127156532,
            "llm_judge_score_mean": 8.826086956521738,
            "llm_judge_score_std": 1.3270488314806714
          },
          "rationale_cider": 0.4199883428717051
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9777976697925547,
        "rationale": {
          "rouge_l_mean": 0.3818405854126195,
          "text_similarity_mean": 0.7794564186454531,
          "llm_judge_score_mean": 9.01176754759875
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.061004555534554854,
          "std_iou": 0.16110323191143627,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.08550185873605948,
            "count": 23,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.055762081784386616,
            "count": 15,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.011152416356877323,
            "count": 3,
            "total": 269
          },
          "mae": {
            "start_mean": 83.0393345724907,
            "end_mean": 3560.761159851301,
            "average_mean": 1821.9002472118957
          },
          "rationale": {
            "rouge_l_mean": 0.28643983747615553,
            "rouge_l_std": 0.08458528508776758,
            "text_similarity_mean": 0.6900391889992257,
            "text_similarity_std": 0.09717182636633187,
            "llm_judge_score_mean": 5.557620817843866,
            "llm_judge_score_std": 1.2939046081652625
          },
          "rationale_cider": 0.1010015559640338
        },
        "02_Job_Interviews": {
          "mean_iou": 0.046207763392073296,
          "std_iou": 0.15754049052434815,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05905511811023622,
            "count": 15,
            "total": 254
          },
          "R@0.5": {
            "recall": 0.04330708661417323,
            "count": 11,
            "total": 254
          },
          "R@0.7": {
            "recall": 0.027559055118110236,
            "count": 7,
            "total": 254
          },
          "mae": {
            "start_mean": 56.542834645669295,
            "end_mean": 59.64205118110236,
            "average_mean": 58.09244291338583
          },
          "rationale": {
            "rouge_l_mean": 0.2890839127106099,
            "rouge_l_std": 0.08712128710599472,
            "text_similarity_mean": 0.68981141297836,
            "text_similarity_std": 0.09697788566820541,
            "llm_judge_score_mean": 5.5,
            "llm_judge_score_std": 1.362460719653387
          },
          "rationale_cider": 0.11704691508302102
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.03725644484584807,
          "std_iou": 0.12138796429638138,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05263157894736842,
            "count": 18,
            "total": 342
          },
          "R@0.5": {
            "recall": 0.02046783625730994,
            "count": 7,
            "total": 342
          },
          "R@0.7": {
            "recall": 0.005847953216374269,
            "count": 2,
            "total": 342
          },
          "mae": {
            "start_mean": 47.2269678362573,
            "end_mean": 48.565856725146205,
            "average_mean": 47.89641228070175
          },
          "rationale": {
            "rouge_l_mean": 0.2819292817320851,
            "rouge_l_std": 0.08394578202794545,
            "text_similarity_mean": 0.7222495460719393,
            "text_similarity_std": 0.09752101116005353,
            "llm_judge_score_mean": 5.652046783625731,
            "llm_judge_score_std": 1.3107668922293196
          },
          "rationale_cider": 0.04695614959957251
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.048156254590825405,
        "mae_average": 642.6297008019944,
        "R@0.3": 0.06572951859788805,
        "R@0.5": 0.03984566821862326,
        "R@0.7": 0.014853141563787276,
        "rationale": {
          "rouge_l_mean": 0.2858176773062835,
          "text_similarity_mean": 0.7007000493498415,
          "llm_judge_score_mean": 5.5698892004898655
        }
      }
    }
  }
}