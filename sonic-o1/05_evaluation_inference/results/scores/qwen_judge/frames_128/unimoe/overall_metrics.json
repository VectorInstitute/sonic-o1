{
  "model": "unimoe",
  "experiment_name": "frames_128",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.21574437675950459,
            "rouge_l_std": 0.053162449066554415,
            "text_similarity_mean": 0.7710515558719635,
            "text_similarity_std": 0.05609521582720652,
            "llm_judge_score_mean": 7.375,
            "llm_judge_score_std": 0.9921567416492215
          },
          "short": {
            "rouge_l_mean": 0.19929696775423117,
            "rouge_l_std": 0.09293702557850382,
            "text_similarity_mean": 0.6810018755495548,
            "text_similarity_std": 0.06216982435896488,
            "llm_judge_score_mean": 5.5625,
            "llm_judge_score_std": 1.7309228030157786
          },
          "cider": {
            "cider_detailed": 0.0022624227102243286,
            "cider_short": 0.011086399015549748
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.22168542959492837,
            "rouge_l_std": 0.05638241269938804,
            "text_similarity_mean": 0.6888275529657092,
            "text_similarity_std": 0.12501524206377027,
            "llm_judge_score_mean": 7.142857142857143,
            "llm_judge_score_std": 1.7535192990020925
          },
          "short": {
            "rouge_l_mean": 0.1626590214442562,
            "rouge_l_std": 0.08188728187586218,
            "text_similarity_mean": 0.6204614170960018,
            "text_similarity_std": 0.150405444000539,
            "llm_judge_score_mean": 4.666666666666667,
            "llm_judge_score_std": 1.2084359562332878
          },
          "cider": {
            "cider_detailed": 0.07132054030870641,
            "cider_short": 0.02949900599499235
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.19662036627949192,
            "rouge_l_std": 0.054635328432421235,
            "text_similarity_mean": 0.6593427268358377,
            "text_similarity_std": 0.19054697820524685,
            "llm_judge_score_mean": 5.846153846153846,
            "llm_judge_score_std": 2.0322838174804474
          },
          "short": {
            "rouge_l_mean": 0.1469149563477829,
            "rouge_l_std": 0.06927074345306371,
            "text_similarity_mean": 0.586834178521083,
            "text_similarity_std": 0.18544229035330215,
            "llm_judge_score_mean": 4.076923076923077,
            "llm_judge_score_std": 1.7303418275695375
          },
          "cider": {
            "cider_detailed": 1.8367694098657604e-05,
            "cider_short": 9.600226846812362e-07
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.21135005754464165,
          "text_similarity_mean": 0.7064072785578368,
          "llm_judge_score_mean": 6.788003663003663
        },
        "short": {
          "rouge_l_mean": 0.16962364851542344,
          "text_similarity_mean": 0.6294324903888798,
          "llm_judge_score_mean": 4.768696581196582
        },
        "cider": {
          "cider_detailed_mean": 0.024533776904343132,
          "cider_short_mean": 0.013528788344408927
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9313725490196079,
          "correct": 95,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.29755159508298923,
            "rouge_l_std": 0.11464521095741959,
            "text_similarity_mean": 0.7406189523491205,
            "text_similarity_std": 0.12004357868262926,
            "llm_judge_score_mean": 8.392156862745098,
            "llm_judge_score_std": 1.3802306055198483
          },
          "rationale_cider": 0.14944334908127627
        },
        "02_Job_Interviews": {
          "accuracy": 0.97,
          "correct": 97,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.277030963067221,
            "rouge_l_std": 0.08072745558778047,
            "text_similarity_mean": 0.7388309174776078,
            "text_similarity_std": 0.088840735581075,
            "llm_judge_score_mean": 8.44,
            "llm_judge_score_std": 1.0131140113531152
          },
          "rationale_cider": 0.21584464145162624
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9304347826086956,
          "correct": 107,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2769301415657742,
            "rouge_l_std": 0.08569387136191868,
            "text_similarity_mean": 0.7364989843219518,
            "text_similarity_std": 0.14113711242604077,
            "llm_judge_score_mean": 7.956521739130435,
            "llm_judge_score_std": 1.68570101195358
          },
          "rationale_cider": 0.17136245832045588
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9439357772094344,
        "rationale": {
          "rouge_l_mean": 0.2838375665719948,
          "text_similarity_mean": 0.73864961804956,
          "llm_judge_score_mean": 8.262892867291844
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.014209444379862473,
          "std_iou": 0.06787044112047104,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.01893939393939394,
            "count": 5,
            "total": 264
          },
          "R@0.5": {
            "recall": 0.003787878787878788,
            "count": 1,
            "total": 264
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 264
          },
          "mae": {
            "start_mean": 1000.2938275157217,
            "end_mean": 4542.631477986333,
            "average_mean": 2771.462652751028
          },
          "rationale": {
            "rouge_l_mean": 0.2430856198578342,
            "rouge_l_std": 0.09900879038285819,
            "text_similarity_mean": 0.5328607151609366,
            "text_similarity_std": 0.18457541378117204,
            "llm_judge_score_mean": 4.113636363636363,
            "llm_judge_score_std": 1.5479592003507112
          },
          "rationale_cider": 0.2614488785056458
        },
        "02_Job_Interviews": {
          "mean_iou": 0.018040394549727435,
          "std_iou": 0.07522396308248604,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.024390243902439025,
            "count": 6,
            "total": 246
          },
          "R@0.5": {
            "recall": 0.008130081300813009,
            "count": 2,
            "total": 246
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 246
          },
          "mae": {
            "start_mean": 653.70334044518,
            "end_mean": 650.2931607307623,
            "average_mean": 651.9982505879711
          },
          "rationale": {
            "rouge_l_mean": 0.22923488946797774,
            "rouge_l_std": 0.08834860242834434,
            "text_similarity_mean": 0.5142952030660902,
            "text_similarity_std": 0.19125033410175735,
            "llm_judge_score_mean": 4.1138211382113825,
            "llm_judge_score_std": 1.5549723654905752
          },
          "rationale_cider": 0.21417933697911024
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.009465935961814165,
          "std_iou": 0.048748929159595514,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.011730205278592375,
            "count": 4,
            "total": 341
          },
          "R@0.5": {
            "recall": 0.0,
            "count": 0,
            "total": 341
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 341
          },
          "mae": {
            "start_mean": 1531.3996539927587,
            "end_mean": 1533.3661602245234,
            "average_mean": 1532.3829071086407
          },
          "rationale": {
            "rouge_l_mean": 0.23073089629950647,
            "rouge_l_std": 0.09434563193375704,
            "text_similarity_mean": 0.5152067508646819,
            "text_similarity_std": 0.20627317737261783,
            "llm_judge_score_mean": 3.9824046920821115,
            "llm_judge_score_std": 1.719215558138232
          },
          "rationale_cider": 0.19948000433027965
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.01390525829713469,
        "mae_average": 1651.94793681588,
        "R@0.3": 0.01835328104014178,
        "R@0.5": 0.003972653362897266,
        "R@0.7": 0.0,
        "rationale": {
          "rouge_l_mean": 0.2343504685417728,
          "text_similarity_mean": 0.520787556363903,
          "llm_judge_score_mean": 4.069954064643286
        }
      }
    }
  }
}