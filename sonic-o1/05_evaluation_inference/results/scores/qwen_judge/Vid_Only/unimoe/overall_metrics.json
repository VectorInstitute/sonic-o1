{
  "model": "unimoe",
  "experiment_name": "Vid_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.2135031143871741,
            "rouge_l_std": 0.04371812276467782,
            "text_similarity_mean": 0.7352305464446545,
            "text_similarity_std": 0.07497948025917764,
            "llm_judge_score_mean": 6.0,
            "llm_judge_score_std": 1.3693063937629153
          },
          "short": {
            "rouge_l_mean": 0.15862685806887158,
            "rouge_l_std": 0.05122587576357171,
            "text_similarity_mean": 0.6191866155713797,
            "text_similarity_std": 0.1431229101429527,
            "llm_judge_score_mean": 4.0,
            "llm_judge_score_std": 1.2747548783981961
          },
          "cider": {
            "cider_detailed": 0.016941001477675147,
            "cider_short": 3.12721250068559e-06
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.17428913441264363,
            "rouge_l_std": 0.03247737481908311,
            "text_similarity_mean": 0.6203617907705761,
            "text_similarity_std": 0.14129162066277104,
            "llm_judge_score_mean": 5.476190476190476,
            "llm_judge_score_std": 1.892819706128742
          },
          "short": {
            "rouge_l_mean": 0.12955269400837036,
            "rouge_l_std": 0.07137840903179565,
            "text_similarity_mean": 0.5112557467960176,
            "text_similarity_std": 0.1440386658861845,
            "llm_judge_score_mean": 3.4285714285714284,
            "llm_judge_score_std": 1.3652639225553764
          },
          "cider": {
            "cider_detailed": 0.005190508834542835,
            "cider_short": 0.01037355794759239
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.15544845343632943,
            "rouge_l_std": 0.03988361143721955,
            "text_similarity_mean": 0.4601102631825667,
            "text_similarity_std": 0.18362758750714617,
            "llm_judge_score_mean": 3.769230769230769,
            "llm_judge_score_std": 1.6711969986461868
          },
          "short": {
            "rouge_l_mean": 0.08613465696486526,
            "rouge_l_std": 0.06869238472116433,
            "text_similarity_mean": 0.4339413350591293,
            "text_similarity_std": 0.2406066441088093,
            "llm_judge_score_mean": 2.6153846153846154,
            "llm_judge_score_std": 1.6426274233894325
          },
          "cider": {
            "cider_detailed": 0.0008212820160541216,
            "cider_short": 0.0005070106796157219
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.18108023407871573,
          "text_similarity_mean": 0.6052342001325991,
          "llm_judge_score_mean": 5.0818070818070815
        },
        "short": {
          "rouge_l_mean": 0.12477140301403573,
          "text_similarity_mean": 0.5214612324755089,
          "llm_judge_score_mean": 3.347985347985348
        },
        "cider": {
          "cider_detailed_mean": 0.0076509307760907015,
          "cider_short_mean": 0.003627898613236266
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.8823529411764706,
          "correct": 90,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.2673156607130253,
            "rouge_l_std": 0.08418783472203012,
            "text_similarity_mean": 0.7136285280187925,
            "text_similarity_std": 0.1438794586307367,
            "llm_judge_score_mean": 7.911764705882353,
            "llm_judge_score_std": 1.721279398635239
          },
          "rationale_cider": 0.18734126825373168
        },
        "02_Job_Interviews": {
          "accuracy": 0.83,
          "correct": 83,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2514845976892455,
            "rouge_l_std": 0.07640867473694084,
            "text_similarity_mean": 0.6957167264819145,
            "text_similarity_std": 0.12368408904530377,
            "llm_judge_score_mean": 7.87,
            "llm_judge_score_std": 1.5404869360043272
          },
          "rationale_cider": 0.1376051574937436
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.808695652173913,
          "correct": 93,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2495410114473337,
            "rouge_l_std": 0.07899692020911694,
            "text_similarity_mean": 0.713605348587684,
            "text_similarity_std": 0.1446811457555151,
            "llm_judge_score_mean": 7.643478260869565,
            "llm_judge_score_std": 1.9028185243170017
          },
          "rationale_cider": 0.0722429541447219
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8403495311167944,
        "rationale": {
          "rouge_l_mean": 0.25611375661653485,
          "text_similarity_mean": 0.7076502010294637,
          "llm_judge_score_mean": 7.808414322250639
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {},
      "aggregated_across_topics": {}
    }
  }
}