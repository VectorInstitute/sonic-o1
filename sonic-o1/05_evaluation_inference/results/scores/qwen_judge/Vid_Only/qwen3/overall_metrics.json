{
  "model": "qwen3",
  "experiment_name": "Vid_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.22016610436791625,
            "rouge_l_std": 0.044254818777529105,
            "text_similarity_mean": 0.7005635090172291,
            "text_similarity_std": 0.1009202716575779,
            "llm_judge_score_mean": 6.4375,
            "llm_judge_score_std": 1.8360538526960477
          },
          "short": {
            "rouge_l_mean": 0.20496671508234124,
            "rouge_l_std": 0.0527089979492162,
            "text_similarity_mean": 0.6828159168362617,
            "text_similarity_std": 0.09021209045894901,
            "llm_judge_score_mean": 5.3125,
            "llm_judge_score_std": 1.2103072956898178
          },
          "cider": {
            "cider_detailed": 0.14123884504032078,
            "cider_short": 0.005729792271757477
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.21307866989336552,
            "rouge_l_std": 0.07893402334315837,
            "text_similarity_mean": 0.6161536276340485,
            "text_similarity_std": 0.1680541312699988,
            "llm_judge_score_mean": 6.333333333333333,
            "llm_judge_score_std": 2.1006423809631216
          },
          "short": {
            "rouge_l_mean": 0.17766132178310517,
            "rouge_l_std": 0.052204607705303024,
            "text_similarity_mean": 0.5595628973983583,
            "text_similarity_std": 0.16099652979420148,
            "llm_judge_score_mean": 5.0,
            "llm_judge_score_std": 1.632993161855452
          },
          "cider": {
            "cider_detailed": 0.03098302788590563,
            "cider_short": 0.0008209983731277451
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.16222011256120344,
            "rouge_l_std": 0.05009146884260957,
            "text_similarity_mean": 0.5372645877874814,
            "text_similarity_std": 0.15683298478687066,
            "llm_judge_score_mean": 4.384615384615385,
            "llm_judge_score_std": 1.862110528755416
          },
          "short": {
            "rouge_l_mean": 0.1465419332099761,
            "rouge_l_std": 0.07470712215291055,
            "text_similarity_mean": 0.45612499805597156,
            "text_similarity_std": 0.2215949225658394,
            "llm_judge_score_mean": 3.6153846153846154,
            "llm_judge_score_std": 1.5951108733329016
          },
          "cider": {
            "cider_detailed": 1.0114609708617963e-05,
            "cider_short": 0.002180197957982887
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.19848829560749506,
          "text_similarity_mean": 0.617993908146253,
          "llm_judge_score_mean": 5.718482905982906
        },
        "short": {
          "rouge_l_mean": 0.17638999002514086,
          "text_similarity_mean": 0.5661679374301972,
          "llm_judge_score_mean": 4.642628205128205
        },
        "cider": {
          "cider_detailed_mean": 0.05741066251197834,
          "cider_short_mean": 0.0029103295342893694
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.8333333333333334,
          "correct": 85,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.27322966923175906,
            "rouge_l_std": 0.09690432407122779,
            "text_similarity_mean": 0.7263256098125496,
            "text_similarity_std": 0.1227543659452576,
            "llm_judge_score_mean": 7.823529411764706,
            "llm_judge_score_std": 2.153026892111326
          },
          "rationale_cider": 0.2209644619563922
        },
        "02_Job_Interviews": {
          "accuracy": 0.9,
          "correct": 90,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2607106514374415,
            "rouge_l_std": 0.09224353327859552,
            "text_similarity_mean": 0.6917312651872635,
            "text_similarity_std": 0.10846499374497708,
            "llm_judge_score_mean": 8.04,
            "llm_judge_score_std": 1.8916659324521337
          },
          "rationale_cider": 0.08732333348316539
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.8521739130434782,
          "correct": 98,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.25793927562312824,
            "rouge_l_std": 0.08610459301342353,
            "text_similarity_mean": 0.7009347804943504,
            "text_similarity_std": 0.13362485671273291,
            "llm_judge_score_mean": 7.530434782608696,
            "llm_judge_score_std": 2.2624409255047664
          },
          "rationale_cider": 0.12718520881642065
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.8618357487922705,
        "rationale": {
          "rouge_l_mean": 0.26395986543077626,
          "text_similarity_mean": 0.7063305518313877,
          "llm_judge_score_mean": 7.7979880647911335
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.03588117872708754,
          "std_iou": 0.10356923606935646,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.06691449814126393,
            "count": 18,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.0037174721189591076,
            "count": 1,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.0037174721189591076,
            "count": 1,
            "total": 269
          },
          "mae": {
            "start_mean": 44.67959479553904,
            "end_mean": 3522.731657992564,
            "average_mean": 1783.7056263940517
          },
          "rationale": {
            "rouge_l_mean": 0.27414241356242214,
            "rouge_l_std": 0.08056796029774979,
            "text_similarity_mean": 0.6610488641217739,
            "text_similarity_std": 0.12747023879656674,
            "llm_judge_score_mean": 5.438661710037175,
            "llm_judge_score_std": 1.261729971605507
          },
          "rationale_cider": 0.14118843005657747
        },
        "02_Job_Interviews": {
          "mean_iou": 0.031893255788090345,
          "std_iou": 0.11824603991859428,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.0392156862745098,
            "count": 10,
            "total": 255
          },
          "R@0.5": {
            "recall": 0.027450980392156862,
            "count": 7,
            "total": 255
          },
          "R@0.7": {
            "recall": 0.00392156862745098,
            "count": 1,
            "total": 255
          },
          "mae": {
            "start_mean": 40.804180392156866,
            "end_mean": 43.70867843137255,
            "average_mean": 42.256429411764714
          },
          "rationale": {
            "rouge_l_mean": 0.2795536486599014,
            "rouge_l_std": 0.08215742585909439,
            "text_similarity_mean": 0.67806362100676,
            "text_similarity_std": 0.10311462957429518,
            "llm_judge_score_mean": 5.435294117647059,
            "llm_judge_score_std": 1.1387684316601818
          },
          "rationale_cider": 0.13376999767923317
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.025298021672332617,
          "std_iou": 0.09725530395353883,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.037900874635568516,
            "count": 13,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.01749271137026239,
            "count": 6,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 343
          },
          "mae": {
            "start_mean": 72.30522157434402,
            "end_mean": 73.98333819241985,
            "average_mean": 73.14427988338194
          },
          "rationale": {
            "rouge_l_mean": 0.2601672029904089,
            "rouge_l_std": 0.07518238569100082,
            "text_similarity_mean": 0.6970705414304927,
            "text_similarity_std": 0.1208140203449904,
            "llm_judge_score_mean": 5.274052478134111,
            "llm_judge_score_std": 1.1708797458194602
          },
          "rationale_cider": 0.08035084723875084
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.0310241520625035,
        "mae_average": 633.0354452297328,
        "R@0.3": 0.04801035301711409,
        "R@0.5": 0.016220387960459454,
        "R@0.7": 0.0025463469154700293,
        "rationale": {
          "rouge_l_mean": 0.2712877550709108,
          "text_similarity_mean": 0.6787276755196755,
          "llm_judge_score_mean": 5.382669435272781
        }
      }
    }
  }
}