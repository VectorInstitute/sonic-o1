{
  "model": "gpt4o",
  "experiment_name": "Vid_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.203763583538719,
            "rouge_l_std": 0.04092389870601215,
            "text_similarity_mean": 0.733963631093502,
            "text_similarity_std": 0.08645913295873872,
            "llm_judge_score_mean": 7.5,
            "llm_judge_score_std": 1.2747548783981961
          },
          "short": {
            "rouge_l_mean": 0.156413977047968,
            "rouge_l_std": 0.04514421376954162,
            "text_similarity_mean": 0.6684594489634037,
            "text_similarity_std": 0.12448153752639941,
            "llm_judge_score_mean": 5.375,
            "llm_judge_score_std": 1.2183492931011204
          },
          "cider": {
            "cider_detailed": 0.04609969857726124,
            "cider_short": 1.26601833177504e-12
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.18523043351542204,
            "rouge_l_std": 0.041528836649699453,
            "text_similarity_mean": 0.6162342954249609,
            "text_similarity_std": 0.16659651183263674,
            "llm_judge_score_mean": 6.380952380952381,
            "llm_judge_score_std": 2.1263917677872484
          },
          "short": {
            "rouge_l_mean": 0.14458734074147275,
            "rouge_l_std": 0.05230704626734027,
            "text_similarity_mean": 0.5215970221019927,
            "text_similarity_std": 0.1770851614282279,
            "llm_judge_score_mean": 4.476190476190476,
            "llm_judge_score_std": 1.4998110236212974
          },
          "cider": {
            "cider_detailed": 0.011300845265198183,
            "cider_short": 0.019200220803138955
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.18056650404593366,
            "rouge_l_std": 0.046382351110656035,
            "text_similarity_mean": 0.5604375451803207,
            "text_similarity_std": 0.1861542545096644,
            "llm_judge_score_mean": 5.3076923076923075,
            "llm_judge_score_std": 2.12619614785927
          },
          "short": {
            "rouge_l_mean": 0.13028234462141056,
            "rouge_l_std": 0.07430264282682712,
            "text_similarity_mean": 0.4899259706815848,
            "text_similarity_std": 0.23758779689061052,
            "llm_judge_score_mean": 3.6153846153846154,
            "llm_judge_score_std": 2.0953597649040363
          },
          "cider": {
            "cider_detailed": 4.230608805564184e-05,
            "cider_short": 0.02313362648200274
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.18985350703335824,
          "text_similarity_mean": 0.6368784905662612,
          "llm_judge_score_mean": 6.396214896214897
        },
        "short": {
          "rouge_l_mean": 0.1437612208036171,
          "text_similarity_mean": 0.5599941472489937,
          "llm_judge_score_mean": 4.4888583638583635
        },
        "cider": {
          "cider_detailed_mean": 0.019147616643505023,
          "cider_short_mean": 0.014111282428802571
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.8627450980392157,
          "correct": 88,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.286232291938133,
            "rouge_l_std": 0.11516164323396089,
            "text_similarity_mean": 0.7294344767635944,
            "text_similarity_std": 0.14259862591833083,
            "llm_judge_score_mean": 8.058823529411764,
            "llm_judge_score_std": 1.8985212481408906
          },
          "rationale_cider": 0.0675633308983818
        },
        "02_Job_Interviews": {
          "accuracy": 0.94,
          "correct": 94,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.2660009640911435,
            "rouge_l_std": 0.08870525910842092,
            "text_similarity_mean": 0.7014586237072945,
            "text_similarity_std": 0.10048033106184585,
            "llm_judge_score_mean": 8.26,
            "llm_judge_score_std": 1.5074481748969017
          },
          "rationale_cider": 0.07511221964952444
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9304347826086956,
          "correct": 107,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.2743224466030063,
            "rouge_l_std": 0.09097276232284714,
            "text_similarity_mean": 0.7356105113159055,
            "text_similarity_std": 0.13130684009851248,
            "llm_judge_score_mean": 8.28695652173913,
            "llm_judge_score_std": 1.5812225176373986
          },
          "rationale_cider": 0.07078988310962724
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9110599602159705,
        "rationale": {
          "rouge_l_mean": 0.27551856754409426,
          "text_similarity_mean": 0.7221678705955981,
          "llm_judge_score_mean": 8.201926683716964
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.047025634652034055,
          "std_iou": 0.14289115049639595,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.05947955390334572,
            "count": 16,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.03345724907063197,
            "count": 9,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.011152416356877323,
            "count": 3,
            "total": 269
          },
          "mae": {
            "start_mean": 33.11740892193309,
            "end_mean": 3509.862423791822,
            "average_mean": 1771.4899163568773
          },
          "rationale": {
            "rouge_l_mean": 0.3459023232948279,
            "rouge_l_std": 0.09127552956660848,
            "text_similarity_mean": 0.7177910921077303,
            "text_similarity_std": 0.09103322359157741,
            "llm_judge_score_mean": 5.587360594795539,
            "llm_judge_score_std": 1.1841534645442755
          },
          "rationale_cider": 0.5190187880519358
        },
        "02_Job_Interviews": {
          "mean_iou": 0.06212898196472538,
          "std_iou": 0.15781652826546924,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.09411764705882353,
            "count": 24,
            "total": 255
          },
          "R@0.5": {
            "recall": 0.050980392156862744,
            "count": 13,
            "total": 255
          },
          "R@0.7": {
            "recall": 0.00784313725490196,
            "count": 2,
            "total": 255
          },
          "mae": {
            "start_mean": 29.128180392156867,
            "end_mean": 30.2306705882353,
            "average_mean": 29.67942549019608
          },
          "rationale": {
            "rouge_l_mean": 0.32734275232649757,
            "rouge_l_std": 0.08939772503081767,
            "text_similarity_mean": 0.7189596685708738,
            "text_similarity_std": 0.08120582458891304,
            "llm_judge_score_mean": 5.564705882352941,
            "llm_judge_score_std": 1.218617591807798
          },
          "rationale_cider": 0.4650758312889894
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.05064466082692683,
          "std_iou": 0.15011236764977398,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.07871720116618076,
            "count": 27,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.043731778425655975,
            "count": 15,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.011661807580174927,
            "count": 4,
            "total": 343
          },
          "mae": {
            "start_mean": 40.48511078717201,
            "end_mean": 41.98221865889213,
            "average_mean": 41.23366472303207
          },
          "rationale": {
            "rouge_l_mean": 0.32314523731532535,
            "rouge_l_std": 0.08155204560242811,
            "text_similarity_mean": 0.7486053441947125,
            "text_similarity_std": 0.08346735249898996,
            "llm_judge_score_mean": 5.437317784256559,
            "llm_judge_score_std": 1.1859261334741291
          },
          "rationale_cider": 0.3228037402892504
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.05326642581456209,
        "mae_average": 614.1343355233685,
        "R@0.3": 0.07743813404278334,
        "R@0.5": 0.04272313988438356,
        "R@0.7": 0.01021912039731807,
        "rationale": {
          "rouge_l_mean": 0.33213010431221696,
          "text_similarity_mean": 0.7284520349577722,
          "llm_judge_score_mean": 5.5297947538016805
        }
      }
    }
  }
}