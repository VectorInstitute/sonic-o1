{
  "model": "videollama",
  "experiment_name": "Audio_Only",
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.10238697161129176,
            "rouge_l_std": 0.02164036356709343,
            "text_similarity_mean": 0.19549510662909597,
            "text_similarity_std": 0.12077127282244246,
            "llm_judge_score_mean": 1.125,
            "llm_judge_score_std": 0.9921567416492215
          },
          "short": {
            "rouge_l_mean": 0.08648643305496354,
            "rouge_l_std": 0.041098960548371054,
            "text_similarity_mean": 0.14080409239977598,
            "text_similarity_std": 0.11747146412962867,
            "llm_judge_score_mean": 1.5,
            "llm_judge_score_std": 0.8660254037844386
          },
          "cider": {
            "cider_detailed": 3.1490218235699e-05,
            "cider_short": 4.998913272962167e-09
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.09740008634482093,
            "rouge_l_std": 0.023095994510806444,
            "text_similarity_mean": 0.2107645067430678,
            "text_similarity_std": 0.059204278415817435,
            "llm_judge_score_mean": 1.5714285714285714,
            "llm_judge_score_std": 0.9035079029052513
          },
          "short": {
            "rouge_l_mean": 0.07669673258248785,
            "rouge_l_std": 0.03644407595707945,
            "text_similarity_mean": 0.1570721184391351,
            "text_similarity_std": 0.08916873150297012,
            "llm_judge_score_mean": 1.4285714285714286,
            "llm_judge_score_std": 0.9035079029052513
          },
          "cider": {
            "cider_detailed": 4.478917468477902e-12,
            "cider_short": 0.00043433473999363704
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.09415201898965053,
            "rouge_l_std": 0.018771788275063934,
            "text_similarity_mean": 0.1672190920664714,
            "text_similarity_std": 0.08422925320951537,
            "llm_judge_score_mean": 1.2307692307692308,
            "llm_judge_score_std": 0.9730085108210399
          },
          "short": {
            "rouge_l_mean": 0.055232488754754956,
            "rouge_l_std": 0.036730748225814305,
            "text_similarity_mean": 0.10689640818880154,
            "text_similarity_std": 0.09909286511084338,
            "llm_judge_score_mean": 1.2307692307692308,
            "llm_judge_score_std": 0.9730085108210399
          },
          "cider": {
            "cider_detailed": 8.35470508879143e-08,
            "cider_short": 8.92990530563159e-06
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.0979796923152544,
          "text_similarity_mean": 0.19115956847954507,
          "llm_judge_score_mean": 1.309065934065934
        },
        "short": {
          "rouge_l_mean": 0.07280521813073544,
          "text_similarity_mean": 0.13492420634257088,
          "llm_judge_score_mean": 1.3864468864468866
        },
        "cider": {
          "cider_detailed_mean": 1.0524589921834793e-05,
          "cider_short_mean": 0.0001477565480708472
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.5196078431372549,
          "correct": 53,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.22740675038857075,
            "rouge_l_std": 0.0773386254809541,
            "text_similarity_mean": 0.6613901951076353,
            "text_similarity_std": 0.1596538246196485,
            "llm_judge_score_mean": 6.0588235294117645,
            "llm_judge_score_std": 2.3339923274390606
          },
          "rationale_cider": 0.07833969988510026
        },
        "02_Job_Interviews": {
          "accuracy": 0.6,
          "correct": 60,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.23102709989408277,
            "rouge_l_std": 0.07101982419014312,
            "text_similarity_mean": 0.6632706567272544,
            "text_similarity_std": 0.1377157263949392,
            "llm_judge_score_mean": 6.79,
            "llm_judge_score_std": 2.0990235825259322
          },
          "rationale_cider": 0.09199916769747107
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.4608695652173913,
          "correct": 53,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.21473352051416073,
            "rouge_l_std": 0.08379626040687695,
            "text_similarity_mean": 0.6430718244539331,
            "text_similarity_std": 0.18057283353551867,
            "llm_judge_score_mean": 5.826086956521739,
            "llm_judge_score_std": 2.5444256875364
          },
          "rationale_cider": 0.05289225439191089
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.5268258027848821,
        "rationale": {
          "rouge_l_mean": 0.22438912359893806,
          "text_similarity_mean": 0.6559108920962743,
          "llm_judge_score_mean": 6.224970161977834
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.038172269871490305,
          "std_iou": 0.07225255722475352,
          "median_iou": 0.01025714285714284,
          "R@0.3": {
            "recall": 0.01486988847583643,
            "count": 4,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.0037174721189591076,
            "count": 1,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 269
          },
          "mae": {
            "start_mean": 195.96518587360592,
            "end_mean": 3705.362260223048,
            "average_mean": 1950.6637230483268
          },
          "rationale": {
            "rouge_l_mean": 0.24911632969609915,
            "rouge_l_std": 0.0947627619511581,
            "text_similarity_mean": 0.5650036113862932,
            "text_similarity_std": 0.17853565015851672,
            "llm_judge_score_mean": 4.639405204460966,
            "llm_judge_score_std": 1.7223895228789277
          },
          "rationale_cider": 0.2920781068648784
        },
        "02_Job_Interviews": {
          "mean_iou": 0.04877468329280952,
          "std_iou": 0.08608867250413782,
          "median_iou": 0.020052883464943737,
          "R@0.3": {
            "recall": 0.015748031496062992,
            "count": 4,
            "total": 254
          },
          "R@0.5": {
            "recall": 0.007874015748031496,
            "count": 2,
            "total": 254
          },
          "R@0.7": {
            "recall": 0.003937007874015748,
            "count": 1,
            "total": 254
          },
          "mae": {
            "start_mean": 131.57825984251969,
            "end_mean": 157.3275196850394,
            "average_mean": 144.45288976377952
          },
          "rationale": {
            "rouge_l_mean": 0.24051144166187557,
            "rouge_l_std": 0.0980316132910363,
            "text_similarity_mean": 0.5439093754164815,
            "text_similarity_std": 0.1917527517575993,
            "llm_judge_score_mean": 4.7362204724409445,
            "llm_judge_score_std": 1.6921853406120897
          },
          "rationale_cider": 0.21701199145696834
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.04062963072441101,
          "std_iou": 0.08523273144807086,
          "median_iou": 0.014166666666665152,
          "R@0.3": {
            "recall": 0.01749271137026239,
            "count": 6,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.011661807580174927,
            "count": 4,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.0,
            "count": 0,
            "total": 343
          },
          "mae": {
            "start_mean": 320.42329154518944,
            "end_mean": 338.996749271137,
            "average_mean": 329.7100204081633
          },
          "rationale": {
            "rouge_l_mean": 0.23892673425739056,
            "rouge_l_std": 0.09615636733871079,
            "text_similarity_mean": 0.5644881234344085,
            "text_similarity_std": 0.190223395882723,
            "llm_judge_score_mean": 4.239067055393586,
            "llm_judge_score_std": 1.6075591310850934
          },
          "rationale_cider": 0.17202207940056485
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.04252552796290362,
        "mae_average": 808.2755444067566,
        "R@0.3": 0.016036877114053938,
        "R@0.5": 0.00775109848238851,
        "R@0.7": 0.0013123359580052493,
        "rationale": {
          "rouge_l_mean": 0.24285150187178842,
          "text_similarity_mean": 0.557800370079061,
          "llm_judge_score_mean": 4.538230910765166
        }
      }
    }
  }
}