{
  "model": "gemini",
  "experiment_name": null,
  "tasks": {
    "t1": {
      "task_name": "task1_summarization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "detailed": {
            "rouge_l_mean": 0.29652681479457055,
            "rouge_l_std": 0.06553069884471927,
            "text_similarity_mean": 0.8581532798707485,
            "text_similarity_std": 0.029875225935854673,
            "llm_judge_score_mean": 8.8125,
            "llm_judge_score_std": 0.7261843774138906
          },
          "short": {
            "rouge_l_mean": 0.25752573649015675,
            "rouge_l_std": 0.0737386693347979,
            "text_similarity_mean": 0.7927049174904823,
            "text_similarity_std": 0.08844602533885809,
            "llm_judge_score_mean": 7.6875,
            "llm_judge_score_std": 1.2103072956898178
          },
          "cider": {
            "cider_detailed": 0.11047204793322225,
            "cider_short": 0.03138748561998228
          }
        },
        "02_Job_Interviews": {
          "detailed": {
            "rouge_l_mean": 0.2647711185144737,
            "rouge_l_std": 0.050668312186306054,
            "text_similarity_mean": 0.8169529097420829,
            "text_similarity_std": 0.05722528758861227,
            "llm_judge_score_mean": 8.857142857142858,
            "llm_judge_score_std": 0.5593971487843205
          },
          "short": {
            "rouge_l_mean": 0.24608927096793226,
            "rouge_l_std": 0.08014793416176813,
            "text_similarity_mean": 0.7261400364694142,
            "text_similarity_std": 0.0828705169406417,
            "llm_judge_score_mean": 7.476190476190476,
            "llm_judge_score_std": 1.1388819755334665
          },
          "cider": {
            "cider_detailed": 0.0037478510492474333,
            "cider_short": 0.055207537994083365
          }
        },
        "03_Parent-Teacher_Conferences": {
          "detailed": {
            "rouge_l_mean": 0.2762881351271424,
            "rouge_l_std": 0.06430206436080868,
            "text_similarity_mean": 0.8218151529630026,
            "text_similarity_std": 0.052610595200432826,
            "llm_judge_score_mean": 8.555555555555555,
            "llm_judge_score_std": 0.5983516452371671
          },
          "short": {
            "rouge_l_mean": 0.2531286458411982,
            "rouge_l_std": 0.07545231781729178,
            "text_similarity_mean": 0.7459158996740977,
            "text_similarity_std": 0.061059094332359926,
            "llm_judge_score_mean": 7.111111111111111,
            "llm_judge_score_std": 1.0999438818457405
          },
          "cider": {
            "cider_detailed": 0.10960087665581214,
            "cider_short": 0.0007009841154356367
          }
        },
        "04_Customer_Service_Interactions": {
          "detailed": {
            "rouge_l_mean": 0.2909538087729732,
            "rouge_l_std": 0.04420149489438533,
            "text_similarity_mean": 0.8141394853591919,
            "text_similarity_std": 0.04426534653455899,
            "llm_judge_score_mean": 8.333333333333334,
            "llm_judge_score_std": 1.0749676997731399
          },
          "short": {
            "rouge_l_mean": 0.3171619205873878,
            "rouge_l_std": 0.08437160529612123,
            "text_similarity_mean": 0.7729566216468811,
            "text_similarity_std": 0.07096607016784674,
            "llm_judge_score_mean": 7.0,
            "llm_judge_score_std": 1.0327955589886444
          },
          "cider": {
            "cider_detailed": 2.158612364929474e-05,
            "cider_short": 0.16069425490329345
          }
        },
        "05_Courtroom_Proceedings": {
          "detailed": {
            "rouge_l_mean": 0.25117272459076034,
            "rouge_l_std": 0.07928546345427315,
            "text_similarity_mean": 0.7921738487023574,
            "text_similarity_std": 0.13192026642502905,
            "llm_judge_score_mean": 7.3076923076923075,
            "llm_judge_score_std": 2.1973625933978003
          },
          "short": {
            "rouge_l_mean": 0.2265241100036442,
            "rouge_l_std": 0.08996690241847358,
            "text_similarity_mean": 0.6751369226437348,
            "text_similarity_std": 0.22715736519622753,
            "llm_judge_score_mean": 5.846153846153846,
            "llm_judge_score_std": 1.5613679330911105
          },
          "cider": {
            "cider_detailed": 0.07488900975320743,
            "cider_short": 0.010605987374435948
          }
        },
        "06_Emergency_Response_Scenarios": {
          "detailed": {
            "rouge_l_mean": 0.2757969475593089,
            "rouge_l_std": 0.06218760138157189,
            "text_similarity_mean": 0.7709836751222611,
            "text_similarity_std": 0.12032286401619417,
            "llm_judge_score_mean": 7.55,
            "llm_judge_score_std": 1.5644487847162016
          },
          "short": {
            "rouge_l_mean": 0.2821281504002289,
            "rouge_l_std": 0.05579435864366492,
            "text_similarity_mean": 0.7298159450292587,
            "text_similarity_std": 0.09859792665491034,
            "llm_judge_score_mean": 6.75,
            "llm_judge_score_std": 1.3369741957120937
          },
          "cider": {
            "cider_detailed": 0.026296521685487045,
            "cider_short": 0.05207225022173384
          }
        },
        "07_Public_Transportation_Conflicts": {
          "detailed": {
            "rouge_l_mean": 0.28064050823557835,
            "rouge_l_std": 0.04347756956189987,
            "text_similarity_mean": 0.8523804460253034,
            "text_similarity_std": 0.04394552951852714,
            "llm_judge_score_mean": 8.142857142857142,
            "llm_judge_score_std": 1.0594569267279519
          },
          "short": {
            "rouge_l_mean": 0.3159729775982411,
            "rouge_l_std": 0.0981340064062688,
            "text_similarity_mean": 0.7712914560522351,
            "text_similarity_std": 0.09026394206818819,
            "llm_judge_score_mean": 7.428571428571429,
            "llm_judge_score_std": 1.178030178747903
          },
          "cider": {
            "cider_detailed": 0.007506571642437169,
            "cider_short": 0.12860458341290731
          }
        },
        "08_Workplace_Team_Meetings": {
          "detailed": {
            "rouge_l_mean": 0.2671953579420369,
            "rouge_l_std": 0.044168928463739224,
            "text_similarity_mean": 0.8272687097390493,
            "text_similarity_std": 0.04089041813855187,
            "llm_judge_score_mean": 8.25,
            "llm_judge_score_std": 0.924211375534118
          },
          "short": {
            "rouge_l_mean": 0.2568340656099063,
            "rouge_l_std": 0.060441827801123164,
            "text_similarity_mean": 0.7796352356672287,
            "text_similarity_std": 0.08998383787424373,
            "llm_judge_score_mean": 6.916666666666667,
            "llm_judge_score_std": 1.114924013354971
          },
          "cider": {
            "cider_detailed": 0.12078857715046824,
            "cider_short": 0.16428691733917913
          }
        },
        "09_HousingApartment_Tours": {
          "detailed": {
            "rouge_l_mean": 0.27712020341717986,
            "rouge_l_std": 0.05242997107506672,
            "text_similarity_mean": 0.7891535287102064,
            "text_similarity_std": 0.07526149434470443,
            "llm_judge_score_mean": 8.208333333333334,
            "llm_judge_score_std": 0.8650224788344456
          },
          "short": {
            "rouge_l_mean": 0.25369293848628177,
            "rouge_l_std": 0.0723446607412628,
            "text_similarity_mean": 0.7547392025589943,
            "text_similarity_std": 0.08203198724751935,
            "llm_judge_score_mean": 6.75,
            "llm_judge_score_std": 1.050793351076541
          },
          "cider": {
            "cider_detailed": 0.09059437632874563,
            "cider_short": 0.008228714468043541
          }
        },
        "10_Restaurant_Service_Encounters": {
          "detailed": {
            "rouge_l_mean": 0.27063297996252444,
            "rouge_l_std": 0.06759169075516312,
            "text_similarity_mean": 0.7861023700755575,
            "text_similarity_std": 0.16603343188472702,
            "llm_judge_score_mean": 7.391304347826087,
            "llm_judge_score_std": 2.058685322043776
          },
          "short": {
            "rouge_l_mean": 0.28912020399231125,
            "rouge_l_std": 0.09169185883194186,
            "text_similarity_mean": 0.7471206784248352,
            "text_similarity_std": 0.16897098270321464,
            "llm_judge_score_mean": 6.826086956521739,
            "llm_judge_score_std": 1.970481598891245
          },
          "cider": {
            "cider_detailed": 0.10805352353362584,
            "cider_short": 0.11874199848498831
          }
        },
        "11_Mental_Health_Counseling": {
          "detailed": {
            "rouge_l_mean": 0.28443630382528157,
            "rouge_l_std": 0.06629138786144877,
            "text_similarity_mean": 0.8297146971409137,
            "text_similarity_std": 0.07517054374732726,
            "llm_judge_score_mean": 8.846153846153847,
            "llm_judge_score_std": 0.532938710021193
          },
          "short": {
            "rouge_l_mean": 0.25628244345394446,
            "rouge_l_std": 0.08746698858947152,
            "text_similarity_mean": 0.768268699829395,
            "text_similarity_std": 0.09969207379321748,
            "llm_judge_score_mean": 7.461538461538462,
            "llm_judge_score_std": 1.2162606385262995
          },
          "cider": {
            "cider_detailed": 0.0990982910678223,
            "cider_short": 0.14450536615697646
          }
        },
        "12_Community_Town_Halls": {
          "detailed": {
            "rouge_l_mean": 0.26042958727755,
            "rouge_l_std": 0.08477364830955211,
            "text_similarity_mean": 0.7913139429357317,
            "text_similarity_std": 0.05499705356563186,
            "llm_judge_score_mean": 7.722222222222222,
            "llm_judge_score_std": 1.2825995978461329
          },
          "short": {
            "rouge_l_mean": 0.24531978058413395,
            "rouge_l_std": 0.09804991266711369,
            "text_similarity_mean": 0.743941166334682,
            "text_similarity_std": 0.12843285080122974,
            "llm_judge_score_mean": 6.666666666666667,
            "llm_judge_score_std": 1.632993161855452
          },
          "cider": {
            "cider_detailed": 0.059855994591731014,
            "cider_short": 0.1633344014605606
          }
        },
        "13_Olympics": {
          "detailed": {
            "rouge_l_mean": 0.23712202296387958,
            "rouge_l_std": 0.04407261193544751,
            "text_similarity_mean": 0.8224339770234149,
            "text_similarity_std": 0.059510698702595836,
            "llm_judge_score_mean": 6.913043478260869,
            "llm_judge_score_std": 1.471853119983544
          },
          "short": {
            "rouge_l_mean": 0.24729931444273728,
            "rouge_l_std": 0.08169830056065522,
            "text_similarity_mean": 0.7575146840966266,
            "text_similarity_std": 0.10172471073133676,
            "llm_judge_score_mean": 6.391304347826087,
            "llm_judge_score_std": 1.6081080007745157
          },
          "cider": {
            "cider_detailed": 0.0030423904993227215,
            "cider_short": 0.08141865581587904
          }
        }
      },
      "aggregated_across_topics": {
        "detailed": {
          "rouge_l_mean": 0.27177588561409693,
          "text_similarity_mean": 0.8132758479546016,
          "llm_judge_score_mean": 8.068472186490581
        },
        "short": {
          "rouge_l_mean": 0.2651599660352388,
          "text_similarity_mean": 0.7511678050706051,
          "llm_judge_score_mean": 6.94706076624973
        },
        "cider": {
          "cider_detailed_mean": 0.0626128936934445,
          "cider_short_mean": 0.08613762595134607
        }
      }
    },
    "t2": {
      "task_name": "task2_mcq",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "accuracy": 0.9901960784313726,
          "correct": 101,
          "total": 102,
          "rationale": {
            "rouge_l_mean": 0.42611036922644596,
            "rouge_l_std": 0.12172702208939794,
            "text_similarity_mean": 0.8245743337799522,
            "text_similarity_std": 0.07487814924132673,
            "llm_judge_score_mean": 9.598039215686274,
            "llm_judge_score_std": 0.5817452431037959
          },
          "rationale_cider": 0.6423049943014726
        },
        "02_Job_Interviews": {
          "accuracy": 0.97,
          "correct": 97,
          "total": 100,
          "rationale": {
            "rouge_l_mean": 0.3999258976380314,
            "rouge_l_std": 0.11925986103549945,
            "text_similarity_mean": 0.8041371941566468,
            "text_similarity_std": 0.08909218096073356,
            "llm_judge_score_mean": 9.32,
            "llm_judge_score_std": 0.823164625090267
          },
          "rationale_cider": 0.3875336039840728
        },
        "03_Parent-Teacher_Conferences": {
          "accuracy": 0.9869565217391304,
          "correct": 227,
          "total": 230,
          "rationale": {
            "rouge_l_mean": 0.3890869344657163,
            "rouge_l_std": 0.10760911581871063,
            "text_similarity_mean": 0.8036679405233135,
            "text_similarity_std": 0.0833243530552749,
            "llm_judge_score_mean": 9.31304347826087,
            "llm_judge_score_std": 0.9265665220850424
          },
          "rationale_cider": 0.4203958086252495
        },
        "04_Customer_Service_Interactions": {
          "accuracy": 0.9487179487179487,
          "correct": 37,
          "total": 39,
          "rationale": {
            "rouge_l_mean": 0.3192025864008883,
            "rouge_l_std": 0.0676224759894827,
            "text_similarity_mean": 0.7843628418751252,
            "text_similarity_std": 0.08756317591059778,
            "llm_judge_score_mean": 8.871794871794872,
            "llm_judge_score_std": 1.1363343490338635
          },
          "rationale_cider": 0.20080329849006906
        },
        "05_Courtroom_Proceedings": {
          "accuracy": 0.9652173913043478,
          "correct": 111,
          "total": 115,
          "rationale": {
            "rouge_l_mean": 0.395503092402921,
            "rouge_l_std": 0.11142815913611853,
            "text_similarity_mean": 0.7964894112037576,
            "text_similarity_std": 0.10890728606256252,
            "llm_judge_score_mean": 9.191304347826087,
            "llm_judge_score_std": 0.9947309199097888
          },
          "rationale_cider": 0.4226155634644825
        },
        "06_Emergency_Response_Scenarios": {
          "accuracy": 0.9540229885057471,
          "correct": 83,
          "total": 87,
          "rationale": {
            "rouge_l_mean": 0.31999144588469147,
            "rouge_l_std": 0.09716017256299828,
            "text_similarity_mean": 0.7863215208909977,
            "text_similarity_std": 0.10958644904399117,
            "llm_judge_score_mean": 8.908045977011493,
            "llm_judge_score_std": 1.3010356962603236
          },
          "rationale_cider": 0.17437104282143853
        },
        "07_Public_Transportation_Conflicts": {
          "accuracy": 0.9607843137254902,
          "correct": 49,
          "total": 51,
          "rationale": {
            "rouge_l_mean": 0.3472636255185249,
            "rouge_l_std": 0.09472003636649932,
            "text_similarity_mean": 0.8021545351720324,
            "text_similarity_std": 0.06139734588199881,
            "llm_judge_score_mean": 9.07843137254902,
            "llm_judge_score_std": 0.5888885261835767
          },
          "rationale_cider": 0.10927116293521595
        },
        "08_Workplace_Team_Meetings": {
          "accuracy": 0.9402985074626866,
          "correct": 63,
          "total": 67,
          "rationale": {
            "rouge_l_mean": 0.37074964992083936,
            "rouge_l_std": 0.10901241556052234,
            "text_similarity_mean": 0.7944539251612194,
            "text_similarity_std": 0.10705685158809101,
            "llm_judge_score_mean": 9.0,
            "llm_judge_score_std": 1.2216944435630523
          },
          "rationale_cider": 0.43004635643912587
        },
        "09_HousingApartment_Tours": {
          "accuracy": 0.9534883720930233,
          "correct": 123,
          "total": 129,
          "rationale": {
            "rouge_l_mean": 0.38644264742210727,
            "rouge_l_std": 0.11822738512684124,
            "text_similarity_mean": 0.800202547348747,
            "text_similarity_std": 0.09558729095772013,
            "llm_judge_score_mean": 9.34108527131783,
            "llm_judge_score_std": 0.9999699532773395
          },
          "rationale_cider": 0.437193148003153
        },
        "10_Restaurant_Service_Encounters": {
          "accuracy": 0.9777777777777777,
          "correct": 88,
          "total": 90,
          "rationale": {
            "rouge_l_mean": 0.3633389381887117,
            "rouge_l_std": 0.10210014478680805,
            "text_similarity_mean": 0.8160332666503058,
            "text_similarity_std": 0.08924101888335989,
            "llm_judge_score_mean": 9.244444444444444,
            "llm_judge_score_std": 0.9105689443799057
          },
          "rationale_cider": 0.3724001431941441
        },
        "11_Mental_Health_Counseling": {
          "accuracy": 0.9846153846153847,
          "correct": 64,
          "total": 65,
          "rationale": {
            "rouge_l_mean": 0.3629128363900598,
            "rouge_l_std": 0.10464313962778489,
            "text_similarity_mean": 0.7911119406039898,
            "text_similarity_std": 0.09045075020048875,
            "llm_judge_score_mean": 9.338461538461539,
            "llm_judge_score_std": 0.5623282117691405
          },
          "rationale_cider": 0.3538443060084952
        },
        "12_Community_Town_Halls": {
          "accuracy": 0.9682539682539683,
          "correct": 183,
          "total": 189,
          "rationale": {
            "rouge_l_mean": 0.40195886859304625,
            "rouge_l_std": 0.11454726597473847,
            "text_similarity_mean": 0.8090396031500802,
            "text_similarity_std": 0.07354137423197481,
            "llm_judge_score_mean": 9.301587301587302,
            "llm_judge_score_std": 0.6738090564060674
          },
          "rationale_cider": 0.5668707948073239
        },
        "13_Olympics": {
          "accuracy": 0.927536231884058,
          "correct": 64,
          "total": 69,
          "rationale": {
            "rouge_l_mean": 0.31814701155341024,
            "rouge_l_std": 0.1054105178949294,
            "text_similarity_mean": 0.8182010028673254,
            "text_similarity_std": 0.0760160769168957,
            "llm_judge_score_mean": 8.942028985507246,
            "llm_judge_score_std": 1.1018305976085918
          },
          "rationale_cider": 0.20152499148505587
        }
      },
      "aggregated_across_topics": {
        "accuracy_mean": 0.9636819603469949,
        "rationale": {
          "rouge_l_mean": 0.36927953104656874,
          "text_similarity_mean": 0.8023653894910379,
          "llm_judge_score_mean": 9.18832821572669
        }
      }
    },
    "t3": {
      "task_name": "task3_temporal_localization",
      "topics": {
        "01_Patient-Doctor_Consultations": {
          "mean_iou": 0.2016161181668154,
          "std_iou": 0.2796408414944442,
          "median_iou": 0.02840909090909264,
          "R@0.3": {
            "recall": 0.26765799256505574,
            "count": 72,
            "total": 269
          },
          "R@0.5": {
            "recall": 0.18587360594795538,
            "count": 50,
            "total": 269
          },
          "R@0.7": {
            "recall": 0.10408921933085502,
            "count": 28,
            "total": 269
          },
          "mae": {
            "start_mean": 7.566360594795536,
            "end_mean": 3486.8729814126386,
            "average_mean": 1747.2196710037176
          },
          "rationale": {
            "rouge_l_mean": 0.29238135730468023,
            "rouge_l_std": 0.08536678405066815,
            "text_similarity_mean": 0.6654356305373202,
            "text_similarity_std": 0.14599353409903343,
            "llm_judge_score_mean": 6.992565055762082,
            "llm_judge_score_std": 1.4784513274207352
          },
          "rationale_cider": 0.22397435132656698
        },
        "02_Job_Interviews": {
          "mean_iou": 0.31895370565318176,
          "std_iou": 0.3222556169575355,
          "median_iou": 0.26178010471204155,
          "R@0.3": {
            "recall": 0.47058823529411764,
            "count": 120,
            "total": 255
          },
          "R@0.5": {
            "recall": 0.30980392156862746,
            "count": 79,
            "total": 255
          },
          "R@0.7": {
            "recall": 0.17254901960784313,
            "count": 44,
            "total": 255
          },
          "mae": {
            "start_mean": 8.640117647058817,
            "end_mean": 10.636450980392153,
            "average_mean": 9.638284313725485
          },
          "rationale": {
            "rouge_l_mean": 0.29668575493434585,
            "rouge_l_std": 0.09200241816212403,
            "text_similarity_mean": 0.669644813619408,
            "text_similarity_std": 0.12131736908037669,
            "llm_judge_score_mean": 7.172549019607843,
            "llm_judge_score_std": 1.4422311733564075
          },
          "rationale_cider": 0.24398839295755445
        },
        "03_Parent-Teacher_Conferences": {
          "mean_iou": 0.2253858097142175,
          "std_iou": 0.28680854414465773,
          "median_iou": 0.07296446334029966,
          "R@0.3": {
            "recall": 0.29927007299270075,
            "count": 164,
            "total": 548
          },
          "R@0.5": {
            "recall": 0.2062043795620438,
            "count": 113,
            "total": 548
          },
          "R@0.7": {
            "recall": 0.10766423357664233,
            "count": 59,
            "total": 548
          },
          "mae": {
            "start_mean": 11.230260948905096,
            "end_mean": 13.914812043795601,
            "average_mean": 12.572536496350349
          },
          "rationale": {
            "rouge_l_mean": 0.27788792973671245,
            "rouge_l_std": 0.08398475911888004,
            "text_similarity_mean": 0.6546616152092053,
            "text_similarity_std": 0.12166311213321802,
            "llm_judge_score_mean": 7.08029197080292,
            "llm_judge_score_std": 1.4874579999497008
          },
          "rationale_cider": 0.15069765103844188
        },
        "04_Customer_Service_Interactions": {
          "mean_iou": 0.3007628630799269,
          "std_iou": 0.30996501540133986,
          "median_iou": 0.21276595744680774,
          "R@0.3": {
            "recall": 0.46017699115044247,
            "count": 52,
            "total": 113
          },
          "R@0.5": {
            "recall": 0.3185840707964602,
            "count": 36,
            "total": 113
          },
          "R@0.7": {
            "recall": 0.13274336283185842,
            "count": 15,
            "total": 113
          },
          "mae": {
            "start_mean": 13.5539203539823,
            "end_mean": 13.272628318584072,
            "average_mean": 13.413274336283186
          },
          "rationale": {
            "rouge_l_mean": 0.29449378275668103,
            "rouge_l_std": 0.08322617717960351,
            "text_similarity_mean": 0.6855572077025355,
            "text_similarity_std": 0.10848687786418272,
            "llm_judge_score_mean": 7.15929203539823,
            "llm_judge_score_std": 1.675256533948856
          },
          "rationale_cider": 0.0892564230176528
        },
        "05_Courtroom_Proceedings": {
          "mean_iou": 0.3253359432535176,
          "std_iou": 0.31020369417116755,
          "median_iou": 0.301204819277108,
          "R@0.3": {
            "recall": 0.5043731778425656,
            "count": 173,
            "total": 343
          },
          "R@0.5": {
            "recall": 0.33527696793002915,
            "count": 115,
            "total": 343
          },
          "R@0.7": {
            "recall": 0.15743440233236153,
            "count": 54,
            "total": 343
          },
          "mae": {
            "start_mean": 15.617618075801744,
            "end_mean": 17.239134110787173,
            "average_mean": 16.428376093294457
          },
          "rationale": {
            "rouge_l_mean": 0.28499548560605675,
            "rouge_l_std": 0.08657923659387426,
            "text_similarity_mean": 0.6775385676598062,
            "text_similarity_std": 0.1357164623296849,
            "llm_judge_score_mean": 7.206997084548105,
            "llm_judge_score_std": 1.6147129712686823
          },
          "rationale_cider": 0.1115320029746358
        },
        "06_Emergency_Response_Scenarios": {
          "mean_iou": 0.19420875469173896,
          "std_iou": 0.2752364336660041,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.31620553359683795,
            "count": 80,
            "total": 253
          },
          "R@0.5": {
            "recall": 0.17391304347826086,
            "count": 44,
            "total": 253
          },
          "R@0.7": {
            "recall": 0.07905138339920949,
            "count": 20,
            "total": 253
          },
          "mae": {
            "start_mean": 13.875727272727266,
            "end_mean": 49.305758893280625,
            "average_mean": 31.59074308300395
          },
          "rationale": {
            "rouge_l_mean": 0.29352047795718467,
            "rouge_l_std": 0.09433764053860065,
            "text_similarity_mean": 0.7168077439540931,
            "text_similarity_std": 0.13620300090184037,
            "llm_judge_score_mean": 6.83399209486166,
            "llm_judge_score_std": 1.6787766908684196
          },
          "rationale_cider": 0.13042760665606093
        },
        "07_Public_Transportation_Conflicts": {
          "mean_iou": 0.2698140653355215,
          "std_iou": 0.28770869406141136,
          "median_iou": 0.2139113693892832,
          "R@0.3": {
            "recall": 0.3826086956521739,
            "count": 44,
            "total": 115
          },
          "R@0.5": {
            "recall": 0.22608695652173913,
            "count": 26,
            "total": 115
          },
          "R@0.7": {
            "recall": 0.12173913043478261,
            "count": 14,
            "total": 115
          },
          "mae": {
            "start_mean": 7.468217391304346,
            "end_mean": 8.062756521739134,
            "average_mean": 7.765486956521738
          },
          "rationale": {
            "rouge_l_mean": 0.2848795618959675,
            "rouge_l_std": 0.10039116629464669,
            "text_similarity_mean": 0.6810955008734827,
            "text_similarity_std": 0.11909207844246615,
            "llm_judge_score_mean": 7.182608695652174,
            "llm_judge_score_std": 1.4895730855905291
          },
          "rationale_cider": 0.1959134400230022
        },
        "08_Workplace_Team_Meetings": {
          "mean_iou": 0.287316723723421,
          "std_iou": 0.30704702152144847,
          "median_iou": 0.17391304347826086,
          "R@0.3": {
            "recall": 0.41509433962264153,
            "count": 66,
            "total": 159
          },
          "R@0.5": {
            "recall": 0.29559748427672955,
            "count": 47,
            "total": 159
          },
          "R@0.7": {
            "recall": 0.1320754716981132,
            "count": 21,
            "total": 159
          },
          "mae": {
            "start_mean": 8.99177987421383,
            "end_mean": 101.68904402515724,
            "average_mean": 55.340411949685524
          },
          "rationale": {
            "rouge_l_mean": 0.2682622877815332,
            "rouge_l_std": 0.07889382445479909,
            "text_similarity_mean": 0.6570814868564125,
            "text_similarity_std": 0.12482953595017927,
            "llm_judge_score_mean": 7.138364779874214,
            "llm_judge_score_std": 1.5559906040903346
          },
          "rationale_cider": 0.14699203937627453
        },
        "09_HousingApartment_Tours": {
          "mean_iou": 0.29435864128715006,
          "std_iou": 0.2957033427674727,
          "median_iou": 0.2467752167477219,
          "R@0.3": {
            "recall": 0.4263565891472868,
            "count": 165,
            "total": 387
          },
          "R@0.5": {
            "recall": 0.2739018087855297,
            "count": 106,
            "total": 387
          },
          "R@0.7": {
            "recall": 0.12661498708010335,
            "count": 49,
            "total": 387
          },
          "mae": {
            "start_mean": 7.678754521963824,
            "end_mean": 9.982576227390183,
            "average_mean": 8.830665374677004
          },
          "rationale": {
            "rouge_l_mean": 0.2984992529144689,
            "rouge_l_std": 0.08793226078265405,
            "text_similarity_mean": 0.7112140336702036,
            "text_similarity_std": 0.12341212108174422,
            "llm_judge_score_mean": 7.160206718346253,
            "llm_judge_score_std": 1.6921059944003547
          },
          "rationale_cider": 0.12863471564533052
        },
        "10_Restaurant_Service_Encounters": {
          "mean_iou": 0.2749652676488147,
          "std_iou": 0.2987418195669755,
          "median_iou": 0.18933823529411717,
          "R@0.3": {
            "recall": 0.41284403669724773,
            "count": 90,
            "total": 218
          },
          "R@0.5": {
            "recall": 0.26605504587155965,
            "count": 58,
            "total": 218
          },
          "R@0.7": {
            "recall": 0.11926605504587157,
            "count": 26,
            "total": 218
          },
          "mae": {
            "start_mean": 8.410761467889909,
            "end_mean": 35.239472477064226,
            "average_mean": 21.825116972477066
          },
          "rationale": {
            "rouge_l_mean": 0.27738647198479843,
            "rouge_l_std": 0.08129114002822677,
            "text_similarity_mean": 0.672087320049695,
            "text_similarity_std": 0.11183409572136817,
            "llm_judge_score_mean": 7.091743119266055,
            "llm_judge_score_std": 1.5943782222746472
          },
          "rationale_cider": 0.11372435055935906
        },
        "11_Mental_Health_Counseling": {
          "mean_iou": 0.29312707678775757,
          "std_iou": 0.29801683225718006,
          "median_iou": 0.23824856290475313,
          "R@0.3": {
            "recall": 0.4473684210526316,
            "count": 68,
            "total": 152
          },
          "R@0.5": {
            "recall": 0.29605263157894735,
            "count": 45,
            "total": 152
          },
          "R@0.7": {
            "recall": 0.13157894736842105,
            "count": 20,
            "total": 152
          },
          "mae": {
            "start_mean": 8.258697368421048,
            "end_mean": 11.64594078947368,
            "average_mean": 9.952319078947363
          },
          "rationale": {
            "rouge_l_mean": 0.266367098134869,
            "rouge_l_std": 0.09218823978187439,
            "text_similarity_mean": 0.618686517002061,
            "text_similarity_std": 0.15743512660893602,
            "llm_judge_score_mean": 7.098684210526316,
            "llm_judge_score_std": 1.4269947645915682
          },
          "rationale_cider": 0.16559866307955548
        },
        "12_Community_Town_Halls": {
          "mean_iou": 0.20453445119860084,
          "std_iou": 0.273404279946976,
          "median_iou": 0.05632991482047405,
          "R@0.3": {
            "recall": 0.2823275862068966,
            "count": 131,
            "total": 464
          },
          "R@0.5": {
            "recall": 0.15948275862068967,
            "count": 74,
            "total": 464
          },
          "R@0.7": {
            "recall": 0.09051724137931035,
            "count": 42,
            "total": 464
          },
          "mae": {
            "start_mean": 37.34078017241379,
            "end_mean": 9.75918534482758,
            "average_mean": 23.549982758620683
          },
          "rationale": {
            "rouge_l_mean": 0.2701159196402161,
            "rouge_l_std": 0.08823974208615888,
            "text_similarity_mean": 0.6429304879736798,
            "text_similarity_std": 0.13741799603339264,
            "llm_judge_score_mean": 6.9719827586206895,
            "llm_judge_score_std": 1.4433526105298817
          },
          "rationale_cider": 0.17423582362816714
        },
        "13_Olympics": {
          "mean_iou": 0.24643955257972547,
          "std_iou": 0.3069441110031874,
          "median_iou": 0.0,
          "R@0.3": {
            "recall": 0.37142857142857144,
            "count": 39,
            "total": 105
          },
          "R@0.5": {
            "recall": 0.23809523809523808,
            "count": 25,
            "total": 105
          },
          "R@0.7": {
            "recall": 0.12380952380952381,
            "count": 13,
            "total": 105
          },
          "mae": {
            "start_mean": 14.167990476190473,
            "end_mean": 15.363485714285714,
            "average_mean": 14.765738095238094
          },
          "rationale": {
            "rouge_l_mean": 0.27605625512552484,
            "rouge_l_std": 0.08139301455637586,
            "text_similarity_mean": 0.6984858643440973,
            "text_similarity_std": 0.10603639232304256,
            "llm_judge_score_mean": 6.695238095238095,
            "llm_judge_score_std": 1.622014089092761
          },
          "rationale_cider": 0.0913478726527278
        }
      },
      "aggregated_across_topics": {
        "mean_iou": 0.26437069024002996,
        "mae_average": 151.76096973173404,
        "R@0.3": 0.3889461725576284,
        "R@0.5": 0.2526867625410623,
        "R@0.7": 0.12301022906883816,
        "rationale": {
          "rouge_l_mean": 0.2831947412133107,
          "text_similarity_mean": 0.6731712914963077,
          "llm_judge_score_mean": 7.06034735680805
        }
      }
    }
  }
}